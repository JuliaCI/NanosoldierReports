Julia Version 1.5.0-DEV.862
Commit b472f88f55 (2020-05-06 01:04 UTC)
Platform Info:
  OS: Linux (x86_64-linux-gnu)
  CPU: Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-9.0.1 (ORCJIT, skylake)
Environment:
  JULIA_DEPOT_PATH = ::/usr/local/share/julia
  JULIA_NUM_THREADS = 2

  Resolving package versions...
  Installed Media ──────────────────────── v0.5.0
  Installed DiffEqFlux ─────────────────── v1.9.0
  Installed CUDAapi ────────────────────── v4.0.0
  Installed DiffEqCallbacks ────────────── v2.13.1
  Installed NLSolversBase ──────────────── v7.6.1
  Installed Juno ───────────────────────── v0.8.1
  Installed ExprTools ──────────────────── v0.1.1
  Installed ForwardDiff ────────────────── v0.10.10
  Installed ArrayLayouts ───────────────── v0.2.6
  Installed CodecZlib ──────────────────── v0.7.0
  Installed Zlib_jll ───────────────────── v1.2.11+9
  Installed DiffResults ────────────────── v1.0.2
  Installed IteratorInterfaceExtensions ── v1.0.0
  Installed QuasiMonteCarlo ────────────── v0.2.0
  Installed Roots ──────────────────────── v1.0.1
  Installed VertexSafeGraphs ───────────── v0.1.2
  Installed DataStructures ─────────────── v0.17.15
  Installed AbstractFFTs ───────────────── v0.5.0
  Installed Cthulhu ────────────────────── v1.0.2
  Installed SimpleTraits ───────────────── v0.9.2
  Installed OpenSpecFun_jll ────────────── v0.5.3+3
  Installed UnPack ─────────────────────── v0.1.0
  Installed RecursiveFactorization ─────── v0.1.0
  Installed Compat ─────────────────────── v3.9.0
  Installed FillArrays ─────────────────── v0.8.8
  Installed ProgressMeter ──────────────── v1.2.0
  Installed Inflate ────────────────────── v0.1.2
  Installed FFTW ───────────────────────── v1.2.1
  Installed LightGraphs ────────────────── v1.3.2
  Installed IntelOpenMP_jll ────────────── v2018.0.3+0
  Installed LabelledArrays ─────────────── v1.2.0
  Installed SpecialFunctions ───────────── v0.10.0
  Installed ColorTypes ─────────────────── v0.10.3
  Installed TranscodingStreams ─────────── v0.9.5
  Installed MacroTools ─────────────────── v0.5.5
  Installed ReverseDiff ────────────────── v1.2.0
  Installed Flux ───────────────────────── v0.10.4
  Installed LLVM ───────────────────────── v1.4.1
  Installed DataAPI ────────────────────── v1.3.0
  Installed Rmath_jll ──────────────────── v0.2.2+0
  Installed Arpack_jll ─────────────────── v3.5.0+3
  Installed ConsoleProgressMonitor ─────── v0.1.2
  Installed FunctionWrappers ───────────── v1.1.1
  Installed DiffEqSensitivity ──────────── v6.13.0
  Installed RecipesBase ────────────────── v1.0.1
  Installed Tracker ────────────────────── v0.2.6
  Installed SpatialIndexing ────────────── v0.1.2
  Installed LeftChildRightSiblingTrees ─── v0.1.2
  Installed TimerOutputs ───────────────── v0.5.5
  Installed ProgressLogging ────────────── v0.1.2
  Installed NLsolve ────────────────────── v4.3.0
  Installed ZygoteRules ────────────────── v0.2.0
  Installed StatsFuns ──────────────────── v0.9.4
  Installed CuArrays ───────────────────── v2.1.0
  Installed ExponentialUtilities ───────── v1.6.0
  Installed CPUTime ────────────────────── v1.0.0
  Installed NaNMath ────────────────────── v0.3.3
  Installed OrderedCollections ─────────── v1.2.0
  Installed TreeViews ──────────────────── v0.3.0
  Installed ZipFile ────────────────────── v0.9.1
  Installed CUDAdrv ────────────────────── v6.3.0
  Installed Zygote ─────────────────────── v0.4.20
  Installed OpenBLAS_jll ───────────────── v0.3.9+4
  Installed RecursiveArrayTools ────────── v2.3.1
  Installed GPUArrays ──────────────────── v3.2.0
  Installed Parameters ─────────────────── v0.12.0
  Installed SparseDiffTools ────────────── v1.7.1
  Installed PositiveFactorizations ─────── v0.2.3
  Installed Distributions ──────────────── v0.23.2
  Installed TerminalLoggers ────────────── v0.1.1
  Installed BinaryProvider ─────────────── v0.5.9
  Installed Colors ─────────────────────── v0.12.0
  Installed LoggingExtras ──────────────── v0.4.0
  Installed Rmath ──────────────────────── v0.6.1
  Installed ChainRulesCore ─────────────── v0.7.5
  Installed Requires ───────────────────── v1.0.1
  Installed CodeTracking ───────────────── v0.5.11
  Installed ArrayInterface ─────────────── v2.8.7
  Installed Distances ──────────────────── v0.8.2
  Installed StaticArrays ───────────────── v0.12.3
  Installed Missings ───────────────────── v0.4.3
  Installed QuadGK ─────────────────────── v2.3.1
  Installed PDMats ─────────────────────── v0.9.12
  Installed Reexport ───────────────────── v0.2.0
  Installed LatinHypercubeSampling ─────── v1.6.4
  Installed DiffEqBase ─────────────────── v6.32.1
  Installed GenericSVD ─────────────────── v0.3.0
  Installed DocStringExtensions ────────── v0.8.1
  Installed IterativeSolvers ───────────── v0.8.4
  Installed TableTraits ────────────────── v1.0.0
  Installed SortingAlgorithms ──────────── v0.3.1
  Installed CompilerSupportLibraries_jll ─ v0.3.3+0
  Installed LineSearches ───────────────── v7.0.1
  Installed FFTW_jll ───────────────────── v3.3.9+5
  Installed CEnum ──────────────────────── v0.2.0
  Installed Arpack ─────────────────────── v0.4.0
  Installed CommonSubexpressions ───────── v0.2.0
  Installed StatsBase ──────────────────── v0.33.0
  Installed NNlib ──────────────────────── v0.6.6
  Installed CUDAnative ─────────────────── v3.0.4
  Installed Adapt ──────────────────────── v1.0.1
  Installed FixedPointNumbers ──────────── v0.8.0
  Installed FiniteDiff ─────────────────── v2.3.1
  Installed DiffRules ──────────────────── v1.0.1
  Installed ArnoldiMethod ──────────────── v0.0.4
  Installed AbstractTrees ──────────────── v0.3.3
  Installed OrdinaryDiffEq ─────────────── v5.35.5
  Installed MKL_jll ────────────────────── v2019.0.117+2
  Installed Sobol ──────────────────────── v1.3.0
  Installed MuladdMacro ────────────────── v0.2.2
  Installed IRTools ────────────────────── v0.3.2
  Installed Optim ──────────────────────── v0.20.6
  Installed BlackBoxOptim ──────────────── v0.5.0
Updating `~/.julia/environments/v1.5/Project.toml`
  [aae7a2af] + DiffEqFlux v1.9.0
Updating `~/.julia/environments/v1.5/Manifest.toml`
  [621f4979] + AbstractFFTs v0.5.0
  [1520ce14] + AbstractTrees v0.3.3
  [79e6a3ab] + Adapt v1.0.1
  [ec485272] + ArnoldiMethod v0.0.4
  [7d9fca2a] + Arpack v0.4.0
  [68821587] + Arpack_jll v3.5.0+3
  [4fba245c] + ArrayInterface v2.8.7
  [4c555306] + ArrayLayouts v0.2.6
  [b99e7846] + BinaryProvider v0.5.9
  [a134a8b2] + BlackBoxOptim v0.5.0
  [fa961155] + CEnum v0.2.0
  [a9c8d775] + CPUTime v1.0.0
  [3895d2a7] + CUDAapi v4.0.0
  [c5f51814] + CUDAdrv v6.3.0
  [be33ccc6] + CUDAnative v3.0.4
  [d360d2e6] + ChainRulesCore v0.7.5
  [da1fd8a2] + CodeTracking v0.5.11
  [944b1d66] + CodecZlib v0.7.0
  [3da002f7] + ColorTypes v0.10.3
  [5ae59095] + Colors v0.12.0
  [bbf7d656] + CommonSubexpressions v0.2.0
  [34da2185] + Compat v3.9.0
  [e66e0078] + CompilerSupportLibraries_jll v0.3.3+0
  [88cd18e8] + ConsoleProgressMonitor v0.1.2
  [f68482b8] + Cthulhu v1.0.2
  [3a865a2d] + CuArrays v2.1.0
  [9a962f9c] + DataAPI v1.3.0
  [864edb3b] + DataStructures v0.17.15
  [2b5f629d] + DiffEqBase v6.32.1
  [459566f4] + DiffEqCallbacks v2.13.1
  [aae7a2af] + DiffEqFlux v1.9.0
  [41bf760c] + DiffEqSensitivity v6.13.0
  [163ba53b] + DiffResults v1.0.2
  [b552c78f] + DiffRules v1.0.1
  [b4f34e82] + Distances v0.8.2
  [31c24e10] + Distributions v0.23.2
  [ffbed154] + DocStringExtensions v0.8.1
  [d4d017d3] + ExponentialUtilities v1.6.0
  [e2ba6199] + ExprTools v0.1.1
  [7a1cc6ca] + FFTW v1.2.1
  [f5851436] + FFTW_jll v3.3.9+5
  [1a297f60] + FillArrays v0.8.8
  [6a86dc24] + FiniteDiff v2.3.1
  [53c48c17] + FixedPointNumbers v0.8.0
  [587475ba] + Flux v0.10.4
  [f6369f11] + ForwardDiff v0.10.10
  [069b7b12] + FunctionWrappers v1.1.1
  [0c68f7d7] + GPUArrays v3.2.0
  [01680d73] + GenericSVD v0.3.0
  [7869d1d1] + IRTools v0.3.2
  [d25df0c9] + Inflate v0.1.2
  [1d5cc7b8] + IntelOpenMP_jll v2018.0.3+0
  [42fd0dbc] + IterativeSolvers v0.8.4
  [82899510] + IteratorInterfaceExtensions v1.0.0
  [e5e0dc1b] + Juno v0.8.1
  [929cbde3] + LLVM v1.4.1
  [2ee39098] + LabelledArrays v1.2.0
  [a5e1c1ea] + LatinHypercubeSampling v1.6.4
  [1d6d02ad] + LeftChildRightSiblingTrees v0.1.2
  [093fc24a] + LightGraphs v1.3.2
  [d3d80556] + LineSearches v7.0.1
  [e6f89c97] + LoggingExtras v0.4.0
  [856f044c] + MKL_jll v2019.0.117+2
  [1914dd2f] + MacroTools v0.5.5
  [e89f7d12] + Media v0.5.0
  [e1d29d7a] + Missings v0.4.3
  [46d2c3a1] + MuladdMacro v0.2.2
  [d41bc354] + NLSolversBase v7.6.1
  [2774e3e8] + NLsolve v4.3.0
  [872c559c] + NNlib v0.6.6
  [77ba4419] + NaNMath v0.3.3
  [4536629a] + OpenBLAS_jll v0.3.9+4
  [efe28fd5] + OpenSpecFun_jll v0.5.3+3
  [429524aa] + Optim v0.20.6
  [bac558e1] + OrderedCollections v1.2.0
  [1dea7af3] + OrdinaryDiffEq v5.35.5
  [90014a1f] + PDMats v0.9.12
  [d96e819e] + Parameters v0.12.0
  [85a6dd25] + PositiveFactorizations v0.2.3
  [33c8b6b6] + ProgressLogging v0.1.2
  [92933f4c] + ProgressMeter v1.2.0
  [1fd47b50] + QuadGK v2.3.1
  [8a4e6c94] + QuasiMonteCarlo v0.2.0
  [3cdcf5f2] + RecipesBase v1.0.1
  [731186ca] + RecursiveArrayTools v2.3.1
  [f2c3362d] + RecursiveFactorization v0.1.0
  [189a3867] + Reexport v0.2.0
  [ae029012] + Requires v1.0.1
  [37e2e3b7] + ReverseDiff v1.2.0
  [79098fc4] + Rmath v0.6.1
  [f50d1b31] + Rmath_jll v0.2.2+0
  [f2b01f46] + Roots v1.0.1
  [699a6c99] + SimpleTraits v0.9.2
  [ed01d8cd] + Sobol v1.3.0
  [a2af1166] + SortingAlgorithms v0.3.1
  [47a9eef4] + SparseDiffTools v1.7.1
  [d4ead438] + SpatialIndexing v0.1.2
  [276daf66] + SpecialFunctions v0.10.0
  [90137ffa] + StaticArrays v0.12.3
  [2913bbd2] + StatsBase v0.33.0
  [4c63d2b9] + StatsFuns v0.9.4
  [3783bdb8] + TableTraits v1.0.0
  [5d786b92] + TerminalLoggers v0.1.1
  [a759f4b9] + TimerOutputs v0.5.5
  [9f7883ad] + Tracker v0.2.6
  [3bb67fe8] + TranscodingStreams v0.9.5
  [a2a6695c] + TreeViews v0.3.0
  [3a884ed6] + UnPack v0.1.0
  [19fa3120] + VertexSafeGraphs v0.1.2
  [a5390f91] + ZipFile v0.9.1
  [83775a58] + Zlib_jll v1.2.11+9
  [e88e6eb3] + Zygote v0.4.20
  [700de1a5] + ZygoteRules v0.2.0
  [2a0f44e3] + Base64
  [ade2ca70] + Dates
  [8bb1440f] + DelimitedFiles
  [8ba89e20] + Distributed
  [9fa8497b] + Future
  [b77e0a4c] + InteractiveUtils
  [76f85450] + LibGit2
  [8f399da3] + Libdl
  [37e2e46d] + LinearAlgebra
  [56ddb016] + Logging
  [d6f4376e] + Markdown
  [a63ad114] + Mmap
  [44cfe95a] + Pkg
  [de0858da] + Printf
  [9abbd945] + Profile
  [3fa0cd96] + REPL
  [9a3f8284] + Random
  [ea8e919c] + SHA
  [9e88b42a] + Serialization
  [1a1011a3] + SharedArrays
  [6462fe0b] + Sockets
  [2f01184e] + SparseArrays
  [10745b16] + Statistics
  [4607b0f0] + SuiteSparse
  [8dfed614] + Test
  [cf7118a7] + UUIDs
  [4ec0a83e] + Unicode
   Building FFTW ─→ `~/.julia/packages/FFTW/5DZuu/deps/build.log`
   Building NNlib → `~/.julia/packages/NNlib/FAI3o/deps/build.log`
    Testing DiffEqFlux
Status `/tmp/jl_ObtCbA/Project.toml`
  [79e6a3ab] Adapt v1.0.1
  [a134a8b2] BlackBoxOptim v0.5.0
  [bcd4f6db] DelayDiffEq v5.23.0
  [2b5f629d] DiffEqBase v6.32.1
  [aae7a2af] DiffEqFlux v1.9.0
  [41bf760c] DiffEqSensitivity v6.13.0
  [163ba53b] DiffResults v1.0.2
  [587475ba] Flux v0.10.4
  [f6369f11] ForwardDiff v0.10.10
  [3933049c] MultistartOptimization v0.1.0
  [76087f3c] NLopt v0.5.1
  [429524aa] Optim v0.20.6
  [1dea7af3] OrdinaryDiffEq v5.35.5
  [33c8b6b6] ProgressLogging v0.1.2
  [731186ca] RecursiveArrayTools v2.3.1
  [ae029012] Requires v1.0.1
  [37e2e3b7] ReverseDiff v1.2.0
  [1bc83da4] SafeTestsets v0.0.1
  [90137ffa] StaticArrays v0.12.3
  [789caeaf] StochasticDiffEq v6.19.2
  [9f7883ad] Tracker v0.2.6
  [e88e6eb3] Zygote v0.4.20
  [700de1a5] ZygoteRules v0.2.0
  [37e2e46d] LinearAlgebra
  [56ddb016] Logging
  [de0858da] Printf
  [10745b16] Statistics
  [8dfed614] Test
Status `/tmp/jl_ObtCbA/Manifest.toml`
  [621f4979] AbstractFFTs v0.5.0
  [1520ce14] AbstractTrees v0.3.3
  [79e6a3ab] Adapt v1.0.1
  [dce04be8] ArgCheck v1.1.0
  [ec485272] ArnoldiMethod v0.0.4
  [7d9fca2a] Arpack v0.4.0
  [68821587] Arpack_jll v3.5.0+3
  [4fba245c] ArrayInterface v2.8.7
  [4c555306] ArrayLayouts v0.2.6
  [9e28174c] BinDeps v1.0.1
  [b99e7846] BinaryProvider v0.5.9
  [a134a8b2] BlackBoxOptim v0.5.0
  [fa961155] CEnum v0.2.0
  [631607c0] CMake v1.2.0
  [d5fb7624] CMakeWrapper v0.2.3
  [a9c8d775] CPUTime v1.0.0
  [3895d2a7] CUDAapi v4.0.0
  [c5f51814] CUDAdrv v6.3.0
  [be33ccc6] CUDAnative v3.0.4
  [d360d2e6] ChainRulesCore v0.7.5
  [da1fd8a2] CodeTracking v0.5.11
  [944b1d66] CodecZlib v0.7.0
  [3da002f7] ColorTypes v0.10.3
  [5ae59095] Colors v0.12.0
  [bbf7d656] CommonSubexpressions v0.2.0
  [34da2185] Compat v3.9.0
  [e66e0078] CompilerSupportLibraries_jll v0.3.3+0
  [88cd18e8] ConsoleProgressMonitor v0.1.2
  [f68482b8] Cthulhu v1.0.2
  [3a865a2d] CuArrays v2.1.0
  [9a962f9c] DataAPI v1.3.0
  [864edb3b] DataStructures v0.17.15
  [bcd4f6db] DelayDiffEq v5.23.0
  [2b5f629d] DiffEqBase v6.32.1
  [459566f4] DiffEqCallbacks v2.13.1
  [aae7a2af] DiffEqFlux v1.9.0
  [77a26b50] DiffEqNoiseProcess v3.10.0
  [41bf760c] DiffEqSensitivity v6.13.0
  [163ba53b] DiffResults v1.0.2
  [b552c78f] DiffRules v1.0.1
  [b4f34e82] Distances v0.8.2
  [31c24e10] Distributions v0.23.2
  [ffbed154] DocStringExtensions v0.8.1
  [d4d017d3] ExponentialUtilities v1.6.0
  [e2ba6199] ExprTools v0.1.1
  [7a1cc6ca] FFTW v1.2.1
  [f5851436] FFTW_jll v3.3.9+5
  [1a297f60] FillArrays v0.8.8
  [6a86dc24] FiniteDiff v2.3.1
  [53c48c17] FixedPointNumbers v0.8.0
  [587475ba] Flux v0.10.4
  [f6369f11] ForwardDiff v0.10.10
  [069b7b12] FunctionWrappers v1.1.1
  [0c68f7d7] GPUArrays v3.2.0
  [01680d73] GenericSVD v0.3.0
  [7869d1d1] IRTools v0.3.2
  [d25df0c9] Inflate v0.1.2
  [1d5cc7b8] IntelOpenMP_jll v2018.0.3+0
  [42fd0dbc] IterativeSolvers v0.8.4
  [82899510] IteratorInterfaceExtensions v1.0.0
  [e5e0dc1b] Juno v0.8.1
  [929cbde3] LLVM v1.4.1
  [2ee39098] LabelledArrays v1.2.0
  [a5e1c1ea] LatinHypercubeSampling v1.6.4
  [1d6d02ad] LeftChildRightSiblingTrees v0.1.2
  [093fc24a] LightGraphs v1.3.2
  [d3d80556] LineSearches v7.0.1
  [e6f89c97] LoggingExtras v0.4.0
  [856f044c] MKL_jll v2019.0.117+2
  [1914dd2f] MacroTools v0.5.5
  [fdba3010] MathProgBase v0.7.8
  [e89f7d12] Media v0.5.0
  [e1d29d7a] Missings v0.4.3
  [46d2c3a1] MuladdMacro v0.2.2
  [3933049c] MultistartOptimization v0.1.0
  [d41bc354] NLSolversBase v7.6.1
  [76087f3c] NLopt v0.5.1
  [2774e3e8] NLsolve v4.3.0
  [872c559c] NNlib v0.6.6
  [77ba4419] NaNMath v0.3.3
  [4536629a] OpenBLAS_jll v0.3.9+4
  [efe28fd5] OpenSpecFun_jll v0.5.3+3
  [429524aa] Optim v0.20.6
  [bac558e1] OrderedCollections v1.2.0
  [1dea7af3] OrdinaryDiffEq v5.35.5
  [90014a1f] PDMats v0.9.12
  [d96e819e] Parameters v0.12.0
  [85a6dd25] PositiveFactorizations v0.2.3
  [33c8b6b6] ProgressLogging v0.1.2
  [92933f4c] ProgressMeter v1.2.0
  [1fd47b50] QuadGK v2.3.1
  [8a4e6c94] QuasiMonteCarlo v0.2.0
  [e6cf234a] RandomNumbers v1.4.0
  [3cdcf5f2] RecipesBase v1.0.1
  [731186ca] RecursiveArrayTools v2.3.1
  [f2c3362d] RecursiveFactorization v0.1.0
  [189a3867] Reexport v0.2.0
  [ae029012] Requires v1.0.1
  [ae5879a3] ResettableStacks v1.0.0
  [37e2e3b7] ReverseDiff v1.2.0
  [79098fc4] Rmath v0.6.1
  [f50d1b31] Rmath_jll v0.2.2+0
  [f2b01f46] Roots v1.0.1
  [1bc83da4] SafeTestsets v0.0.1
  [699a6c99] SimpleTraits v0.9.2
  [ed01d8cd] Sobol v1.3.0
  [a2af1166] SortingAlgorithms v0.3.1
  [47a9eef4] SparseDiffTools v1.7.1
  [d4ead438] SpatialIndexing v0.1.2
  [276daf66] SpecialFunctions v0.10.0
  [90137ffa] StaticArrays v0.12.3
  [2913bbd2] StatsBase v0.33.0
  [4c63d2b9] StatsFuns v0.9.4
  [789caeaf] StochasticDiffEq v6.19.2
  [3783bdb8] TableTraits v1.0.0
  [5d786b92] TerminalLoggers v0.1.1
  [a759f4b9] TimerOutputs v0.5.5
  [9f7883ad] Tracker v0.2.6
  [3bb67fe8] TranscodingStreams v0.9.5
  [a2a6695c] TreeViews v0.3.0
  [30578b45] URIParser v0.4.1
  [3a884ed6] UnPack v0.1.0
  [19fa3120] VertexSafeGraphs v0.1.2
  [a5390f91] ZipFile v0.9.1
  [83775a58] Zlib_jll v1.2.11+9
  [e88e6eb3] Zygote v0.4.20
  [700de1a5] ZygoteRules v0.2.0
  [2a0f44e3] Base64
  [ade2ca70] Dates
  [8bb1440f] DelimitedFiles
  [8ba89e20] Distributed
  [9fa8497b] Future
  [b77e0a4c] InteractiveUtils
  [76f85450] LibGit2
  [8f399da3] Libdl
  [37e2e46d] LinearAlgebra
  [56ddb016] Logging
  [d6f4376e] Markdown
  [a63ad114] Mmap
  [44cfe95a] Pkg
  [de0858da] Printf
  [9abbd945] Profile
  [3fa0cd96] REPL
  [9a3f8284] Random
  [ea8e919c] SHA
  [9e88b42a] Serialization
  [1a1011a3] SharedArrays
  [6462fe0b] Sockets
  [2f01184e] SparseArrays
  [10745b16] Statistics
  [4607b0f0] SuiteSparse
  [8dfed614] Test
  [cf7118a7] UUIDs
  [4ec0a83e] Unicode
WARNING: could not import Compiler.just_construct_ssa into Wrap
416.7086995514811301.86465432569827213.21697213391016149.3929796071194115.2247517913104296.3438136311290279.3936123278511765.5625775751988754.73408970504229446.2626159904189439.55562467607837434.29338639796445430.54262397471921328.93102825992030330.78230280655101637.5616921386252547.3899178101139352.31771747076819448.8559023431533241.1409918445188933.4998573173513127.7136171838481823.8213532431337521.2574969683678819.43364473086034417.9080234863235616.40169427585258414.75827680393393112.9155083938401710.8813206837866058.7182734287630626.5317427624672214.45717155975515762.6440088213855261.23592071815229730.34657206429314730.032376452495214510.26367482940779230.905567097641851.72975726545156162.4738072183786732.93352765725007243.03618679634409672.84498125290393672.49864919108605132.1348949032518721.84171784634708581.64735382952383951.53548374223780541.46787860302471571.40348733006078131.31057624087927451.1723646987133380.98792068128521930.77000396330597080.54099857867771020.327773302321376360.156017818275280080.0445653704883056740.00091322139101779030.0190239786206280320.080410475436539270.159197271761418070.229589835455646730.27324885152407110.28364198270243850.26573545746272670.231707259387955280.195075966066423880.16585180213779310.148215404734272720.140797690163480650.13867239213660060.135911948945067140.127791420961522520.112133203758811030.089657382441687130.063454110359997350.037871264121909730.017167018654110830.0042911560636956050.000132331621225468740.0034254726393664930.0112719546828020260.020185633126746330.027176635651557520.0305475552150724330.0301433293399415230.027042574606087690.0228944748810245740.0192068480124802340.0168492892211720180.0158974341901012840.0157962049318957630.0157121461393411320.0149121397453862370.013034469259305310.0101844853779321530.0068527418229991870.00371107376408517140.4165147233362722.05749907210333312.0370229088210226.5135197460485113.4493268314854941.75595334745193780.84094112859804390.37442848437232390.168534450628907680.113993987252263160.146123478717827850.226085226249190860.330333604729165340.444534373326884170.55998839915566550.67148736086149520.77600647672168410.87189532854251120.95837015434837681.03519180224068191.10245973190405681.1604782310606711.20967136813183121.2505271011940631.28356141199421251.30929540278075081.32824101618583621.34089244428137851.34772122926775031.34917388070383741.34567112361001651.33760820437140931.32535586876996651.30926175199351081.2896520048541721.26683303750284851.2410933007046591.21270505766819881.18192610099156161.14900136156496921.11416445784295411.077639125372521.03964052899088681.00037646561251430.96004837917381170.9188522812487860.87697958751648160.83461759464484320.79194990569877520.74915676965992480.70641524364564860.6638991331799460.6217785606913130.58021964301761350.53938387483431670.499427333461248070.460499739708821740.42274337822788430.386291899150562330.35126903336119110.317787254515817130.28594642748894370.2558324889077440.22751621015976570.201052096409242430.17647747594999640.153811832936124440.13305643256661220.114194281298790780.097190454764755960.081992813256698670.068533108927257250.0567284800940232450.046483294056760050.037691293225716780.0302379803044163580.0240031626280547870.018863561059381280.0146954113248494580.0113769316606047150.0087905797423000960.0068250187158452330.0053767511287227460.0043513374548950640.0036642088220860870.0032410697523287770.0030179127204255330.00294069263292744520.002964718794096250.0030538328905185820.00317944571738445650.00331950398506860.00345745205301092220.00358124310142483430.00368244138331800960.00375544316037069530.0037968299933879190.003804855491655320.0037790561592185770.0037199694116348114416.70869955148316301.87186997430365213.22801705361297149.40648614478837115.2240412249364696.3454391345538679.3977976779125865.5678714713974554.73969630607274446.2681431647925939.5608383606454234.2980229596167530.54633410011867428.9332381463373130.78208250143966437.5587566610162847.3870414634704752.3184906512701248.8589544323429841.14383255919601433.5012140860317727.71329342894746323.81952936942857221.2543767187669819.4294856183711817.90303100693667616.39608500849614714.75230897079328312.90945584534311610.8754625577971288.712882525457966.5270686024001344.4534149599774862.6412962392444671.2342795663056390.3459158046638280.032506748634095760.264310414146053160.90639787468961761.7305156642179892.4743407762289472.9338239544033673.0363286740031682.84506488225275162.49872238818174442.13494430274363721.8416920826943291.6471951659831451.53515198186780831.46736343542057451.40280885504569251.30977827422533061.1715059590677530.98706641658549010.76921762520158130.54033333964074280.32726603243509670.155685347538229820.044402053227207340.00089265358725995560.0191039242369983630.080534769824929170.159307303717266780.229641600525029880.273224851436197740.283552996888048250.26561206968046450.23158584804701470.194985613398158870.165806707487119760.14821387238102470.140826097601411540.13871147379454670.135943057155115480.12780127946737040.112116466759106760.089616889144780110.063399129505176940.037814459743258160.017120978004210560.00426522788223897450.00013060182132885350.00344585479851722280.011306698777051870.0202248123961467770.02721155913652230.0305730930624841660.030158440940405340.027049241017987240.022895782224433470.0192051454721658860.016845112348903880.0158894783750984330.015782221974403340.015690315650227170.0148822708785586060.0129985681020147110.0101464819170533940.0068175358724189420.003683289147449469Test Summary: | Pass  Total
Layers Tests  |    6      6
Test Summary: | Pass  Total
Fast Layers   |    9      9
Training   0%|                                          |  ETA: N/A
494.2089032199091loss: 494   1%|▍                                        |  ETA: 0:08:19
416.7086995514831loss: 417   2%|▉                                        |  ETA: 0:04:14
301.87186997430365loss: 302   3%|█▎                                       |  ETA: 0:02:48
213.22801705361297loss: 213   4%|█▋                                       |  ETA: 0:02:05
149.40648614478843loss: 149   5%|██                                       |  ETA: 0:01:39
115.22404122493646loss: 115   6%|██▌                                      |  ETA: 0:01:22
96.34543913455384loss: 96.3   7%|██▊                                     |  ETA: 0:01:10
79.39779767791259loss: 79.4   8%|███▎                                    |  ETA: 0:01:01
65.56787147139745loss: 65.6   9%|███▋                                    |  ETA: 0:00:53
54.739696306072744loss: 54.7  10%|████                                    |  ETA: 0:00:48
46.26814316479258loss: 46.3  11%|████▍                                   |  ETA: 0:00:43
39.560838360645405loss: 39.6  12%|████▊                                   |  ETA: 0:00:39
34.298022959616745loss: 34.3  13%|█████▎                                  |  ETA: 0:00:36
30.54633410011867loss: 30.5  14%|█████▋                                  |  ETA: 0:00:33
28.933238146337313loss: 28.9  15%|██████                                  |  ETA: 0:00:30
30.78208250143967loss: 30.8  16%|██████▍                                 |  ETA: 0:00:28
37.55875666101628loss: 37.6  17%|██████▊                                 |  ETA: 0:00:26
47.38704146347047loss: 47.4  18%|███████▎                                |  ETA: 0:00:25
52.318490651270146loss: 52.3  19%|███████▋                                |  ETA: 0:00:23
48.85895443234298loss: 48.9  20%|████████                                |  ETA: 0:00:22
41.143832559196loss: 41.1  21%|████████▍                               |  ETA: 0:00:21
33.50121408603177loss: 33.5  22%|████████▊                               |  ETA: 0:00:20
27.713293428947466loss: 27.7  23%|█████████▎                              |  ETA: 0:00:18
23.819529369428558loss: 23.8  24%|█████████▋                              |  ETA: 0:00:18
21.254376718766984loss: 21.3  25%|██████████                              |  ETA: 0:00:17
19.42948561837119loss: 19.4  26%|██████████▍                             |  ETA: 0:00:16
17.90303100693668loss: 17.9  27%|██████████▊                             |  ETA: 0:00:15
16.396085008496147loss: 16.4  28%|███████████▎                            |  ETA: 0:00:14
14.752308970793283loss: 14.8  29%|███████████▋                            |  ETA: 0:00:14
12.909455845343116loss: 12.9  30%|████████████                            |  ETA: 0:00:13
10.875462557797128loss: 10.9  31%|████████████▍                           |  ETA: 0:00:13
8.71288252545796loss: 8.71  32%|████████████▊                           |  ETA: 0:00:12
6.527068602400136loss: 6.53  33%|█████████████▎                          |  ETA: 0:00:12
4.4534149599774855loss: 4.45  34%|█████████████▋                          |  ETA: 0:00:11
2.6412962392444665loss: 2.64  35%|██████████████                          |  ETA: 0:00:11
1.2342795663056394loss: 1.23  36%|██████████████▍                         |  ETA: 0:00:10
0.34591580466382804loss: 0.346  37%|██████████████▍                        |  ETA: 0:00:10
0.03250674863409575loss: 0.0325  38%|██████████████▌                       |  ETA: 0:00:09
0.2643104141460533loss: 0.264  39%|███████████████▎                       |  ETA: 0:00:09
0.9063978746896175loss: 0.906  40%|███████████████▋                       |  ETA: 0:00:09
1.7305156642179889loss: 1.73  41%|████████████████▍                       |  ETA: 0:00:08
2.474340776228947loss: 2.47  42%|████████████████▊                       |  ETA: 0:00:08
2.933823954403367loss: 2.93  43%|█████████████████▎                      |  ETA: 0:00:08
3.036328674003168loss: 3.04  44%|█████████████████▋                      |  ETA: 0:00:07
2.845064882252751loss: 2.85  45%|██████████████████                      |  ETA: 0:00:07
2.498722388181745loss: 2.5  46%|██████████████████▉                      |  ETA: 0:00:07
2.1349443027436377loss: 2.13  47%|██████████████████▊                     |  ETA: 0:00:07
1.8416920826943293loss: 1.84  48%|███████████████████▎                    |  ETA: 0:00:06
1.647195165983145loss: 1.65  49%|███████████████████▋                    |  ETA: 0:00:06
1.5351519818678083loss: 1.54  50%|████████████████████                    |  ETA: 0:00:06
1.467363435420574loss: 1.47  51%|████████████████████▍                   |  ETA: 0:00:06
1.402808855045693loss: 1.4  52%|█████████████████████▍                   |  ETA: 0:00:05
1.3097782742253306loss: 1.31  53%|█████████████████████▎                  |  ETA: 0:00:05
1.171505959067753loss: 1.17  54%|█████████████████████▋                  |  ETA: 0:00:05
0.9870664165854897loss: 0.987  55%|█████████████████████▌                 |  ETA: 0:00:05
0.7692176252015813loss: 0.769  56%|█████████████████████▉                 |  ETA: 0:00:05
0.5403333396407428loss: 0.54  57%|██████████████████████▊                 |  ETA: 0:00:05
0.3272660324350968loss: 0.327  58%|██████████████████████▋                |  ETA: 0:00:04
0.1556853475382298loss: 0.156  59%|███████████████████████                |  ETA: 0:00:04
0.04440205322720733loss: 0.0444  60%|██████████████████████▊               |  ETA: 0:00:04
0.0008926535872599556loss: 0.000893  61%|██████████████████████              |  ETA: 0:00:04
0.019103924236998377loss: 0.0191  62%|███████████████████████▌              |  ETA: 0:00:04
0.08053476982492919loss: 0.0805  63%|████████████████████████              |  ETA: 0:00:04
0.15930730371726676loss: 0.159  64%|█████████████████████████              |  ETA: 0:00:03
0.22964160052502988loss: 0.23  65%|██████████████████████████              |  ETA: 0:00:03
0.2732248514361978loss: 0.273  66%|█████████████████████████▊             |  ETA: 0:00:03
0.2835529968880482loss: 0.284  67%|██████████████████████████▏            |  ETA: 0:00:03
0.2656120696804645loss: 0.266  68%|██████████████████████████▌            |  ETA: 0:00:03
0.23158584804701468loss: 0.232  69%|██████████████████████████▉            |  ETA: 0:00:03
0.19498561339815892loss: 0.195  70%|███████████████████████████▎           |  ETA: 0:00:03
0.1658067074871198loss: 0.166  71%|███████████████████████████▊           |  ETA: 0:00:03
0.14821387238102476loss: 0.148  72%|████████████████████████████▏          |  ETA: 0:00:02
0.14082609760141152loss: 0.141  73%|████████████████████████████▌          |  ETA: 0:00:02
0.13871147379454676loss: 0.139  74%|████████████████████████████▉          |  ETA: 0:00:02
0.13594305715511545loss: 0.136  75%|█████████████████████████████▎         |  ETA: 0:00:02
0.1278012794673704loss: 0.128  76%|█████████████████████████████▋         |  ETA: 0:00:02
0.11211646675910676loss: 0.112  77%|██████████████████████████████         |  ETA: 0:00:02
0.0896168891447801loss: 0.0896  78%|█████████████████████████████▋        |  ETA: 0:00:02
0.06339912950517693loss: 0.0634  79%|██████████████████████████████        |  ETA: 0:00:02
0.03781445974325816loss: 0.0378  80%|██████████████████████████████▍       |  ETA: 0:00:02
0.017120978004210557loss: 0.0171  81%|██████████████████████████████▊       |  ETA: 0:00:01
0.004265227882238975loss: 0.00427  82%|██████████████████████████████▍      |  ETA: 0:00:01
0.00013060182132885353loss: 0.000131  83%|█████████████████████████████▉      |  ETA: 0:00:01
0.0034458547985172228loss: 0.00345  84%|███████████████████████████████▏     |  ETA: 0:00:01
0.01130669877705187loss: 0.0113  85%|████████████████████████████████▎     |  ETA: 0:00:01
0.020224812396146763loss: 0.0202  86%|████████████████████████████████▋     |  ETA: 0:00:01
0.0272115591365223loss: 0.0272  87%|█████████████████████████████████     |  ETA: 0:00:01
0.030573093062484166loss: 0.0306  88%|█████████████████████████████████▌    |  ETA: 0:00:01
0.03015844094040534loss: 0.0302  89%|█████████████████████████████████▉    |  ETA: 0:00:01
0.027049241017987252loss: 0.027  90%|███████████████████████████████████▏   |  ETA: 0:00:01
0.02289578222443347loss: 0.0229  91%|██████████████████████████████████▋   |  ETA: 0:00:01
0.019205145472165897loss: 0.0192  92%|███████████████████████████████████   |  ETA: 0:00:01
0.01684511234890388loss: 0.0168  93%|███████████████████████████████████▍  |  ETA: 0:00:00
0.01588947837509843loss: 0.0159  94%|███████████████████████████████████▊  |  ETA: 0:00:00
0.015782221974403335loss: 0.0158  95%|████████████████████████████████████▏ |  ETA: 0:00:00
0.01569031565022717loss: 0.0157  96%|████████████████████████████████████▌ |  ETA: 0:00:00
0.014882270878558595loss: 0.0149  97%|████████████████████████████████████▉ |  ETA: 0:00:00
0.01299856810201471loss: 0.013  98%|██████████████████████████████████████▎|  ETA: 0:00:00
0.010146481917053394loss: 0.0101  99%|█████████████████████████████████████▋|  ETA: 0:00:00
0.0068175358724189405loss: 0.00682 100%|█████████████████████████████████████| Time: 0:00:06
0.00013060182132885353Training 100%|██████████████████████████████████████████| Time: 0:00:07
494.2089032199091473.8759825703607567.1074818633421318.3855315564830028.9648899689183876.117368644930780.16955330167694750.00127452976279778434.4571905150957083e-72.8044413366351072e-82.7293451721924503e-82.729347900600587e-8494.2089032199091473.8237904460742767.225842645127621.36751439899320811.4350691360731788.0109706995672490.32915359623862510.0036303407653235878.934029714091469e-56.246060306129264e-50.00031047251577495510.0058696171661135780.000109832425486265727.733918863503838e-64.2537266437804656e-78.726103760581099e-74.2537266437804656e-72.72458808565546e-72.766701471525958e-72.72458808565546e-72.6483736867625894e-72.648412606822959e-72.6483736867625894e-72.5881397656192965e-72.588142311285918e-72.5881397656192965e-72.508740247077843e-72.508742514801445e-72.508740247077843e-72.508742104639958e-7Starting optimization with optimizer BlackBoxOptim.DiffEvoOpt{BlackBoxOptim.FitPopulation{Float64},BlackBoxOptim.SimpleSelector,BlackBoxOptim.AdaptiveDiffEvoRandBin{3},BlackBoxOptim.RandomBound{BlackBoxOptim.ContinuousRectSearchSpace}}
0.00 secs, 0 evals, 0 steps
70.88430844761649963.06183320254210.80922221348674599.533110089540642194.9266536712151.1556797273485411e6187.0772099820403623.69441161489971435.4379523948815631236.250506801032830.6284219545582388.150413616387151.31798151208978525.70861432916268.9194911160333114.1652492865933722.378368388102025170.09149182827437734.366284292842950.1936810170607450.1936810170607425.82607264419739693.8561701321299910.80922221348674599.533110089540642361.8207683809096397.843344878987993282.39983849967689.602474831606451.317981512089723.69441161489971421.8493299548990323.694411614899714267818.0486072037693.0178066877590825.82607264419739631.706982367978714107.54157005971534142.2895190055642222.378368388102025123.8264407018236447.07728990442061582.47332526214451453.999616830705793.017806687759081210.140225183000223.694411614899714257.94556440219937.11741032765914497879.79550543966173.6872450738629837.11741032765914455.6969721226487242830.6284219545582173.687245073862981291.145827272284855.696972122648724335.30352406976317731.102545643623676.6106019725005321.2357972129566787.8877648065879550.1936810170607422.37836838810202555.6969721226487241098.497256962503637.1174103276591441200.9526139078287177.167049493473831.7069823679787145528.9516311949837.1787006700981811.85089882191072255.69697212264872453.365264603775669737.28889416955350.193681017060748953.8172011700646827.823685492663440.508348064547121.2618086011931198.17056775918235164.95673354880174163.6665528493335167.375828876459144000.440929188393563.9099681976384570.88430844761649123.8264407018236124.5813904722182547.7968253484299146.095602431827260776.86741730387458.9454702635217388.4821138437311769.49695084011937177.78841053308034255.2174904846299670.8843084476164937.117410327659144106.63906316077603
Optimization stopped after 101 steps and 0.31 seconds
Termination reason: Max number of steps (100) reached
Steps per second = 322.56
Function evals per second = 536.53
Improvements/step = 0.36000
Total function evaluations = 168


Best candidate found: [3.22631, 3.43214, 2.21895, 2.22308]

Fitness: 0.795720744

Training   0%|                                          |  ETA: N/A
74.23333058147179loss: 74.2   1%|▍                                       |  ETA: 0:00:31
40.41651472333627loss: 40.4   2%|▊                                       |  ETA: 0:00:16
22.057499072103333loss: 22.1   3%|█▎                                      |  ETA: 0:00:10
12.037022908821022loss: 12   4%|█▋                                        |  ETA: 0:00:08
6.513519746048511loss: 6.51   5%|██                                      |  ETA: 0:00:06
3.449326831485494loss: 3.45   6%|██▍                                     |  ETA: 0:00:05
1.7559533474519378loss: 1.76   7%|██▊                                     |  ETA: 0:00:04
0.8409411285980439loss: 0.841   8%|███▏                                   |  ETA: 0:00:04
0.3744284843723239loss: 0.374   9%|███▌                                   |  ETA: 0:00:03
0.16853445062890768loss: 0.169  10%|███▉                                   |  ETA: 0:00:03
0.11399398725226316loss: 0.114  11%|████▎                                  |  ETA: 0:00:03
0.14612347871782785loss: 0.146  12%|████▋                                  |  ETA: 0:00:02
0.22608522624919086loss: 0.226  13%|█████▏                                 |  ETA: 0:00:02
0.33033360472916534loss: 0.33  14%|█████▋                                  |  ETA: 0:00:02
0.44453437332688417loss: 0.445  15%|█████▉                                 |  ETA: 0:00:02
0.5599883991556655loss: 0.56  16%|██████▍                                 |  ETA: 0:00:02
0.6714873608614952loss: 0.671  17%|██████▋                                |  ETA: 0:00:02
0.7760064767216841loss: 0.776  18%|███████                                |  ETA: 0:00:02
0.8718953285425112loss: 0.872  19%|███████▍                               |  ETA: 0:00:01
0.9583701543483768loss: 0.958  20%|███████▊                               |  ETA: 0:00:01
1.0351918022406819loss: 1.04  21%|████████▍                               |  ETA: 0:00:01
1.1024597319040568loss: 1.1  22%|█████████                                |  ETA: 0:00:01
1.160478231060671loss: 1.16  23%|█████████▎                              |  ETA: 0:00:01
1.2096713681318312loss: 1.21  24%|█████████▋                              |  ETA: 0:00:01
1.250527101194063loss: 1.25  25%|██████████                              |  ETA: 0:00:01
1.2835614119942125loss: 1.28  26%|██████████▍                             |  ETA: 0:00:01
1.3092954027807508loss: 1.31  27%|██████████▊                             |  ETA: 0:00:01
1.3282410161858362loss: 1.33  28%|███████████▎                            |  ETA: 0:00:01
1.3408924442813785loss: 1.34  29%|███████████▋                            |  ETA: 0:00:01
1.3477212292677503loss: 1.35  30%|████████████                            |  ETA: 0:00:01
1.3491738807038374loss: 1.35  31%|████████████▍                           |  ETA: 0:00:01
1.3456711236100165loss: 1.35  32%|████████████▊                           |  ETA: 0:00:01
1.3376082043714093loss: 1.34  33%|█████████████▎                          |  ETA: 0:00:01
1.3253558687699665loss: 1.33  34%|█████████████▋                          |  ETA: 0:00:01
1.3092617519935108loss: 1.31  35%|██████████████                          |  ETA: 0:00:01
1.289652004854172loss: 1.29  36%|██████████████▍                         |  ETA: 0:00:01
1.2668330375028485loss: 1.27  37%|██████████████▊                         |  ETA: 0:00:01
1.241093300704659loss: 1.24  38%|███████████████▎                        |  ETA: 0:00:01
1.2127050576681988loss: 1.21  39%|███████████████▋                        |  ETA: 0:00:01
1.1819261009915616loss: 1.18  40%|████████████████                        |  ETA: 0:00:01
1.1490013615649692loss: 1.15  41%|████████████████▍                       |  ETA: 0:00:01
1.1141644578429541loss: 1.11  42%|████████████████▊                       |  ETA: 0:00:01
1.07763912537252loss: 1.08  43%|█████████████████▎                      |  ETA: 0:00:00
1.0396405289908868loss: 1.04  44%|█████████████████▋                      |  ETA: 0:00:00
1.0003764656125143loss: 1  45%|███████████████████▍                       |  ETA: 0:00:00
0.9600483791738117loss: 0.96  46%|██████████████████▍                     |  ETA: 0:00:00
0.918852281248786loss: 0.919  47%|██████████████████▍                    |  ETA: 0:00:00
0.8769795875164816loss: 0.877  48%|██████████████████▊                    |  ETA: 0:00:00
0.8346175946448432loss: 0.835  49%|███████████████████▏                   |  ETA: 0:00:00
0.7919499056987752loss: 0.792  50%|███████████████████▌                   |  ETA: 0:00:00
0.7491567696599248loss: 0.749  51%|███████████████████▉                   |  ETA: 0:00:00
0.7064152436456486loss: 0.706  52%|████████████████████▎                  |  ETA: 0:00:00
0.663899133179946loss: 0.664  53%|████████████████████▋                  |  ETA: 0:00:00
0.621778560691313loss: 0.622  54%|█████████████████████                  |  ETA: 0:00:00
0.5802196430176135loss: 0.58  55%|██████████████████████                  |  ETA: 0:00:00
0.5393838748343167loss: 0.539  56%|█████████████████████▉                 |  ETA: 0:00:00
0.49942733346124807loss: 0.499  57%|██████████████████████▎                |  ETA: 0:00:00
0.46049973970882174loss: 0.46  58%|███████████████████████▎                |  ETA: 0:00:00
0.4227433782278843loss: 0.423  59%|███████████████████████                |  ETA: 0:00:00
0.38629189915056233loss: 0.386  60%|███████████████████████▍               |  ETA: 0:00:00
0.3512690333611911loss: 0.351  61%|███████████████████████▊               |  ETA: 0:00:00
0.31778725451581713loss: 0.318  62%|████████████████████████▏              |  ETA: 0:00:00
0.2859464274889437loss: 0.286  63%|████████████████████████▋              |  ETA: 0:00:00
0.255832488907744loss: 0.256  64%|█████████████████████████              |  ETA: 0:00:00
0.2275162101597657loss: 0.228  65%|█████████████████████████▍             |  ETA: 0:00:00
0.20105209640924243loss: 0.201  66%|█████████████████████████▊             |  ETA: 0:00:00
0.1764774759499964loss: 0.176  67%|██████████████████████████▏            |  ETA: 0:00:00
0.15381183293612444loss: 0.154  68%|██████████████████████████▌            |  ETA: 0:00:00
0.1330564325666122loss: 0.133  69%|██████████████████████████▉            |  ETA: 0:00:00
0.11419428129879078loss: 0.114  70%|███████████████████████████▎           |  ETA: 0:00:00
0.09719045476475596loss: 0.0972  71%|███████████████████████████           |  ETA: 0:00:00
0.08199281325669867loss: 0.082  72%|████████████████████████████▏          |  ETA: 0:00:00
0.06853310892725725loss: 0.0685  73%|███████████████████████████▊          |  ETA: 0:00:00
0.056728480094023245loss: 0.0567  74%|████████████████████████████▏         |  ETA: 0:00:00
0.04648329405676005loss: 0.0465  75%|████████████████████████████▌         |  ETA: 0:00:00
0.03769129322571678loss: 0.0377  76%|████████████████████████████▉         |  ETA: 0:00:00
0.030237980304416358loss: 0.0302  77%|█████████████████████████████▎        |  ETA: 0:00:00
0.024003162628054787loss: 0.024  78%|██████████████████████████████▍        |  ETA: 0:00:00
0.01886356105938128loss: 0.0189  79%|██████████████████████████████        |  ETA: 0:00:00
0.014695411324849458loss: 0.0147  80%|██████████████████████████████▍       |  ETA: 0:00:00
0.011376931660604715loss: 0.0114  81%|██████████████████████████████▊       |  ETA: 0:00:00
0.008790579742300096loss: 0.00879  82%|██████████████████████████████▍      |  ETA: 0:00:00
0.006825018715845233loss: 0.00683  83%|██████████████████████████████▊      |  ETA: 0:00:00
0.005376751128722746loss: 0.00538  84%|███████████████████████████████▏     |  ETA: 0:00:00
0.004351337454895064loss: 0.00435  85%|███████████████████████████████▌     |  ETA: 0:00:00
0.003664208822086087loss: 0.00366  86%|███████████████████████████████▉     |  ETA: 0:00:00
0.003241069752328777loss: 0.00324  87%|████████████████████████████████▎    |  ETA: 0:00:00
0.003017912720425533loss: 0.00302  88%|████████████████████████████████▌    |  ETA: 0:00:00
0.0029406926329274452loss: 0.00294  89%|████████████████████████████████▉    |  ETA: 0:00:00
0.00296471879409625loss: 0.00296  90%|█████████████████████████████████▎   |  ETA: 0:00:00
0.003053832890518582loss: 0.00305  91%|█████████████████████████████████▋   |  ETA: 0:00:00
0.0031794457173844565loss: 0.00318  92%|██████████████████████████████████   |  ETA: 0:00:00
0.0033195039850686loss: 0.00332  93%|██████████████████████████████████▍  |  ETA: 0:00:00
0.0034574520530109222loss: 0.00346  94%|██████████████████████████████████▊  |  ETA: 0:00:00
0.0035812431014248343loss: 0.00358  95%|███████████████████████████████████▏ |  ETA: 0:00:00
0.0036824413833180096loss: 0.00368  96%|███████████████████████████████████▌ |  ETA: 0:00:00
0.0037554431603706953loss: 0.00376  97%|███████████████████████████████████▉ |  ETA: 0:00:00
0.003796829993387919loss: 0.0038  98%|█████████████████████████████████████▎|  ETA: 0:00:00
0.00380485549165532loss: 0.0038  99%|█████████████████████████████████████▋|  ETA: 0:00:00
0.003779056159218577loss: 0.00378 100%|█████████████████████████████████████| Time: 0:00:00
0.0029406926329274452Training 100%|██████████████████████████████████████████| Time: 0:00:00
74.233330581471791.56135771993787721.51943291733128061.51473100623542180.68242629663728870.191372695910355050.00019358396316548111.7955751251456144e-61.0477433167206356e-107.924036678678786e-177.924038666328425e-17Training   0%|                                          |  ETA: N/A
494.2089032199091loss: 494   1%|▍                                        |  ETA: 0:00:37
416.7086995514831loss: 417   2%|▉                                        |  ETA: 0:00:19
301.87186997430365loss: 302   3%|█▎                                       |  ETA: 0:00:13
213.22801705361297loss: 213   4%|█▋                                       |  ETA: 0:00:10
149.40648614478843loss: 149   5%|██                                       |  ETA: 0:00:08
115.22404122493646loss: 115   6%|██▌                                      |  ETA: 0:00:07
96.34543913455384loss: 96.3   7%|██▊                                     |  ETA: 0:00:06
79.39779767791259loss: 79.4   8%|███▎                                    |  ETA: 0:00:05
65.56787147139745loss: 65.6   9%|███▋                                    |  ETA: 0:00:05
54.739696306072744loss: 54.7  10%|████                                    |  ETA: 0:00:04
46.26814316479258loss: 46.3  11%|████▍                                   |  ETA: 0:00:04
39.560838360645405loss: 39.6  12%|████▊                                   |  ETA: 0:00:04
34.298022959616745loss: 34.3  13%|█████▎                                  |  ETA: 0:00:04
30.54633410011867loss: 30.5  14%|█████▋                                  |  ETA: 0:00:03
28.933238146337313loss: 28.9  15%|██████                                  |  ETA: 0:00:03
30.78208250143967loss: 30.8  16%|██████▍                                 |  ETA: 0:00:03
37.55875666101628loss: 37.6  17%|██████▊                                 |  ETA: 0:00:03
47.38704146347047loss: 47.4  18%|███████▎                                |  ETA: 0:00:03
52.318490651270146loss: 52.3  19%|███████▋                                |  ETA: 0:00:03
48.85895443234298loss: 48.9  20%|████████                                |  ETA: 0:00:03
41.143832559196loss: 41.1  21%|████████▍                               |  ETA: 0:00:03
33.50121408603177loss: 33.5  22%|████████▊                               |  ETA: 0:00:02
27.713293428947466loss: 27.7  23%|█████████▎                              |  ETA: 0:00:02
23.819529369428558loss: 23.8  24%|█████████▋                              |  ETA: 0:00:02
21.254376718766984loss: 21.3  25%|██████████                              |  ETA: 0:00:02
19.42948561837119loss: 19.4  26%|██████████▍                             |  ETA: 0:00:02
17.90303100693668loss: 17.9  27%|██████████▊                             |  ETA: 0:00:02
16.396085008496147loss: 16.4  28%|███████████▎                            |  ETA: 0:00:02
14.752308970793283loss: 14.8  29%|███████████▋                            |  ETA: 0:00:02
12.909455845343116loss: 12.9  30%|████████████                            |  ETA: 0:00:02
10.875462557797128loss: 10.9  31%|████████████▍                           |  ETA: 0:00:02
8.71288252545796loss: 8.71  32%|████████████▊                           |  ETA: 0:00:02
6.527068602400136loss: 6.53  33%|█████████████▎                          |  ETA: 0:00:02
4.4534149599774855loss: 4.45  34%|█████████████▋                          |  ETA: 0:00:02
2.6412962392444665loss: 2.64  35%|██████████████                          |  ETA: 0:00:02
1.2342795663056394loss: 1.23  36%|██████████████▍                         |  ETA: 0:00:02
0.34591580466382804loss: 0.346  37%|██████████████▍                        |  ETA: 0:00:02
0.03250674863409575loss: 0.0325  38%|██████████████▌                       |  ETA: 0:00:01
0.2643104141460533loss: 0.264  39%|███████████████▎                       |  ETA: 0:00:01
0.9063978746896175loss: 0.906  40%|███████████████▋                       |  ETA: 0:00:01
1.7305156642179889loss: 1.73  41%|████████████████▍                       |  ETA: 0:00:01
2.474340776228947loss: 2.47  42%|████████████████▊                       |  ETA: 0:00:01
2.933823954403367loss: 2.93  43%|█████████████████▎                      |  ETA: 0:00:01
3.036328674003168loss: 3.04  44%|█████████████████▋                      |  ETA: 0:00:01
2.845064882252751loss: 2.85  45%|██████████████████                      |  ETA: 0:00:01
2.498722388181745loss: 2.5  46%|██████████████████▉                      |  ETA: 0:00:01
2.1349443027436377loss: 2.13  47%|██████████████████▊                     |  ETA: 0:00:01
1.8416920826943293loss: 1.84  48%|███████████████████▎                    |  ETA: 0:00:01
1.647195165983145loss: 1.65  49%|███████████████████▋                    |  ETA: 0:00:01
1.5351519818678083loss: 1.54  50%|████████████████████                    |  ETA: 0:00:01
1.467363435420574loss: 1.47  51%|████████████████████▍                   |  ETA: 0:00:01
1.402808855045693loss: 1.4  52%|█████████████████████▍                   |  ETA: 0:00:01
1.3097782742253306loss: 1.31  53%|█████████████████████▎                  |  ETA: 0:00:01
1.171505959067753loss: 1.17  54%|█████████████████████▋                  |  ETA: 0:00:01
0.9870664165854897loss: 0.987  55%|█████████████████████▌                 |  ETA: 0:00:01
0.7692176252015813loss: 0.769  56%|█████████████████████▉                 |  ETA: 0:00:01
0.5403333396407428loss: 0.54  57%|██████████████████████▊                 |  ETA: 0:00:01
0.3272660324350968loss: 0.327  58%|██████████████████████▋                |  ETA: 0:00:01
0.1556853475382298loss: 0.156  59%|███████████████████████                |  ETA: 0:00:01
0.04440205322720733loss: 0.0444  60%|██████████████████████▊               |  ETA: 0:00:01
0.0008926535872599556loss: 0.000893  61%|██████████████████████              |  ETA: 0:00:01
0.019103924236998377loss: 0.0191  62%|███████████████████████▌              |  ETA: 0:00:01
0.08053476982492919loss: 0.0805  63%|████████████████████████              |  ETA: 0:00:01
0.15930730371726676loss: 0.159  64%|█████████████████████████              |  ETA: 0:00:01
0.22964160052502988loss: 0.23  65%|██████████████████████████              |  ETA: 0:00:01
0.2732248514361978loss: 0.273  66%|█████████████████████████▊             |  ETA: 0:00:01
0.2835529968880482loss: 0.284  67%|██████████████████████████▏            |  ETA: 0:00:01
0.2656120696804645loss: 0.266  68%|██████████████████████████▌            |  ETA: 0:00:01
0.23158584804701468loss: 0.232  69%|██████████████████████████▉            |  ETA: 0:00:01
0.19498561339815892loss: 0.195  70%|███████████████████████████▎           |  ETA: 0:00:01
0.1658067074871198loss: 0.166  71%|███████████████████████████▊           |  ETA: 0:00:01
0.14821387238102476loss: 0.148  72%|████████████████████████████▏          |  ETA: 0:00:01
0.14082609760141152loss: 0.141  73%|████████████████████████████▌          |  ETA: 0:00:01
0.13871147379454676loss: 0.139  74%|████████████████████████████▉          |  ETA: 0:00:01
0.13594305715511545loss: 0.136  75%|█████████████████████████████▎         |  ETA: 0:00:01
0.1278012794673704loss: 0.128  76%|█████████████████████████████▋         |  ETA: 0:00:00
0.11211646675910676loss: 0.112  77%|██████████████████████████████         |  ETA: 0:00:00
0.0896168891447801loss: 0.0896  78%|█████████████████████████████▋        |  ETA: 0:00:00
0.06339912950517693loss: 0.0634  79%|██████████████████████████████        |  ETA: 0:00:00
0.03781445974325816loss: 0.0378  80%|██████████████████████████████▍       |  ETA: 0:00:00
0.017120978004210557loss: 0.0171  81%|██████████████████████████████▊       |  ETA: 0:00:00
0.004265227882238975loss: 0.00427  82%|██████████████████████████████▍      |  ETA: 0:00:00
0.00013060182132885353loss: 0.000131  83%|█████████████████████████████▉      |  ETA: 0:00:00
0.0034458547985172228loss: 0.00345  84%|███████████████████████████████▏     |  ETA: 0:00:00
0.01130669877705187loss: 0.0113  85%|████████████████████████████████▎     |  ETA: 0:00:00
0.020224812396146763loss: 0.0202  86%|████████████████████████████████▋     |  ETA: 0:00:00
0.0272115591365223loss: 0.0272  87%|█████████████████████████████████     |  ETA: 0:00:00
0.030573093062484166loss: 0.0306  88%|█████████████████████████████████▌    |  ETA: 0:00:00
0.03015844094040534loss: 0.0302  89%|█████████████████████████████████▉    |  ETA: 0:00:00
0.027049241017987252loss: 0.027  90%|███████████████████████████████████▏   |  ETA: 0:00:00
0.02289578222443347loss: 0.0229  91%|██████████████████████████████████▋   |  ETA: 0:00:00
0.019205145472165897loss: 0.0192  92%|███████████████████████████████████   |  ETA: 0:00:00
0.01684511234890388loss: 0.0168  93%|███████████████████████████████████▍  |  ETA: 0:00:00
0.01588947837509843loss: 0.0159  94%|███████████████████████████████████▊  |  ETA: 0:00:00
0.015782221974403335loss: 0.0158  95%|████████████████████████████████████▏ |  ETA: 0:00:00
0.01569031565022717loss: 0.0157  96%|████████████████████████████████████▌ |  ETA: 0:00:00
0.014882270878558595loss: 0.0149  97%|████████████████████████████████████▉ |  ETA: 0:00:00
0.01299856810201471loss: 0.013  98%|██████████████████████████████████████▎|  ETA: 0:00:00
0.010146481917053394loss: 0.0101  99%|█████████████████████████████████████▋|  ETA: 0:00:00
0.0068175358724189405loss: 0.00682 100%|█████████████████████████████████████| Time: 0:00:01
0.00013060182132885353Training 100%|██████████████████████████████████████████| Time: 0:00:01
494.2089032199091473.8759825703607567.1074818633421318.3855315564830028.9648899689183876.117368644930780.16955330167694750.00127452976279778434.4571905150957083e-72.8044413366351072e-82.7293451721924503e-82.729347900600587e-8494.2089032199091473.8237904460742767.225842645127621.36751439899320811.4350691360731788.0109706995672490.32915359623862510.0036303407653235878.934029714091469e-56.246060306129264e-50.00031047251577495510.0058696171661135780.000109832425486265727.733918863503838e-64.2537266437804656e-78.726103760581099e-74.2537266437804656e-72.72458808565546e-72.766701471525958e-72.72458808565546e-72.6483736867625894e-72.648412606822959e-72.6483736867625894e-72.5881397656192965e-72.588142311285918e-72.5881397656192965e-72.508740247077843e-72.508742514801445e-72.508740247077843e-72.508742104639958e-7fval:4.94e+02  norm:3.16e+00
Layers SciML Tests: Error During Test at /home/pkgeval/.julia/packages/SafeTestsets/A83XK/src/SafeTestsets.jl:25
  Got exception outside of a @test
  LoadError: TypeError: in typeassert, expected NLopt.Callback_Data, got a value of type Core.Compiler.UseRef
  Stacktrace:
   [1] nlopt_callback_wrapper(::UInt32, ::Ptr{Float64}, ::Ptr{Float64}, ::Ptr{Nothing}) at /home/pkgeval/.julia/packages/NLopt/eqN9a/src/NLopt.jl:411
   [2] optimize! at /home/pkgeval/.julia/packages/NLopt/eqN9a/src/NLopt.jl:604 [inlined]
   [3] optimize at /home/pkgeval/.julia/packages/NLopt/eqN9a/src/NLopt.jl:611 [inlined]
   [4] sciml_train(::typeof(Main.##373.loss_adjoint), ::Array{Float64,1}, ::NLopt.Opt, ::Base.Iterators.Cycle{Tuple{DiffEqFlux.NullData}}; maxeval::Int64) at /home/pkgeval/.julia/packages/DiffEqFlux/gMXy7/src/require.jl:34
   [5] sciml_train(::Function, ::Array{Float64,1}, ::NLopt.Opt, ::Base.Iterators.Cycle{Tuple{DiffEqFlux.NullData}}) at /home/pkgeval/.julia/packages/DiffEqFlux/gMXy7/src/require.jl:13 (repeats 2 times)
   [6] top-level scope at /home/pkgeval/.julia/packages/DiffEqFlux/gMXy7/test/layers_sciml.jl:147
   [7] include(::Function, ::Module, ::String) at ./Base.jl:380
   [8] include at ./Base.jl:368 [inlined]
   [9] include(::String) at /home/pkgeval/.julia/packages/SafeTestsets/A83XK/src/SafeTestsets.jl:23
   [10] top-level scope at /home/pkgeval/.julia/packages/DiffEqFlux/gMXy7/test/runtests.jl:11
   [11] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [12] top-level scope at /home/pkgeval/.julia/packages/DiffEqFlux/gMXy7/test/runtests.jl:11
   [13] eval(::Module, ::Any) at ./boot.jl:331
   [14] top-level scope at /home/pkgeval/.julia/packages/SafeTestsets/A83XK/src/SafeTestsets.jl:23
   [15] top-level scope at /home/pkgeval/.julia/packages/DiffEqFlux/gMXy7/test/runtests.jl:11
   [16] top-level scope at timing.jl:174
   [17] include(::String) at ./client.jl:457
   [18] top-level scope at none:6
   [19] eval(::Module, ::Any) at ./boot.jl:331
   [20] exec_options(::Base.JLOptions) at ./client.jl:272
   [21] _start() at ./client.jl:506
  in expression starting at /home/pkgeval/.julia/packages/DiffEqFlux/gMXy7/test/layers_sciml.jl:147
  
Test Summary:      | Pass  Error  Total
Layers SciML Tests |   14      1     15
ERROR: LoadError: Some tests did not pass: 14 passed, 0 failed, 1 errored, 0 broken.
in expression starting at /home/pkgeval/.julia/packages/DiffEqFlux/gMXy7/test/runtests.jl:7
ERROR: Package DiffEqFlux errored during testing
Stacktrace:
 [1] pkgerror(::String, ::Vararg{String,N} where N) at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/Types.jl:52
 [2] test(::Pkg.Types.Context, ::Array{Pkg.Types.PackageSpec,1}; coverage::Bool, julia_args::Cmd, test_args::Cmd, test_fn::Nothing) at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/Operations.jl:1515
 [3] test(::Pkg.Types.Context, ::Array{Pkg.Types.PackageSpec,1}; coverage::Bool, test_fn::Nothing, julia_args::Cmd, test_args::Cmd, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:316
 [4] test(::Pkg.Types.Context, ::Array{Pkg.Types.PackageSpec,1}) at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:303
 [5] #test#68 at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:297 [inlined]
 [6] test at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:297 [inlined]
 [7] #test#67 at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:296 [inlined]
 [8] test at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:296 [inlined]
 [9] test(::String; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:295
 [10] test(::String) at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:295
 [11] top-level scope at none:16
