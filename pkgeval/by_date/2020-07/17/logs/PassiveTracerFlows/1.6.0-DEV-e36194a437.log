Julia Version 1.6.0-DEV.483
Commit e36194a437 (2020-07-17 16:26 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-9.0.1 (ORCJIT, skylake-avx512)
Environment:
  JULIA_DEPOT_PATH = ::/usr/local/share/julia
  JULIA_NUM_THREADS = 2

  Resolving package versions...
  Installed CUDAapi ──────────── v2.1.0
  Installed Reexport ─────────── v0.2.0
  Installed Ratios ───────────── v0.4.0
  Installed BinaryProvider ───── v0.5.10
  Installed JSON ─────────────── v0.21.0
  Installed PassiveTracerFlows ─ v0.2.0
  Installed FourierFlows ─────── v0.4.5
  Installed LLVM ─────────────── v1.7.0
  Installed AxisAlgorithms ───── v1.0.0
  Installed VersionParsing ───── v1.2.0
  Installed WoodburyMatrices ─── v0.5.2
  Installed Adapt ────────────── v1.1.0
  Installed Zlib_jll ─────────── v1.2.11+14
  Installed OrderedCollections ─ v1.3.0
  Installed DataStructures ───── v0.17.19
  Installed NNlib ────────────── v0.6.6
  Installed CuArrays ─────────── v1.7.0
  Installed Conda ────────────── v1.4.1
  Installed CEnum ────────────── v0.2.0
  Installed Requires ─────────── v1.0.1
  Installed AbstractFFTs ─────── v0.5.0
  Installed FFTW ─────────────── v1.1.0
  Installed TranscodingStreams ─ v0.9.5
  Installed JLD2 ─────────────── v0.1.14
  Installed CUDAnative ───────── v2.9.1
  Installed CodecZlib ────────── v0.7.0
  Installed TimerOutputs ─────── v0.5.6
  Installed Parsers ──────────── v1.0.7
  Installed MacroTools ───────── v0.5.5
  Installed OffsetArrays ─────── v1.1.1
  Installed GPUArrays ────────── v2.0.1
  Installed CUDAdrv ──────────── v5.1.0
  Installed Interpolations ───── v0.12.10
  Installed FileIO ───────────── v1.3.0
  Installed StaticArrays ─────── v0.12.4
Updating `~/.julia/environments/v1.6/Project.toml`
  [dc26d6a1] + PassiveTracerFlows v0.2.0
Updating `~/.julia/environments/v1.6/Manifest.toml`
  [621f4979] + AbstractFFTs v0.5.0
  [79e6a3ab] + Adapt v1.1.0
  [13072b0f] + AxisAlgorithms v1.0.0
  [b99e7846] + BinaryProvider v0.5.10
  [fa961155] + CEnum v0.2.0
  [3895d2a7] + CUDAapi v2.1.0
  [c5f51814] + CUDAdrv v5.1.0
  [be33ccc6] + CUDAnative v2.9.1
  [944b1d66] + CodecZlib v0.7.0
  [8f4d0f93] + Conda v1.4.1
  [3a865a2d] + CuArrays v1.7.0
  [864edb3b] + DataStructures v0.17.19
  [7a1cc6ca] + FFTW v1.1.0
  [5789e2e9] + FileIO v1.3.0
  [2aec4490] + FourierFlows v0.4.5
  [0c68f7d7] + GPUArrays v2.0.1
  [a98d9a8b] + Interpolations v0.12.10
  [033835bb] + JLD2 v0.1.14
  [682c06a0] + JSON v0.21.0
  [929cbde3] + LLVM v1.7.0
  [1914dd2f] + MacroTools v0.5.5
  [872c559c] + NNlib v0.6.6
  [6fe1bfb0] + OffsetArrays v1.1.1
  [bac558e1] + OrderedCollections v1.3.0
  [69de0a69] + Parsers v1.0.7
  [dc26d6a1] + PassiveTracerFlows v0.2.0
  [c84ed2f1] + Ratios v0.4.0
  [189a3867] + Reexport v0.2.0
  [ae029012] + Requires v1.0.1
  [90137ffa] + StaticArrays v0.12.4
  [a759f4b9] + TimerOutputs v0.5.6
  [3bb67fe8] + TranscodingStreams v0.9.5
  [81def892] + VersionParsing v1.2.0
  [efce3f68] + WoodburyMatrices v0.5.2
  [83775a58] + Zlib_jll v1.2.11+14
  [2a0f44e3] + Base64
  [ade2ca70] + Dates
  [8ba89e20] + Distributed
  [b77e0a4c] + InteractiveUtils
  [76f85450] + LibGit2
  [8f399da3] + Libdl
  [37e2e46d] + LinearAlgebra
  [56ddb016] + Logging
  [d6f4376e] + Markdown
  [a63ad114] + Mmap
  [44cfe95a] + Pkg
  [de0858da] + Printf
  [3fa0cd96] + REPL
  [9a3f8284] + Random
  [ea8e919c] + SHA
  [9e88b42a] + Serialization
  [1a1011a3] + SharedArrays
  [6462fe0b] + Sockets
  [2f01184e] + SparseArrays
  [10745b16] + Statistics
  [8dfed614] + Test
  [cf7118a7] + UUIDs
  [4ec0a83e] + Unicode
   Building NNlib → `~/.julia/packages/NNlib/FAI3o/deps/build.log`
   Building Conda → `~/.julia/packages/Conda/3rPhK/deps/build.log`
   Building FFTW ─→ `~/.julia/packages/FFTW/loJ3F/deps/build.log`
    Testing PassiveTracerFlows
Status `/tmp/jl_47U9ZX/Project.toml`
  [a2441757] Coverage v1.1.1
  [3a865a2d] CuArrays v1.7.0
  [7a1cc6ca] FFTW v1.1.0
  [2aec4490] FourierFlows v0.4.5
  [033835bb] JLD2 v0.1.14
  [dc26d6a1] PassiveTracerFlows v0.2.0
  [189a3867] Reexport v0.2.0
  [37e2e46d] LinearAlgebra
  [9a3f8284] Random
  [10745b16] Statistics
  [8dfed614] Test
Status `/tmp/jl_47U9ZX/Manifest.toml`
  [621f4979] AbstractFFTs v0.5.0
  [79e6a3ab] Adapt v1.1.0
  [13072b0f] AxisAlgorithms v1.0.0
  [b99e7846] BinaryProvider v0.5.10
  [fa961155] CEnum v0.2.0
  [3895d2a7] CUDAapi v2.1.0
  [c5f51814] CUDAdrv v5.1.0
  [be33ccc6] CUDAnative v2.9.1
  [944b1d66] CodecZlib v0.7.0
  [8f4d0f93] Conda v1.4.1
  [a2441757] Coverage v1.1.1
  [c36e975a] CoverageTools v1.1.0
  [3a865a2d] CuArrays v1.7.0
  [864edb3b] DataStructures v0.17.19
  [7a1cc6ca] FFTW v1.1.0
  [5789e2e9] FileIO v1.3.0
  [2aec4490] FourierFlows v0.4.5
  [0c68f7d7] GPUArrays v2.0.1
  [cd3eb016] HTTP v0.8.16
  [83e8ac13] IniFile v0.5.0
  [a98d9a8b] Interpolations v0.12.10
  [033835bb] JLD2 v0.1.14
  [682c06a0] JSON v0.21.0
  [929cbde3] LLVM v1.7.0
  [1914dd2f] MacroTools v0.5.5
  [739be429] MbedTLS v1.0.2
  [c8ffd9c3] MbedTLS_jll v2.16.6+1
  [872c559c] NNlib v0.6.6
  [6fe1bfb0] OffsetArrays v1.1.1
  [bac558e1] OrderedCollections v1.3.0
  [69de0a69] Parsers v1.0.7
  [dc26d6a1] PassiveTracerFlows v0.2.0
  [c84ed2f1] Ratios v0.4.0
  [189a3867] Reexport v0.2.0
  [ae029012] Requires v1.0.1
  [90137ffa] StaticArrays v0.12.4
  [a759f4b9] TimerOutputs v0.5.6
  [3bb67fe8] TranscodingStreams v0.9.5
  [81def892] VersionParsing v1.2.0
  [efce3f68] WoodburyMatrices v0.5.2
  [83775a58] Zlib_jll v1.2.11+14
  [2a0f44e3] Base64
  [ade2ca70] Dates
  [8ba89e20] Distributed
  [b77e0a4c] InteractiveUtils
  [76f85450] LibGit2
  [8f399da3] Libdl
  [37e2e46d] LinearAlgebra
  [56ddb016] Logging
  [d6f4376e] Markdown
  [a63ad114] Mmap
  [44cfe95a] Pkg
  [de0858da] Printf
  [3fa0cd96] REPL
  [9a3f8284] Random
  [ea8e919c] SHA
  [9e88b42a] Serialization
  [1a1011a3] SharedArrays
  [6462fe0b] Sockets
  [2f01184e] SparseArrays
  [10745b16] Statistics
  [8dfed614] Test
  [cf7118a7] UUIDs
  [4ec0a83e] Unicode
┌ Warning: `haskey(::TargetIterator, name::String)` is deprecated, use `Target(; name = name) !== nothing` instead.
│   caller = llvm_support(version::VersionNumber) at CUDAnative.jl:163
└ @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/CUDAnative.jl:163
┌ Warning: `haskey(::TargetIterator, name::String)` is deprecated, use `Target(; name = name) !== nothing` instead.
│   caller = llvm_support(version::VersionNumber) at CUDAnative.jl:163
└ @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/CUDAnative.jl:163
┌ Warning: `haskey(::TargetIterator, name::String)` is deprecated, use `Target(; name = name) !== nothing` instead.
│   caller = llvm_support(version::VersionNumber) at CUDAnative.jl:163
└ @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/CUDAnative.jl:163
┌ Warning: Incompatibility detected between CUDA and LLVM 8.0+; disabling debug info emission for CUDA kernels
└ @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/CUDAnative.jl:88
┌ Warning: `haskey(::TargetIterator, name::String)` is deprecated, use `Target(; name = name) !== nothing` instead.
│   caller = llvm_support(version::VersionNumber) at CUDAnative.jl:163
└ @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/CUDAnative.jl:163
testing on CPU
Test Summary: | Pass  Total
TracerAdvDiff |    6      6
testing on GPU
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
TracerAdvDiff: Error During Test at /home/pkgeval/.julia/packages/PassiveTracerFlows/Lpy5T/test/runtests.jl:34
  Test threw exception
  Expression: test_constvel(stepper, dt, nsteps, dev)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#93"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#94"{CUDAnative.CompilerJob,DataStructures.MultiDict{Core.MethodInstance,LLVM.Function},CUDAnative.var"#postprocess#92"}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(#undef), Core.Box(#undef), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Vector{LLVM.Function}}()), Core.Box(nothing), CUDAnative.var"#postprocess#92"()), emit_function=CUDAnative.var"#hook_emit_function#97"{CUDAnative.CompilerJob,Vector{Core.MethodInstance}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#98"{CUDAnative.CompilerJob,Vector{Core.MethodInstance}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[], Core.Box(nothing)), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function, lookup, generic_context) at reflection.jl:1011 got unsupported keyword argument "cached"
  Stacktrace:
    [1] kwerr(kw::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#93",CUDAnative.var"#hook_module_activation#94"{CUDAnative.CompilerJob,DataStructures.MultiDict{Core.MethodInstance,LLVM.Function},CUDAnative.var"#postprocess#92"},CUDAnative.var"#hook_emit_function#97"{CUDAnative.CompilerJob,Vector{Core.MethodInstance}},CUDAnative.var"#hook_emitted_function#98"{CUDAnative.CompilerJob,Vector{Core.MethodInstance}},Bool,Int32}}, args::Type)
      @ Base ./error.jl:157
    [2] compile_method_instance(job::CUDAnative.CompilerJob, method_instance::Core.MethodInstance, world::UInt64)
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/compiler/irgen.jl:148
    [3] macro expansion
      @ ~/.julia/packages/TimerOutputs/dVnaw/src/TimerOutput.jl:206 [inlined]
    [4] irgen(job::CUDAnative.CompilerJob, method_instance::Core.MethodInstance, world::UInt64)
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/compiler/irgen.jl:165
    [5] macro expansion
      @ ~/.julia/packages/TimerOutputs/dVnaw/src/TimerOutput.jl:206 [inlined]
    [6] macro expansion
      @ ~/.julia/packages/CUDAnative/JfXpo/src/compiler/driver.jl:104 [inlined]
    [7] macro expansion
      @ ~/.julia/packages/TimerOutputs/dVnaw/src/TimerOutput.jl:206 [inlined]
    [8] codegen(target::Symbol, job::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool)
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/compiler/driver.jl:103
    [9] emit_function!(mod::LLVM.Module, cap::VersionNumber, f::Function, types::Tuple{DataType}, name::String)
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/compiler/rtlib.jl:144
   [10] build_runtime(cap::VersionNumber)
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#139#142"{VersionNumber,String})()
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/compiler/rtlib.jl:189
   [12] get!(default::CUDAnative.var"#139#142"{VersionNumber,String}, h::Dict{String,LLVM.Module}, key::String)
      @ Base ./dict.jl:451
   [13] load_runtime(cap::VersionNumber)
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/compiler/rtlib.jl:182
   [14] codegen(target::Symbol, job::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool)
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/compiler/driver.jl:99
   [15] compile(target::Symbol, job::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool)
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/compiler/driver.jl:52
   [16] #compile#150
      @ ~/.julia/packages/CUDAnative/JfXpo/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion
      @ ~/.julia/packages/CUDAnative/JfXpo/src/execution.jl:393 [inlined]
   [18] cufunction(f::GPUArrays.var"#25#26", tt::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceMatrix{Float64,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(+),Tuple{Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{CUDAnative.CuRefValue{typeof(^)},Base.Broadcast.Extruded{CUDAnative.CuDeviceMatrix{Float64,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},CUDAnative.CuRefValue{Val{2}}}},Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{CUDAnative.CuRefValue{typeof(^)},Base.Broadcast.Extruded{CUDAnative.CuDeviceMatrix{Float64,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},CUDAnative.CuRefValue{Val{2}}}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}})
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/execution.jl:359
   [19] cufunction(f::Function, tt::Type)
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/execution.jl:359
   [20] macro expansion
      @ ~/.julia/packages/CUDAnative/JfXpo/src/execution.jl:179 [inlined]
   [21] _gpu_call(#unused#::CuArrays.CuArrayBackend, f::Function, A::CuMatrix{Float64,Nothing}, args::Tuple{CuMatrix{Float64,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(+),Tuple{Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{Base.RefValue{typeof(^)},Base.Broadcast.Extruded{CuMatrix{Float64,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.RefValue{Val{2}}}},Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{Base.RefValue{typeof(^)},Base.Broadcast.Extruded{CuMatrix{Float64,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.RefValue{Val{2}}}}}}}, blocks_threads::Tuple{Tuple{Int64},Tuple{Int64}})
      @ CuArrays ~/.julia/packages/CuArrays/1njKF/src/gpuarray_interface.jl:62
   [22] gpu_call
      @ ~/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [23] gpu_call(kernel::Function, A::CuMatrix{Float64,Nothing}, args::Tuple{CuMatrix{Float64,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(+),Tuple{Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{Base.RefValue{typeof(^)},Base.Broadcast.Extruded{CuMatrix{Float64,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.RefValue{Val{2}}}},Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{Base.RefValue{typeof(^)},Base.Broadcast.Extruded{CuMatrix{Float64,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.RefValue{Val{2}}}}}}})
      @ GPUArrays ~/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [24] copyto!
      @ ~/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto!
      @ ./broadcast.jl:886 [inlined]
   [26] copy(bc::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(+),Tuple{Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{Base.RefValue{typeof(^)},CuMatrix{Float64,Nothing},Base.RefValue{Val{2}}}},Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{Base.RefValue{typeof(^)},CuMatrix{Float64,Nothing},Base.RefValue{Val{2}}}}}})
      @ Base.Broadcast ./broadcast.jl:862
   [27] materialize(bc::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(+),Tuple{Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{Base.RefValue{typeof(^)},CuMatrix{Float64,Nothing},Base.RefValue{Val{2}}}},Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{Base.RefValue{typeof(^)},CuMatrix{Float64,Nothing},Base.RefValue{Val{2}}}}}})
      @ Base.Broadcast ./broadcast.jl:837
   [28] TwoDGrid(nx::Int64, Lx::Float64, ny::Int64, Ly::Float64; x0::Float64, y0::Float64, nthreads::Int64, effort::UInt32, T::Type, dealias::Float64, ArrayType::Type)
      @ FourierFlows ~/.julia/packages/FourierFlows/Xyzjv/src/domains.jl:148
   [29] #TwoDGrid#68
      @ ~/.julia/packages/FourierFlows/Xyzjv/src/CuFourierFlows.jl:8 [inlined]
   [30] Problem(; nx::Int64, Lx::Float64, ny::Int64, Ly::Float64, kap::Float64, eta::Float64, u::Function, v::Function, dt::Float64, stepper::String, steadyflow::Bool, T::Type, dev::GPU)
      @ PassiveTracerFlows.TracerAdvDiff ~/.julia/packages/PassiveTracerFlows/Lpy5T/src/traceradvdiff.jl:43
   [31] test_constvel(stepper::String, dt::Float64, nsteps::Int64, dev::GPU)
      @ Main ~/.julia/packages/PassiveTracerFlows/Lpy5T/test/test_traceradvdiff.jl:15
   [32] macro expansion
      @ ~/.julia/packages/PassiveTracerFlows/Lpy5T/test/runtests.jl:34 [inlined]
   [33] macro expansion
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114 [inlined]
   [34] macro expansion
      @ ~/.julia/packages/PassiveTracerFlows/Lpy5T/test/runtests.jl:28 [inlined]
   [35] top-level scope
      @ ./timing.jl:233
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
TracerAdvDiff: Error During Test at /home/pkgeval/.julia/packages/PassiveTracerFlows/Lpy5T/test/runtests.jl:36
  Test threw exception
  Expression: test_timedependentvel(stepper, dt, tfinal, dev)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#93"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#94"{CUDAnative.CompilerJob,DataStructures.MultiDict{Core.MethodInstance,LLVM.Function},CUDAnative.var"#postprocess#92"}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(#undef), Core.Box(#undef), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Vector{LLVM.Function}}()), Core.Box(nothing), CUDAnative.var"#postprocess#92"()), emit_function=CUDAnative.var"#hook_emit_function#97"{CUDAnative.CompilerJob,Vector{Core.MethodInstance}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#98"{CUDAnative.CompilerJob,Vector{Core.MethodInstance}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[], Core.Box(nothing)), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function, lookup, generic_context) at reflection.jl:1011 got unsupported keyword argument "cached"
  Stacktrace:
    [1] kwerr(kw::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#93",CUDAnative.var"#hook_module_activation#94"{CUDAnative.CompilerJob,DataStructures.MultiDict{Core.MethodInstance,LLVM.Function},CUDAnative.var"#postprocess#92"},CUDAnative.var"#hook_emit_function#97"{CUDAnative.CompilerJob,Vector{Core.MethodInstance}},CUDAnative.var"#hook_emitted_function#98"{CUDAnative.CompilerJob,Vector{Core.MethodInstance}},Bool,Int32}}, args::Type)
      @ Base ./error.jl:157
    [2] compile_method_instance(job::CUDAnative.CompilerJob, method_instance::Core.MethodInstance, world::UInt64)
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/compiler/irgen.jl:148
    [3] macro expansion
      @ ~/.julia/packages/TimerOutputs/dVnaw/src/TimerOutput.jl:206 [inlined]
    [4] irgen(job::CUDAnative.CompilerJob, method_instance::Core.MethodInstance, world::UInt64)
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/compiler/irgen.jl:165
    [5] macro expansion
      @ ~/.julia/packages/TimerOutputs/dVnaw/src/TimerOutput.jl:206 [inlined]
    [6] macro expansion
      @ ~/.julia/packages/CUDAnative/JfXpo/src/compiler/driver.jl:104 [inlined]
    [7] macro expansion
      @ ~/.julia/packages/TimerOutputs/dVnaw/src/TimerOutput.jl:206 [inlined]
    [8] codegen(target::Symbol, job::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool)
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/compiler/driver.jl:103
    [9] emit_function!(mod::LLVM.Module, cap::VersionNumber, f::Function, types::Tuple{DataType}, name::String)
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/compiler/rtlib.jl:144
   [10] build_runtime(cap::VersionNumber)
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#139#142"{VersionNumber,String})()
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/compiler/rtlib.jl:189
   [12] get!(default::CUDAnative.var"#139#142"{VersionNumber,String}, h::Dict{String,LLVM.Module}, key::String)
      @ Base ./dict.jl:451
   [13] load_runtime(cap::VersionNumber)
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/compiler/rtlib.jl:182
   [14] codegen(target::Symbol, job::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool)
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/compiler/driver.jl:99
   [15] compile(target::Symbol, job::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool)
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/compiler/driver.jl:52
   [16] #compile#150
      @ ~/.julia/packages/CUDAnative/JfXpo/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion
      @ ~/.julia/packages/CUDAnative/JfXpo/src/execution.jl:393 [inlined]
   [18] cufunction(f::GPUArrays.var"#25#26", tt::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceMatrix{Float64,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(+),Tuple{Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{CUDAnative.CuRefValue{typeof(^)},Base.Broadcast.Extruded{CUDAnative.CuDeviceMatrix{Float64,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},CUDAnative.CuRefValue{Val{2}}}},Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{CUDAnative.CuRefValue{typeof(^)},Base.Broadcast.Extruded{CUDAnative.CuDeviceMatrix{Float64,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},CUDAnative.CuRefValue{Val{2}}}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}})
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/execution.jl:359
   [19] cufunction(f::Function, tt::Type)
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/execution.jl:359
   [20] macro expansion
      @ ~/.julia/packages/CUDAnative/JfXpo/src/execution.jl:179 [inlined]
   [21] _gpu_call(#unused#::CuArrays.CuArrayBackend, f::Function, A::CuMatrix{Float64,Nothing}, args::Tuple{CuMatrix{Float64,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(+),Tuple{Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{Base.RefValue{typeof(^)},Base.Broadcast.Extruded{CuMatrix{Float64,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.RefValue{Val{2}}}},Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{Base.RefValue{typeof(^)},Base.Broadcast.Extruded{CuMatrix{Float64,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.RefValue{Val{2}}}}}}}, blocks_threads::Tuple{Tuple{Int64},Tuple{Int64}})
      @ CuArrays ~/.julia/packages/CuArrays/1njKF/src/gpuarray_interface.jl:62
   [22] gpu_call
      @ ~/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [23] gpu_call(kernel::Function, A::CuMatrix{Float64,Nothing}, args::Tuple{CuMatrix{Float64,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(+),Tuple{Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{Base.RefValue{typeof(^)},Base.Broadcast.Extruded{CuMatrix{Float64,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.RefValue{Val{2}}}},Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{Base.RefValue{typeof(^)},Base.Broadcast.Extruded{CuMatrix{Float64,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.RefValue{Val{2}}}}}}})
      @ GPUArrays ~/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [24] copyto!
      @ ~/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto!
      @ ./broadcast.jl:886 [inlined]
   [26] copy(bc::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(+),Tuple{Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{Base.RefValue{typeof(^)},CuMatrix{Float64,Nothing},Base.RefValue{Val{2}}}},Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{Base.RefValue{typeof(^)},CuMatrix{Float64,Nothing},Base.RefValue{Val{2}}}}}})
      @ Base.Broadcast ./broadcast.jl:862
   [27] materialize(bc::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(+),Tuple{Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{Base.RefValue{typeof(^)},CuMatrix{Float64,Nothing},Base.RefValue{Val{2}}}},Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{Base.RefValue{typeof(^)},CuMatrix{Float64,Nothing},Base.RefValue{Val{2}}}}}})
      @ Base.Broadcast ./broadcast.jl:837
   [28] TwoDGrid(nx::Int64, Lx::Float64, ny::Int64, Ly::Float64; x0::Float64, y0::Float64, nthreads::Int64, effort::UInt32, T::Type, dealias::Float64, ArrayType::Type)
      @ FourierFlows ~/.julia/packages/FourierFlows/Xyzjv/src/domains.jl:148
   [29] #TwoDGrid#68
      @ ~/.julia/packages/FourierFlows/Xyzjv/src/CuFourierFlows.jl:8 [inlined]
   [30] Problem(; nx::Int64, Lx::Float64, ny::Int64, Ly::Float64, kap::Float64, eta::Float64, u::Function, v::Function, dt::Float64, stepper::String, steadyflow::Bool, T::Type, dev::GPU)
      @ PassiveTracerFlows.TracerAdvDiff ~/.julia/packages/PassiveTracerFlows/Lpy5T/src/traceradvdiff.jl:43
   [31] test_timedependentvel(stepper::String, dt::Float64, tfinal::Float64, dev::GPU; uvel::Float64, αv::Float64)
      @ Main ~/.julia/packages/PassiveTracerFlows/Lpy5T/test/test_traceradvdiff.jl:55
   [32] test_timedependentvel(stepper::String, dt::Float64, tfinal::Float64, dev::GPU)
      @ Main ~/.julia/packages/PassiveTracerFlows/Lpy5T/test/test_traceradvdiff.jl:45
   [33] macro expansion
      @ ~/.julia/packages/PassiveTracerFlows/Lpy5T/test/runtests.jl:36 [inlined]
   [34] macro expansion
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114 [inlined]
   [35] macro expansion
      @ ~/.julia/packages/PassiveTracerFlows/Lpy5T/test/runtests.jl:28 [inlined]
   [36] top-level scope
      @ ./timing.jl:233
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
TracerAdvDiff: Error During Test at /home/pkgeval/.julia/packages/PassiveTracerFlows/Lpy5T/test/runtests.jl:38
  Test threw exception
  Expression: test_diffusion(stepper, dt, tfinal, dev; steadyflow = true)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#93"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#94"{CUDAnative.CompilerJob,DataStructures.MultiDict{Core.MethodInstance,LLVM.Function},CUDAnative.var"#postprocess#92"}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(#undef), Core.Box(#undef), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Vector{LLVM.Function}}()), Core.Box(nothing), CUDAnative.var"#postprocess#92"()), emit_function=CUDAnative.var"#hook_emit_function#97"{CUDAnative.CompilerJob,Vector{Core.MethodInstance}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#98"{CUDAnative.CompilerJob,Vector{Core.MethodInstance}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[], Core.Box(nothing)), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function, lookup, generic_context) at reflection.jl:1011 got unsupported keyword argument "cached"
  Stacktrace:
    [1] kwerr(kw::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#93",CUDAnative.var"#hook_module_activation#94"{CUDAnative.CompilerJob,DataStructures.MultiDict{Core.MethodInstance,LLVM.Function},CUDAnative.var"#postprocess#92"},CUDAnative.var"#hook_emit_function#97"{CUDAnative.CompilerJob,Vector{Core.MethodInstance}},CUDAnative.var"#hook_emitted_function#98"{CUDAnative.CompilerJob,Vector{Core.MethodInstance}},Bool,Int32}}, args::Type)
      @ Base ./error.jl:157
    [2] compile_method_instance(job::CUDAnative.CompilerJob, method_instance::Core.MethodInstance, world::UInt64)
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/compiler/irgen.jl:148
    [3] macro expansion
      @ ~/.julia/packages/TimerOutputs/dVnaw/src/TimerOutput.jl:206 [inlined]
    [4] irgen(job::CUDAnative.CompilerJob, method_instance::Core.MethodInstance, world::UInt64)
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/compiler/irgen.jl:165
    [5] macro expansion
      @ ~/.julia/packages/TimerOutputs/dVnaw/src/TimerOutput.jl:206 [inlined]
    [6] macro expansion
      @ ~/.julia/packages/CUDAnative/JfXpo/src/compiler/driver.jl:104 [inlined]
    [7] macro expansion
      @ ~/.julia/packages/TimerOutputs/dVnaw/src/TimerOutput.jl:206 [inlined]
    [8] codegen(target::Symbol, job::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool)
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/compiler/driver.jl:103
    [9] emit_function!(mod::LLVM.Module, cap::VersionNumber, f::Function, types::Tuple{DataType}, name::String)
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/compiler/rtlib.jl:144
   [10] build_runtime(cap::VersionNumber)
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#139#142"{VersionNumber,String})()
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/compiler/rtlib.jl:189
   [12] get!(default::CUDAnative.var"#139#142"{VersionNumber,String}, h::Dict{String,LLVM.Module}, key::String)
      @ Base ./dict.jl:451
   [13] load_runtime(cap::VersionNumber)
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/compiler/rtlib.jl:182
   [14] codegen(target::Symbol, job::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool)
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/compiler/driver.jl:99
   [15] compile(target::Symbol, job::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool)
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/compiler/driver.jl:52
   [16] #compile#150
      @ ~/.julia/packages/CUDAnative/JfXpo/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion
      @ ~/.julia/packages/CUDAnative/JfXpo/src/execution.jl:393 [inlined]
   [18] cufunction(f::GPUArrays.var"#25#26", tt::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceMatrix{Float64,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(+),Tuple{Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{CUDAnative.CuRefValue{typeof(^)},Base.Broadcast.Extruded{CUDAnative.CuDeviceMatrix{Float64,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},CUDAnative.CuRefValue{Val{2}}}},Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{CUDAnative.CuRefValue{typeof(^)},Base.Broadcast.Extruded{CUDAnative.CuDeviceMatrix{Float64,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},CUDAnative.CuRefValue{Val{2}}}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}})
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/execution.jl:359
   [19] cufunction(f::Function, tt::Type)
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/execution.jl:359
   [20] macro expansion
      @ ~/.julia/packages/CUDAnative/JfXpo/src/execution.jl:179 [inlined]
   [21] _gpu_call(#unused#::CuArrays.CuArrayBackend, f::Function, A::CuMatrix{Float64,Nothing}, args::Tuple{CuMatrix{Float64,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(+),Tuple{Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{Base.RefValue{typeof(^)},Base.Broadcast.Extruded{CuMatrix{Float64,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.RefValue{Val{2}}}},Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{Base.RefValue{typeof(^)},Base.Broadcast.Extruded{CuMatrix{Float64,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.RefValue{Val{2}}}}}}}, blocks_threads::Tuple{Tuple{Int64},Tuple{Int64}})
      @ CuArrays ~/.julia/packages/CuArrays/1njKF/src/gpuarray_interface.jl:62
   [22] gpu_call
      @ ~/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [23] gpu_call(kernel::Function, A::CuMatrix{Float64,Nothing}, args::Tuple{CuMatrix{Float64,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(+),Tuple{Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{Base.RefValue{typeof(^)},Base.Broadcast.Extruded{CuMatrix{Float64,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.RefValue{Val{2}}}},Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{Base.RefValue{typeof(^)},Base.Broadcast.Extruded{CuMatrix{Float64,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.RefValue{Val{2}}}}}}})
      @ GPUArrays ~/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [24] copyto!
      @ ~/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto!
      @ ./broadcast.jl:886 [inlined]
   [26] copy(bc::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(+),Tuple{Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{Base.RefValue{typeof(^)},CuMatrix{Float64,Nothing},Base.RefValue{Val{2}}}},Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{Base.RefValue{typeof(^)},CuMatrix{Float64,Nothing},Base.RefValue{Val{2}}}}}})
      @ Base.Broadcast ./broadcast.jl:862
   [27] materialize(bc::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(+),Tuple{Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{Base.RefValue{typeof(^)},CuMatrix{Float64,Nothing},Base.RefValue{Val{2}}}},Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{Base.RefValue{typeof(^)},CuMatrix{Float64,Nothing},Base.RefValue{Val{2}}}}}})
      @ Base.Broadcast ./broadcast.jl:837
   [28] TwoDGrid(nx::Int64, Lx::Float64, ny::Int64, Ly::Float64; x0::Float64, y0::Float64, nthreads::Int64, effort::UInt32, T::Type, dealias::Float64, ArrayType::Type)
      @ FourierFlows ~/.julia/packages/FourierFlows/Xyzjv/src/domains.jl:148
   [29] #TwoDGrid#68
      @ ~/.julia/packages/FourierFlows/Xyzjv/src/CuFourierFlows.jl:8 [inlined]
   [30] Problem(; nx::Int64, Lx::Float64, ny::Int64, Ly::Float64, kap::Float64, eta::Float64, u::Function, v::Function, dt::Float64, stepper::String, steadyflow::Bool, T::Type, dev::GPU)
      @ PassiveTracerFlows.TracerAdvDiff ~/.julia/packages/PassiveTracerFlows/Lpy5T/src/traceradvdiff.jl:43
   [31] test_diffusion(stepper::String, dt::Float64, tfinal::Float64, dev::GPU; steadyflow::Bool)
      @ Main ~/.julia/packages/PassiveTracerFlows/Lpy5T/test/test_traceradvdiff.jl:91
   [32] macro expansion
      @ ~/.julia/packages/PassiveTracerFlows/Lpy5T/test/runtests.jl:38 [inlined]
   [33] macro expansion
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114 [inlined]
   [34] macro expansion
      @ ~/.julia/packages/PassiveTracerFlows/Lpy5T/test/runtests.jl:28 [inlined]
   [35] top-level scope
      @ ./timing.jl:233
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
TracerAdvDiff: Error During Test at /home/pkgeval/.julia/packages/PassiveTracerFlows/Lpy5T/test/runtests.jl:40
  Test threw exception
  Expression: test_diffusion(stepper, dt, tfinal, dev; steadyflow = false)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#93"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#94"{CUDAnative.CompilerJob,DataStructures.MultiDict{Core.MethodInstance,LLVM.Function},CUDAnative.var"#postprocess#92"}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(#undef), Core.Box(#undef), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Vector{LLVM.Function}}()), Core.Box(nothing), CUDAnative.var"#postprocess#92"()), emit_function=CUDAnative.var"#hook_emit_function#97"{CUDAnative.CompilerJob,Vector{Core.MethodInstance}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#98"{CUDAnative.CompilerJob,Vector{Core.MethodInstance}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[], Core.Box(nothing)), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function, lookup, generic_context) at reflection.jl:1011 got unsupported keyword argument "cached"
  Stacktrace:
    [1] kwerr(kw::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#93",CUDAnative.var"#hook_module_activation#94"{CUDAnative.CompilerJob,DataStructures.MultiDict{Core.MethodInstance,LLVM.Function},CUDAnative.var"#postprocess#92"},CUDAnative.var"#hook_emit_function#97"{CUDAnative.CompilerJob,Vector{Core.MethodInstance}},CUDAnative.var"#hook_emitted_function#98"{CUDAnative.CompilerJob,Vector{Core.MethodInstance}},Bool,Int32}}, args::Type)
      @ Base ./error.jl:157
    [2] compile_method_instance(job::CUDAnative.CompilerJob, method_instance::Core.MethodInstance, world::UInt64)
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/compiler/irgen.jl:148
    [3] macro expansion
      @ ~/.julia/packages/TimerOutputs/dVnaw/src/TimerOutput.jl:206 [inlined]
    [4] irgen(job::CUDAnative.CompilerJob, method_instance::Core.MethodInstance, world::UInt64)
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/compiler/irgen.jl:165
    [5] macro expansion
      @ ~/.julia/packages/TimerOutputs/dVnaw/src/TimerOutput.jl:206 [inlined]
    [6] macro expansion
      @ ~/.julia/packages/CUDAnative/JfXpo/src/compiler/driver.jl:104 [inlined]
    [7] macro expansion
      @ ~/.julia/packages/TimerOutputs/dVnaw/src/TimerOutput.jl:206 [inlined]
    [8] codegen(target::Symbol, job::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool)
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/compiler/driver.jl:103
    [9] emit_function!(mod::LLVM.Module, cap::VersionNumber, f::Function, types::Tuple{DataType}, name::String)
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/compiler/rtlib.jl:144
   [10] build_runtime(cap::VersionNumber)
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#139#142"{VersionNumber,String})()
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/compiler/rtlib.jl:189
   [12] get!(default::CUDAnative.var"#139#142"{VersionNumber,String}, h::Dict{String,LLVM.Module}, key::String)
      @ Base ./dict.jl:451
   [13] load_runtime(cap::VersionNumber)
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/compiler/rtlib.jl:182
   [14] codegen(target::Symbol, job::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool)
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/compiler/driver.jl:99
   [15] compile(target::Symbol, job::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool)
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/compiler/driver.jl:52
   [16] #compile#150
      @ ~/.julia/packages/CUDAnative/JfXpo/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion
      @ ~/.julia/packages/CUDAnative/JfXpo/src/execution.jl:393 [inlined]
   [18] cufunction(f::GPUArrays.var"#25#26", tt::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceMatrix{Float64,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(+),Tuple{Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{CUDAnative.CuRefValue{typeof(^)},Base.Broadcast.Extruded{CUDAnative.CuDeviceMatrix{Float64,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},CUDAnative.CuRefValue{Val{2}}}},Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{CUDAnative.CuRefValue{typeof(^)},Base.Broadcast.Extruded{CUDAnative.CuDeviceMatrix{Float64,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},CUDAnative.CuRefValue{Val{2}}}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}})
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/execution.jl:359
   [19] cufunction(f::Function, tt::Type)
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/execution.jl:359
   [20] macro expansion
      @ ~/.julia/packages/CUDAnative/JfXpo/src/execution.jl:179 [inlined]
   [21] _gpu_call(#unused#::CuArrays.CuArrayBackend, f::Function, A::CuMatrix{Float64,Nothing}, args::Tuple{CuMatrix{Float64,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(+),Tuple{Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{Base.RefValue{typeof(^)},Base.Broadcast.Extruded{CuMatrix{Float64,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.RefValue{Val{2}}}},Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{Base.RefValue{typeof(^)},Base.Broadcast.Extruded{CuMatrix{Float64,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.RefValue{Val{2}}}}}}}, blocks_threads::Tuple{Tuple{Int64},Tuple{Int64}})
      @ CuArrays ~/.julia/packages/CuArrays/1njKF/src/gpuarray_interface.jl:62
   [22] gpu_call
      @ ~/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [23] gpu_call(kernel::Function, A::CuMatrix{Float64,Nothing}, args::Tuple{CuMatrix{Float64,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(+),Tuple{Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{Base.RefValue{typeof(^)},Base.Broadcast.Extruded{CuMatrix{Float64,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.RefValue{Val{2}}}},Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{Base.RefValue{typeof(^)},Base.Broadcast.Extruded{CuMatrix{Float64,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.RefValue{Val{2}}}}}}})
      @ GPUArrays ~/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [24] copyto!
      @ ~/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto!
      @ ./broadcast.jl:886 [inlined]
   [26] copy(bc::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(+),Tuple{Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{Base.RefValue{typeof(^)},CuMatrix{Float64,Nothing},Base.RefValue{Val{2}}}},Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{Base.RefValue{typeof(^)},CuMatrix{Float64,Nothing},Base.RefValue{Val{2}}}}}})
      @ Base.Broadcast ./broadcast.jl:862
   [27] materialize(bc::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(+),Tuple{Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{Base.RefValue{typeof(^)},CuMatrix{Float64,Nothing},Base.RefValue{Val{2}}}},Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{Base.RefValue{typeof(^)},CuMatrix{Float64,Nothing},Base.RefValue{Val{2}}}}}})
      @ Base.Broadcast ./broadcast.jl:837
   [28] TwoDGrid(nx::Int64, Lx::Float64, ny::Int64, Ly::Float64; x0::Float64, y0::Float64, nthreads::Int64, effort::UInt32, T::Type, dealias::Float64, ArrayType::Type)
      @ FourierFlows ~/.julia/packages/FourierFlows/Xyzjv/src/domains.jl:148
   [29] #TwoDGrid#68
      @ ~/.julia/packages/FourierFlows/Xyzjv/src/CuFourierFlows.jl:8 [inlined]
   [30] Problem(; nx::Int64, Lx::Float64, ny::Int64, Ly::Float64, kap::Float64, eta::Float64, u::Function, v::Function, dt::Float64, stepper::String, steadyflow::Bool, T::Type, dev::GPU)
      @ PassiveTracerFlows.TracerAdvDiff ~/.julia/packages/PassiveTracerFlows/Lpy5T/src/traceradvdiff.jl:43
   [31] test_diffusion(stepper::String, dt::Float64, tfinal::Float64, dev::GPU; steadyflow::Bool)
      @ Main ~/.julia/packages/PassiveTracerFlows/Lpy5T/test/test_traceradvdiff.jl:91
   [32] macro expansion
      @ ~/.julia/packages/PassiveTracerFlows/Lpy5T/test/runtests.jl:40 [inlined]
   [33] macro expansion
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114 [inlined]
   [34] macro expansion
      @ ~/.julia/packages/PassiveTracerFlows/Lpy5T/test/runtests.jl:28 [inlined]
   [35] top-level scope
      @ ./timing.jl:233
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
TracerAdvDiff: Error During Test at /home/pkgeval/.julia/packages/PassiveTracerFlows/Lpy5T/test/runtests.jl:42
  Test threw exception
  Expression: test_hyperdiffusion(stepper, dt, tfinal, dev)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#93"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#94"{CUDAnative.CompilerJob,DataStructures.MultiDict{Core.MethodInstance,LLVM.Function},CUDAnative.var"#postprocess#92"}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(#undef), Core.Box(#undef), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Vector{LLVM.Function}}()), Core.Box(nothing), CUDAnative.var"#postprocess#92"()), emit_function=CUDAnative.var"#hook_emit_function#97"{CUDAnative.CompilerJob,Vector{Core.MethodInstance}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#98"{CUDAnative.CompilerJob,Vector{Core.MethodInstance}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[], Core.Box(nothing)), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function, lookup, generic_context) at reflection.jl:1011 got unsupported keyword argument "cached"
  Stacktrace:
    [1] kwerr(kw::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#93",CUDAnative.var"#hook_module_activation#94"{CUDAnative.CompilerJob,DataStructures.MultiDict{Core.MethodInstance,LLVM.Function},CUDAnative.var"#postprocess#92"},CUDAnative.var"#hook_emit_function#97"{CUDAnative.CompilerJob,Vector{Core.MethodInstance}},CUDAnative.var"#hook_emitted_function#98"{CUDAnative.CompilerJob,Vector{Core.MethodInstance}},Bool,Int32}}, args::Type)
      @ Base ./error.jl:157
    [2] compile_method_instance(job::CUDAnative.CompilerJob, method_instance::Core.MethodInstance, world::UInt64)
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/compiler/irgen.jl:148
    [3] macro expansion
      @ ~/.julia/packages/TimerOutputs/dVnaw/src/TimerOutput.jl:206 [inlined]
    [4] irgen(job::CUDAnative.CompilerJob, method_instance::Core.MethodInstance, world::UInt64)
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/compiler/irgen.jl:165
    [5] macro expansion
      @ ~/.julia/packages/TimerOutputs/dVnaw/src/TimerOutput.jl:206 [inlined]
    [6] macro expansion
      @ ~/.julia/packages/CUDAnative/JfXpo/src/compiler/driver.jl:104 [inlined]
    [7] macro expansion
      @ ~/.julia/packages/TimerOutputs/dVnaw/src/TimerOutput.jl:206 [inlined]
    [8] codegen(target::Symbol, job::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool)
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/compiler/driver.jl:103
    [9] emit_function!(mod::LLVM.Module, cap::VersionNumber, f::Function, types::Tuple{DataType}, name::String)
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/compiler/rtlib.jl:144
   [10] build_runtime(cap::VersionNumber)
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#139#142"{VersionNumber,String})()
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/compiler/rtlib.jl:189
   [12] get!(default::CUDAnative.var"#139#142"{VersionNumber,String}, h::Dict{String,LLVM.Module}, key::String)
      @ Base ./dict.jl:451
   [13] load_runtime(cap::VersionNumber)
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/compiler/rtlib.jl:182
   [14] codegen(target::Symbol, job::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool)
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/compiler/driver.jl:99
   [15] compile(target::Symbol, job::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool)
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/compiler/driver.jl:52
   [16] #compile#150
      @ ~/.julia/packages/CUDAnative/JfXpo/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion
      @ ~/.julia/packages/CUDAnative/JfXpo/src/execution.jl:393 [inlined]
   [18] cufunction(f::GPUArrays.var"#25#26", tt::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceMatrix{Float64,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(+),Tuple{Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{CUDAnative.CuRefValue{typeof(^)},Base.Broadcast.Extruded{CUDAnative.CuDeviceMatrix{Float64,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},CUDAnative.CuRefValue{Val{2}}}},Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{CUDAnative.CuRefValue{typeof(^)},Base.Broadcast.Extruded{CUDAnative.CuDeviceMatrix{Float64,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},CUDAnative.CuRefValue{Val{2}}}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}})
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/execution.jl:359
   [19] cufunction(f::Function, tt::Type)
      @ CUDAnative ~/.julia/packages/CUDAnative/JfXpo/src/execution.jl:359
   [20] macro expansion
      @ ~/.julia/packages/CUDAnative/JfXpo/src/execution.jl:179 [inlined]
   [21] _gpu_call(#unused#::CuArrays.CuArrayBackend, f::Function, A::CuMatrix{Float64,Nothing}, args::Tuple{CuMatrix{Float64,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(+),Tuple{Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{Base.RefValue{typeof(^)},Base.Broadcast.Extruded{CuMatrix{Float64,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.RefValue{Val{2}}}},Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{Base.RefValue{typeof(^)},Base.Broadcast.Extruded{CuMatrix{Float64,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.RefValue{Val{2}}}}}}}, blocks_threads::Tuple{Tuple{Int64},Tuple{Int64}})
      @ CuArrays ~/.julia/packages/CuArrays/1njKF/src/gpuarray_interface.jl:62
   [22] gpu_call
      @ ~/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [23] gpu_call(kernel::Function, A::CuMatrix{Float64,Nothing}, args::Tuple{CuMatrix{Float64,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(+),Tuple{Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{Base.RefValue{typeof(^)},Base.Broadcast.Extruded{CuMatrix{Float64,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.RefValue{Val{2}}}},Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{Base.RefValue{typeof(^)},Base.Broadcast.Extruded{CuMatrix{Float64,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.RefValue{Val{2}}}}}}})
      @ GPUArrays ~/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [24] copyto!
      @ ~/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto!
      @ ./broadcast.jl:886 [inlined]
   [26] copy(bc::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(+),Tuple{Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{Base.RefValue{typeof(^)},CuMatrix{Float64,Nothing},Base.RefValue{Val{2}}}},Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{Base.RefValue{typeof(^)},CuMatrix{Float64,Nothing},Base.RefValue{Val{2}}}}}})
      @ Base.Broadcast ./broadcast.jl:862
   [27] materialize(bc::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(+),Tuple{Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{Base.RefValue{typeof(^)},CuMatrix{Float64,Nothing},Base.RefValue{Val{2}}}},Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CuArrays.culiteral_pow),Tuple{Base.RefValue{typeof(^)},CuMatrix{Float64,Nothing},Base.RefValue{Val{2}}}}}})
      @ Base.Broadcast ./broadcast.jl:837
   [28] TwoDGrid(nx::Int64, Lx::Float64, ny::Int64, Ly::Float64; x0::Float64, y0::Float64, nthreads::Int64, effort::UInt32, T::Type, dealias::Float64, ArrayType::Type)
      @ FourierFlows ~/.julia/packages/FourierFlows/Xyzjv/src/domains.jl:148
   [29] #TwoDGrid#68
      @ ~/.julia/packages/FourierFlows/Xyzjv/src/CuFourierFlows.jl:8 [inlined]
   [30] TwoDGrid
      @ ~/.julia/packages/FourierFlows/Xyzjv/src/CuFourierFlows.jl:8 [inlined]
   [31] test_hyperdiffusion(stepper::String, dt::Float64, tfinal::Float64, dev::GPU; steadyflow::Bool)
      @ Main ~/.julia/packages/PassiveTracerFlows/Lpy5T/test/test_traceradvdiff.jl:135
   [32] test_hyperdiffusion(stepper::String, dt::Float64, tfinal::Float64, dev::GPU)
      @ Main ~/.julia/packages/PassiveTracerFlows/Lpy5T/test/test_traceradvdiff.jl:121
   [33] macro expansion
      @ ~/.julia/packages/PassiveTracerFlows/Lpy5T/test/runtests.jl:42 [inlined]
   [34] macro expansion
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114 [inlined]
   [35] macro expansion
      @ ~/.julia/packages/PassiveTracerFlows/Lpy5T/test/runtests.jl:28 [inlined]
   [36] top-level scope
      @ ./timing.jl:233
  
Test Summary: | Pass  Error  Total
TracerAdvDiff |    1      5      6
ERROR: LoadError: Some tests did not pass: 1 passed, 0 failed, 5 errored, 0 broken.
in expression starting at /home/pkgeval/.julia/packages/PassiveTracerFlows/Lpy5T/test/runtests.jl:21
ERROR: Package PassiveTracerFlows errored during testing
Stacktrace:
  [1] pkgerror(::String, ::Vararg{String,N} where N)
    @ Pkg.Types /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Pkg/src/Types.jl:52
  [2] test(ctx::Pkg.Types.Context, pkgs::Vector{Pkg.Types.PackageSpec}; coverage::Bool, julia_args::Cmd, test_args::Cmd, test_fn::Nothing)
    @ Pkg.Operations /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Pkg/src/Operations.jl:1561
  [3] test(ctx::Pkg.Types.Context, pkgs::Vector{Pkg.Types.PackageSpec}; coverage::Bool, test_fn::Nothing, julia_args::Cmd, test_args::Cmd, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}})
    @ Pkg.API /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Pkg/src/API.jl:327
  [4] test(ctx::Pkg.Types.Context, pkgs::Vector{Pkg.Types.PackageSpec})
    @ Pkg.API /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Pkg/src/API.jl:314
  [5] #test#61
    @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Pkg/src/API.jl:67 [inlined]
  [6] test
    @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Pkg/src/API.jl:67 [inlined]
  [7] #test#60
    @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Pkg/src/API.jl:66 [inlined]
  [8] test
    @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Pkg/src/API.jl:66 [inlined]
  [9] test(pkg::String; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}})
    @ Pkg.API /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Pkg/src/API.jl:65
 [10] test(pkg::String)
    @ Pkg.API /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Pkg/src/API.jl:65
 [11] top-level scope
    @ none:16
