Julia Version 1.5.0-DEV.139
Commit c2abaeedf8 (2020-01-22 06:54 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-8.0.1 (ORCJIT, skylake)
Environment:
  JULIA_DEPOT_PATH = ::/usr/local/share/julia
  JULIA_NUM_THREADS = 2

 Resolving package versions...
 Installed GaussianMixtures ─── v0.3.0
 Installed Missings ─────────── v0.4.3
 Installed OpenSpecFun_jll ──── v0.5.3+1
 Installed FillArrays ───────── v0.8.4
 Installed Parameters ───────── v0.12.0
 Installed FileIO ───────────── v1.2.1
 Installed StatsBase ────────── v0.32.0
 Installed Distributions ────── v0.22.3
 Installed StatsFuns ────────── v0.9.3
 Installed CMakeWrapper ─────── v0.2.3
 Installed ScikitLearnBase ──── v0.5.0
 Installed DataStructures ───── v0.17.9
 Installed Arpack_jll ───────── v3.5.0+2
 Installed DataAPI ──────────── v1.1.0
 Installed Arpack ───────────── v0.4.0
 Installed OpenBLAS_jll ─────── v0.3.7+4
 Installed Blosc ────────────── v0.5.1
 Installed Rmath ────────────── v0.6.0
 Installed SortingAlgorithms ── v0.3.1
 Installed SpecialFunctions ─── v0.9.0
 Installed NearestNeighbors ─── v0.4.4
 Installed Compat ───────────── v2.2.0
 Installed JLD ──────────────── v0.9.1
 Installed Distances ────────── v0.8.2
 Installed OrderedCollections ─ v1.1.0
 Installed PDMats ───────────── v0.9.10
 Installed Clustering ───────── v0.13.3
 Installed CMake ────────────── v1.1.2
 Installed LegacyStrings ────── v0.4.1
 Installed URIParser ────────── v0.4.0
 Installed QuadGK ───────────── v2.3.1
 Installed StaticArrays ─────── v0.12.1
 Installed BinDeps ──────────── v1.0.0
 Installed BinaryProvider ───── v0.5.8
 Installed HDF5 ─────────────── v0.12.5
  Updating `~/.julia/environments/v1.5/Project.toml`
  [cc18c42c] + GaussianMixtures v0.3.0
  Updating `~/.julia/environments/v1.5/Manifest.toml`
  [7d9fca2a] + Arpack v0.4.0
  [68821587] + Arpack_jll v3.5.0+2
  [9e28174c] + BinDeps v1.0.0
  [b99e7846] + BinaryProvider v0.5.8
  [a74b3585] + Blosc v0.5.1
  [631607c0] + CMake v1.1.2
  [d5fb7624] + CMakeWrapper v0.2.3
  [aaaa29a8] + Clustering v0.13.3
  [34da2185] + Compat v2.2.0
  [9a962f9c] + DataAPI v1.1.0
  [864edb3b] + DataStructures v0.17.9
  [b4f34e82] + Distances v0.8.2
  [31c24e10] + Distributions v0.22.3
  [5789e2e9] + FileIO v1.2.1
  [1a297f60] + FillArrays v0.8.4
  [cc18c42c] + GaussianMixtures v0.3.0
  [f67ccb44] + HDF5 v0.12.5
  [4138dd39] + JLD v0.9.1
  [1b4a561d] + LegacyStrings v0.4.1
  [e1d29d7a] + Missings v0.4.3
  [b8a86587] + NearestNeighbors v0.4.4
  [4536629a] + OpenBLAS_jll v0.3.7+4
  [efe28fd5] + OpenSpecFun_jll v0.5.3+1
  [bac558e1] + OrderedCollections v1.1.0
  [90014a1f] + PDMats v0.9.10
  [d96e819e] + Parameters v0.12.0
  [1fd47b50] + QuadGK v2.3.1
  [79098fc4] + Rmath v0.6.0
  [6e75b9c4] + ScikitLearnBase v0.5.0
  [a2af1166] + SortingAlgorithms v0.3.1
  [276daf66] + SpecialFunctions v0.9.0
  [90137ffa] + StaticArrays v0.12.1
  [2913bbd2] + StatsBase v0.32.0
  [4c63d2b9] + StatsFuns v0.9.3
  [30578b45] + URIParser v0.4.0
  [2a0f44e3] + Base64 
  [ade2ca70] + Dates 
  [8bb1440f] + DelimitedFiles 
  [8ba89e20] + Distributed 
  [b77e0a4c] + InteractiveUtils 
  [76f85450] + LibGit2 
  [8f399da3] + Libdl 
  [37e2e46d] + LinearAlgebra 
  [56ddb016] + Logging 
  [d6f4376e] + Markdown 
  [a63ad114] + Mmap 
  [44cfe95a] + Pkg 
  [de0858da] + Printf 
  [9abbd945] + Profile 
  [3fa0cd96] + REPL 
  [9a3f8284] + Random 
  [ea8e919c] + SHA 
  [9e88b42a] + Serialization 
  [1a1011a3] + SharedArrays 
  [6462fe0b] + Sockets 
  [2f01184e] + SparseArrays 
  [10745b16] + Statistics 
  [4607b0f0] + SuiteSparse 
  [8dfed614] + Test 
  [cf7118a7] + UUIDs 
  [4ec0a83e] + Unicode 
  Building CMake → `~/.julia/packages/CMake/nSK2r/deps/build.log`
  Updating `/tmp/jl_ppnOM5/Project.toml`
 [no changes]
  Updating `/tmp/jl_ppnOM5/Manifest.toml`
 [no changes]
  Building Blosc → `~/.julia/packages/Blosc/lzFr0/deps/build.log`
  Updating `/tmp/jl_ohqkwo/Project.toml`
 [no changes]
  Updating `/tmp/jl_ohqkwo/Manifest.toml`
 [no changes]
  Building Rmath → `~/.julia/packages/Rmath/BoBag/deps/build.log`
  Updating `/tmp/jl_tiOQ8I/Project.toml`
 [no changes]
  Updating `/tmp/jl_tiOQ8I/Manifest.toml`
 [no changes]
  Building HDF5 ─→ `~/.julia/packages/HDF5/Zh9on/deps/build.log`
  Updating `/tmp/jl_Q3Hank/Project.toml`
 [no changes]
  Updating `/tmp/jl_Q3Hank/Manifest.toml`
 [no changes]
   Testing GaussianMixtures
  Updating `/tmp/jl_JWxjxm/Project.toml`
 [no changes]
  Updating `/tmp/jl_JWxjxm/Manifest.toml`
 [no changes]
Running sandbox
Status `/tmp/jl_JWxjxm/Project.toml`
  [aaaa29a8] Clustering v0.13.3
  [34da2185] Compat v2.2.0
  [31c24e10] Distributions v0.22.3
  [5789e2e9] FileIO v1.2.1
  [cc18c42c] GaussianMixtures v0.3.0
  [4138dd39] JLD v0.9.1
  [90014a1f] PDMats v0.9.10
  [6e75b9c4] ScikitLearnBase v0.5.0
  [276daf66] SpecialFunctions v0.9.0
  [2913bbd2] StatsBase v0.32.0
  [8bb1440f] DelimitedFiles 
  [8ba89e20] Distributed 
  [37e2e46d] LinearAlgebra 
  [56ddb016] Logging 
  [de0858da] Printf 
  [9a3f8284] Random 
  [10745b16] Statistics 
[ Info: Testing Data
(100000, -1.3072750747972948e6, [86367.17905641464, 13632.820943585368], [-7540.599821744484 -8689.155092996049 4088.789460218305; 7756.774272611296 8563.35639261223 -4648.959022291843], [[86257.95995700225 1358.2163269173004 -2114.9463843733447; 1358.2163269173004 86049.29348541277 925.3419515539345; -2114.9463843733447 925.3419515539345 86296.58770084192], [12957.956927312713 -1129.766455224356 1945.1825132671293; -1129.7664552243555 13622.108105818537 -507.81494314458104; 1945.1825132671293 -507.8149431445809 13984.113854171645]])
┌ Warning: rmprocs: process 1 not removed
└ @ Distributed /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Distributed/src/cluster.jl:1015
[ Info: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.470575e+03
      1       9.022056e+02      -5.683695e+02 |        8
      2       8.598541e+02      -4.235154e+01 |        0
      3       8.598541e+02       0.000000e+00 |        0
K-means converged with 3 iterations (objv = 859.8540903466246)
┌ Info: K-means with 272 data points using 3 iterations
└ 11.3 data points per parameter
[ Info: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
┌ Info: EM with 272 data points 0 iterations avll -2.070865
└ 5.8 data points per parameter
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:221
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:221
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:221
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:221
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:631 [inlined]
└ @ Core ./broadcast.jl:631
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:230
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:230
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:631 [inlined]
└ @ Core ./broadcast.jl:631
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:631 [inlined]
└ @ Core ./broadcast.jl:631
[ Info: iteration 1, lowerbound -3.767187
[ Info: iteration 2, lowerbound -3.614409
[ Info: iteration 3, lowerbound -3.453778
[ Info: iteration 4, lowerbound -3.287822
[ Info: iteration 5, lowerbound -3.138932
[ Info: iteration 6, lowerbound -3.022132
[ Info: dropping number of Gaussions to 7
[ Info: iteration 7, lowerbound -2.931828
[ Info: dropping number of Gaussions to 5
[ Info: iteration 8, lowerbound -2.866663
[ Info: dropping number of Gaussions to 4
[ Info: iteration 9, lowerbound -2.804805
[ Info: iteration 10, lowerbound -2.764888
[ Info: iteration 11, lowerbound -2.743346
[ Info: dropping number of Gaussions to 3
[ Info: iteration 12, lowerbound -2.720566
[ Info: iteration 13, lowerbound -2.692266
[ Info: iteration 14, lowerbound -2.661545
[ Info: iteration 15, lowerbound -2.625112
[ Info: iteration 16, lowerbound -2.584207
[ Info: iteration 17, lowerbound -2.540919
[ Info: iteration 18, lowerbound -2.497803
[ Info: iteration 19, lowerbound -2.457077
[ Info: iteration 20, lowerbound -2.419813
[ Info: iteration 21, lowerbound -2.385829
[ Info: iteration 22, lowerbound -2.354805
[ Info: iteration 23, lowerbound -2.328407
[ Info: iteration 24, lowerbound -2.311345
[ Info: iteration 25, lowerbound -2.307822
[ Info: dropping number of Gaussions to 2
[ Info: iteration 26, lowerbound -2.302917
[ Info: iteration 27, lowerbound -2.299259
[ Info: iteration 28, lowerbound -2.299256
[ Info: iteration 29, lowerbound -2.299254
[ Info: iteration 30, lowerbound -2.299254
[ Info: iteration 31, lowerbound -2.299253
[ Info: iteration 32, lowerbound -2.299253
[ Info: iteration 33, lowerbound -2.299253
[ Info: iteration 34, lowerbound -2.299253
[ Info: iteration 35, lowerbound -2.299253
[ Info: iteration 36, lowerbound -2.299253
[ Info: iteration 37, lowerbound -2.299253
[ Info: iteration 38, lowerbound -2.299253
[ Info: iteration 39, lowerbound -2.299253
[ Info: iteration 40, lowerbound -2.299253
[ Info: iteration 41, lowerbound -2.299253
[ Info: iteration 42, lowerbound -2.299253
[ Info: iteration 43, lowerbound -2.299253
[ Info: iteration 44, lowerbound -2.299253
[ Info: iteration 45, lowerbound -2.299253
[ Info: iteration 46, lowerbound -2.299253
[ Info: iteration 47, lowerbound -2.299253
[ Info: iteration 48, lowerbound -2.299253
[ Info: iteration 49, lowerbound -2.299253
[ Info: iteration 50, lowerbound -2.299253
[ Info: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
History[Thu Jan 23 00:07:22 2020: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
, Thu Jan 23 00:07:30 2020: K-means with 272 data points using 3 iterations
11.3 data points per parameter
, Thu Jan 23 00:07:33 2020: EM with 272 data points 0 iterations avll -2.070865
5.8 data points per parameter
, Thu Jan 23 00:07:35 2020: GMM converted to Variational GMM
, Thu Jan 23 00:07:44 2020: iteration 1, lowerbound -3.767187
, Thu Jan 23 00:07:44 2020: iteration 2, lowerbound -3.614409
, Thu Jan 23 00:07:44 2020: iteration 3, lowerbound -3.453778
, Thu Jan 23 00:07:44 2020: iteration 4, lowerbound -3.287822
, Thu Jan 23 00:07:44 2020: iteration 5, lowerbound -3.138932
, Thu Jan 23 00:07:44 2020: iteration 6, lowerbound -3.022132
, Thu Jan 23 00:07:45 2020: dropping number of Gaussions to 7
, Thu Jan 23 00:07:45 2020: iteration 7, lowerbound -2.931828
, Thu Jan 23 00:07:45 2020: dropping number of Gaussions to 5
, Thu Jan 23 00:07:45 2020: iteration 8, lowerbound -2.866663
, Thu Jan 23 00:07:45 2020: dropping number of Gaussions to 4
, Thu Jan 23 00:07:45 2020: iteration 9, lowerbound -2.804805
, Thu Jan 23 00:07:45 2020: iteration 10, lowerbound -2.764888
, Thu Jan 23 00:07:45 2020: iteration 11, lowerbound -2.743346
, Thu Jan 23 00:07:45 2020: dropping number of Gaussions to 3
, Thu Jan 23 00:07:45 2020: iteration 12, lowerbound -2.720566
, Thu Jan 23 00:07:45 2020: iteration 13, lowerbound -2.692266
, Thu Jan 23 00:07:45 2020: iteration 14, lowerbound -2.661545
, Thu Jan 23 00:07:45 2020: iteration 15, lowerbound -2.625112
, Thu Jan 23 00:07:45 2020: iteration 16, lowerbound -2.584207
, Thu Jan 23 00:07:45 2020: iteration 17, lowerbound -2.540919
, Thu Jan 23 00:07:45 2020: iteration 18, lowerbound -2.497803
, Thu Jan 23 00:07:45 2020: iteration 19, lowerbound -2.457077
, Thu Jan 23 00:07:45 2020: iteration 20, lowerbound -2.419813
, Thu Jan 23 00:07:45 2020: iteration 21, lowerbound -2.385829
, Thu Jan 23 00:07:45 2020: iteration 22, lowerbound -2.354805
, Thu Jan 23 00:07:45 2020: iteration 23, lowerbound -2.328407
, Thu Jan 23 00:07:45 2020: iteration 24, lowerbound -2.311345
, Thu Jan 23 00:07:45 2020: iteration 25, lowerbound -2.307822
, Thu Jan 23 00:07:45 2020: dropping number of Gaussions to 2
, Thu Jan 23 00:07:45 2020: iteration 26, lowerbound -2.302917
, Thu Jan 23 00:07:45 2020: iteration 27, lowerbound -2.299259
, Thu Jan 23 00:07:45 2020: iteration 28, lowerbound -2.299256
, Thu Jan 23 00:07:45 2020: iteration 29, lowerbound -2.299254
, Thu Jan 23 00:07:45 2020: iteration 30, lowerbound -2.299254
, Thu Jan 23 00:07:45 2020: iteration 31, lowerbound -2.299253
, Thu Jan 23 00:07:45 2020: iteration 32, lowerbound -2.299253
, Thu Jan 23 00:07:45 2020: iteration 33, lowerbound -2.299253
, Thu Jan 23 00:07:45 2020: iteration 34, lowerbound -2.299253
, Thu Jan 23 00:07:45 2020: iteration 35, lowerbound -2.299253
, Thu Jan 23 00:07:45 2020: iteration 36, lowerbound -2.299253
, Thu Jan 23 00:07:45 2020: iteration 37, lowerbound -2.299253
, Thu Jan 23 00:07:45 2020: iteration 38, lowerbound -2.299253
, Thu Jan 23 00:07:45 2020: iteration 39, lowerbound -2.299253
, Thu Jan 23 00:07:45 2020: iteration 40, lowerbound -2.299253
, Thu Jan 23 00:07:45 2020: iteration 41, lowerbound -2.299253
, Thu Jan 23 00:07:45 2020: iteration 42, lowerbound -2.299253
, Thu Jan 23 00:07:45 2020: iteration 43, lowerbound -2.299253
, Thu Jan 23 00:07:45 2020: iteration 44, lowerbound -2.299253
, Thu Jan 23 00:07:45 2020: iteration 45, lowerbound -2.299253
, Thu Jan 23 00:07:45 2020: iteration 46, lowerbound -2.299253
, Thu Jan 23 00:07:45 2020: iteration 47, lowerbound -2.299253
, Thu Jan 23 00:07:45 2020: iteration 48, lowerbound -2.299253
, Thu Jan 23 00:07:45 2020: iteration 49, lowerbound -2.299253
, Thu Jan 23 00:07:45 2020: iteration 50, lowerbound -2.299253
, Thu Jan 23 00:07:45 2020: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [178.04509222607587, 95.95490777392428]
β = [178.04509222607587, 95.95490777392428]
m = [4.250300733269406 79.28686694435439; 2.0002292577748486 53.85198717245854]
ν = [180.04509222607587, 97.95490777392428]
W = LinearAlgebra.UpperTriangular{Float64,Array{Float64,2}}[[0.1840415554747801 -0.007644049042334075; 0.0 0.008581705166323925], [0.3758763611957041 -0.008953123827356772; 0.0 0.012748664777411756]]
Kind: diag, size256
nx: 100000 sum(zeroth order stats): 100000.00000000009
avll from stats: -0.9913481605542761
avll from llpg:  -0.9913481605541794
avll direct:     -0.9913481605541794
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 100000.0
avll from stats: -0.9728717646750994
avll from llpg:  -0.9728717646750994
avll direct:     -0.9728717646750995
sum posterior: 100000.0
32×26 Array{Float64,2}:
  0.17056      0.137603     -0.173015    -0.0046638    0.0913673   -0.0182484  -0.0770959   -0.0479815   -0.131876     0.124622     0.104044     0.116481      0.0457142  -0.157298    -0.0279986    0.0195937    0.0753189    0.0920637    0.149546     0.0392775   0.037605     0.0732857   -0.0805772   -0.000345558   0.0410162  -0.106334
  0.0698645   -0.139985     -0.0792667   -0.0278009   -0.0194123    0.0165811   0.0667587   -0.0562846    0.0401966   -0.207472     0.0339286   -0.14308       0.0562658   0.0135029   -0.0615602   -0.0735935   -0.0761292   -0.0325221    0.0299616   -0.05421     0.204774    -0.0503829   -0.0527616    0.0833101     0.0253917   0.126221
  0.00114201   0.0417378     0.106725    -0.085388    -0.0391144    0.0486731  -0.090137    -0.0845969   -0.135584     0.155578     0.220701     0.0621006    -0.12516    -0.0620047   -0.0513216   -0.0864778    0.0586054   -0.128972     0.043843    -0.0234148  -0.120257     0.0838045    0.00881865   0.0154322    -0.114219    0.0317636
 -0.110312     0.078619     -0.0263615   -0.0201243    0.00371477  -0.0407534  -0.0544748   -0.133733    -0.0700781    0.0898364   -0.0707939    0.00275795   -0.172709   -0.118442     0.00581108  -0.132641     0.0651177   -0.00223276   0.0504969    0.0859689   0.0228192    0.04547      0.00748092   0.0455617    -0.0172092  -0.245046
 -0.0127156   -0.0805383     0.0187637   -0.076885     0.0194623    0.163733   -0.246637     0.0868078   -0.0890117    0.0466014   -0.0250528   -0.0293701     0.0631258  -0.0863321    0.167373    -0.224335     0.029738    -0.0230762    0.0785794    0.0085158   0.0372333    0.0444893    0.0418695    0.0599397    -0.0072003   0.14831
  0.0308761    0.0921732     0.0445819   -0.112541    -0.0860353    0.260299    0.106837     0.00935086  -0.0224169   -0.0472893   -0.0543528    0.12866      -0.123886   -0.0298064    0.069799     0.0120981   -0.0221712   -0.00966113  -0.0263329    0.121969   -0.0886832    0.172778     0.0246179    0.00637746   -0.0826514  -0.0601046
 -0.0592054   -0.020435      0.21116      0.208716    -0.157686     0.0120393   0.0806155    0.0362105    0.11246      0.145208    -0.0590091    0.136862     -0.233031    0.00127674   0.0142608   -0.318967     0.12698      0.0663914   -0.213814    -0.158122    0.00281878   0.184563    -0.140156    -0.0277013     0.0636013   0.130359
 -0.178956    -0.00356991    0.0977243   -0.0858124    0.00669721  -0.135875   -0.100308     0.0509181    0.136307    -0.13474     -0.0718814    0.0024433     0.019297    0.184238     0.0456883   -0.142009     0.141664    -0.100878     0.0223146    0.159153   -0.0520617    0.00377813   0.0288479   -0.0999307    -0.115563   -0.0821577
  0.0491576   -0.0943853     0.00512534  -0.0348966   -0.174385    -0.0475073   0.0445877   -0.0977737   -0.101838    -0.0876263   -0.0328766   -0.0844161     0.11607    -0.0290326    0.119565     0.0458615   -0.0774836    0.129868    -0.0234275    0.0226105   0.0527847   -0.107882     0.0852209   -0.0499662     0.11805    -0.0641082
  0.106347    -0.0304988     0.0778201    0.0621976   -0.0445829    0.102161   -0.113854    -0.0518246   -0.0349263   -0.1659       0.105677     0.10878       0.0637803   0.0514728    0.171788    -0.0108268    0.0820008   -0.05684      0.136717    -0.097483   -0.109267     0.0158208    0.0815943    0.0743609     0.0395816  -0.19676
  0.0335141    0.074658      0.0247348    0.0486733    0.115608    -0.0338783  -0.0585699   -0.258492     0.13192      0.0013283   -0.135858     0.0434343    -0.0449656   0.0719502   -0.0204751    0.192726    -0.193013    -0.0760718    0.0969408   -0.104166   -0.134223    -0.0419338   -0.154587    -0.11059       0.115585    0.0741852
 -0.0124924    0.000238687  -0.199172    -0.164896    -0.0282444    0.264687   -0.162202     0.0807707   -0.111813     0.0346811   -0.0762787   -0.0123167    -0.131735   -0.0527125    0.00382141   0.186505    -0.113094     0.041846    -0.032472     0.1966      0.107625     0.0613159    0.0279568   -0.214976      0.0145633  -0.0314976
 -0.0714671   -0.131619      0.0160243   -0.107324     0.13076      0.0226786   0.175459     0.0852367   -0.00414661  -0.0100531    0.0867626   -0.100047      0.20314     0.0517756    0.0190685   -0.064414    -0.00140225   0.00788205  -0.0994267   -0.243774   -0.0265495   -0.0343601    0.0792256    0.191172      0.0739364   0.0836444
  0.0868831   -0.101374      0.0625332    0.0842302   -0.00169158   0.018817    0.236018    -0.00349023   0.151217     0.119552    -0.00375783  -0.237087     -0.266094    0.00227409   0.0548092    0.0973684    0.141123    -0.182324     0.0439995    0.154491   -0.0245228    0.201934     0.0665143    0.0402391    -0.0412081  -0.016537
 -0.0884518   -0.209164      0.0568287    0.028591    -0.0914976   -0.068829    0.124381    -0.151343    -0.092954    -0.0170344    0.0114086   -0.0908864     0.0976852   0.0288019   -0.0727589   -0.0388951    0.1487      -0.0985281    0.0200961   -0.0390896  -0.0241819   -0.174381    -0.126886     0.231172     -0.0248021  -0.0969948
 -0.0456201   -0.114234      0.29349     -0.024357     0.13605      0.10844    -0.19211     -0.0920533    0.164479     0.0638488    0.121809     0.177877     -0.0298577   0.0552581    0.0642884   -0.0129056    0.102086     0.133882     0.0586519    0.0379974   0.010003    -0.112274    -0.0725466    0.13441      -0.161842    0.067332
  0.00515455   0.0713557    -0.225479     0.0499531    0.0610659   -0.0938029  -0.0802897   -0.0835221   -0.0441299   -0.0399784    0.166129    -0.0611655    -0.0410985  -0.253459     0.0233901    0.0253518   -0.0604078   -0.0717905    0.00477965  -0.090557   -0.0428962   -0.218049    -0.039318     0.0175129     0.0417883   0.0995279
 -0.0226943   -0.0599107     0.0745431   -0.0184728    0.0427318    0.0683409  -0.100993     0.00113274   0.0379385    0.00226076  -0.179802    -0.275959     -0.0259302   0.0272367   -0.020381     0.111427    -0.0274329   -0.15374     -0.0719798   -0.0418114   0.241811    -0.0773565    0.156401     0.0461018    -0.01102     0.000241117
  0.0644625    0.130642      0.038219    -0.056079     0.0559388   -0.171977   -0.0887097   -0.00777565   0.0044114    0.0667491   -0.0882955   -0.0756731     0.0276511  -0.0311006   -0.144383    -0.0938554   -0.0467145    0.0896102   -0.15636     -0.0620566   0.0957581    0.0464678    0.0852841   -0.0507106    -0.0557272  -0.0735866
  0.021085    -0.11332       0.0692073   -0.115379     0.0390758   -0.138298    0.117536    -0.0960143    0.0380268   -0.0955132    0.0337784   -0.0321613     0.126889    0.115126    -0.0253132    0.0155419    0.0518341   -0.0755883    0.0290651    0.08907    -0.117471    -0.0121399   -0.0509067   -0.0398895     0.15911     0.0290053
 -0.00725331  -0.0496603    -0.053949    -0.0466958    0.132473     0.0802759   0.0059198    0.140629    -0.0518884    0.0494277   -0.0137639    0.0965695    -0.399584   -0.0182113   -0.0786138    0.0556622    0.0520273   -0.0935783    0.0995324    0.0639943  -0.162465    -0.101985    -0.0202146    0.187986     -0.062623    0.14724
  0.132497    -0.0898634     0.0570579   -0.0902656   -0.00338162   0.162502   -0.105744     0.223958     0.171592    -0.00638567  -0.0203553   -0.0252335    -0.0773218   0.120607     0.0822237   -0.0750784    0.173229    -0.037205    -0.0454948    0.0911554   0.0990206    0.0677469    0.00189983  -0.0596129     0.0253143  -0.0348418
 -0.0840832   -0.0855839     0.06442      0.0563551    0.0120544   -0.0867931   0.0105893    0.0423537   -0.0529465    0.210476     0.0436685    0.27564       0.0431259  -0.0883742   -0.0739855    0.18125      0.0314506    0.0541634    0.126966     0.110638    0.0817607    0.143487     0.00508168   0.129781      0.252934   -0.0381494
 -0.0561324   -0.204247     -0.0947624    0.0992082   -0.11033      0.134404    0.178925    -0.042169    -0.085018    -0.0080917    0.00740674   0.147206      0.0521195   0.102431    -0.0786176    0.105103     0.0790937    0.195534     0.0899294    0.0757948  -0.0333837    0.302343    -0.103537    -0.0923462    -0.0142313   0.0666196
  0.118176    -0.0334663     0.0947763   -0.0556074   -0.0204095   -0.0132256   0.0560163   -0.104847    -0.14364      0.176373    -0.192625     0.0693447     0.173145   -0.0310115   -0.0928892   -0.111346     0.0249229   -0.0600298    0.161913     0.015489   -0.0345296    0.0926579   -0.0137499    0.0442459     0.114245    0.0823469
  0.128353     0.0686103    -0.020221     0.112057    -0.0811438    0.0321123  -0.022683     0.206819    -0.110289     0.113151     0.122574    -0.0404253     0.0614039  -0.125187     0.00782432   0.0737515    0.0346135    0.132261     0.0994055    0.0578921   0.0571439   -0.147718    -0.121463    -0.0895346     0.0121287   0.0583524
  0.0817911   -0.109071      0.0983117    0.130475    -0.0957835   -0.0908592   0.0159293    0.131372     0.00337089  -0.0620127    0.00445135   0.0596343    -0.0961626   0.0441272    0.034056     0.155677     0.0824869    0.12218     -0.0319917   -0.0374758   0.0548309    0.0854521   -0.0648699    0.0776205    -0.188432   -0.0440577
 -0.101433     0.238478     -0.0342198   -0.00394351   0.270118     0.102869   -0.00593291   0.0729583   -0.136149    -0.0462826    0.153269     0.0796409    -0.0690646   0.073105    -0.111729     0.0215949    0.167118    -0.10549     -0.173807    -0.198664   -0.105535     0.0671362    0.145798    -0.0219867     0.100874   -0.041031
 -0.101473    -0.0985232     0.137798    -0.0523648    0.0121378    0.0144407   0.0813164   -0.0680756    0.0632397   -0.0406275   -0.00211473  -0.128939     -0.0231478   0.151271    -0.0265807    0.00426254   0.0632193    0.0868814    0.0537081   -0.0167961  -0.100936     0.142911    -0.00258275   0.0142405    -0.0819479   0.0268082
 -0.0375195   -0.023799      0.145071     0.127284    -0.106389     0.0208834  -0.125086     0.0236637    0.152449     0.0274587   -0.111372     0.021537      0.0491693  -0.140494    -0.0619079   -0.104725     0.0106794   -0.0116238   -0.0464916   -0.0779967  -0.127937     0.107341    -0.178109    -0.0607807    -0.0487922   0.103569
 -0.049188     0.103906     -0.0695153    0.0630139   -0.0629036    0.121811    0.0355331    0.0597316    0.0496751   -0.0849571   -0.0648486   -0.0545887     0.183517   -0.0134306   -0.114998     0.017453    -0.151042    -0.0529262    0.0272919   -0.0373439  -0.00355554   0.0595786    0.034643     0.0300896    -0.103347   -0.0807717
  0.0216072   -0.00236561   -0.0812843    0.191569    -0.0577442   -0.0728235  -0.126148     0.0910262   -0.131916     0.104808    -0.0370023   -0.000454478  -0.120283    0.0987593   -0.0559267   -0.121099     0.0143161    0.117656    -0.0483334    0.150119    0.0814669    0.130438     0.0564861    0.0339948    -0.107588   -0.0627699kind diag, method split
┌ Info: 0: avll = 
└   tll[1] = -1.410436349908187
[ Info: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.410518
[ Info: iteration 2, average log likelihood -1.410435
[ Info: iteration 3, average log likelihood -1.409546
[ Info: iteration 4, average log likelihood -1.401835
[ Info: iteration 5, average log likelihood -1.388967
[ Info: iteration 6, average log likelihood -1.384348
[ Info: iteration 7, average log likelihood -1.383101
[ Info: iteration 8, average log likelihood -1.382306
[ Info: iteration 9, average log likelihood -1.381719
[ Info: iteration 10, average log likelihood -1.381288
[ Info: iteration 11, average log likelihood -1.380935
[ Info: iteration 12, average log likelihood -1.380597
[ Info: iteration 13, average log likelihood -1.380257
[ Info: iteration 14, average log likelihood -1.379923
[ Info: iteration 15, average log likelihood -1.379596
[ Info: iteration 16, average log likelihood -1.379261
[ Info: iteration 17, average log likelihood -1.378904
[ Info: iteration 18, average log likelihood -1.378491
[ Info: iteration 19, average log likelihood -1.377897
[ Info: iteration 20, average log likelihood -1.377140
[ Info: iteration 21, average log likelihood -1.376475
[ Info: iteration 22, average log likelihood -1.376002
[ Info: iteration 23, average log likelihood -1.375667
[ Info: iteration 24, average log likelihood -1.375412
[ Info: iteration 25, average log likelihood -1.375178
[ Info: iteration 26, average log likelihood -1.374909
[ Info: iteration 27, average log likelihood -1.374636
[ Info: iteration 28, average log likelihood -1.374440
[ Info: iteration 29, average log likelihood -1.374297
[ Info: iteration 30, average log likelihood -1.374176
[ Info: iteration 31, average log likelihood -1.374066
[ Info: iteration 32, average log likelihood -1.373976
[ Info: iteration 33, average log likelihood -1.373915
[ Info: iteration 34, average log likelihood -1.373878
[ Info: iteration 35, average log likelihood -1.373856
[ Info: iteration 36, average log likelihood -1.373844
[ Info: iteration 37, average log likelihood -1.373836
[ Info: iteration 38, average log likelihood -1.373831
[ Info: iteration 39, average log likelihood -1.373828
[ Info: iteration 40, average log likelihood -1.373826
[ Info: iteration 41, average log likelihood -1.373824
[ Info: iteration 42, average log likelihood -1.373823
[ Info: iteration 43, average log likelihood -1.373823
[ Info: iteration 44, average log likelihood -1.373822
[ Info: iteration 45, average log likelihood -1.373821
[ Info: iteration 46, average log likelihood -1.373821
[ Info: iteration 47, average log likelihood -1.373821
[ Info: iteration 48, average log likelihood -1.373820
[ Info: iteration 49, average log likelihood -1.373820
[ Info: iteration 50, average log likelihood -1.373820
┌ Info: EM with 100000 data points 50 iterations avll -1.373820
└ 952.4 data points per parameter
┌ Info: 1
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4105183653163
│     -1.4104350728832733
│      ⋮
└     -1.3738197161513894
[ Info: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.373954
[ Info: iteration 2, average log likelihood -1.373825
[ Info: iteration 3, average log likelihood -1.373100
[ Info: iteration 4, average log likelihood -1.367163
[ Info: iteration 5, average log likelihood -1.354851
[ Info: iteration 6, average log likelihood -1.346642
[ Info: iteration 7, average log likelihood -1.343704
[ Info: iteration 8, average log likelihood -1.342047
[ Info: iteration 9, average log likelihood -1.340247
[ Info: iteration 10, average log likelihood -1.338397
[ Info: iteration 11, average log likelihood -1.336962
[ Info: iteration 12, average log likelihood -1.335859
[ Info: iteration 13, average log likelihood -1.334904
[ Info: iteration 14, average log likelihood -1.334166
[ Info: iteration 15, average log likelihood -1.333668
[ Info: iteration 16, average log likelihood -1.333308
[ Info: iteration 17, average log likelihood -1.333018
[ Info: iteration 18, average log likelihood -1.332768
[ Info: iteration 19, average log likelihood -1.332550
[ Info: iteration 20, average log likelihood -1.332362
[ Info: iteration 21, average log likelihood -1.332202
[ Info: iteration 22, average log likelihood -1.332059
[ Info: iteration 23, average log likelihood -1.331925
[ Info: iteration 24, average log likelihood -1.331783
[ Info: iteration 25, average log likelihood -1.331619
[ Info: iteration 26, average log likelihood -1.331404
[ Info: iteration 27, average log likelihood -1.331096
[ Info: iteration 28, average log likelihood -1.330688
[ Info: iteration 29, average log likelihood -1.330245
[ Info: iteration 30, average log likelihood -1.329837
[ Info: iteration 31, average log likelihood -1.329509
[ Info: iteration 32, average log likelihood -1.329285
[ Info: iteration 33, average log likelihood -1.329137
[ Info: iteration 34, average log likelihood -1.329029
[ Info: iteration 35, average log likelihood -1.328936
[ Info: iteration 36, average log likelihood -1.328840
[ Info: iteration 37, average log likelihood -1.328732
[ Info: iteration 38, average log likelihood -1.328602
[ Info: iteration 39, average log likelihood -1.328460
[ Info: iteration 40, average log likelihood -1.328311
[ Info: iteration 41, average log likelihood -1.328166
[ Info: iteration 42, average log likelihood -1.328043
[ Info: iteration 43, average log likelihood -1.327949
[ Info: iteration 44, average log likelihood -1.327881
[ Info: iteration 45, average log likelihood -1.327829
[ Info: iteration 46, average log likelihood -1.327779
[ Info: iteration 47, average log likelihood -1.327697
[ Info: iteration 48, average log likelihood -1.327479
[ Info: iteration 49, average log likelihood -1.327049
[ Info: iteration 50, average log likelihood -1.326588
┌ Info: EM with 100000 data points 50 iterations avll -1.326588
└ 473.9 data points per parameter
┌ Info: 2
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.3739543715177984
│     -1.3738245271851852
│      ⋮
└     -1.326587667257929
[ Info: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.326355
[ Info: iteration 2, average log likelihood -1.325773
[ Info: iteration 3, average log likelihood -1.324441
[ Info: iteration 4, average log likelihood -1.315414
[ Info: iteration 5, average log likelihood -1.296969
[ Info: iteration 6, average log likelihood -1.286475
[ Info: iteration 7, average log likelihood -1.281648
[ Info: iteration 8, average log likelihood -1.278267
[ Info: iteration 9, average log likelihood -1.275543
[ Info: iteration 10, average log likelihood -1.273629
[ Info: iteration 11, average log likelihood -1.272067
[ Info: iteration 12, average log likelihood -1.270588
[ Info: iteration 13, average log likelihood -1.269276
[ Info: iteration 14, average log likelihood -1.268263
[ Info: iteration 15, average log likelihood -1.267521
[ Info: iteration 16, average log likelihood -1.266924
[ Info: iteration 17, average log likelihood -1.266375
[ Info: iteration 18, average log likelihood -1.265853
[ Info: iteration 19, average log likelihood -1.265358
[ Info: iteration 20, average log likelihood -1.265004
[ Info: iteration 21, average log likelihood -1.264827
[ Info: iteration 22, average log likelihood -1.264700
[ Info: iteration 23, average log likelihood -1.264589
[ Info: iteration 24, average log likelihood -1.264489
[ Info: iteration 25, average log likelihood -1.264397
[ Info: iteration 26, average log likelihood -1.264317
[ Info: iteration 27, average log likelihood -1.264249
[ Info: iteration 28, average log likelihood -1.264192
[ Info: iteration 29, average log likelihood -1.264143
[ Info: iteration 30, average log likelihood -1.264104
[ Info: iteration 31, average log likelihood -1.264071
[ Info: iteration 32, average log likelihood -1.264044
[ Info: iteration 33, average log likelihood -1.264022
[ Info: iteration 34, average log likelihood -1.264005
[ Info: iteration 35, average log likelihood -1.263991
[ Info: iteration 36, average log likelihood -1.263981
[ Info: iteration 37, average log likelihood -1.263974
[ Info: iteration 38, average log likelihood -1.263968
[ Info: iteration 39, average log likelihood -1.263964
[ Info: iteration 40, average log likelihood -1.263961
[ Info: iteration 41, average log likelihood -1.263959
[ Info: iteration 42, average log likelihood -1.263958
[ Info: iteration 43, average log likelihood -1.263956
[ Info: iteration 44, average log likelihood -1.263955
[ Info: iteration 45, average log likelihood -1.263955
[ Info: iteration 46, average log likelihood -1.263954
[ Info: iteration 47, average log likelihood -1.263954
[ Info: iteration 48, average log likelihood -1.263954
[ Info: iteration 49, average log likelihood -1.263953
[ Info: iteration 50, average log likelihood -1.263953
┌ Info: EM with 100000 data points 50 iterations avll -1.263953
└ 236.4 data points per parameter
┌ Info: 3
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.3263553045219996
│     -1.3257729750160199
│      ⋮
└     -1.2639531530158599
[ Info: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.264240
[ Info: iteration 2, average log likelihood -1.263952
[ Info: iteration 3, average log likelihood -1.262891
[ Info: iteration 4, average log likelihood -1.250095
[ Info: iteration 5, average log likelihood -1.215497
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.189611
[ Info: iteration 7, average log likelihood -1.189653
[ Info: iteration 8, average log likelihood -1.180477
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      7
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.173489
[ Info: iteration 10, average log likelihood -1.190020
[ Info: iteration 11, average log likelihood -1.180667
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 12, average log likelihood -1.174710
[ Info: iteration 13, average log likelihood -1.180782
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 14, average log likelihood -1.172591
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 15, average log likelihood -1.177877
[ Info: iteration 16, average log likelihood -1.182517
[ Info: iteration 17, average log likelihood -1.174417
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      7
│     13
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 18, average log likelihood -1.168426
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 19, average log likelihood -1.181122
[ Info: iteration 20, average log likelihood -1.181073
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 21, average log likelihood -1.173046
[ Info: iteration 22, average log likelihood -1.178203
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 23, average log likelihood -1.170043
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 24, average log likelihood -1.176153
[ Info: iteration 25, average log likelihood -1.180994
[ Info: iteration 26, average log likelihood -1.172894
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      7
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 27, average log likelihood -1.166832
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     13
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 28, average log likelihood -1.184579
[ Info: iteration 29, average log likelihood -1.182681
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 30, average log likelihood -1.170803
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 31, average log likelihood -1.175658
[ Info: iteration 32, average log likelihood -1.179466
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 33, average log likelihood -1.172432
[ Info: iteration 34, average log likelihood -1.178263
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 35, average log likelihood -1.170113
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 36, average log likelihood -1.176123
[ Info: iteration 37, average log likelihood -1.180939
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     13
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 38, average log likelihood -1.172837
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      7
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 39, average log likelihood -1.173988
[ Info: iteration 40, average log likelihood -1.185795
[ Info: iteration 41, average log likelihood -1.175753
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 42, average log likelihood -1.169566
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 43, average log likelihood -1.175324
[ Info: iteration 44, average log likelihood -1.179381
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 45, average log likelihood -1.172399
[ Info: iteration 46, average log likelihood -1.178219
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 47, average log likelihood -1.170045
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      7
│     13
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 48, average log likelihood -1.175998
[ Info: iteration 49, average log likelihood -1.187906
[ Info: iteration 50, average log likelihood -1.174321
┌ Info: EM with 100000 data points 50 iterations avll -1.174321
└ 118.1 data points per parameter
┌ Info: 4
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.2642398987336811
│     -1.2639521755194447
│      ⋮
└     -1.1743214659218666
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     13
│     14
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 1, average log likelihood -1.167939
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     13
│     14
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 2, average log likelihood -1.167321
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     13
│     14
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 3, average log likelihood -1.165379
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│     13
│     14
│     25
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.149919
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     13
│     14
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.122574
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│     13
│     14
│     20
│     21
│     25
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.093780
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      5
│     13
│     14
│     27
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.100381
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      6
│     13
│     14
│     20
│     25
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.095083
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      3
│     13
│     14
│     21
│     25
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.098593
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│     13
│     14
│     20
│     25
│     27
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.099323
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      5
│     13
│     14
│     25
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 11, average log likelihood -1.096693
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      6
│     13
│     14
│     20
│     21
│     25
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 12, average log likelihood -1.091931
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│     13
│     14
│     25
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 13, average log likelihood -1.097945
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      3
│     13
│     14
│     20
│     25
│     27
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 14, average log likelihood -1.083620
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      5
│     13
│     14
│     21
│     25
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 15, average log likelihood -1.090770
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      6
│     13
│     14
│     20
│     25
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 16, average log likelihood -1.098498
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│     13
│     14
│     25
│     27
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 17, average log likelihood -1.093607
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      3
│     13
│     14
│     20
│     21
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 18, average log likelihood -1.084240
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      5
│     13
│     14
│     25
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 19, average log likelihood -1.095976
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      6
│     13
│     14
│     20
│     25
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 20, average log likelihood -1.088691
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│     13
│     14
│     21
│     27
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 21, average log likelihood -1.093875
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      3
│     13
│     14
│     20
│     25
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 22, average log likelihood -1.087636
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      5
│     13
│     14
│     25
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 23, average log likelihood -1.092193
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      6
│     13
│     14
│     20
│      ⋮
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 24, average log likelihood -1.086183
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│     13
│     14
│     25
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 25, average log likelihood -1.103720
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      3
│     13
│     14
│     20
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 26, average log likelihood -1.085597
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      5
│     13
│     14
│     21
│     25
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 27, average log likelihood -1.084393
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│     13
│     14
│     20
│     27
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 28, average log likelihood -1.097548
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      6
│     13
│     14
│     25
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 29, average log likelihood -1.086973
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│     13
│     14
│     20
│     21
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 30, average log likelihood -1.087497
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      3
│      5
│     13
│     14
│      ⋮
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 31, average log likelihood -1.084682
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│     13
│     14
│     20
│     25
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 32, average log likelihood -1.102160
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│     13
│     14
│     21
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 33, average log likelihood -1.089390
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      6
│     13
│     14
│     20
│     25
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 34, average log likelihood -1.082541
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│     13
│     14
│     27
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 35, average log likelihood -1.092919
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      3
│      5
│     13
│     14
│      ⋮
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 36, average log likelihood -1.078552
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│     13
│     14
│     25
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 37, average log likelihood -1.107651
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│     13
│     14
│     20
│     27
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 38, average log likelihood -1.084694
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      6
│     13
│     14
│     21
│     25
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 39, average log likelihood -1.086214
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│     13
│     14
│     20
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 40, average log likelihood -1.096095
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      3
│      5
│     13
│     14
│     25
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 41, average log likelihood -1.081525
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│     13
│     14
│     20
│     21
│     25
│     27
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 42, average log likelihood -1.096413
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│     13
│     14
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 43, average log likelihood -1.096884
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      6
│     13
│     14
│     20
│     25
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 44, average log likelihood -1.082747
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│     13
│     14
│     21
│     27
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 45, average log likelihood -1.087871
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      3
│     13
│     14
│     20
│     25
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 46, average log likelihood -1.090917
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      5
│     13
│     14
│     25
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 47, average log likelihood -1.094497
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│     13
│     14
│     20
│     21
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 48, average log likelihood -1.087873
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      6
│     13
│     14
│     25
│     27
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 49, average log likelihood -1.092374
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│     13
│     14
│     20
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 50, average log likelihood -1.092525
┌ Info: EM with 100000 data points 50 iterations avll -1.092525
└ 59.0 data points per parameter
┌ Info: 5
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.167939028546526
│     -1.1673214726572807
│      ⋮
└     -1.0925251567235303
┌ Info: Total log likelihood: 
│   tll =
│    251-element Array{Float64,1}:
│     -1.410436349908187
│     -1.4105183653163
│     -1.4104350728832733
│     -1.4095457950322334
│      ⋮
│     -1.0878733841852257
│     -1.0923743007864193
└     -1.0925251567235303
32×26 Array{Float64,2}:
 -0.0140098   -0.0645668    -0.034888    -0.00889753   0.0363916    0.0653029   -0.0980317   -0.00269281   0.0444916    0.00121982  -0.228717    -0.294936   -0.0587325   0.0565169   -0.0204473     0.109669    -0.0291811  -0.173866    -0.0726974   -0.0602309    0.230459    -0.099912    0.148694      0.0429839   -0.0142937    0.0411532
 -0.0705396   -0.203152     -0.108635     0.101705    -0.0916089    0.116665     0.188944    -0.041848    -0.0824722   -7.63663e-5  -0.0447271    0.161168    0.0475879   0.0971622   -0.0798392     0.094597     0.0779251   0.205822     0.0844019    0.0947057   -0.0277209    0.292245   -0.115912     -0.0900914   -0.00891019   0.0528265
  0.0132336   -0.127905      0.0799697   -0.106663     0.0288753   -0.103545     0.0887171   -0.0888163    0.0396679   -0.0935556    0.0515837   -0.0191753   0.112011    0.120584    -0.0238483    -0.0145006    0.058006   -0.0695262    0.0288906    0.0888216   -0.117852     0.0127536  -0.0387787    -0.02419      0.118601     0.0409417
  0.0939797   -0.0892965     0.045242    -0.0894046   -0.00391203   0.131841    -0.100066     0.225566     0.171896    -0.00797258  -0.0281689   -0.0180504  -0.0721482   0.0897373    0.0812142    -0.102414     0.176409   -0.063488    -0.0140872    0.0647589    0.0544442    0.0745805   0.015605     -0.0596523    0.0263485   -0.0382658
 -0.0320639    0.0648808    -0.140847     0.0364965    0.0416416   -0.0996881   -0.0812177   -0.105525    -0.0492693   -0.0468435    0.103042    -0.0581789  -0.0229579  -0.213624     0.0171612     0.00170094  -0.02718    -0.0759661    0.00518242  -0.0351065   -0.059384    -0.225451   -0.0439433     0.0124891   -0.00137152   0.0590027
 -0.0575151    0.0220493     0.0929322   -0.0622822   -0.0500783    0.00795093  -0.0709773   -0.0996062   -0.133923     0.144989     0.170501     0.0517148  -0.114692   -0.0414595   -0.0480572    -0.090345     0.0723601  -0.130962     0.0506607    0.00812491  -0.100766     0.0885738  -0.000499293  -0.0097694   -0.138414    -0.0164573
  0.0016849   -0.0674248     0.0687366    0.0710904   -0.039746    -0.0310312   -0.036304    -0.0133763   -0.071791     0.0250555    0.0354373    0.0714374   0.024349    0.0428788    0.0169533    -0.00567284   0.0651669   0.013827     0.0685792    0.0230386    0.00664716   0.0163459   0.0168506     0.112265     0.0276973   -0.104379
 -0.0282694   -0.0419094     0.156408     0.142959    -0.0907551   -0.023145    -0.124669     0.0138937    0.171622     0.022255    -0.123172     0.0172865   0.0555909  -0.14205     -0.046562     -0.106526     0.0195768  -0.0319031   -0.0196393   -0.0642483   -0.13219      0.131795   -0.16883      -0.0612517   -0.076234     0.0809926
  0.0883098   -0.0134065     0.056037    -0.154633    -0.112178    -0.0155847    0.0620202   -0.0889439   -0.138306     0.168708    -0.263313     0.0556866   0.215849   -0.0198315   -0.0909862    -0.106992    -0.204052   -0.0578559    0.243261    -1.44931     -0.0180694    0.063501    0.041955      0.0334656    0.118939     0.0945758
  0.159031    -0.047984      0.125575    -0.0835493    0.0500539   -0.0345832    0.0452529   -0.0829954   -0.144143     0.183607    -0.169095     0.0722255   0.144082   -0.0630889   -0.0964527    -0.112983     0.220071   -0.0653688    0.152917     1.334       -0.0697318    0.0979163  -0.0269087     0.0211682    0.111743     0.088885
  0.0912004    0.136662     -0.264408    -0.940686     0.0820619   -0.0847205    0.0278296   -0.026325    -0.132309     0.123562     0.0303832    0.232562    0.0528373  -0.250373    -0.0349648     0.023365     0.0698325   0.1122       0.143666     0.0485238    0.0264486    0.0866421  -0.0615353     8.10437e-5   0.0256488   -0.122826
  0.257487     0.138106     -0.0346197    0.951937     0.0972544    0.111402    -0.143154    -0.0777546   -0.153596     0.124922     0.217042     0.0399309   0.105343   -0.0587807   -0.0313913     0.016483     0.0788195   0.0899315    0.163476     0.0227542    0.028177     0.0554816  -0.102809     -0.00216024   0.0575531   -0.0790763
 -0.00572542   0.126551      0.0188472   -0.466348     0.103163    -0.173876     0.0208723   -0.0083982    0.0812777    0.0501344   -0.0685794    0.0975463   0.0280175  -0.0300167   -0.121089     -0.0890402   -0.0735733   0.127815    -0.154535     0.0497319    0.0800331   -0.0178222  -0.0528667    -0.0512098   -0.168359    -0.0693819
  0.119112     0.111671      0.058588     0.371034     0.00548016  -0.158616    -0.153347    -0.019177    -0.111317     0.0821958   -0.0584348   -0.331674    0.0279524  -0.0307844   -0.114812     -0.0964964   -0.0201615   0.0359322   -0.155277    -0.299912     0.115694     0.0794952   0.274855     -0.050947     0.0235589   -0.0688862
 -0.167168     0.085557      0.255907     0.156468    -0.174926     0.0275027    0.0331627    0.0212254    0.0440745    0.166336    -0.08548      0.124871   -0.199087   -0.00955177  -0.0738834    -0.319208    -0.366329    0.0721586   -0.219569    -0.19826      0.0424012    0.237974   -0.336004     -0.0297095    0.0954889    0.111648
  0.0814227   -0.0690728     0.169878     0.206551    -0.133112     0.00366925   0.12872      0.0105484    0.222383     0.135365    -0.0479605    0.148515   -0.214296    0.0168836    0.0550157    -0.318751     0.554288    0.0307629   -0.208176    -0.137057    -0.029146     0.123076   -0.0346422    -0.0273577    0.0223474    0.186856
 -0.0821703   -0.00726266    0.137089    -0.0510463   -0.0924306    0.0338866    0.0689173   -0.109954     0.0680711    0.0752797   -0.00919869  -0.107737   -0.0348004   0.144982    -0.0703518     0.0227486    0.0632829   0.073311    -0.548437     0.00278558   0.0620189    0.177404   -0.013407      0.0050408   -0.00893713   0.0628135
 -0.151752    -0.0386391     0.137862    -0.0460747    0.0156173   -0.00885885   0.038128    -0.0427547    0.0337947   -0.168759    -0.0447431   -0.158123   -0.0378699   0.155962    -0.0130437    -0.01869      0.0631028   0.0951923    0.788422    -0.0123258   -0.234901     0.0314674  -0.00693791    0.0456056   -0.130428     0.0105766
  0.0528128   -0.0965801    -0.00319822  -0.035437    -0.199535    -0.0313699    0.0461797   -0.0498863   -0.113624    -0.0932846   -0.0319618   -0.0904954   0.0872367  -0.00164718   0.11012       0.0548394   -0.0863913   0.143233    -0.0157852    0.019272     0.0549016   -0.102607    0.0845448    -0.0476844    0.109112    -0.0577991
  0.0756283   -0.102825      0.0734597    0.0846171    0.0402509    0.00973144   0.233182     0.0552731    0.135091     0.100687    -0.0011017   -0.23609    -0.277675    0.00309161   0.0542585     0.0816752    0.138509   -0.177588     0.0228323    0.144746    -0.0334536    0.21132     0.059858      0.0389366   -0.0664001   -0.0220671
  0.0401819    0.0690983     0.0258948    0.0371524    0.113392    -0.0373433   -0.0938954   -0.225675     0.141904    -0.0171905   -0.125354     0.0429165  -0.0480694   0.118112    -0.0187116     0.1894      -0.17262    -0.076917     0.106134    -0.0764647   -0.113473    -0.0434876  -0.146432     -0.106214     0.0913224    0.0700268
 -0.0608155    0.0902435    -0.050815     0.0474124   -0.0607256    0.0893592    0.0349221    0.0592049    0.0293214   -0.101488    -0.0666549   -0.052949    0.172284    0.0285388   -0.0855721     0.00432398  -0.0776131  -0.0684289    0.0232138   -0.00258619  -0.00833789   0.0693754   0.0272742     0.0207473   -0.0920962   -0.0769494
 -0.0519978    0.0871876    -0.0413988   -0.0214812    0.234664     0.0947284   -0.00232622   0.106283    -0.102641     0.010049     0.105701     0.0800806  -0.230078    0.0128404   -0.102059      0.0487067    0.0895072  -0.0999519   -0.0433882   -0.0577967   -0.121425    -0.0306148   0.0671851     0.0767212    0.0200887    0.0468494
  0.0113033   -0.0230173    -0.00274346  -0.0104826    0.0329929    0.0366315    0.0879502    0.132111    -0.0669407    0.0267642    0.107455    -0.0883652   0.131996   -0.0201523    0.029936     -0.00946164   0.0199181   0.0626537   -0.0125039   -0.0995021    0.0192838   -0.090713   -0.0280203     0.0887876    0.0519931    0.0764668
 -0.0517938   -0.0371047    -0.0544158   -0.0192819   -0.0156138   -0.0168406    0.0148301   -0.0984763   -0.0513017   -0.062004    -0.0206429   -0.0519145  -0.032782   -0.050096    -0.0313228    -0.110268     0.0389688  -0.0227698    0.0395428    0.0112278    0.102182    -0.0398415  -0.020746      0.0909338    0.0129766   -0.0792783
 -0.0130462   -0.0801804     0.0242656   -0.0860687   -0.0208763    0.152965    -0.223005     0.0809632   -0.107268     0.0506867   -0.0261631   -0.0305955   0.0709192  -0.0872823    0.160154     -0.24804      0.0175009  -0.0236334    0.0853156    0.0085027    0.033492     0.0554237   0.0484559     0.0477708   -0.00154307   0.158921
 -0.018689    -0.000622764  -0.183555    -0.179768    -0.0264693    0.254638    -0.157836     0.0771158   -0.0816073   -0.0068889   -0.0545323   -0.0447329  -0.123149   -0.0405622    0.000431963   0.171155    -0.110147    0.0406008   -0.0318634    0.197082     0.115518     0.0691352   0.0447746    -0.193285     0.0277117   -0.036547
  0.0407197    0.0818007     0.0915869   -0.11183     -0.0854669    0.252506     0.08587      0.0107648   -0.0334538   -0.13566     -0.0819179    0.107114   -0.120308   -0.0389176    0.0718997    -0.0217953   -0.0108451  -0.00389477  -0.0240186    0.121467    -0.0914418    0.198941    0.0253109     0.0170275   -0.0900854   -0.0591098
 -0.0500897   -0.113879      0.297409     0.0217609    0.140054     0.169152    -0.176223    -0.129178     0.19131     -0.0486918    0.133554     0.182449    0.106971    0.0111878    0.177723     -0.170668     0.263025    0.132463    -0.00324427   0.0578792    0.0793613   -0.0240487  -0.663583      0.139931    -0.139356     0.116857
 -0.0413903   -0.147467      0.313405    -0.071454     0.13645      0.0058691   -0.24178     -0.0527813    0.102597     0.126953     0.112215     0.167385   -0.13804     0.108146    -0.0238709     0.105927    -0.0261753   0.133882     0.0781635    0.0555979   -0.0146909   -0.145562    0.395662      0.134914    -0.183888     0.0339635
  0.0806749   -0.00523466    0.0990417   -0.172943    -0.0951712   -0.0789308    0.0166231    0.147574    -0.0072223   -0.0619283    0.0035624    0.0771893  -0.0963721   0.0734827    0.0119579     0.161129     0.40951     0.065989    -0.166168    -0.0367864    0.0554919    0.0965749  -0.190527      0.0777709   -0.205832    -0.056015
  0.0828232   -0.22654       0.162932     0.512942    -0.0905878   -0.0921847    0.00488169   0.137008    -0.00526229  -0.0657634    0.0119779    0.0316353  -0.0957372   0.0531589    0.0396367     0.13114     -0.275863    0.114943     0.0962318   -0.0392885    0.0555044    0.0847619   0.105499      0.0503093   -0.286759    -0.0390463[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│     13
│     14
│     21
│     25
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 1, average log likelihood -1.084923
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      3
│     13
│     14
│     20
│      ⋮
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 2, average log likelihood -1.073891
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      5
│     13
│     14
│     21
│     25
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 3, average log likelihood -1.082044
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      3
│      6
│     13
│     14
│      ⋮
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.071141
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      5
│     13
│     14
│     21
│     25
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.084289
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      3
│     13
│     14
│     20
│      ⋮
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.073157
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      5
│     13
│     14
│     21
│     25
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.080216
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      3
│      6
│     13
│     14
│      ⋮
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.069227
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      5
│     13
│     14
│     21
│     25
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.081804
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      3
│      5
│     13
│     14
│      ⋮
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.070862
┌ Info: EM with 100000 data points 10 iterations avll -1.070862
└ 59.0 data points per parameter
kind diag, method kmeans
[ Info: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.930986e+05
      1       6.839811e+05      -2.091175e+05 |       32
      2       6.602709e+05      -2.371021e+04 |       32
      3       6.425896e+05      -1.768132e+04 |       32
      4       6.290151e+05      -1.357446e+04 |       32
      5       6.194898e+05      -9.525344e+03 |       32
      6       6.127791e+05      -6.710636e+03 |       32
      7       6.092159e+05      -3.563177e+03 |       32
      8       6.077072e+05      -1.508728e+03 |       32
      9       6.068465e+05      -8.607099e+02 |       32
     10       6.059994e+05      -8.471491e+02 |       32
     11       6.051769e+05      -8.224157e+02 |       32
     12       6.044696e+05      -7.073054e+02 |       32
     13       6.039069e+05      -5.627218e+02 |       32
     14       6.035419e+05      -3.649732e+02 |       32
     15       6.032954e+05      -2.465278e+02 |       32
     16       6.030926e+05      -2.027867e+02 |       32
     17       6.029067e+05      -1.859609e+02 |       32
     18       6.027536e+05      -1.530853e+02 |       32
     19       6.026171e+05      -1.364656e+02 |       32
     20       6.024746e+05      -1.425143e+02 |       32
     21       6.023113e+05      -1.632778e+02 |       32
     22       6.020764e+05      -2.348819e+02 |       32
     23       6.017652e+05      -3.112254e+02 |       32
     24       6.014368e+05      -3.284094e+02 |       31
     25       6.011010e+05      -3.357665e+02 |       32
     26       6.008402e+05      -2.608121e+02 |       32
     27       6.006831e+05      -1.570961e+02 |       32
     28       6.005403e+05      -1.428243e+02 |       32
     29       6.003666e+05      -1.737448e+02 |       32
     30       6.001775e+05      -1.890997e+02 |       32
     31       5.999669e+05      -2.105690e+02 |       32
     32       5.997388e+05      -2.281260e+02 |       32
     33       5.995314e+05      -2.073985e+02 |       32
     34       5.993773e+05      -1.541093e+02 |       32
     35       5.992442e+05      -1.330630e+02 |       32
     36       5.991335e+05      -1.107277e+02 |       32
     37       5.990202e+05      -1.132288e+02 |       32
     38       5.989017e+05      -1.185238e+02 |       32
     39       5.987770e+05      -1.247415e+02 |       31
     40       5.986718e+05      -1.051711e+02 |       31
     41       5.985979e+05      -7.389776e+01 |       31
     42       5.985423e+05      -5.559483e+01 |       31
     43       5.985019e+05      -4.038404e+01 |       31
     44       5.984719e+05      -3.006666e+01 |       30
     45       5.984480e+05      -2.390616e+01 |       32
     46       5.984277e+05      -2.028463e+01 |       29
     47       5.984100e+05      -1.764635e+01 |       30
     48       5.983985e+05      -1.154967e+01 |       29
     49       5.983900e+05      -8.452720e+00 |       26
     50       5.983831e+05      -6.870478e+00 |       24
K-means terminated without convergence after 50 iterations (objv = 598383.1483204457)
┌ Info: K-means with 32000 data points using 50 iterations
└ 37.0 data points per parameter
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.317524
[ Info: iteration 2, average log likelihood -1.288772
[ Info: iteration 3, average log likelihood -1.261344
[ Info: iteration 4, average log likelihood -1.229119
[ Info: iteration 5, average log likelihood -1.193400
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     2
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.153645
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      9
│     11
│     16
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.121654
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      7
│     21
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.126169
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     10
│     20
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.112668
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      9
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.107323
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     16
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 11, average log likelihood -1.079752
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      7
│     21
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 12, average log likelihood -1.088755
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     10
│     20
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 13, average log likelihood -1.092419
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 14, average log likelihood -1.081573
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      9
│     11
│     16
│     25
│     27
│     28
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 15, average log likelihood -1.059141
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      7
│     21
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 16, average log likelihood -1.122368
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     10
│     20
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 17, average log likelihood -1.093736
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      9
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 18, average log likelihood -1.084031
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│     11
│     16
│     25
│     29
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 19, average log likelihood -1.068202
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     21
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 20, average log likelihood -1.113479
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     10
│     20
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 21, average log likelihood -1.084418
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 22, average log likelihood -1.096413
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      9
│     11
│     16
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 23, average log likelihood -1.070080
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      7
│     21
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 24, average log likelihood -1.101369
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     10
│     20
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 25, average log likelihood -1.084749
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     9
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 26, average log likelihood -1.097788
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      7
│     11
│     16
│     25
│     28
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 27, average log likelihood -1.063765
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      2
│     21
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 28, average log likelihood -1.109728
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     10
│     20
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 29, average log likelihood -1.090035
[ Info: iteration 30, average log likelihood -1.104989
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      9
│     11
│     16
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 31, average log likelihood -1.069195
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      2
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 32, average log likelihood -1.100381
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      7
│     10
│     20
│     21
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 33, average log likelihood -1.066529
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      9
│     17
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 34, average log likelihood -1.105882
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     16
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 35, average log likelihood -1.096547
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     25
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 36, average log likelihood -1.088061
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     20
│     21
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 37, average log likelihood -1.071926
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      2
│      9
│     10
│     11
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 38, average log likelihood -1.081225
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     16
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 39, average log likelihood -1.100352
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      7
│     25
│     28
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 40, average log likelihood -1.082813
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     17
│     20
│     21
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 41, average log likelihood -1.085313
[ Info: iteration 42, average log likelihood -1.114590
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     10
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 43, average log likelihood -1.079245
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     16
│     25
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 44, average log likelihood -1.074568
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     17
│     20
│     21
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 45, average log likelihood -1.084363
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      9
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 46, average log likelihood -1.099669
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      7
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 47, average log likelihood -1.074133
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     10
│     11
│     16
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 48, average log likelihood -1.052979
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      9
│     17
│     20
│     21
│     25
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 49, average log likelihood -1.065289
[ Info: iteration 50, average log likelihood -1.108130
┌ Info: EM with 100000 data points 50 iterations avll -1.108130
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
  0.0536854   -0.0939585   -0.00279051  -0.0352344   -0.199372    -0.0305568     0.0450537   -0.0497065   -0.114541   -0.0936369   -0.0316707   -0.0909598    0.0843118   -0.00231776   0.109623     0.0557481   -0.0864278    0.143079     -0.0156153    0.0193935    0.0560143   -0.100214    0.0844589   -0.0475471    0.108348    -0.0578081
 -0.0315577    0.0077498    0.205352     0.178555    -0.14958      0.0092744     0.0846186    0.0212962    0.14        0.146884    -0.0694016    0.13925     -0.199246     0.00300633  -0.00989466  -0.318905     0.100044     0.0541416    -0.211361    -0.167632     0.00730847   0.173035   -0.170779    -0.0288357    0.0554328    0.143521
 -0.0548425   -0.182921     0.069187     0.00618642  -0.0795366   -0.0805329     0.103545    -0.145413    -0.0852556  -0.0186169    0.00743613  -0.0823762    0.0981706    0.00992182  -0.0727124   -0.0344145    0.12657     -0.0654915     0.00981561  -0.0478618    0.00768769  -0.179576   -0.101013     0.211277    -0.0241067   -0.0783353
 -0.0134129   -0.079809     0.0237687   -0.0856544   -0.0198609    0.154022     -0.223007     0.0793914   -0.107572    0.0508548   -0.02605     -0.0306617    0.0692408   -0.0889523    0.157948    -0.247183     0.0174545   -0.024708      0.0841001    0.00736632   0.0351175    0.0551164   0.0474987    0.0488288   -0.00165164   0.154534
 -0.112886    -0.0178445    0.137459    -0.0506715   -0.0389606    0.014744      0.0592339   -0.0825302    0.0550021  -0.0366785   -0.029625    -0.133829    -0.0363632    0.1506      -0.0461508    0.00617768   0.0627636    0.084113      0.075819    -0.00647558  -0.0794552    0.107818   -0.0104411    0.0209064   -0.0646134    0.0388067
 -0.0870938   -0.118078     0.0169833   -0.108845     0.123729     0.0229056     0.173173     0.0539087   -0.0308836  -0.00834534   0.122806    -0.121892     0.186574     0.0422982    0.0139036   -0.0654898    0.00641064   0.00850829   -0.0982422   -0.236145    -0.0226212   -0.0390701   0.0806091    0.181625     0.0805872    0.0874733
  0.0648351    0.0271277    0.100514     0.333273    -0.062023    -0.140139      0.0154065    0.0339009   -0.0647373  -0.0718639    0.0978456    0.0119223   -0.056956     0.0390689   -0.00852296   0.0694796    0.155595    -0.000866132   0.0324144   -0.00292528   0.0262607    0.0808812  -0.0114481   -0.11222     -0.161651    -0.0974136
  0.173276     0.137381    -0.148784    -0.00364226   0.0897841    0.0118905    -0.0571485   -0.0536806   -0.14234     0.124301     0.123217     0.135629     0.0788516   -0.153752    -0.0332342    0.0198274    0.0737258    0.100055      0.152983     0.036376     0.0280933    0.0718674  -0.0820523   -0.00107589   0.0410345   -0.0996515
  0.0649315    0.114504     0.0887148   -0.175631     0.01623     -0.134895     -0.0769638   -0.00108281  -0.0113994   0.0136211   -0.0207398   -0.156476     0.0244176   -0.0229597   -0.168245    -0.0350493   -0.0298335    0.0698493    -0.100204    -0.16304      0.0516508    0.0197938   0.130414    -0.0262325   -0.0425017   -0.0819104
 -0.0888374    0.0892651   -0.0163429    0.0168426    0.00761291  -0.0721248    -0.0614258   -0.105195    -0.0775028   0.0509203   -0.0886147    0.00664743  -0.186548    -0.136907    -0.00538263  -0.151099     0.0807598    0.0134955     0.0155171    0.0220017    0.0454907   -0.0176533   0.0172832    0.0889508   -0.0264905   -0.211084
 -0.0109028    0.00729964  -0.176599    -0.199581    -0.0303076    0.244352     -0.151952     0.0607328   -0.0690789  -0.0119863   -0.0531287   -0.0618357   -0.117265    -0.0411739   -0.00554674   0.162936    -0.108869     0.0318322    -0.0354067    0.189996     0.106064     0.0620621   0.042625    -0.178051     0.0321862   -0.0364949
 -0.0527124    0.0856721   -0.040924    -0.0255129    0.242629     0.0952445    -0.00152397   0.106536    -0.0984645   0.0013746    0.102813     0.0782376   -0.23554      0.00965484  -0.104938     0.0463063    0.0833675   -0.0976874    -0.0477586   -0.0584974   -0.136686    -0.031361    0.0687398    0.0800262    0.0170795    0.0472612
 -0.0716962   -0.202272    -0.101611     0.0986478   -0.0837921    0.116449      0.183256    -0.042146    -0.0845884  -0.00231525  -0.0492448    0.158034     0.0492424    0.0960483   -0.0795229    0.0940583    0.0746507    0.204027      0.0835245    0.0914379   -0.0296241    0.291299   -0.117085    -0.0883983   -0.00956261   0.0546954
 -0.0409262    0.0954545   -0.0606525    0.0618071   -0.0616817    0.113345      0.0417683    0.0427004    0.0249467  -0.0875244   -0.0729536   -0.0553021    0.173796    -0.0314777   -0.101236     0.0228051   -0.118259    -0.0646921     0.0433634   -0.0336808   -0.00927304   0.0566921   0.0183852    0.0288088   -0.0624768   -0.0757526
  0.124446    -0.0300987    0.0912818   -0.119512    -0.0304558   -0.025731      0.0534596   -0.0837704   -0.140286    0.176205    -0.216622     0.0649827    0.178363    -0.0416464   -0.0935845   -0.109409     0.0126919   -0.0616872     0.195161    -0.0358115   -0.0438253    0.0797816   0.00790565   0.0267354    0.115256     0.090895
  0.0447433    0.0669154    0.0928874   -0.11787     -0.0757334    0.245963      0.0841991    0.00170697  -0.0260783  -0.140502    -0.0676274    0.0977517   -0.100705    -0.0316488    0.064513    -0.0238087   -0.0101608   -0.00690941   -0.0189313    0.118448    -0.0660787    0.172371    0.00953838   0.017019    -0.0712668   -0.0554669
  0.0364137   -0.121489    -0.09916     -0.0572697   -0.010265     0.0766501     0.114358    -0.0678519    0.025351   -0.169731     0.0636107   -0.142206     0.158617     0.00291891  -0.0674585   -0.0352381   -0.0389137   -0.0373063     0.0211753   -0.0751219    0.152662    -0.0314122  -0.00462884   0.0588128    0.061283     0.09308
  0.0444945    0.00215338  -0.0191405    0.190153    -0.0559228   -0.0768325    -0.123824     0.0637334   -0.0908676   0.0748921   -0.0384335   -0.0157782   -0.0978276    0.0953462   -0.0460954   -0.118158    -0.0183677    0.110864     -0.0461011    0.141124     0.0854102    0.117788    0.0535965    0.0465863   -0.129554    -0.084027
  0.106909    -0.0902036    0.0489402   -0.0889987   -0.00663282   0.139653     -0.0956715    0.211661     0.175249   -0.0115718   -0.0260794   -0.0244342   -0.0816992    0.0884962    0.0779733   -0.0987069    0.182558    -0.0598934    -0.0175835    0.0542755    0.0556184    0.0637711   0.0112528   -0.0651928    0.0196053   -0.0228104
  0.0920316   -0.0203641    0.475253     0.0750169   -0.0429695    0.0460983    -0.109904    -0.0155979   -0.0724661  -0.178865     0.0954707    0.120637     0.0445904    0.0364151    0.354264    -0.00604354   0.0890395   -0.0590696     0.130748    -0.138045    -0.0865204   -0.0273718   0.0652297    0.0770252    0.0242126   -0.173633
  0.0546404    0.0958918    0.0216933    0.0412514    0.125974    -0.0507879    -0.115443    -0.225323     0.139968   -0.0147053   -0.149514     0.0466886   -0.046031     0.195116    -0.0250612    0.161571    -0.257636    -0.056968      0.0956382   -0.100287    -0.101408    -0.0503173  -0.165521    -0.100432     0.165007     0.122367
 -0.0109386   -0.060834    -0.0355629   -0.00861126   0.0366249    0.0651804    -0.099725    -0.00369359   0.0430541   0.00395236  -0.227541    -0.289428    -0.0581881    0.0622561   -0.0206398    0.111239    -0.0306076   -0.175514     -0.0717241   -0.0616321    0.227425    -0.0982017   0.146806     0.0424789   -0.00989205   0.0407678
 -0.0779355   -0.0735574    0.0586373    0.0155218    0.0111679   -0.0942807     0.0218086    0.0356504   -0.0529747   0.189505     0.0250995    0.251778     0.045714    -0.0663536   -0.0914016    0.170604     0.0288487    0.0578879     0.118532     0.0999139    0.0761938    0.132049    0.00440722   0.123418     0.253127    -0.0552192
 -0.0202037   -0.0413385    0.143238     0.161523    -0.0908583   -0.0245777    -0.124507     0.0151931    0.171751    0.0208148   -0.106224     0.023407     0.060447    -0.150828    -0.0439065   -0.0872938    0.00857068  -0.0257912    -0.013273    -0.0747367   -0.132049     0.130886   -0.172484    -0.0563922   -0.0749887    0.0843118
  0.0856868   -0.167878     0.141118     0.269467    -0.0628329   -0.0966272     0.0171316    0.123406     0.0071251  -0.0670536    0.0136045    0.0776203   -0.0802399    0.0751703    0.0141842    0.110256     0.112162     0.0701096    -0.0216938   -0.0455267    0.0594252    0.0808223  -0.0513517    0.0780802   -0.378301    -0.0444137
 -0.043968    -0.136467     0.305423    -0.0167545    0.136483     0.0887591    -0.206251    -0.0903935    0.149069    0.0405057    0.121541     0.175664    -0.0172437    0.0574456    0.0768217   -0.0297757    0.119652     0.133719      0.0377809    0.0536928    0.0319416   -0.0799819  -0.126532     0.137843    -0.162874     0.0755247
  0.0251139    0.102092    -0.494912     0.0532379    0.0621593   -0.0690401    -0.0869615   -0.134032    -0.0836491  -0.0221227    0.162935    -0.0619541   -0.00899414  -0.203717    -0.0220656    0.0326685   -0.0657322   -0.0734857     0.0094776   -0.109996    -0.0539669   -0.17968    -0.0279611    0.0307411    0.012766     0.0724245
  0.0751756   -0.0967753    0.0645016    0.0803957    0.060516     0.000618384   0.219392     0.0209998    0.143413    0.0933983   -0.00910498  -0.210416    -0.262123     0.00464202   0.0471044    0.0936193    0.120801    -0.168994      0.024453     0.127785    -0.0386172    0.19554     0.0458958    0.0260832   -0.0618828   -0.0210625
  0.00510618  -0.198845     0.0593431   -0.106132     0.0232748   -0.122083      0.110011    -0.0935368    0.0371535  -0.0944321    0.060897    -0.0587887    0.15731      0.113605    -0.0406589   -0.00259328   0.04899     -0.0690985     0.0262965    0.0771643   -0.112183    -0.0131472  -0.0542467   -0.0271231    0.156361     0.0388122
 -0.155306    -0.0112285    0.0906686   -0.0914387    0.0110953   -0.134134     -0.0903288    0.0309653    0.0440929  -0.107293    -0.0595246    0.010168     0.0122637    0.186662     0.0585042   -0.1439       0.140946    -0.105353      0.0184629    0.158166    -0.0560647    0.0016125   0.0090615   -0.0980809   -0.11062     -0.0327568
 -0.0315598    0.0525057    0.0984047   -0.0523369   -0.0421979    0.0492617    -0.0786564   -0.107306    -0.146026    0.174596     0.221724     0.0593832   -0.122484    -0.0617297   -0.0546661   -0.0611449    0.0554026   -0.130894      0.0499401   -0.0258943   -0.104066     0.0840281   0.00891458   0.00210321  -0.140381     0.00275919
  0.12637      0.0763451   -0.0106781    0.112329    -0.0740381    0.0440818    -0.0211978    0.187492    -0.103581    0.0578585    0.114786    -0.036224     0.0514724   -0.0941145    0.0599471    0.0604442    0.0352269    0.122816      0.0928394    0.0490764    0.063767    -0.146182   -0.1264      -0.0161078    0.0114599    0.05555[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 1, average log likelihood -1.071428
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│     10
│     11
│     16
│     25
│     27
│     28
│     29
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 2, average log likelihood -1.028827
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      7
│      9
│     16
│     17
│     20
│     21
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 3, average log likelihood -1.024691
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│     10
│     11
│     27
│     28
│     29
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.053511
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     16
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.035849
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      9
│     10
│     11
│     16
│      ⋮
│     28
│     29
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.012267
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      7
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.064094
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│     10
│     11
│     16
│     25
│     27
│     28
│     29
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.022740
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      9
│     16
│     17
│     20
│     21
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.028607
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│     10
│     11
│     27
│     28
│     29
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.048950
┌ Info: EM with 100000 data points 10 iterations avll -1.048950
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
  0.059389    -0.0255267   -0.0594292    0.0828804     0.0202594    -0.0178514   -0.244665     0.0610892    0.120115      0.0465156   -0.017181    -0.0731935  -0.0622312    0.04619      0.0911159   -0.0403676    0.201784    -0.0206205    0.0863479    0.00134189   0.0109569   -0.0447029   -0.0362506   -0.142168     0.0490567   -0.0901622
 -0.130985    -0.0267371   -0.0921136   -0.0193718    -0.18883      -0.105209    -0.15846     -0.0978249   -0.198092      0.0743792   -0.001174    -0.0515492   0.0798864   -0.0487138    0.0114171   -0.00104884  -0.107243    -0.0685357   -0.0168951    0.0542195   -0.168141     0.0736442   -0.0150183   -0.101966     0.123307     0.148521
  0.125666    -0.0101324    0.0773955    0.0250333     0.265403      0.158162     0.00833655   0.0103177   -0.069294      0.0420432   -0.141499     0.130229   -0.0526598    0.189298    -0.253223    -0.0439444   -0.00229719  -0.0660116    0.0539592    0.00401178   0.107566    -0.0384331    0.208282    -0.181662    -0.02845      0.241089
 -0.0880098   -0.0595783    0.0758376   -0.104508      0.00432981   -0.0437239    0.179491    -0.162779    -0.0722389    -0.127649    -0.0101763   -0.11421     0.0600867   -0.21768      0.140907     0.0277617   -0.0823104    0.118092    -0.0694323   -0.0843139   -0.066212    -0.123518     0.247445    -0.167861    -0.103615     0.177386
 -0.0626433    0.00741736   0.087854     0.0407378    -0.0135449    -0.00776753  -0.10744      0.109912    -0.0333734    -0.0449547   -0.0162948    0.0567335  -0.02796      0.0741335    0.0453704    0.0248546   -0.0109828   -0.0854952    0.0326737   -0.0801336    0.0973021   -0.0295697   -0.0770204   -0.02417      0.265718     0.10564
 -0.0504771   -0.250632     0.0116671    0.0354414    -0.119476     -0.0842185    0.120215    -0.0761675   -0.0858003    -0.0680506   -0.119902    -0.12876    -0.0628631   -0.0408734   -0.00915935  -0.0453585    0.0911538    0.218081     0.0701577    0.0952891    0.0409343    0.0545996   -0.12279     -0.0797723    0.00682988  -0.0506882
  0.0442533   -0.18883     -0.08758     -0.032012      0.211407     -0.0984879   -0.0487745   -0.0428365    0.105561      0.0489611    0.0360461    0.233573    0.0615429    0.00573445  -0.0287014    0.111258     0.0749019   -0.0409788    0.0319059    0.239535    -0.146457    -0.0846459   -0.090353    -0.0540713   -0.112028    -0.0401163
 -0.0512738   -0.0199411    0.0171949    0.143773     -0.0511697    -0.125325     0.0399101   -0.0739207   -0.253621      0.0810561    0.0516587   -0.0915958   0.00116283  -0.0203664    0.0736881   -0.0262072   -0.0252313    0.00297603   0.107805     0.049563    -0.038188     0.00106887  -0.00218767  -0.0921455   -0.0125185    0.144014
  0.116691    -0.0864534    0.111879     0.0157051    -0.110303     -0.150925     0.0338229    0.147976     0.0166835    -0.0572158    0.126477    -0.239874    0.168527     0.0503259   -0.22105     -0.0099154   -0.046485    -0.0384347   -0.128786     0.0142337    0.0401926   -0.255465     0.0780421   -0.13399      0.238562    -0.140363
  0.0888188    0.0907325   -0.0735135    0.20739      -0.0997474    -0.00263782  -0.0749716   -0.0614878   -0.144897     -0.0977227    0.179234    -0.0235872   0.0241368    0.128071    -0.047726     0.0799631   -0.126902    -0.132047    -0.0450904    0.145722    -0.120106     0.0356924   -0.0268622    0.0879795    0.10195      0.18967
  0.105526     0.0187017    0.0340551    0.05427       0.0742313    -0.124743    -0.0689522    0.0627768    0.108675      0.0633257   -0.134014    -0.181365    0.118779    -0.141365    -0.0517587   -0.163131    -0.126198    -0.159933    -0.0190977   -0.170107     0.0259029   -0.147925    -0.0635068   -0.0862568    0.123162     0.0296021
 -0.0905964   -0.0128899   -0.186011    -0.0774335     0.000290775  -0.115459     0.00906077  -0.239999     0.104945     -0.0561528    0.0465106   -0.137513   -0.174855     0.113478    -0.00361235   0.0864007   -0.110876     0.0325766   -0.261395    -0.141907    -0.133476    -0.0685695   -0.0617942   -0.0575635    0.140083     0.0832131
 -0.00652324  -0.147169    -0.027671    -0.148247     -0.0611378    -0.0142991    0.0524843    0.157104    -0.000608948   0.0308453    0.0511999    0.101764   -0.00255359  -0.0550295   -0.0584386    0.161224    -0.143211    -0.0152992    0.0210988    0.214564    -0.148222     0.122471    -0.0346325   -0.0485223   -0.106812     0.069324
 -0.109629     0.198422    -0.0795004   -0.079303      0.068847     -0.123994    -0.0713879   -0.14466     -0.273274      0.00853305   0.0402814    0.0225038  -0.0555233    0.0662873   -0.101906     0.0435768    0.0676738   -0.12766     -0.0365566   -0.171017     0.0286078    0.223327    -0.0863112    0.0532782   -0.0447819    0.0271503
  0.0534017   -0.0283151   -0.035317    -0.0868743     0.0883425     0.11303     -0.144579    -0.0615103    0.0877149     0.116264     0.0107565   -0.0170975   0.0318151    0.172735    -0.0703628    0.0163064    0.0749419   -0.00951138  -0.164226    -0.121776     0.103901    -0.0289778   -0.135381     0.124274     0.00248848   0.0434503
 -0.048502    -0.104025    -0.0913011   -0.0817252    -0.0570501    -0.0166977   -0.0254024    0.00738802  -0.0340698     0.100539     0.0304064   -0.155468    0.0945431    0.238729    -0.0618102   -0.0380574   -0.121703     0.173705    -0.0657209   -0.0485272    0.0517094   -0.210823    -0.120595    -0.0406757    0.0368181   -0.0270215
  0.260285    -0.0094317    0.0301064   -0.000863899   0.18966       0.0809409   -0.0933671    0.072585    -0.181735      0.0657227   -0.119708    -0.0022058   0.113126    -0.104378     0.0314817    0.113036     0.115873    -0.00951676  -0.167728    -0.0592151   -0.0757624    0.118457     0.111846     0.00211082  -0.0698812   -0.0718002
  0.0780603   -0.135401    -0.103437    -0.151524     -0.0526767     0.0429567    0.111387    -0.0398803    0.0240552    -0.150652    -0.024989     0.0663298  -0.0446241    0.0251748   -0.0179133   -0.0649673   -0.0227634   -0.0540542    0.0217431    0.0642513    0.0024835    0.065265    -0.156839    -0.0809557    0.168634    -0.0445306
  0.0452255    0.0755773   -0.101106    -0.064216      0.153808     -0.00533111  -0.0676279   -0.00160386   0.0233581    -0.0585703    0.129246    -0.0317809  -0.180811    -0.0923468    0.0293904   -0.0669661    0.127237    -0.0532588   -0.179338    -0.0119746   -0.023451     0.0236974   -0.0249269   -0.149998    -0.036534    -0.0675304
  0.131333     0.159855     0.0307091    0.0719286     0.0881406     0.0866219   -0.0677927    0.14082      0.27303      -0.107931    -0.0508196   -0.0903222   0.0947482   -0.0190268   -0.0692412   -0.0775332    0.0434567    0.019391     0.0496522   -0.0826982    0.0198242   -0.0850253    0.071825    -0.10159     -0.0627973   -0.064991
 -0.075275    -0.00122754  -0.00976714  -0.201493     -0.0277255     0.0808756    0.0251464   -0.131755     0.0359133     0.00215144   0.0958494   -0.152679   -0.0529469    0.0235831    0.0249955   -0.127374    -0.107875     0.0414472    0.0701612   -0.0257517   -0.0847364   -0.0400014    0.0100082   -0.115894    -0.0520625    0.0597987
  0.0943784   -0.0769643   -0.0936174    0.12806       0.178905     -0.00219733  -0.016745     0.181651     0.0848859    -0.0751553    0.0724507   -0.0767209   0.201736    -0.00366991   0.104856     0.12758     -0.0228012   -0.0404503   -0.0994953    0.084201     0.087142     0.0706171   -0.0186507    0.0153565    0.0196461    0.162634
  0.0668236   -0.153191     0.00513711  -0.0698707     0.038715      0.0227691   -0.189282    -0.079476    -0.101529      0.0373065   -4.56523e-5  -0.131526    0.0998643   -0.0939103   -0.158994    -0.0740893   -0.0827361   -0.163481     0.112646     0.00788694  -0.00465677  -0.121967     0.0484224    0.0679811   -0.0894628    0.193394
  0.0462206    0.0565282    0.0267636    0.092393      0.0337004     0.0698203   -0.22149      0.0351874   -0.176278     -0.0169038    0.0175748   -0.0327317   0.0865703   -0.19557     -0.00654803  -0.0637039    0.0711511   -0.00253622  -0.0813983    0.112639     0.289443     0.0534951   -0.0333139   -0.00347958   0.00872308  -0.189004
  0.175298    -0.179891    -0.0449863    0.0664615     0.082124      0.142735     0.0403544    0.0863253   -0.0390832    -0.00193993   0.072038    -0.0667969   0.132774    -0.103155    -0.0755832    0.0400148    0.166088    -0.0304681   -0.116138     0.226143    -0.0170384    0.0761143    0.0785112    0.0274629    0.0292766    0.0643786
 -0.0889475    0.176654     0.305074     0.0316755     0.0871208    -0.0665339    0.0498727   -0.0455194    0.120669      0.0865405    0.0985531   -0.229465   -0.0529993    0.210903    -0.0265323   -0.082896    -0.11967     -0.228233     0.0999472   -0.171958    -0.108184    -0.298397     0.114489     0.0104589   -0.120322    -0.0605135
  0.0379335    0.15406      0.16156     -0.116964     -0.102433      0.0448687   -0.0725316   -0.0841268   -0.166945      0.0493045    0.108216    -0.0537636   0.105949    -0.0373302    0.0795169   -0.109779     0.0412618   -0.0227043    0.0281775   -0.0306168   -0.138312     0.0132461    0.0349851   -0.0041045    0.0405993   -0.0326211
  0.0696361   -0.0161869    0.04605      0.0341354     0.03892       0.111632    -0.0079913    0.0476208   -0.00759067   -0.0436233    0.0708154   -0.127911   -0.115463    -0.0510354   -0.0734417    0.0356299    0.0163337    0.0126158   -0.0604417   -0.209641     0.0170621   -0.0265202   -0.166189    -0.0421704    0.0362746   -0.175795
  0.0298926    0.0142304    0.172925    -0.251518     -0.00331915   -0.168463    -0.00959707   0.115864    -0.0475798     0.0874755   -0.03321     -0.263153   -0.100235     0.0268074   -0.0424777    0.0610183    0.1204       0.0257071   -0.0421177    0.0550349    0.0497949    0.117545     0.1411       0.156023     0.0388711   -0.150113
 -0.0561715    0.0977441    0.125198     0.0461238    -0.113489      0.00537438  -0.146971    -0.0390685    0.0803692    -0.0141884    0.0756184    0.0912636  -0.165242    -0.146976    -0.14927      0.00784071   0.127895     0.0517998    0.00219876   0.21525     -0.0216291   -0.0391554   -0.0783403    0.0396453    0.019629     0.218584
 -0.0153078   -0.087815     0.032548    -0.0619134    -0.0226202     0.0293017    0.0071273   -0.0589679   -0.189171      0.161308     0.0486819    0.0018113   0.104677    -0.048108    -0.0647051   -0.0322688   -0.0564141   -0.0985613   -0.124805     0.124996    -0.0591331    0.0604142    0.00850386   0.0681218   -0.12484     -0.0722333
  0.022914     0.194544     0.00799523   0.042939      0.00441179    0.1445       0.10451     -0.170782     0.0104697     0.0625579   -0.120787     0.0584047  -0.15763      0.0888483   -0.0199854    0.170868    -0.223755    -0.0532559    0.0458054    0.158669    -0.264197    -0.0170932   -0.0648229    0.0683625    0.00949062  -0.0221399kind full, method split
┌ Info: 0: avll = 
└   tll[1] = -1.4194973360824075
[ Info: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.419515
[ Info: iteration 2, average log likelihood -1.419469
[ Info: iteration 3, average log likelihood -1.419438
[ Info: iteration 4, average log likelihood -1.419402
[ Info: iteration 5, average log likelihood -1.419358
[ Info: iteration 6, average log likelihood -1.419300
[ Info: iteration 7, average log likelihood -1.419210
[ Info: iteration 8, average log likelihood -1.419045
[ Info: iteration 9, average log likelihood -1.418695
[ Info: iteration 10, average log likelihood -1.417985
[ Info: iteration 11, average log likelihood -1.416832
[ Info: iteration 12, average log likelihood -1.415582
[ Info: iteration 13, average log likelihood -1.414752
[ Info: iteration 14, average log likelihood -1.414383
[ Info: iteration 15, average log likelihood -1.414247
[ Info: iteration 16, average log likelihood -1.414199
[ Info: iteration 17, average log likelihood -1.414180
[ Info: iteration 18, average log likelihood -1.414173
[ Info: iteration 19, average log likelihood -1.414170
[ Info: iteration 20, average log likelihood -1.414169
[ Info: iteration 21, average log likelihood -1.414168
[ Info: iteration 22, average log likelihood -1.414168
[ Info: iteration 23, average log likelihood -1.414168
[ Info: iteration 24, average log likelihood -1.414168
[ Info: iteration 25, average log likelihood -1.414167
[ Info: iteration 26, average log likelihood -1.414167
[ Info: iteration 27, average log likelihood -1.414167
[ Info: iteration 28, average log likelihood -1.414167
[ Info: iteration 29, average log likelihood -1.414167
[ Info: iteration 30, average log likelihood -1.414167
[ Info: iteration 31, average log likelihood -1.414167
[ Info: iteration 32, average log likelihood -1.414166
[ Info: iteration 33, average log likelihood -1.414166
[ Info: iteration 34, average log likelihood -1.414166
[ Info: iteration 35, average log likelihood -1.414166
[ Info: iteration 36, average log likelihood -1.414166
[ Info: iteration 37, average log likelihood -1.414166
[ Info: iteration 38, average log likelihood -1.414166
[ Info: iteration 39, average log likelihood -1.414166
[ Info: iteration 40, average log likelihood -1.414166
[ Info: iteration 41, average log likelihood -1.414166
[ Info: iteration 42, average log likelihood -1.414166
[ Info: iteration 43, average log likelihood -1.414166
[ Info: iteration 44, average log likelihood -1.414166
[ Info: iteration 45, average log likelihood -1.414166
[ Info: iteration 46, average log likelihood -1.414166
[ Info: iteration 47, average log likelihood -1.414166
[ Info: iteration 48, average log likelihood -1.414166
[ Info: iteration 49, average log likelihood -1.414166
[ Info: iteration 50, average log likelihood -1.414166
┌ Info: EM with 100000 data points 50 iterations avll -1.414166
└ 952.4 data points per parameter
┌ Info: 1
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4195154827606373
│     -1.419468599923863
│      ⋮
└     -1.414165701667603
[ Info: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.414181
[ Info: iteration 2, average log likelihood -1.414118
[ Info: iteration 3, average log likelihood -1.414065
[ Info: iteration 4, average log likelihood -1.414001
[ Info: iteration 5, average log likelihood -1.413926
[ Info: iteration 6, average log likelihood -1.413841
[ Info: iteration 7, average log likelihood -1.413754
[ Info: iteration 8, average log likelihood -1.413674
[ Info: iteration 9, average log likelihood -1.413606
[ Info: iteration 10, average log likelihood -1.413550
[ Info: iteration 11, average log likelihood -1.413505
[ Info: iteration 12, average log likelihood -1.413466
[ Info: iteration 13, average log likelihood -1.413431
[ Info: iteration 14, average log likelihood -1.413398
[ Info: iteration 15, average log likelihood -1.413366
[ Info: iteration 16, average log likelihood -1.413337
[ Info: iteration 17, average log likelihood -1.413309
[ Info: iteration 18, average log likelihood -1.413284
[ Info: iteration 19, average log likelihood -1.413260
[ Info: iteration 20, average log likelihood -1.413238
[ Info: iteration 21, average log likelihood -1.413217
[ Info: iteration 22, average log likelihood -1.413198
[ Info: iteration 23, average log likelihood -1.413181
[ Info: iteration 24, average log likelihood -1.413165
[ Info: iteration 25, average log likelihood -1.413150
[ Info: iteration 26, average log likelihood -1.413137
[ Info: iteration 27, average log likelihood -1.413125
[ Info: iteration 28, average log likelihood -1.413114
[ Info: iteration 29, average log likelihood -1.413105
[ Info: iteration 30, average log likelihood -1.413096
[ Info: iteration 31, average log likelihood -1.413089
[ Info: iteration 32, average log likelihood -1.413082
[ Info: iteration 33, average log likelihood -1.413076
[ Info: iteration 34, average log likelihood -1.413071
[ Info: iteration 35, average log likelihood -1.413066
[ Info: iteration 36, average log likelihood -1.413062
[ Info: iteration 37, average log likelihood -1.413058
[ Info: iteration 38, average log likelihood -1.413054
[ Info: iteration 39, average log likelihood -1.413051
[ Info: iteration 40, average log likelihood -1.413048
[ Info: iteration 41, average log likelihood -1.413045
[ Info: iteration 42, average log likelihood -1.413043
[ Info: iteration 43, average log likelihood -1.413040
[ Info: iteration 44, average log likelihood -1.413038
[ Info: iteration 45, average log likelihood -1.413036
[ Info: iteration 46, average log likelihood -1.413034
[ Info: iteration 47, average log likelihood -1.413032
[ Info: iteration 48, average log likelihood -1.413030
[ Info: iteration 49, average log likelihood -1.413028
[ Info: iteration 50, average log likelihood -1.413027
┌ Info: EM with 100000 data points 50 iterations avll -1.413027
└ 473.9 data points per parameter
┌ Info: 2
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4141806219406605
│     -1.4141179911563182
│      ⋮
└     -1.4130267126223262
[ Info: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.413038
[ Info: iteration 2, average log likelihood -1.412987
[ Info: iteration 3, average log likelihood -1.412945
[ Info: iteration 4, average log likelihood -1.412896
[ Info: iteration 5, average log likelihood -1.412835
[ Info: iteration 6, average log likelihood -1.412760
[ Info: iteration 7, average log likelihood -1.412673
[ Info: iteration 8, average log likelihood -1.412576
[ Info: iteration 9, average log likelihood -1.412475
[ Info: iteration 10, average log likelihood -1.412375
[ Info: iteration 11, average log likelihood -1.412283
[ Info: iteration 12, average log likelihood -1.412199
[ Info: iteration 13, average log likelihood -1.412126
[ Info: iteration 14, average log likelihood -1.412064
[ Info: iteration 15, average log likelihood -1.412010
[ Info: iteration 16, average log likelihood -1.411964
[ Info: iteration 17, average log likelihood -1.411923
[ Info: iteration 18, average log likelihood -1.411886
[ Info: iteration 19, average log likelihood -1.411852
[ Info: iteration 20, average log likelihood -1.411822
[ Info: iteration 21, average log likelihood -1.411793
[ Info: iteration 22, average log likelihood -1.411767
[ Info: iteration 23, average log likelihood -1.411743
[ Info: iteration 24, average log likelihood -1.411721
[ Info: iteration 25, average log likelihood -1.411701
[ Info: iteration 26, average log likelihood -1.411683
[ Info: iteration 27, average log likelihood -1.411666
[ Info: iteration 28, average log likelihood -1.411651
[ Info: iteration 29, average log likelihood -1.411637
[ Info: iteration 30, average log likelihood -1.411624
[ Info: iteration 31, average log likelihood -1.411612
[ Info: iteration 32, average log likelihood -1.411601
[ Info: iteration 33, average log likelihood -1.411590
[ Info: iteration 34, average log likelihood -1.411580
[ Info: iteration 35, average log likelihood -1.411571
[ Info: iteration 36, average log likelihood -1.411563
[ Info: iteration 37, average log likelihood -1.411554
[ Info: iteration 38, average log likelihood -1.411547
[ Info: iteration 39, average log likelihood -1.411539
[ Info: iteration 40, average log likelihood -1.411532
[ Info: iteration 41, average log likelihood -1.411526
[ Info: iteration 42, average log likelihood -1.411519
[ Info: iteration 43, average log likelihood -1.411513
[ Info: iteration 44, average log likelihood -1.411507
[ Info: iteration 45, average log likelihood -1.411502
[ Info: iteration 46, average log likelihood -1.411496
[ Info: iteration 47, average log likelihood -1.411491
[ Info: iteration 48, average log likelihood -1.411486
[ Info: iteration 49, average log likelihood -1.411482
[ Info: iteration 50, average log likelihood -1.411477
┌ Info: EM with 100000 data points 50 iterations avll -1.411477
└ 236.4 data points per parameter
┌ Info: 3
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4130383561695903
│     -1.4129872458563724
│      ⋮
└     -1.4114774441369848
[ Info: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.411482
[ Info: iteration 2, average log likelihood -1.411428
[ Info: iteration 3, average log likelihood -1.411378
[ Info: iteration 4, average log likelihood -1.411319
[ Info: iteration 5, average log likelihood -1.411246
[ Info: iteration 6, average log likelihood -1.411159
[ Info: iteration 7, average log likelihood -1.411058
[ Info: iteration 8, average log likelihood -1.410949
[ Info: iteration 9, average log likelihood -1.410837
[ Info: iteration 10, average log likelihood -1.410730
[ Info: iteration 11, average log likelihood -1.410630
[ Info: iteration 12, average log likelihood -1.410539
[ Info: iteration 13, average log likelihood -1.410457
[ Info: iteration 14, average log likelihood -1.410383
[ Info: iteration 15, average log likelihood -1.410317
[ Info: iteration 16, average log likelihood -1.410258
[ Info: iteration 17, average log likelihood -1.410205
[ Info: iteration 18, average log likelihood -1.410157
[ Info: iteration 19, average log likelihood -1.410113
[ Info: iteration 20, average log likelihood -1.410072
[ Info: iteration 21, average log likelihood -1.410035
[ Info: iteration 22, average log likelihood -1.409999
[ Info: iteration 23, average log likelihood -1.409965
[ Info: iteration 24, average log likelihood -1.409933
[ Info: iteration 25, average log likelihood -1.409901
[ Info: iteration 26, average log likelihood -1.409871
[ Info: iteration 27, average log likelihood -1.409842
[ Info: iteration 28, average log likelihood -1.409813
[ Info: iteration 29, average log likelihood -1.409785
[ Info: iteration 30, average log likelihood -1.409759
[ Info: iteration 31, average log likelihood -1.409733
[ Info: iteration 32, average log likelihood -1.409708
[ Info: iteration 33, average log likelihood -1.409685
[ Info: iteration 34, average log likelihood -1.409662
[ Info: iteration 35, average log likelihood -1.409640
[ Info: iteration 36, average log likelihood -1.409620
[ Info: iteration 37, average log likelihood -1.409600
[ Info: iteration 38, average log likelihood -1.409582
[ Info: iteration 39, average log likelihood -1.409564
[ Info: iteration 40, average log likelihood -1.409547
[ Info: iteration 41, average log likelihood -1.409531
[ Info: iteration 42, average log likelihood -1.409516
[ Info: iteration 43, average log likelihood -1.409501
[ Info: iteration 44, average log likelihood -1.409488
[ Info: iteration 45, average log likelihood -1.409475
[ Info: iteration 46, average log likelihood -1.409462
[ Info: iteration 47, average log likelihood -1.409450
[ Info: iteration 48, average log likelihood -1.409439
[ Info: iteration 49, average log likelihood -1.409428
[ Info: iteration 50, average log likelihood -1.409417
┌ Info: EM with 100000 data points 50 iterations avll -1.409417
└ 118.1 data points per parameter
┌ Info: 4
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4114817553732883
│     -1.411428094131539
│      ⋮
└     -1.4094173910378318
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.409416
[ Info: iteration 2, average log likelihood -1.409353
[ Info: iteration 3, average log likelihood -1.409291
[ Info: iteration 4, average log likelihood -1.409217
[ Info: iteration 5, average log likelihood -1.409123
[ Info: iteration 6, average log likelihood -1.409006
[ Info: iteration 7, average log likelihood -1.408865
[ Info: iteration 8, average log likelihood -1.408706
[ Info: iteration 9, average log likelihood -1.408539
[ Info: iteration 10, average log likelihood -1.408372
[ Info: iteration 11, average log likelihood -1.408214
[ Info: iteration 12, average log likelihood -1.408067
[ Info: iteration 13, average log likelihood -1.407934
[ Info: iteration 14, average log likelihood -1.407815
[ Info: iteration 15, average log likelihood -1.407709
[ Info: iteration 16, average log likelihood -1.407613
[ Info: iteration 17, average log likelihood -1.407528
[ Info: iteration 18, average log likelihood -1.407451
[ Info: iteration 19, average log likelihood -1.407382
[ Info: iteration 20, average log likelihood -1.407320
[ Info: iteration 21, average log likelihood -1.407265
[ Info: iteration 22, average log likelihood -1.407215
[ Info: iteration 23, average log likelihood -1.407170
[ Info: iteration 24, average log likelihood -1.407129
[ Info: iteration 25, average log likelihood -1.407093
[ Info: iteration 26, average log likelihood -1.407059
[ Info: iteration 27, average log likelihood -1.407029
[ Info: iteration 28, average log likelihood -1.407001
[ Info: iteration 29, average log likelihood -1.406975
[ Info: iteration 30, average log likelihood -1.406951
[ Info: iteration 31, average log likelihood -1.406929
[ Info: iteration 32, average log likelihood -1.406909
[ Info: iteration 33, average log likelihood -1.406890
[ Info: iteration 34, average log likelihood -1.406872
[ Info: iteration 35, average log likelihood -1.406855
[ Info: iteration 36, average log likelihood -1.406839
[ Info: iteration 37, average log likelihood -1.406825
[ Info: iteration 38, average log likelihood -1.406811
[ Info: iteration 39, average log likelihood -1.406798
[ Info: iteration 40, average log likelihood -1.406785
[ Info: iteration 41, average log likelihood -1.406774
[ Info: iteration 42, average log likelihood -1.406763
[ Info: iteration 43, average log likelihood -1.406752
[ Info: iteration 44, average log likelihood -1.406742
[ Info: iteration 45, average log likelihood -1.406732
[ Info: iteration 46, average log likelihood -1.406723
[ Info: iteration 47, average log likelihood -1.406714
[ Info: iteration 48, average log likelihood -1.406705
[ Info: iteration 49, average log likelihood -1.406697
[ Info: iteration 50, average log likelihood -1.406689
┌ Info: EM with 100000 data points 50 iterations avll -1.406689
└ 59.0 data points per parameter
┌ Info: 5
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4094156021909696
│     -1.4093528955935568
│      ⋮
└     -1.4066886782032466
┌ Info: Total log likelihood: 
│   tll =
│    251-element Array{Float64,1}:
│     -1.4194973360824075
│     -1.4195154827606373
│     -1.419468599923863
│     -1.4194377703504348
│      ⋮
│     -1.4067053107634306
│     -1.406696876732237
└     -1.4066886782032466
32×26 Array{Float64,2}:
 -0.0689482  -0.0822302   -0.0915792   -0.211253    0.0500844    0.119076     0.109378    0.0838367  -0.174449    -0.29557    -0.188751   -0.167441     0.0949809   -0.14417     -0.238406    -0.202797   -0.109735    -0.237583    0.176041    -0.0658809    0.0799635   -0.130629    -0.150868    0.00111727   0.0572191  -0.0481346
  0.126797   -0.0235178    0.117041     0.0967464  -0.00162065  -0.131069    -0.0852611  -0.0423614  -0.0408187    0.143948    0.143906    0.0141721   -0.0277013   -0.00158499   0.00712554   0.0317184   0.00657293   0.114623   -0.219843     0.0466771   -0.115056    -0.161665     0.024098   -0.119737    -0.0250974   0.0509954
  0.256888    0.18817     -0.120401    -0.131148    0.0784456   -0.33428      0.0408837  -0.0916174   0.252085     0.132577    0.0709203  -0.255777    -0.0466708    0.149346     0.36377      0.504992   -0.0567782   -0.241556    0.336784     0.0604635   -0.207942     0.32693      0.169231    0.395123     0.312332    0.0229346
  0.0334721   0.329934    -0.291828    -0.149039    0.243075     0.41681     -0.196625    0.255447    0.0819745    0.177274   -0.352627   -0.215471     0.132121    -0.032709     0.0906649    0.0882081  -0.0328367    0.122079   -0.0671844   -0.0556829    0.138443     0.718407    -0.146938    0.104329     0.0617406   0.164134
 -0.116818   -0.0932664   -0.0651307    0.150557   -0.0703935   -0.168387     0.270205   -0.49555    -0.0301847   -0.148891    0.434426    0.644227    -0.491906     0.33819      0.0543       0.0600103   0.0485711   -0.246178   -0.00530879   0.136562     0.0393858   -0.382843     0.486948   -0.401193     0.24549    -0.50356
 -0.237987    0.0249928   -0.088872    -0.115603   -0.180765     0.488004     0.376575   -0.325057    0.108288    -0.549879    0.163459    0.285833     0.892301     0.309456    -0.104789     0.328863    0.168114    -0.362772   -0.343377     0.158798    -0.0736662   -0.0908588    0.633998   -0.0473093    0.19396     0.0296798
 -0.07982     0.101563     0.11215      0.0850893  -0.111009    -0.171055    -0.0654971  -0.483435    0.46236     -0.0199761  -0.0369983  -0.373849     0.116296    -0.318891    -0.0268      -0.246427    0.171145     0.298264   -0.0506832    0.0323739    0.519898    -0.304506    -0.477727   -0.553464    -0.270365   -0.0891316
  0.0220508   0.283717     0.376185    -0.50417     0.287739     0.353281     0.155617   -0.595386    0.269149     0.173235   -0.0347678   0.858614     0.082769    -0.53918      0.193918    -0.266017    0.0157863   -0.0186985  -0.0813479   -0.0557756    0.222939    -0.134188    -0.3719     -0.0622895    0.0924211  -0.0477635
 -0.272078   -0.188219     0.210124    -0.222349    0.0895141    0.279557    -1.06751     0.597427    0.0819156   -0.250189   -0.150486   -0.0763868    0.285449     0.444043    -0.695012     0.112151   -0.645501    -0.511386    0.318832    -0.0542063   -0.593548    -0.252814     0.290137   -0.219692     0.754246   -0.648956
 -1.61163    -0.154334     0.00371703  -0.10844    -0.312344     0.520327    -0.615006    0.492341    0.10274      0.0199161  -0.428923   -0.393636     0.679368     0.892286    -0.133899    -0.509076    0.263225    -0.240153    0.105325    -0.577381     0.178757    -0.343381    -0.081853   -0.128655    -0.513061    0.194597
  0.41403    -0.645405     0.133213     0.11625    -0.0244289    0.051932    -0.40622     0.013553   -0.00929538  -0.673247    0.223959   -0.0218821   -0.0561767    0.309303     0.142265    -0.420046    0.201412     0.205388   -0.329504    -0.781585     0.00501042   0.415512    -0.148528   -0.377241     0.194836   -0.0569823
 -0.0120782  -0.0206365    0.199272    -0.400888    0.294689    -0.133334    -0.868911    0.347428    0.134163     0.534305    0.09645     0.0365968   -0.478857     0.622591     0.398256    -0.271436   -0.0739107   -0.164152   -0.46039     -0.212682    -0.194988     0.16122     -0.660071    0.153425     0.218751    0.0618335
 -0.121653   -0.360676    -0.139796     0.349458   -0.027918    -0.508533    -0.204635    0.281599    0.139201    -0.0793871   0.472297   -0.527879    -0.00767367   0.883605    -0.41399      0.313026    0.323409    -0.101804   -0.217326     0.0749274   -0.227345    -0.315843     0.230857    0.124031    -0.23505     0.18311
  0.0562094  -0.275123     0.612972     0.332517   -0.615837    -0.0412522   -0.521427   -0.213023    0.4059       0.0990074   0.457006   -0.23035      0.0637149    0.122448     0.26814      0.53158    -0.0241069    0.409773    0.203874     0.154982    -0.148348    -0.152841    -0.0519095   0.291721    -0.21785     0.600461
  0.409373   -0.545944     0.0544227    0.452943   -0.34698     -0.122035    -0.0224265   0.302244   -0.289045     0.315774    0.281717   -0.00107128  -0.212258    -0.462539     0.0204809   -0.347425   -0.472151    -0.115035    0.0826099   -0.215005    -0.559025    -0.381625    -0.177682    0.334675    -0.557894    0.00150768
 -0.087726   -0.100477     0.544761     0.159396   -0.513674     0.278538     0.133067    0.49203    -0.668408     0.120176    0.103537    0.0609509    0.0164482    0.0579621   -0.224941    -0.29039     0.203215     0.168185   -0.490619     0.392091    -0.099394    -0.497968     0.191097   -0.109489    -0.29525    -0.220384
 -0.526651   -0.301568     0.0432184   -0.142051   -0.243965    -0.127172     0.0223601  -0.151656   -0.709171    -0.474095    0.0151817   0.230648    -0.2687       0.0571913   -0.383675    -0.597873   -0.415365     0.229094    0.242879     0.00539483   1.10018      0.37383     -0.614709   -0.684573     0.16735     0.633461
  0.426899    0.218793     0.210754    -0.621102    0.319814    -0.18051      0.0582182   0.183741   -0.426964     0.197591   -0.386758   -0.263039    -0.223096    -0.87527     -0.024786    -0.849744   -0.212742     0.553653    0.0881608    0.410602    -0.0774357   -0.254028    -0.68948    -0.0374107   -0.0620632  -0.0891865
 -0.359231   -0.277944     0.180229     0.753732   -0.393558     0.110325     0.173406   -0.0465643  -0.327245    -0.0945807  -0.887862   -0.0344002   -0.350236    -0.555895    -0.548583     0.139952    0.450147    -0.351594    0.427812     0.522871     0.275238     0.291418    -0.368741   -0.545752     0.514769    0.364959
 -0.712255    0.27574     -0.299647     0.615139    0.058054    -0.095556     0.0628639  -0.600233   -0.0951769    0.592493   -0.188527    0.582366    -0.445718    -0.0229287   -0.323092     0.361739    0.261242     0.235046   -0.517781     0.562675     0.351424     0.00965142  -0.078955   -0.240589    -0.244218    0.391863
 -0.511316    0.464976    -0.339173    -0.114007    0.0919194   -0.12763      0.585821    0.0878844  -0.0277259   -0.130065   -0.187465   -0.521916     0.576741    -0.454128    -0.199226     0.378577   -0.547524     0.0252364   0.363367     0.451387    -0.291418    -0.139767     0.600807    0.0245006   -0.241566    0.0729668
  0.339446   -0.0240125   -0.333795     0.560113    0.0960987    0.10324      0.799235   -0.317387   -0.0119699   -0.0966807  -0.167323   -0.153694     0.308601    -0.5792      -0.282497     0.36331     0.318567     0.137854    0.356468     0.319849     0.278544    -0.035029     0.422299    0.0751901   -0.640322   -0.0652691
  0.274243   -0.420453     0.318958     0.410045   -0.228069    -0.549453    -0.0659619  -0.309177    0.11003      0.0394571   0.521821    0.0655982   -0.16689      0.251742    -0.148218    -0.0503379   0.131243     0.096862   -0.209431     0.0162193    0.0767458   -0.579143     0.12271    -0.478715    -0.10752    -0.133361
 -0.507357   -0.00031304  -0.0824817   -0.078      -0.21007      0.352527     0.0955666   0.100586    0.163531    -0.194421   -0.0536634  -0.147091     0.282204     0.0922708   -0.185732     0.227098    0.0768143   -0.225547    0.0972324    0.230675    -0.0217168   -0.0410612    0.0262049   0.205042    -0.189291    0.00196647
 -0.259518   -0.452516    -0.454894    -0.533008    0.843216    -0.703964     0.110225   -0.687245    0.147978    -0.550117   -0.0918882   0.0957662   -0.280522     0.294771     0.0485931    0.268993   -0.420209    -0.053765    0.370913     0.0257218   -0.0413237    0.465774    -0.2111     -0.114142     0.549889    0.468553
  1.19166     0.178037     0.00394519   0.0343264   0.121731    -0.579815     0.524004   -0.2648     -0.268156     0.236612    0.225223    0.295841    -0.529499    -0.810577     0.581716     0.366723   -0.150359     0.0653933   0.0889323    0.424433    -0.262787     0.112972     0.261296    0.171334     0.419081   -0.146782
  0.309114    0.668026    -0.0236848   -0.31012    -0.0848105    0.50992     -0.108031   -0.471306    0.0149571    0.0473327  -0.237956    0.4682      -0.0698171   -0.0362435    0.38522      0.156892   -0.0201371   -0.719917    0.10166     -0.5588      -0.24505      0.469454    -0.271062    0.284704     0.479123   -0.227612
  0.870898    0.233928    -0.318568    -0.423565    0.516505     0.0674239   -0.262245    0.135651    0.109678     0.192417    0.782366    0.050353     0.351166     0.411951     0.610805    -0.0644257  -0.375974     0.325752   -0.237203    -0.495172    -0.171106    -0.0755438    0.715397    0.37755     -0.371156   -0.228032
  0.213429    0.279131    -0.0324933    0.114132    0.160859     0.122854    -0.361816    0.711094   -0.169199     0.447544   -0.316821   -1.00825      0.0952678   -0.0869169    0.109201     0.288738    0.0881594    0.38357    -0.677718     0.0563979   -0.608386     0.4706      -0.102304   -0.173139     0.288156    0.648018
  0.551529   -0.122843     0.0651914   -0.335583   -0.160565    -0.125379    -0.289906    0.638254   -0.0510768   -0.393097   -0.298504   -0.994618     0.331993    -0.228661     0.093995    -0.363644    0.102767    -0.0480094   0.547203    -0.118131     0.332653     0.418271    -0.28012     0.419932     0.0167277  -0.287453
  0.264234   -0.331165    -0.0925303   -0.116651    1.16104      0.354441     0.211299    0.571726   -0.559841     0.408193   -0.214291    0.35901     -0.545083     0.0694145   -0.319965    -0.479442   -0.035333    -0.503983    0.143561    -0.836254     0.149627     0.295703     0.294942    0.280355     0.179183   -0.139081
 -0.155007    0.501646    -0.538191    -0.405334    0.578174     0.00210489  -0.117707   -0.150928   -0.505488     0.21812    -0.143935    0.00963899  -0.144182    -0.129347    -0.7288      -0.488845    0.135903    -0.35342    -0.535633    -0.298745    -0.0387898   -0.331032     0.0260111  -0.688769     0.267139   -0.401784[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.406681
[ Info: iteration 2, average log likelihood -1.406673
[ Info: iteration 3, average log likelihood -1.406665
[ Info: iteration 4, average log likelihood -1.406658
[ Info: iteration 5, average log likelihood -1.406651
[ Info: iteration 6, average log likelihood -1.406643
[ Info: iteration 7, average log likelihood -1.406636
[ Info: iteration 8, average log likelihood -1.406629
[ Info: iteration 9, average log likelihood -1.406623
[ Info: iteration 10, average log likelihood -1.406616
┌ Info: EM with 100000 data points 10 iterations avll -1.406616
└ 59.0 data points per parameter
kind full, method kmeans
[ Info: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.882496e+05
      1       7.005159e+05      -1.877337e+05 |       32
      2       6.863724e+05      -1.414354e+04 |       32
      3       6.818506e+05      -4.521738e+03 |       32
      4       6.795440e+05      -2.306640e+03 |       32
      5       6.781604e+05      -1.383548e+03 |       32
      6       6.771595e+05      -1.000925e+03 |       32
      7       6.763795e+05      -7.799932e+02 |       32
      8       6.757457e+05      -6.338490e+02 |       32
      9       6.751939e+05      -5.517675e+02 |       32
     10       6.747284e+05      -4.654633e+02 |       32
     11       6.742838e+05      -4.446345e+02 |       32
     12       6.738545e+05      -4.292673e+02 |       32
     13       6.734886e+05      -3.659075e+02 |       32
     14       6.731644e+05      -3.242317e+02 |       32
     15       6.728839e+05      -2.804785e+02 |       32
     16       6.726331e+05      -2.507707e+02 |       32
     17       6.724037e+05      -2.294257e+02 |       32
     18       6.722040e+05      -1.996749e+02 |       32
     19       6.720293e+05      -1.747477e+02 |       32
     20       6.718832e+05      -1.460758e+02 |       32
     21       6.717414e+05      -1.417886e+02 |       32
     22       6.716146e+05      -1.268755e+02 |       32
     23       6.715100e+05      -1.045091e+02 |       32
     24       6.714251e+05      -8.492944e+01 |       32
     25       6.713389e+05      -8.620121e+01 |       32
     26       6.712504e+05      -8.851210e+01 |       32
     27       6.711544e+05      -9.602182e+01 |       32
     28       6.710613e+05      -9.311917e+01 |       32
     29       6.709672e+05      -9.408588e+01 |       32
     30       6.708718e+05      -9.537264e+01 |       32
     31       6.707832e+05      -8.856085e+01 |       32
     32       6.707122e+05      -7.106857e+01 |       32
     33       6.706391e+05      -7.306169e+01 |       32
     34       6.705649e+05      -7.420794e+01 |       32
     35       6.704948e+05      -7.006609e+01 |       32
     36       6.704331e+05      -6.177785e+01 |       32
     37       6.703827e+05      -5.036414e+01 |       32
     38       6.703327e+05      -4.995989e+01 |       32
     39       6.702827e+05      -5.005320e+01 |       32
     40       6.702376e+05      -4.509386e+01 |       32
     41       6.701948e+05      -4.278942e+01 |       32
     42       6.701579e+05      -3.690942e+01 |       32
     43       6.701241e+05      -3.376172e+01 |       32
     44       6.700905e+05      -3.365775e+01 |       32
     45       6.700583e+05      -3.217267e+01 |       32
     46       6.700277e+05      -3.058174e+01 |       32
     47       6.699971e+05      -3.059498e+01 |       32
     48       6.699669e+05      -3.018236e+01 |       32
     49       6.699366e+05      -3.031644e+01 |       32
     50       6.699047e+05      -3.188452e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 669904.7371361067)
┌ Info: K-means with 32000 data points using 50 iterations
└ 37.0 data points per parameter
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.418217
[ Info: iteration 2, average log likelihood -1.413251
[ Info: iteration 3, average log likelihood -1.411909
[ Info: iteration 4, average log likelihood -1.410949
[ Info: iteration 5, average log likelihood -1.409968
[ Info: iteration 6, average log likelihood -1.409042
[ Info: iteration 7, average log likelihood -1.408355
[ Info: iteration 8, average log likelihood -1.407935
[ Info: iteration 9, average log likelihood -1.407687
[ Info: iteration 10, average log likelihood -1.407526
[ Info: iteration 11, average log likelihood -1.407408
[ Info: iteration 12, average log likelihood -1.407314
[ Info: iteration 13, average log likelihood -1.407234
[ Info: iteration 14, average log likelihood -1.407164
[ Info: iteration 15, average log likelihood -1.407101
[ Info: iteration 16, average log likelihood -1.407044
[ Info: iteration 17, average log likelihood -1.406992
[ Info: iteration 18, average log likelihood -1.406943
[ Info: iteration 19, average log likelihood -1.406898
[ Info: iteration 20, average log likelihood -1.406856
[ Info: iteration 21, average log likelihood -1.406816
[ Info: iteration 22, average log likelihood -1.406779
[ Info: iteration 23, average log likelihood -1.406744
[ Info: iteration 24, average log likelihood -1.406710
[ Info: iteration 25, average log likelihood -1.406679
[ Info: iteration 26, average log likelihood -1.406649
[ Info: iteration 27, average log likelihood -1.406621
[ Info: iteration 28, average log likelihood -1.406595
[ Info: iteration 29, average log likelihood -1.406571
[ Info: iteration 30, average log likelihood -1.406547
[ Info: iteration 31, average log likelihood -1.406526
[ Info: iteration 32, average log likelihood -1.406505
[ Info: iteration 33, average log likelihood -1.406486
[ Info: iteration 34, average log likelihood -1.406468
[ Info: iteration 35, average log likelihood -1.406451
[ Info: iteration 36, average log likelihood -1.406435
[ Info: iteration 37, average log likelihood -1.406419
[ Info: iteration 38, average log likelihood -1.406405
[ Info: iteration 39, average log likelihood -1.406391
[ Info: iteration 40, average log likelihood -1.406378
[ Info: iteration 41, average log likelihood -1.406366
[ Info: iteration 42, average log likelihood -1.406354
[ Info: iteration 43, average log likelihood -1.406343
[ Info: iteration 44, average log likelihood -1.406332
[ Info: iteration 45, average log likelihood -1.406322
[ Info: iteration 46, average log likelihood -1.406312
[ Info: iteration 47, average log likelihood -1.406302
[ Info: iteration 48, average log likelihood -1.406293
[ Info: iteration 49, average log likelihood -1.406284
[ Info: iteration 50, average log likelihood -1.406276
┌ Info: EM with 100000 data points 50 iterations avll -1.406276
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
  0.359904     0.287145   -0.0813142    -0.36435      0.144659   -0.27035     -0.14875     0.205523     0.135021    0.129119     0.121057    -0.416566     0.0396003   0.280897     0.30866     0.35039     -0.105753   -0.41761      0.332287    -0.0868288   -0.408851    0.368278    0.146998    0.757383     0.345664   -0.0231179
 -0.0910049   -0.166462   -0.137331     -0.177286    -0.0341431   0.402188     0.153111    0.281718     0.0504868  -0.197173     0.56671     -0.00740313   0.30356     0.404162    -0.400942   -0.204212     0.0518266  -0.018009    -0.858693     0.101993    -0.290282   -0.586386    0.269782   -0.13974     -0.144456   -0.156037
  0.0574622   -0.394752    0.150928     -0.488354     1.01168     0.453649    -0.0970538   0.487913    -0.425219    0.634247    -0.0740604    0.630305    -0.471365   -0.0721522   -0.0454958  -0.547747     0.20578    -0.338672    -0.276522    -0.502846     0.281327    0.293284   -0.265084    0.717813     0.0938948   0.320595
  0.0638975    0.0870488  -0.0974128     0.00459252   0.0585846  -0.0671624   -0.11924    -0.0836093    0.205179    0.0583127    0.107028    -0.0700382    0.0677704   0.174316     0.0461765   0.230247     0.0155833   0.0651396   -0.119598     0.022342    -0.0536108   0.0863498   0.0466043  -0.0936842    0.0933816   0.0647162
  0.0656349    0.370402    0.255892     -0.134597    -0.65881     0.337169     0.153593    0.433487    -0.320618    0.0350785   -0.184636    -0.45664      0.15167     0.244148    -0.613566   -0.108232     0.674753   -0.146818     0.0310174    0.184401     0.950507    0.0708664  -0.0722143   0.527373    -0.583403   -0.205234
  0.279712     0.181027    0.100348     -0.386462     0.480756   -0.643595     0.0736017   0.395094    -0.422411    0.431712    -0.21323     -0.580083    -0.453734   -0.622716    -0.245774   -0.777547    -0.246977    0.706797     0.00972139   0.389597    -0.321272   -0.554142   -0.509425   -0.178832    -0.215177   -0.122313
 -0.417605    -0.266556    0.103754      0.761282    -0.361737    0.00201121   0.190201   -0.250919    -0.214591    0.106151    -0.59405      0.0877583   -0.42171    -0.41948     -0.48367     0.111088     0.340255   -0.134736     0.202872     0.442732     0.2827      0.195679   -0.375262   -0.546338     0.193526    0.457125
 -0.0430593   -0.0584636   0.0058631    -0.115054     0.0061264   0.115748     0.193597    0.0453303   -0.270937   -0.173457    -0.187525    -0.0596283    0.0712583  -0.229377    -0.174933   -0.20139     -0.0937262  -0.212044     0.150356    -0.001956     0.0441638  -0.153571   -0.0593917  -0.00164714   0.0292374  -0.0274104
 -0.603741     0.449399   -0.206242     -0.329314     0.479799    0.309386    -0.235281    0.139088    -0.0346977   0.268805    -0.626825    -0.0468625   -0.524457    0.508579    -0.052959   -0.0535587   -0.175312    0.128941    -0.16196      0.103837     0.415947    1.31867    -0.262986    0.0961971    0.255314    0.392389
  0.346724    -0.241363    0.524414     -0.436166    -0.318185   -0.0383546   -0.80451    -0.236807    -0.394777   -0.244911     0.149115     0.266553    -0.365214    0.465772     0.313391   -0.661213     0.0995991   0.0852431   -0.416771    -0.402896     0.0689799   0.172843   -0.667561   -0.42384      0.605498    0.230613
  0.150811     0.322895   -0.101057      0.064062     0.19581     0.218648    -0.381389    0.7997      -0.280801    0.383893    -0.438142    -1.14928      0.207292   -0.216566     0.147786    0.366976     0.0213317   0.307458    -0.526278     0.00374197  -0.654859    0.574303   -0.146329   -0.0885171    0.271133    0.839789
 -0.198637    -0.250808    0.558804      0.31906     -0.241989   -0.15457     -0.0830692  -0.00632537  -0.457957    0.101547     0.201159     0.170217    -0.262267   -0.00419031  -0.0223315  -0.163187     0.16841     0.391229    -0.379821     0.199544    -0.0892648  -0.48491     0.0198552  -0.267797    -0.218917    0.029191
  0.516483    -0.375041    0.177259      0.423781    -0.266978   -0.260428    -0.229852    0.316083    -0.103587    0.501439     0.426259    -0.129901    -0.27811     0.0323327    0.100955    0.0279214   -0.290557   -0.166709    -0.0820873   -0.0593179   -0.708332   -0.460126    0.100827    0.300039    -0.403292    0.0320546
 -0.115199     0.832954    0.0836944    -0.464252    -0.015949    0.6335       0.0755416  -0.421747     0.0312839   0.0777141   -0.452521     0.374878     0.432176   -0.848785    -0.0889669  -0.244722    -0.143778   -0.0916709   -0.130104    -0.0374713    0.0844182  -0.032663   -0.463083   -0.175382     0.0968087  -0.0864216
 -1.62798     -0.126017    0.0174031    -0.0818048   -0.300016    0.484088    -0.656103    0.471194     0.110484   -0.0126162   -0.365335    -0.416223     0.707059    0.995721    -0.10847    -0.418656     0.2376     -0.269228     0.0960628   -0.503387     0.0893847  -0.321091    0.0159885  -0.145248    -0.46959     0.207163
  0.239485     0.682606    0.283387      0.148592    -0.220044    0.167857    -0.228669    0.199125     0.538757    0.513729     0.422224    -0.0694182    0.161719    0.406361     0.556504    0.431461     0.478147    0.23613     -0.68991      0.354679    -0.0922493   0.0694287   0.0698885  -0.084919    -0.0761929  -0.208741
 -0.565582     0.359806   -0.121996     -0.690668     0.250032   -0.439882     0.154574   -0.69755      0.748249   -0.589313    -0.0788245   -0.192676    -0.0689958   0.152879    -0.102142    0.189185     0.319288   -0.0420277    0.252393     0.139562     0.201667    0.121414   -0.350237   -0.078199     0.33077     0.328668
 -0.317385    -0.81729    -0.977444     -0.444392     0.77378    -0.493547     0.226521   -0.476629    -0.605896   -0.655654     0.00326461   0.297632    -0.276339    0.241913    -0.0916321   0.0621831   -0.742129   -0.326591     0.373201    -0.0807601   -0.0751438   0.26923     0.0822001  -0.0693134    0.444137    0.0831711
 -0.0919756   -0.96053    -0.189289      0.629167    -0.049887   -0.62798     -0.381469    0.136391     0.318143   -0.183144     0.613031    -0.642252    -0.298893    1.17758     -0.0827434   0.268064     0.368581    0.223254    -0.0971804   -0.0961413   -0.0505887   0.175281    0.0445988   0.00573122   0.0107934   0.433533
  1.00063     -0.361399    0.282645      0.23432      0.102885    0.108066     0.013326    0.513649    -0.41791    -0.535111    -0.390077    -0.403465     0.0551098  -0.317113     0.159442   -0.254982     0.174345    0.216455     0.34669     -0.322332     0.1096      0.679616    0.316026   -0.0175813    0.249828   -0.327318
 -0.394633    -0.168698    0.0621017     0.211874    -0.191113    0.0258282    0.217542   -0.174567    -0.29316    -0.109482    -0.151412     0.122395    -0.0573037  -0.24134     -0.47233    -0.00443724   0.103595   -0.00937278  -0.0320091    0.296384     0.255122   -0.119895   -0.223927   -0.328761    -0.130101    0.175881
  0.152008    -0.132882   -0.0734325    -0.162076     0.117235   -0.0704543   -0.434423    0.195244     0.299296   -0.0762642   -0.0326093   -0.468108     0.160887   -0.159545     0.181189   -0.462707    -0.116145    0.0940652    0.0123276   -0.338474     0.209853    0.144626   -0.674362   -0.140902    -0.196486   -0.188981
  0.602903     0.123573   -0.150486     -0.338069     0.264053    0.16026     -0.102318    0.160416    -0.0245092  -0.00416527   0.222067    -0.00233263   0.10123     0.012108     0.460106   -0.121687    -0.426607   -0.260781     0.128328    -0.394136    -0.411943   -0.0115953   0.180691    0.42284      0.0281849  -0.386598
  0.00741416   0.492606   -0.495009     -0.0956019    0.643918   -0.095476    -0.0738935  -0.308435    -0.641992    0.355913    -0.0260822    0.235139    -0.398238    0.134979    -0.515316   -0.272514     0.256305   -0.313811    -0.433314    -0.387863     0.131935   -0.0914751   0.332012   -0.840761     0.371237   -0.346336
  0.863869     0.1995     -0.230466      0.00966277  -0.231549   -0.413564     0.470207   -0.295619    -0.518897    0.230104     0.142022     0.394881    -0.673042   -0.86564      0.63239     0.12112      0.0428329   0.0378075    0.18564      0.403746     0.256076    0.206503   -0.0337867   0.209143     0.17371    -0.051871
  0.193713    -0.440252    0.344508      0.13921     -0.184703   -0.451568     0.146557   -0.732135     0.345469   -0.199346     0.428127     0.485604    -0.0051171   0.128632    -0.410265   -0.220795     0.131526   -0.136824     0.0843353   -0.0156437    0.436836   -0.918727    0.113136   -0.398252    -0.115708   -0.728792
 -0.386673    -0.239886    0.219988     -0.234067     0.0220503   0.311236    -0.76291     0.708603    -0.216991   -0.248748    -0.307794    -0.207809     0.182481    0.288075    -0.76833    -0.0635238   -0.350233   -0.763766     0.200679    -0.0393108   -0.352755   -0.22421     0.112694   -0.115703     0.669631   -0.643118
 -0.172151     0.0375647   0.000271332   0.278266    -0.299249    0.208436     0.131011   -0.718178     0.0766028  -0.396332     0.220007     0.559702     0.277776    0.455812     0.34808     0.596038     0.105769   -0.670251    -0.114181    -0.316881    -0.131286    0.0766873   0.606594    0.13227      0.301261    0.170079
 -0.228648     0.259903   -0.277816      0.110863    -0.1049     -0.0118717    0.634477   -0.00716728  -0.224687   -0.31492     -0.0692748   -0.250668     0.752938   -0.408248    -0.263799    0.383563    -0.16224     0.0608814    0.27463      0.54699     -0.15065    -0.115995    0.767399   -0.127878    -0.207456   -0.109462
 -0.0953669   -0.430382    0.677142      0.194517    -0.719762   -0.112964    -0.368691   -0.191721     0.341237   -0.0415908    0.259207    -0.303813     0.249457   -0.184022     0.181371    0.435036    -0.254497    0.571418     0.393969     0.263877     0.0286938  -0.155437   -0.160557    0.366937    -0.23846     0.854411
  0.00699827   0.0620766  -0.683325      0.420098     0.561214    0.14687      0.350371   -0.369045     0.675904    0.374224    -0.142923    -0.177777     0.0537338  -0.335846    -0.273706    0.34043     -0.109833    0.0376843    0.339841     0.0917033    0.274052   -0.164233    0.245711    0.35721     -0.73332     0.301248
  0.446232    -0.0133605   0.281818     -0.446118     0.782819   -0.0902225   -0.0400147  -0.533541     0.627377    0.345162     0.0604626    0.776811    -0.161566   -0.0349904    0.686771    0.163834    -0.464918    0.376755     0.143757    -0.0354268   -0.0741013   0.298138   -0.0602906  -0.486268     0.354601    0.00125458[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.406268
[ Info: iteration 2, average log likelihood -1.406260
[ Info: iteration 3, average log likelihood -1.406252
[ Info: iteration 4, average log likelihood -1.406244
[ Info: iteration 5, average log likelihood -1.406237
[ Info: iteration 6, average log likelihood -1.406230
[ Info: iteration 7, average log likelihood -1.406223
[ Info: iteration 8, average log likelihood -1.406217
[ Info: iteration 9, average log likelihood -1.406210
[ Info: iteration 10, average log likelihood -1.406204
┌ Info: EM with 100000 data points 10 iterations avll -1.406204
└ 59.0 data points per parameter
[ Info: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.678561e+05
      1       2.230230e+04      -1.455538e+05 |        2
      2       7.823675e+03      -1.447862e+04 |        0
      3       7.823675e+03       0.000000e+00 |        0
K-means converged with 3 iterations (objv = 7823.67549422947)
┌ Info: K-means with 900 data points using 3 iterations
└ 150.0 data points per parameter
[ Info: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
[ Info: iteration 1, average log likelihood -2.043155
[ Info: iteration 2, average log likelihood -2.043154
[ Info: iteration 3, average log likelihood -2.043154
[ Info: iteration 4, average log likelihood -2.043154
[ Info: iteration 5, average log likelihood -2.043154
[ Info: iteration 6, average log likelihood -2.043154
[ Info: iteration 7, average log likelihood -2.043154
[ Info: iteration 8, average log likelihood -2.043154
[ Info: iteration 9, average log likelihood -2.043154
[ Info: iteration 10, average log likelihood -2.043154
┌ Info: EM with 900 data points 10 iterations avll -2.043154
└ 81.8 data points per parameter
   Testing GaussianMixtures tests passed 
