Julia Version 1.5.0-DEV.142
Commit 6d86384ead (2020-01-23 18:34 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-8.0.1 (ORCJIT, skylake)
Environment:
  JULIA_DEPOT_PATH = ::/usr/local/share/julia
  JULIA_NUM_THREADS = 2

 Resolving package versions...
 Installed GaussianMixtures ─── v0.3.0
 Installed JLD ──────────────── v0.9.1
 Installed Parameters ───────── v0.12.0
 Installed StatsFuns ────────── v0.9.3
 Installed HDF5 ─────────────── v0.12.5
 Installed NearestNeighbors ─── v0.4.4
 Installed StaticArrays ─────── v0.12.1
 Installed Clustering ───────── v0.13.3
 Installed SortingAlgorithms ── v0.3.1
 Installed Missings ─────────── v0.4.3
 Installed Distances ────────── v0.8.2
 Installed Arpack_jll ───────── v3.5.0+2
 Installed DataAPI ──────────── v1.1.0
 Installed DataStructures ───── v0.17.9
 Installed PDMats ───────────── v0.9.11
 Installed ScikitLearnBase ──── v0.5.0
 Installed Rmath ────────────── v0.6.0
 Installed CMake ────────────── v1.1.2
 Installed QuadGK ───────────── v2.3.1
 Installed Blosc ────────────── v0.5.1
 Installed BinDeps ──────────── v1.0.0
 Installed SpecialFunctions ─── v0.9.0
 Installed OrderedCollections ─ v1.1.0
 Installed OpenBLAS_jll ─────── v0.3.7+4
 Installed StatsBase ────────── v0.32.0
 Installed CMakeWrapper ─────── v0.2.3
 Installed FileIO ───────────── v1.2.1
 Installed FillArrays ───────── v0.8.4
 Installed Compat ───────────── v2.2.0
 Installed Arpack ───────────── v0.4.0
 Installed BinaryProvider ───── v0.5.8
 Installed URIParser ────────── v0.4.0
 Installed OpenSpecFun_jll ──── v0.5.3+1
 Installed LegacyStrings ────── v0.4.1
 Installed Distributions ────── v0.22.3
  Updating `~/.julia/environments/v1.5/Project.toml`
  [cc18c42c] + GaussianMixtures v0.3.0
  Updating `~/.julia/environments/v1.5/Manifest.toml`
  [7d9fca2a] + Arpack v0.4.0
  [68821587] + Arpack_jll v3.5.0+2
  [9e28174c] + BinDeps v1.0.0
  [b99e7846] + BinaryProvider v0.5.8
  [a74b3585] + Blosc v0.5.1
  [631607c0] + CMake v1.1.2
  [d5fb7624] + CMakeWrapper v0.2.3
  [aaaa29a8] + Clustering v0.13.3
  [34da2185] + Compat v2.2.0
  [9a962f9c] + DataAPI v1.1.0
  [864edb3b] + DataStructures v0.17.9
  [b4f34e82] + Distances v0.8.2
  [31c24e10] + Distributions v0.22.3
  [5789e2e9] + FileIO v1.2.1
  [1a297f60] + FillArrays v0.8.4
  [cc18c42c] + GaussianMixtures v0.3.0
  [f67ccb44] + HDF5 v0.12.5
  [4138dd39] + JLD v0.9.1
  [1b4a561d] + LegacyStrings v0.4.1
  [e1d29d7a] + Missings v0.4.3
  [b8a86587] + NearestNeighbors v0.4.4
  [4536629a] + OpenBLAS_jll v0.3.7+4
  [efe28fd5] + OpenSpecFun_jll v0.5.3+1
  [bac558e1] + OrderedCollections v1.1.0
  [90014a1f] + PDMats v0.9.11
  [d96e819e] + Parameters v0.12.0
  [1fd47b50] + QuadGK v2.3.1
  [79098fc4] + Rmath v0.6.0
  [6e75b9c4] + ScikitLearnBase v0.5.0
  [a2af1166] + SortingAlgorithms v0.3.1
  [276daf66] + SpecialFunctions v0.9.0
  [90137ffa] + StaticArrays v0.12.1
  [2913bbd2] + StatsBase v0.32.0
  [4c63d2b9] + StatsFuns v0.9.3
  [30578b45] + URIParser v0.4.0
  [2a0f44e3] + Base64 
  [ade2ca70] + Dates 
  [8bb1440f] + DelimitedFiles 
  [8ba89e20] + Distributed 
  [b77e0a4c] + InteractiveUtils 
  [76f85450] + LibGit2 
  [8f399da3] + Libdl 
  [37e2e46d] + LinearAlgebra 
  [56ddb016] + Logging 
  [d6f4376e] + Markdown 
  [a63ad114] + Mmap 
  [44cfe95a] + Pkg 
  [de0858da] + Printf 
  [9abbd945] + Profile 
  [3fa0cd96] + REPL 
  [9a3f8284] + Random 
  [ea8e919c] + SHA 
  [9e88b42a] + Serialization 
  [1a1011a3] + SharedArrays 
  [6462fe0b] + Sockets 
  [2f01184e] + SparseArrays 
  [10745b16] + Statistics 
  [4607b0f0] + SuiteSparse 
  [8dfed614] + Test 
  [cf7118a7] + UUIDs 
  [4ec0a83e] + Unicode 
  Building CMake → `~/.julia/packages/CMake/nSK2r/deps/build.log`
  Updating `/tmp/jl_fsPJfm/Project.toml`
 [no changes]
  Updating `/tmp/jl_fsPJfm/Manifest.toml`
 [no changes]
  Building Blosc → `~/.julia/packages/Blosc/lzFr0/deps/build.log`
  Updating `/tmp/jl_inzYK2/Project.toml`
 [no changes]
  Updating `/tmp/jl_inzYK2/Manifest.toml`
 [no changes]
  Building HDF5 ─→ `~/.julia/packages/HDF5/Zh9on/deps/build.log`
  Updating `/tmp/jl_RbnoKH/Project.toml`
 [no changes]
  Updating `/tmp/jl_RbnoKH/Manifest.toml`
 [no changes]
  Building Rmath → `~/.julia/packages/Rmath/BoBag/deps/build.log`
  Updating `/tmp/jl_oStuRH/Project.toml`
 [no changes]
  Updating `/tmp/jl_oStuRH/Manifest.toml`
 [no changes]
   Testing GaussianMixtures
  Updating `/tmp/jl_bLUFD1/Project.toml`
 [no changes]
  Updating `/tmp/jl_bLUFD1/Manifest.toml`
 [no changes]
Running sandbox
Status `/tmp/jl_bLUFD1/Project.toml`
  [aaaa29a8] Clustering v0.13.3
  [34da2185] Compat v2.2.0
  [31c24e10] Distributions v0.22.3
  [5789e2e9] FileIO v1.2.1
  [cc18c42c] GaussianMixtures v0.3.0
  [4138dd39] JLD v0.9.1
  [90014a1f] PDMats v0.9.11
  [6e75b9c4] ScikitLearnBase v0.5.0
  [276daf66] SpecialFunctions v0.9.0
  [2913bbd2] StatsBase v0.32.0
  [8bb1440f] DelimitedFiles 
  [8ba89e20] Distributed 
  [37e2e46d] LinearAlgebra 
  [56ddb016] Logging 
  [de0858da] Printf 
  [9a3f8284] Random 
  [10745b16] Statistics 
[ Info: Testing Data
(100000, -869432.9159067723, [98702.14878216047, 1297.85121783959], [-403.5637517620705 -3122.0250232169706 585.2261558676196; 455.7065587618234 3020.734888948974 -1106.4099580438724], [[99340.73080060478 -1521.2640915073605 96.71004521950803; -1521.2640915073605 93031.35201957154 1858.8473810514915; 96.71004521950795 1858.8473810514915 96871.79705766484], [1044.1354268309644 1086.745908496077 -166.69879177200318; 1086.7459084960772 7430.732751943384 -2094.510242502341; -166.6987917720032 -2094.510242502341 2422.671311997223]])
┌ Warning: rmprocs: process 1 not removed
└ @ Distributed /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Distributed/src/cluster.jl:1015
[ Info: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.401960e+03
      1       1.103132e+03      -2.988279e+02 |        7
      2       1.006802e+03      -9.632996e+01 |        5
      3       9.947138e+02      -1.208840e+01 |        3
      4       9.692734e+02      -2.544041e+01 |        2
      5       9.633224e+02      -5.950981e+00 |        2
      6       9.564442e+02      -6.878257e+00 |        2
      7       9.498319e+02      -6.612256e+00 |        2
      8       9.440753e+02      -5.756569e+00 |        2
      9       9.185530e+02      -2.552235e+01 |        0
     10       9.185530e+02       0.000000e+00 |        0
K-means converged with 10 iterations (objv = 918.5529754180425)
┌ Info: K-means with 272 data points using 10 iterations
└ 11.3 data points per parameter
[ Info: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
┌ Info: EM with 272 data points 0 iterations avll -2.077199
└ 5.8 data points per parameter
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:221
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:221
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:221
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:221
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:631 [inlined]
└ @ Core ./broadcast.jl:631
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:230
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:230
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:631 [inlined]
└ @ Core ./broadcast.jl:631
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:631 [inlined]
└ @ Core ./broadcast.jl:631
[ Info: iteration 1, lowerbound -3.774769
[ Info: iteration 2, lowerbound -3.656147
[ Info: iteration 3, lowerbound -3.534655
[ Info: iteration 4, lowerbound -3.391699
[ Info: iteration 5, lowerbound -3.227217
[ Info: iteration 6, lowerbound -3.056940
[ Info: iteration 7, lowerbound -2.914063
[ Info: dropping number of Gaussions to 6
[ Info: iteration 8, lowerbound -2.811482
[ Info: dropping number of Gaussions to 5
[ Info: iteration 9, lowerbound -2.728132
[ Info: dropping number of Gaussions to 4
[ Info: iteration 10, lowerbound -2.664538
[ Info: dropping number of Gaussions to 3
[ Info: iteration 11, lowerbound -2.597250
[ Info: iteration 12, lowerbound -2.525331
[ Info: iteration 13, lowerbound -2.459271
[ Info: iteration 14, lowerbound -2.404431
[ Info: iteration 15, lowerbound -2.362789
[ Info: iteration 16, lowerbound -2.332319
[ Info: iteration 17, lowerbound -2.313065
[ Info: iteration 18, lowerbound -2.307464
[ Info: dropping number of Gaussions to 2
[ Info: iteration 19, lowerbound -2.302927
[ Info: iteration 20, lowerbound -2.299261
[ Info: iteration 21, lowerbound -2.299256
[ Info: iteration 22, lowerbound -2.299255
[ Info: iteration 23, lowerbound -2.299254
[ Info: iteration 24, lowerbound -2.299253
[ Info: iteration 25, lowerbound -2.299253
[ Info: iteration 26, lowerbound -2.299253
[ Info: iteration 27, lowerbound -2.299253
[ Info: iteration 28, lowerbound -2.299253
[ Info: iteration 29, lowerbound -2.299253
[ Info: iteration 30, lowerbound -2.299253
[ Info: iteration 31, lowerbound -2.299253
[ Info: iteration 32, lowerbound -2.299253
[ Info: iteration 33, lowerbound -2.299253
[ Info: iteration 34, lowerbound -2.299253
[ Info: iteration 35, lowerbound -2.299253
[ Info: iteration 36, lowerbound -2.299253
[ Info: iteration 37, lowerbound -2.299253
[ Info: iteration 38, lowerbound -2.299253
[ Info: iteration 39, lowerbound -2.299253
[ Info: iteration 40, lowerbound -2.299253
[ Info: iteration 41, lowerbound -2.299253
[ Info: iteration 42, lowerbound -2.299253
[ Info: iteration 43, lowerbound -2.299253
[ Info: iteration 44, lowerbound -2.299253
[ Info: iteration 45, lowerbound -2.299253
[ Info: iteration 46, lowerbound -2.299253
[ Info: iteration 47, lowerbound -2.299253
[ Info: iteration 48, lowerbound -2.299253
[ Info: iteration 49, lowerbound -2.299253
[ Info: iteration 50, lowerbound -2.299253
[ Info: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
History[Sat Jan 25 04:58:44 2020: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
, Sat Jan 25 04:58:52 2020: K-means with 272 data points using 10 iterations
11.3 data points per parameter
, Sat Jan 25 04:58:54 2020: EM with 272 data points 0 iterations avll -2.077199
5.8 data points per parameter
, Sat Jan 25 04:58:56 2020: GMM converted to Variational GMM
, Sat Jan 25 04:59:04 2020: iteration 1, lowerbound -3.774769
, Sat Jan 25 04:59:04 2020: iteration 2, lowerbound -3.656147
, Sat Jan 25 04:59:04 2020: iteration 3, lowerbound -3.534655
, Sat Jan 25 04:59:04 2020: iteration 4, lowerbound -3.391699
, Sat Jan 25 04:59:04 2020: iteration 5, lowerbound -3.227217
, Sat Jan 25 04:59:04 2020: iteration 6, lowerbound -3.056940
, Sat Jan 25 04:59:04 2020: iteration 7, lowerbound -2.914063
, Sat Jan 25 04:59:05 2020: dropping number of Gaussions to 6
, Sat Jan 25 04:59:05 2020: iteration 8, lowerbound -2.811482
, Sat Jan 25 04:59:05 2020: dropping number of Gaussions to 5
, Sat Jan 25 04:59:05 2020: iteration 9, lowerbound -2.728132
, Sat Jan 25 04:59:05 2020: dropping number of Gaussions to 4
, Sat Jan 25 04:59:05 2020: iteration 10, lowerbound -2.664538
, Sat Jan 25 04:59:05 2020: dropping number of Gaussions to 3
, Sat Jan 25 04:59:05 2020: iteration 11, lowerbound -2.597250
, Sat Jan 25 04:59:05 2020: iteration 12, lowerbound -2.525331
, Sat Jan 25 04:59:05 2020: iteration 13, lowerbound -2.459271
, Sat Jan 25 04:59:05 2020: iteration 14, lowerbound -2.404431
, Sat Jan 25 04:59:05 2020: iteration 15, lowerbound -2.362789
, Sat Jan 25 04:59:05 2020: iteration 16, lowerbound -2.332319
, Sat Jan 25 04:59:05 2020: iteration 17, lowerbound -2.313065
, Sat Jan 25 04:59:05 2020: iteration 18, lowerbound -2.307464
, Sat Jan 25 04:59:05 2020: dropping number of Gaussions to 2
, Sat Jan 25 04:59:05 2020: iteration 19, lowerbound -2.302927
, Sat Jan 25 04:59:05 2020: iteration 20, lowerbound -2.299261
, Sat Jan 25 04:59:05 2020: iteration 21, lowerbound -2.299256
, Sat Jan 25 04:59:05 2020: iteration 22, lowerbound -2.299255
, Sat Jan 25 04:59:05 2020: iteration 23, lowerbound -2.299254
, Sat Jan 25 04:59:05 2020: iteration 24, lowerbound -2.299253
, Sat Jan 25 04:59:05 2020: iteration 25, lowerbound -2.299253
, Sat Jan 25 04:59:05 2020: iteration 26, lowerbound -2.299253
, Sat Jan 25 04:59:05 2020: iteration 27, lowerbound -2.299253
, Sat Jan 25 04:59:05 2020: iteration 28, lowerbound -2.299253
, Sat Jan 25 04:59:05 2020: iteration 29, lowerbound -2.299253
, Sat Jan 25 04:59:05 2020: iteration 30, lowerbound -2.299253
, Sat Jan 25 04:59:05 2020: iteration 31, lowerbound -2.299253
, Sat Jan 25 04:59:05 2020: iteration 32, lowerbound -2.299253
, Sat Jan 25 04:59:05 2020: iteration 33, lowerbound -2.299253
, Sat Jan 25 04:59:05 2020: iteration 34, lowerbound -2.299253
, Sat Jan 25 04:59:05 2020: iteration 35, lowerbound -2.299253
, Sat Jan 25 04:59:05 2020: iteration 36, lowerbound -2.299253
, Sat Jan 25 04:59:05 2020: iteration 37, lowerbound -2.299253
, Sat Jan 25 04:59:05 2020: iteration 38, lowerbound -2.299253
, Sat Jan 25 04:59:05 2020: iteration 39, lowerbound -2.299253
, Sat Jan 25 04:59:05 2020: iteration 40, lowerbound -2.299253
, Sat Jan 25 04:59:05 2020: iteration 41, lowerbound -2.299253
, Sat Jan 25 04:59:05 2020: iteration 42, lowerbound -2.299253
, Sat Jan 25 04:59:05 2020: iteration 43, lowerbound -2.299253
, Sat Jan 25 04:59:05 2020: iteration 44, lowerbound -2.299253
, Sat Jan 25 04:59:05 2020: iteration 45, lowerbound -2.299253
, Sat Jan 25 04:59:05 2020: iteration 46, lowerbound -2.299253
, Sat Jan 25 04:59:05 2020: iteration 47, lowerbound -2.299253
, Sat Jan 25 04:59:05 2020: iteration 48, lowerbound -2.299253
, Sat Jan 25 04:59:05 2020: iteration 49, lowerbound -2.299253
, Sat Jan 25 04:59:05 2020: iteration 50, lowerbound -2.299253
, Sat Jan 25 04:59:05 2020: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [178.0450922260141, 95.9549077739859]
β = [178.0450922260141, 95.9549077739859]
m = [4.250300733269907 79.28686694436182; 2.000229257775368 53.85198717246128]
ν = [180.0450922260141, 97.9549077739859]
W = LinearAlgebra.UpperTriangular{Float64,Array{Float64,2}}[[0.18404155547484394 -0.007644049042327432; 0.0 0.008581705166333458], [0.3758763611948425 -0.008953123827346088; 0.0 0.012748664777409394]]
Kind: diag, size256
nx: 100000 sum(zeroth order stats): 100000.00000000003
avll from stats: -0.9992514887861624
avll from llpg:  -0.9992514887861632
avll direct:     -0.9992514887861632
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 100000.0
avll from stats: -1.008522186419291
avll from llpg:  -1.008522186419291
avll direct:     -1.0085221864192913
sum posterior: 100000.0
32×26 Array{Float64,2}:
 -0.137901     0.0463821   -0.0492045   -0.105878    -0.0904149   -0.0797292     0.102321     -0.124321     0.0830406   0.064135   -0.0348549  -0.0350884    0.1197       0.102668     0.0844977    0.0549837    0.0101059  -0.126932     0.0383225    -0.000192325   0.108255     0.022707     0.00365857  -0.0854656     0.156671     0.0282253
  0.109569    -0.112194     0.146267     0.0363823   -0.0678936    0.00559279    0.00786391   -0.218652    -0.0778203   0.113114    0.257759    0.0331376   -0.0735024    0.0485591   -0.01764      0.146709     0.0624869   0.0513595   -0.0129887    -0.172487      0.0877957    0.0720367   -0.0399875   -0.0701621    -0.0237471   -0.0372486
  0.138788     0.105166    -0.026522    -0.0387668    0.0289191    0.0163159     0.0442734     0.212034    -0.0488966  -0.18435    -0.0212562   0.0936989    0.0329862    0.0272368    0.0272689    0.0131263   -0.047433   -0.164072    -0.0626212    -0.164862      0.172676    -0.0535043    0.0830572    0.065476      0.0903007    0.0433952
 -0.114865     0.107364     0.0337671    0.120303    -0.0180673   -0.024203      0.0139904    -0.0969714    0.143314    0.156946    0.101257   -0.0477736    0.0591524   -0.0519064    0.0670078    0.0410322   -0.0490111   0.0145669   -0.0717791     0.192969     -0.0629931    0.153637    -0.141309    -0.0459131     0.155817    -0.0841584
 -0.00879072   0.0712903    0.0473389    0.112746    -0.0681679    0.0319697    -0.087164     -0.0374332   -0.0805897  -0.0213512  -0.0761868   0.0541464   -0.0180421    0.0620061    0.116627    -0.0323146    0.0838682  -0.114054    -0.0886934     0.0570479     0.0800858   -0.159588     0.327986    -0.0864681    -0.065011    -0.0472468
 -0.00666896   0.0850004    0.022614    -0.0847958   -0.0897132   -0.0387517    -0.102724     -0.00276247  -0.08459     0.0106529  -0.134733    0.0315319   -0.145177     0.0519413    0.0750548    0.0675647    0.227656   -0.0510518    0.109148     -0.0970666     0.111034    -0.0481665   -0.0634653   -0.0960234     0.0578506   -0.0739109
 -0.199344     0.0198258    0.0908498    0.0971795   -0.0568159   -0.0494299     0.0705526     0.166472     0.0144666  -0.241048   -0.0914057   0.198816     0.127582    -0.0121906    0.044028     0.0883138   -0.0297381  -0.122072    -0.0821877    -0.0721156    -0.200249     0.109073     0.0480811   -0.0212204     0.0305896   -0.074033
 -0.136319     0.00953418   0.0550545    0.110688    -0.0953537    0.0107093     0.082529      0.0137314   -0.0277644  -0.0199902   0.184248    0.0158398   -0.0556064    0.0727166   -0.0791261    0.282904    -0.144449    0.0658132    0.0527773    -0.167085     -0.0880261    0.12191      0.068896    -0.0274395    -0.0950927   -0.265272
 -0.144133     0.0672975   -0.0289019    0.0178568   -0.00685523   0.159647     -0.0219416     0.169392    -0.194413    0.164312    0.115539    0.0979689    0.0132113    0.0435796    0.0398663    0.00264161  -0.0530001  -0.129171     0.0707128    -0.00413352   -0.0199947    0.114991     0.123843    -0.0520959    -0.0225838    0.0139367
 -0.170907    -0.119972    -0.00137843  -0.0833721   -0.1628      -0.0614247     0.0197833     0.0177272   -0.0464054   0.0383553  -0.106485    0.0370973    0.0144356   -0.0924401   -0.191062     0.0169941   -0.0630624  -0.039486     0.0561273    -0.0873479    -0.00984616   0.159184    -0.00674181  -0.0520264    -0.0226876   -0.102266
  0.125276    -0.120489    -0.0667959   -0.230599    -0.0802333    0.00308755    0.0594121     0.0460357   -0.143142    0.136513    0.300302    0.0466376   -0.0541964    0.048385    -0.105286     0.140952     0.0900529   0.109439    -0.0276219    -0.0839831     0.0729982   -0.047839     0.107116     0.0238621     0.0995106   -0.0654048
  0.014088     0.120455     0.0670681   -0.102192     0.0897976    0.165185      0.051347      0.0165689   -0.0114231  -0.0300207   0.0623021   0.105065    -0.108525    -0.0380219   -0.0810539   -0.161294    -0.0222149  -0.188336     0.0672111    -0.0170394     0.0829748   -0.152313    -0.0910349    0.0708151    -0.023776    -0.0810232
 -0.0649073    0.147889    -0.00254695  -0.00876714   0.0886518    0.139573     -0.152236      0.247754    -0.0732625  -0.0339327  -0.0333593  -0.126986    -0.0751216    0.118686    -0.0296067    0.0285189   -0.127172   -0.146014     0.000504141   0.105621      0.0159973   -0.191948    -0.122645    -0.047429     -0.0692346    0.148961
 -0.0151973    0.00683898   0.00903352  -0.00781808  -0.0246784    0.000839861  -0.000407861   0.112651    -0.0731377  -0.062472   -0.0183735   0.0791436   -0.0466629    0.0661237    0.0937548   -0.025328     0.0371513   0.0338454    0.0320757    -0.0530798    -0.0293291   -0.0730247   -0.12484     -0.176816      0.134224    -0.0301074
 -0.00474729   0.0237628   -0.0413107    0.115802    -0.152187    -0.0717652     0.0376179     0.0882451    0.058633    0.0860406   0.0176857   0.0257309   -0.105356     0.028886     0.0366923   -0.0694468   -0.0704041   0.0605056   -0.0132586     0.152381      0.243415    -0.0390679   -0.00195095  -0.000110954   0.00531881  -0.0206679
  0.00338553   0.00672205   0.104642    -0.0321836   -0.142998     0.0304048     0.0939432    -0.245213    -0.16683    -0.0452176  -0.121685   -0.0184878   -0.136736     0.13161      0.100994    -0.0378723    0.0394493  -0.0263517    0.0940844     0.0316957    -0.00595401  -0.206655     0.149433     0.207741      0.171905     0.135895
  0.161131    -0.085373     0.0733728    0.130428    -0.236289     0.0716414     0.0201258     0.105941     0.11442     0.12064     0.073669   -0.0740415    0.0151495   -0.0971732    0.00706412  -0.233513    -0.0646032   0.140224    -0.102972      0.151355      0.0168493   -0.130227    -0.0553824   -0.0398323     0.0902034   -0.0557527
 -0.0726347   -0.0898419    0.172905    -0.118092    -0.115862     0.135521      0.010721      0.100587     0.202458   -0.0745027   0.139395    0.0715611    0.117317    -0.0801682    0.0606711   -0.129857    -0.060646   -0.0911989   -0.0138551    -0.0803699    -0.0635643   -0.00890958  -0.0653906    0.00673126    0.13835     -0.153792
 -0.0988695    0.213636    -0.0477504    0.0495161    0.0526734    0.080153     -0.0432389     0.00451869   0.0758945  -0.0296412  -0.0299054   0.134143     0.0357459    0.0310915   -0.0802501    0.136251    -0.233143   -0.049944     0.0606487    -0.0438766     0.254372     0.259941    -0.036926    -0.0155373     0.124264     0.0570753
 -0.052529    -0.0607206    0.176535     0.139842    -0.159628    -0.0902467     0.107211      0.025655     0.0868282   0.171758   -0.0718822  -0.134786     0.140362    -0.0947287   -0.0175187    0.00942444  -0.0104083  -0.0974049   -0.174231     -0.165095     -0.184249     0.0136994   -0.345261     0.0184581     0.0335326    0.103169
 -0.0528434    0.00990496  -0.03976      0.00387604  -0.0461131    0.0549049    -0.0575197    -0.0338779   -0.0794202  -0.101195    0.0960283  -0.0142915   -0.103352    -0.0591945    0.0144403   -0.0661866    0.13083    -0.14323      0.238816     -0.166486     -0.0861115   -0.0690126   -0.102013    -0.0931053     0.042726     0.0142621
 -0.0215253    0.123006     0.0704645    0.082652     0.107062     0.0617792    -0.0347151    -0.0137996    0.116125   -0.0479187   0.0448797   0.020731     0.118446    -0.123768    -0.160945    -0.0128232    0.0899844  -0.109634    -0.0912439    -0.0483888    -0.0973358   -0.0436693    0.0604478    0.124076     -0.0307791    0.0591007
 -0.20769     -0.0158508   -0.156296    -0.117998    -0.0994416    0.0273276     0.200924     -0.0537473    0.0549121   0.002296    0.161749   -0.00239698   0.154082    -0.0677885    0.189519     0.075482     0.158819   -0.0309678   -0.0340673    -0.0783167    -0.113384    -0.0922673   -0.128929    -0.0261472     0.0882523   -0.0593982
 -0.0638382    0.043083    -0.00221534   0.0893989   -0.0216842   -0.106411     -0.330335      0.0802966    0.0446061   0.0488143  -0.0758886   0.0175417   -0.0900046   -0.0912997   -0.112665    -0.0456588   -0.0267445  -0.04835     -0.0354974    -0.194634      0.0429213    0.145718    -0.0371448    0.00841795   -0.149564     0.0567731
 -0.0672797    0.103996     0.0474227    0.025605    -0.00760653  -0.133365     -0.0346686    -0.0430928    0.0332192   0.0415326  -0.0975434  -0.0753718   -0.0732752    0.0304918   -0.0976423   -0.132657    -0.106389   -0.00371068  -0.0788259     0.0325275    -0.0514003   -0.0547886    0.164134     0.139039     -0.0156221   -0.12106
 -0.0955615    0.0240634    0.0196638    0.034149    -0.0845061   -0.176443     -0.00720617    0.0925251    0.147511    0.023312   -0.0399115  -0.0281067   -0.104399     0.0169328    0.00125211  -0.0195967    0.0316526  -0.05858      0.089412     -0.136918     -0.0477093   -0.169839    -0.0302997   -0.00545737    0.0230991    0.0670574
 -0.101079    -0.0445384    0.0518565    0.0941692    0.214198    -0.198807      0.141384      0.0861853    0.0321041   0.0305454  -0.0948446  -0.147346    -0.0959346   -0.0429635   -0.0373254    0.168834     0.0541956   0.0636191    0.0186441    -0.0201798    -0.1935       0.114142     0.103856     0.0388146    -0.246208     0.140505
  0.11611     -0.0618481    0.0435471    0.147162     0.013191     0.0574584    -0.0946047     0.14546     -0.210242   -0.178742   -0.0233436  -0.00731114   0.172268    -0.171032    -0.0664051   -0.0342607    0.159098    0.105784    -0.0777686    -0.0674121     0.0809343    0.0148443    0.0774188   -0.0251037    -0.0487107   -0.00550824
 -0.024179    -0.0105836    0.159795    -0.0330625    0.070145     0.201        -0.203321     -0.00178486  -0.0182492  -0.0110046   0.0177288  -0.0435587   -0.0365091   -0.0839293    0.109886    -0.0965647    0.106739   -0.165178    -0.0535269     0.0761022     0.129802    -0.191317    -0.202366     0.0422207    -0.0475519    0.111357
 -0.0205154   -0.0738884    0.0974236   -0.0087229   -0.110542    -0.00417211    0.0527975    -0.0726364   -0.11629    -0.0799799   0.0435764   0.0690117   -0.00281763  -0.160365     0.0549644    0.00849231   0.102543    0.0842044   -0.0432321     0.141528     -0.0728956   -0.189212     0.0526531    0.0345941     0.0269107   -0.0148599
  0.0640492    0.0292798    0.0421486    0.136103     0.011018     0.0366297    -0.246124      0.0174262   -0.0837707  -0.0627927   0.0667757   0.0101633    0.106209     0.0848757   -0.0566464   -0.0441854    0.0139039   0.032637     0.0356102    -0.0406239    -0.226322     0.0651168   -0.0660027    0.101393      0.0393475    0.105414
 -0.018694     0.106986    -0.0860907    0.0685101    0.00202388   0.133224     -0.149714      0.0238956   -0.0930893  -0.0781784  -0.0974399  -0.00547125   0.0124355   -0.00528506   0.0967412   -0.019726     0.0160597  -0.0284895    0.0895148    -0.0530067    -0.0285218    0.0291765    0.0198891   -0.0115761     0.00137745   0.0122088kind diag, method split
┌ Info: 0: avll = 
└   tll[1] = -1.3863583432791935
[ Info: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.386430
[ Info: iteration 2, average log likelihood -1.386375
[ Info: iteration 3, average log likelihood -1.386147
[ Info: iteration 4, average log likelihood -1.383442
[ Info: iteration 5, average log likelihood -1.371791
[ Info: iteration 6, average log likelihood -1.361752
[ Info: iteration 7, average log likelihood -1.358852
[ Info: iteration 8, average log likelihood -1.357379
[ Info: iteration 9, average log likelihood -1.356179
[ Info: iteration 10, average log likelihood -1.355160
[ Info: iteration 11, average log likelihood -1.354351
[ Info: iteration 12, average log likelihood -1.353472
[ Info: iteration 13, average log likelihood -1.352323
[ Info: iteration 14, average log likelihood -1.350756
[ Info: iteration 15, average log likelihood -1.348704
[ Info: iteration 16, average log likelihood -1.348091
[ Info: iteration 17, average log likelihood -1.347899
[ Info: iteration 18, average log likelihood -1.347808
[ Info: iteration 19, average log likelihood -1.347752
[ Info: iteration 20, average log likelihood -1.347711
[ Info: iteration 21, average log likelihood -1.347679
[ Info: iteration 22, average log likelihood -1.347652
[ Info: iteration 23, average log likelihood -1.347629
[ Info: iteration 24, average log likelihood -1.347610
[ Info: iteration 25, average log likelihood -1.347593
[ Info: iteration 26, average log likelihood -1.347579
[ Info: iteration 27, average log likelihood -1.347566
[ Info: iteration 28, average log likelihood -1.347556
[ Info: iteration 29, average log likelihood -1.347547
[ Info: iteration 30, average log likelihood -1.347539
[ Info: iteration 31, average log likelihood -1.347532
[ Info: iteration 32, average log likelihood -1.347527
[ Info: iteration 33, average log likelihood -1.347522
[ Info: iteration 34, average log likelihood -1.347518
[ Info: iteration 35, average log likelihood -1.347515
[ Info: iteration 36, average log likelihood -1.347512
[ Info: iteration 37, average log likelihood -1.347509
[ Info: iteration 38, average log likelihood -1.347507
[ Info: iteration 39, average log likelihood -1.347506
[ Info: iteration 40, average log likelihood -1.347504
[ Info: iteration 41, average log likelihood -1.347503
[ Info: iteration 42, average log likelihood -1.347502
[ Info: iteration 43, average log likelihood -1.347501
[ Info: iteration 44, average log likelihood -1.347501
[ Info: iteration 45, average log likelihood -1.347500
[ Info: iteration 46, average log likelihood -1.347500
[ Info: iteration 47, average log likelihood -1.347499
[ Info: iteration 48, average log likelihood -1.347499
[ Info: iteration 49, average log likelihood -1.347499
[ Info: iteration 50, average log likelihood -1.347499
┌ Info: EM with 100000 data points 50 iterations avll -1.347499
└ 952.4 data points per parameter
┌ Info: 1
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.3864295853650583
│     -1.386374888203823
│      ⋮
└     -1.3474986512674867
[ Info: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.347615
[ Info: iteration 2, average log likelihood -1.347506
[ Info: iteration 3, average log likelihood -1.347088
[ Info: iteration 4, average log likelihood -1.343328
[ Info: iteration 5, average log likelihood -1.330632
[ Info: iteration 6, average log likelihood -1.320637
[ Info: iteration 7, average log likelihood -1.317330
[ Info: iteration 8, average log likelihood -1.315886
[ Info: iteration 9, average log likelihood -1.315060
[ Info: iteration 10, average log likelihood -1.314535
[ Info: iteration 11, average log likelihood -1.314155
[ Info: iteration 12, average log likelihood -1.313823
[ Info: iteration 13, average log likelihood -1.313491
[ Info: iteration 14, average log likelihood -1.313140
[ Info: iteration 15, average log likelihood -1.312764
[ Info: iteration 16, average log likelihood -1.312363
[ Info: iteration 17, average log likelihood -1.311937
[ Info: iteration 18, average log likelihood -1.311476
[ Info: iteration 19, average log likelihood -1.310961
[ Info: iteration 20, average log likelihood -1.310374
[ Info: iteration 21, average log likelihood -1.309700
[ Info: iteration 22, average log likelihood -1.308916
[ Info: iteration 23, average log likelihood -1.308013
[ Info: iteration 24, average log likelihood -1.307027
[ Info: iteration 25, average log likelihood -1.306027
[ Info: iteration 26, average log likelihood -1.305113
[ Info: iteration 27, average log likelihood -1.304393
[ Info: iteration 28, average log likelihood -1.303908
[ Info: iteration 29, average log likelihood -1.303588
[ Info: iteration 30, average log likelihood -1.303361
[ Info: iteration 31, average log likelihood -1.303189
[ Info: iteration 32, average log likelihood -1.303055
[ Info: iteration 33, average log likelihood -1.302950
[ Info: iteration 34, average log likelihood -1.302866
[ Info: iteration 35, average log likelihood -1.302800
[ Info: iteration 36, average log likelihood -1.302750
[ Info: iteration 37, average log likelihood -1.302711
[ Info: iteration 38, average log likelihood -1.302682
[ Info: iteration 39, average log likelihood -1.302660
[ Info: iteration 40, average log likelihood -1.302644
[ Info: iteration 41, average log likelihood -1.302631
[ Info: iteration 42, average log likelihood -1.302621
[ Info: iteration 43, average log likelihood -1.302612
[ Info: iteration 44, average log likelihood -1.302606
[ Info: iteration 45, average log likelihood -1.302600
[ Info: iteration 46, average log likelihood -1.302595
[ Info: iteration 47, average log likelihood -1.302591
[ Info: iteration 48, average log likelihood -1.302587
[ Info: iteration 49, average log likelihood -1.302584
[ Info: iteration 50, average log likelihood -1.302581
┌ Info: EM with 100000 data points 50 iterations avll -1.302581
└ 473.9 data points per parameter
┌ Info: 2
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.3476154529332967
│     -1.3475062508150977
│      ⋮
└     -1.302580699417771
[ Info: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.302734
[ Info: iteration 2, average log likelihood -1.302593
[ Info: iteration 3, average log likelihood -1.302297
[ Info: iteration 4, average log likelihood -1.299769
[ Info: iteration 5, average log likelihood -1.289036
[ Info: iteration 6, average log likelihood -1.273107
[ Info: iteration 7, average log likelihood -1.262425
[ Info: iteration 8, average log likelihood -1.256920
[ Info: iteration 9, average log likelihood -1.253793
[ Info: iteration 10, average log likelihood -1.251744
[ Info: iteration 11, average log likelihood -1.250143
[ Info: iteration 12, average log likelihood -1.248667
[ Info: iteration 13, average log likelihood -1.247280
[ Info: iteration 14, average log likelihood -1.246031
[ Info: iteration 15, average log likelihood -1.245038
[ Info: iteration 16, average log likelihood -1.244372
[ Info: iteration 17, average log likelihood -1.244216
[ Info: iteration 18, average log likelihood -1.244179
[ Info: iteration 19, average log likelihood -1.244151
[ Info: iteration 20, average log likelihood -1.244127
[ Info: iteration 21, average log likelihood -1.244104
[ Info: iteration 22, average log likelihood -1.244082
[ Info: iteration 23, average log likelihood -1.244061
[ Info: iteration 24, average log likelihood -1.244040
[ Info: iteration 25, average log likelihood -1.244019
[ Info: iteration 26, average log likelihood -1.243998
[ Info: iteration 27, average log likelihood -1.243977
[ Info: iteration 28, average log likelihood -1.243956
[ Info: iteration 29, average log likelihood -1.243936
[ Info: iteration 30, average log likelihood -1.243917
[ Info: iteration 31, average log likelihood -1.243899
[ Info: iteration 32, average log likelihood -1.243882
[ Info: iteration 33, average log likelihood -1.243867
[ Info: iteration 34, average log likelihood -1.243854
[ Info: iteration 35, average log likelihood -1.243840
[ Info: iteration 36, average log likelihood -1.243828
[ Info: iteration 37, average log likelihood -1.243817
[ Info: iteration 38, average log likelihood -1.243806
[ Info: iteration 39, average log likelihood -1.243796
[ Info: iteration 40, average log likelihood -1.243786
[ Info: iteration 41, average log likelihood -1.243775
[ Info: iteration 42, average log likelihood -1.243764
[ Info: iteration 43, average log likelihood -1.243751
[ Info: iteration 44, average log likelihood -1.243737
[ Info: iteration 45, average log likelihood -1.243720
[ Info: iteration 46, average log likelihood -1.243702
[ Info: iteration 47, average log likelihood -1.243681
[ Info: iteration 48, average log likelihood -1.243657
[ Info: iteration 49, average log likelihood -1.243630
[ Info: iteration 50, average log likelihood -1.243598
┌ Info: EM with 100000 data points 50 iterations avll -1.243598
└ 236.4 data points per parameter
┌ Info: 3
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.3027338065742406
│     -1.3025925952246937
│      ⋮
└     -1.2435982666111343
[ Info: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.243778
[ Info: iteration 2, average log likelihood -1.243480
[ Info: iteration 3, average log likelihood -1.242228
[ Info: iteration 4, average log likelihood -1.232256
[ Info: iteration 5, average log likelihood -1.212904
[ Info: iteration 6, average log likelihood -1.196175
[ Info: iteration 7, average log likelihood -1.184365
[ Info: iteration 8, average log likelihood -1.175034
[ Info: iteration 9, average log likelihood -1.168413
[ Info: iteration 10, average log likelihood -1.165045
[ Info: iteration 11, average log likelihood -1.162817
[ Info: iteration 12, average log likelihood -1.160084
[ Info: iteration 13, average log likelihood -1.157606
[ Info: iteration 14, average log likelihood -1.157316
[ Info: iteration 15, average log likelihood -1.157276
[ Info: iteration 16, average log likelihood -1.157263
[ Info: iteration 17, average log likelihood -1.157258
[ Info: iteration 18, average log likelihood -1.157256
[ Info: iteration 19, average log likelihood -1.157255
[ Info: iteration 20, average log likelihood -1.157254
[ Info: iteration 21, average log likelihood -1.157254
[ Info: iteration 22, average log likelihood -1.157254
[ Info: iteration 23, average log likelihood -1.157253
[ Info: iteration 24, average log likelihood -1.157253
[ Info: iteration 25, average log likelihood -1.157253
[ Info: iteration 26, average log likelihood -1.157253
[ Info: iteration 27, average log likelihood -1.157253
[ Info: iteration 28, average log likelihood -1.157253
[ Info: iteration 29, average log likelihood -1.157253
[ Info: iteration 30, average log likelihood -1.157252
[ Info: iteration 31, average log likelihood -1.157252
[ Info: iteration 32, average log likelihood -1.157252
[ Info: iteration 33, average log likelihood -1.157252
[ Info: iteration 34, average log likelihood -1.157252
[ Info: iteration 35, average log likelihood -1.157252
[ Info: iteration 36, average log likelihood -1.157252
[ Info: iteration 37, average log likelihood -1.157252
[ Info: iteration 38, average log likelihood -1.157251
[ Info: iteration 39, average log likelihood -1.157251
[ Info: iteration 40, average log likelihood -1.157251
[ Info: iteration 41, average log likelihood -1.157251
[ Info: iteration 42, average log likelihood -1.157251
[ Info: iteration 43, average log likelihood -1.157251
[ Info: iteration 44, average log likelihood -1.157251
[ Info: iteration 45, average log likelihood -1.157250
[ Info: iteration 46, average log likelihood -1.157250
[ Info: iteration 47, average log likelihood -1.157250
[ Info: iteration 48, average log likelihood -1.157250
[ Info: iteration 49, average log likelihood -1.157250
[ Info: iteration 50, average log likelihood -1.157250
┌ Info: EM with 100000 data points 50 iterations avll -1.157250
└ 118.1 data points per parameter
┌ Info: 4
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.2437783529079622
│     -1.243479875348385
│      ⋮
└     -1.1572499520300514
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.157529
[ Info: iteration 2, average log likelihood -1.157178
[ Info: iteration 3, average log likelihood -1.155410
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     23
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.140772
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      9
│     23
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.104423
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      2
│      9
│     23
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.074435
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      9
│     23
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.060823
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      9
│     23
│     29
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.049951
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      2
│      9
│     23
│     24
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.051462
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      9
│     23
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.068470
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      9
│     23
│     24
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 11, average log likelihood -1.052953
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      2
│      9
│     23
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 12, average log likelihood -1.053545
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      9
│     23
│     24
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 13, average log likelihood -1.054195
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      9
│     23
│     29
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 14, average log likelihood -1.060346
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      2
│      9
│     23
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 15, average log likelihood -1.055483
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      9
│     23
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 16, average log likelihood -1.057142
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      9
│     23
│     24
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 17, average log likelihood -1.046395
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      2
│      9
│     23
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 18, average log likelihood -1.062191
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      9
│     23
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 19, average log likelihood -1.071380
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      9
│     23
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 20, average log likelihood -1.060768
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      2
│      9
│     23
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 21, average log likelihood -1.045930
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      9
│     23
│     29
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 22, average log likelihood -1.053642
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      9
│     23
│     24
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 23, average log likelihood -1.054456
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      2
│      9
│     23
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 24, average log likelihood -1.057258
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      9
│     23
│     24
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 25, average log likelihood -1.058009
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      9
│     23
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 26, average log likelihood -1.057432
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      2
│      9
│     23
│     24
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 27, average log likelihood -1.043120
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      9
│     23
│     29
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 28, average log likelihood -1.066122
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      9
│     23
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 29, average log likelihood -1.060463
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      2
│      9
│     23
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 30, average log likelihood -1.046432
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      9
│     23
│     24
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 31, average log likelihood -1.052221
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      9
│     23
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 32, average log likelihood -1.067198
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      2
│      9
│     23
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 33, average log likelihood -1.060621
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      9
│     23
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 34, average log likelihood -1.066547
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      9
│     23
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 35, average log likelihood -1.050741
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      2
│      9
│     23
│     29
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 36, average log likelihood -1.042719
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      9
│     23
│     24
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 37, average log likelihood -1.060264
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      9
│     23
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 38, average log likelihood -1.062182
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      2
│      9
│     23
│     24
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 39, average log likelihood -1.047183
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      9
│     23
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 40, average log likelihood -1.063433
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      9
│     23
│     24
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 41, average log likelihood -1.048242
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      2
│      9
│     23
│     29
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 42, average log likelihood -1.055230
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      9
│     23
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 43, average log likelihood -1.066116
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      9
│     23
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 44, average log likelihood -1.051077
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      2
│      9
│     23
│     24
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 45, average log likelihood -1.041226
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      9
│     23
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 46, average log likelihood -1.073064
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      9
│     23
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 47, average log likelihood -1.065696
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      2
│      9
│     23
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 48, average log likelihood -1.055967
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      9
│     23
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 49, average log likelihood -1.056617
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      9
│     23
│     29
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 50, average log likelihood -1.047702
┌ Info: EM with 100000 data points 50 iterations avll -1.047702
└ 59.0 data points per parameter
┌ Info: 5
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.1575291539043704
│     -1.1571781041413658
│      ⋮
└     -1.0477019106313006
┌ Info: Total log likelihood: 
│   tll =
│    251-element Array{Float64,1}:
│     -1.3863583432791935
│     -1.3864295853650583
│     -1.386374888203823
│     -1.3861469758872984
│      ⋮
│     -1.0559667117529334
│     -1.0566165638693468
└     -1.0477019106313006
32×26 Array{Float64,2}:
 -0.000784713   0.0626192    0.0481722    -0.0554426    0.0151548    0.103887     0.0388303    0.0672826   -0.0460545   -0.0415888   0.00945755   0.0639087   -0.0929418    0.0118034    0.00514002   -0.0964628   -0.0063366  -0.0831376     0.0508176   -0.0366154    0.02325     -0.10286     -0.109039   -0.0601119     0.0736318   -0.0691446
 -0.187085      0.0267507    0.0902638     0.0947298   -0.0456966   -0.0613686    0.0651682    0.159193     0.0168699   -0.228603   -0.0759788    0.205198     0.104065    -0.00581802   0.0436899     0.0851052   -0.0207759  -0.117841     -0.0896613   -0.070241    -0.193406     0.112318     0.0350299  -0.0191306     0.0253275   -0.094195
  0.146474      0.10264      0.00461627   -0.0282288    0.0349425    0.00848953   0.0970945    0.216784    -0.0762949   -0.187      -0.0182095    0.102678     0.0199328    0.0349585    0.0318494    -0.00346692  -0.0134244  -0.158958     -0.061511    -0.170236     0.168847    -0.0542282    0.0913828   0.0606173     0.0916954    0.0434435
  0.0574597    -0.00162623   0.142186      0.139906     0.00885073   0.0576703   -0.245009     0.0186633   -0.0830943   -0.0645618   0.0810058    0.0160581    0.103348     0.0801282   -0.0426792    -0.0452563    0.0240802   0.0318237     0.0876388   -0.0401352   -0.226538     0.0878316   -0.0500789   0.108162      0.0391177    0.109079
 -0.0282641    -0.0320778    0.155442      0.00821397   0.0719824    0.173565    -0.201074     0.00309299   0.00303689  -0.0070899   0.0329293   -0.053035    -0.0157955   -0.0865092    0.129296     -0.12824      0.14809    -0.174748     -0.04998      0.0708316    0.127729    -0.19062     -0.195908    0.0642321    -0.0474717    0.0756021
 -0.135498      0.0217623    0.0185191     0.115896    -0.081239    -0.0129797    0.0646921    0.0120146   -0.0259846   -0.0365682   0.185167    -0.00757112  -0.0245687    0.0683412   -0.109607      0.212973    -0.155426    0.0687875     0.050063    -0.161516    -0.0791217    0.129582     0.0428623  -0.0362073    -0.105913    -0.28427
 -0.0241716     0.124437     0.0601803     0.0899683    0.105464     0.075482    -0.0352239   -0.0908806    0.104941    -0.0381675   0.0417898    0.0323425    0.113626    -0.12718     -0.219717     -0.0578617    0.0950527  -0.0948105    -0.0707136   -0.0439716   -0.0943189   -0.0421895    0.0674627   0.135316     -0.00568784   0.0541952
 -0.0180946    -0.0773732    0.0717651     0.0434306   -0.112402    -0.00257676   0.0418275   -0.0759102   -0.127428    -0.0697888   0.0435595    0.0478262   -0.00891414  -0.170299     0.0693407    -0.00128606   0.102715    0.0899105    -0.0273901    0.142557    -0.0790829   -0.188571     0.0568619  -0.0113803     0.0192753   -0.0150857
 -0.064245      0.0437916    0.000611274   0.0889933   -0.021656    -0.113452    -0.335769     0.017143     0.0659732    0.0336752  -0.0601073    0.0181848   -0.0484099   -0.0864193   -0.10796      -0.0571786   -0.0251178  -0.0481399    -0.0439914   -0.184641     0.06221      0.111356    -0.0176296  -0.0152269    -0.139586     0.0425163
 -0.0306586     0.0560266    0.0596076     0.112356    -0.0724394    0.0226228   -0.0297034   -0.0132079   -0.072826    -0.0353388  -0.0864812    0.0565777   -0.0113638    0.082798     0.137091     -0.0301676    0.0917303  -0.119591     -0.0994686    0.0807983    0.0880664   -0.235179     0.310261   -0.0871345    -0.0614451   -0.0355298
 -0.234447     -0.0544914   -0.0347995    -0.117297    -0.151923    -0.0524703    0.139117    -0.635947     0.0857597    0.0867067   0.266952    -0.00788642   0.0189799   -0.073911     0.193795      0.0375481    0.115124   -0.0311705    -0.0366515   -0.224191    -0.110189    -0.160424    -0.114684   -0.000369703   0.0243273   -0.00847839
 -0.215834      0.0263313   -0.187455     -0.0842622   -0.0539884    0.10945      0.228224     0.62765      0.0210614   -0.0951785   0.131001     0.0115546    0.278006    -0.102773     0.199336      0.102604     0.203902   -0.0289663    -0.0272538    0.00470614  -0.117867     0.0565961   -0.132916   -0.0829069     0.13672     -0.072993
 -0.0601065    -0.28181      0.161521     -0.263847    -0.169892     0.11958      0.211691     0.0316697    0.474312    -0.0299846   0.101297    -0.00815272   0.118104    -0.0880705    0.0442837    -0.149532    -0.0439564  -0.147684     -0.381554    -0.0337985   -0.091974    -1.93907     -0.0619029   0.273501      0.0834697   -0.15293
 -0.127115     -0.194644     0.199843      0.165979    -0.224379     0.142645    -0.173807     0.168262     0.370306    -0.367764    0.26287      0.0816019    0.110714    -0.142141     0.0654658    -0.168329    -0.0621994  -0.0206941    -0.0321055   -0.122722    -0.106995     0.631168    -0.107843   -0.123526      0.157298    -0.158641
 -0.064617      0.23013      0.167065     -0.341547    -0.151562     0.165274    -0.13276      0.119056    -0.125052     0.0464766   0.173562     0.15543      0.118204     0.0123121    0.069581     -0.0952925   -0.102627   -0.0223465     0.280516    -0.116018    -0.0369276   -0.998214    -0.0451068  -0.0664645     0.170017    -0.162022
 -0.0188834    -0.0167974    0.180319     -0.213071    -0.048166     0.116832     0.129097     0.0296647    0.192641    -0.0927664  -0.00980283   0.0640094    0.119357    -0.0671458    0.0621384    -0.114832    -0.0187247  -0.176729     -0.0222529   -0.0581596   -0.00616315   1.83934     -0.0205842  -0.0338896     0.147898    -0.150149
 -0.00192462    0.0176488   -0.0579295     0.113421    -0.796959    -0.0376728    0.0378475    0.264956     0.061297     0.0838268   0.0580506    0.0416763   -0.174065     0.0208687    0.0434956    -0.00728982  -0.0595681   0.0540095    -0.0132494    0.224285     0.186705    -0.0807784   -0.0172945   0.0158109     0.0977473    0.0550202
 -0.00151079    0.0256763   -0.01963       0.117684     0.37977     -0.109583     0.038218    -0.120888     0.0465163    0.0910138  -0.0811393    0.0237079   -0.0412948    0.0264876    0.0386153    -0.13014     -0.0505671   0.0598339    -0.0104855    0.0662587    0.275781    -0.0313207    0.0355896   0.0537432    -0.13405     -0.0996145
 -0.0859413     0.0235958    0.0223208     0.0335909   -0.0913393   -0.163075    -0.00161398   0.0952837    0.173021     0.0308999   0.0199127   -0.0363929   -0.112102     0.0157613    0.000889553   0.0131835    0.0390288  -0.0355488     0.0958507   -0.128117     0.00204463  -0.185659    -0.0308976  -0.0270627     0.0187424    0.075841
  0.11301      -0.112003     0.142386      0.029571    -0.0626313    0.0200045    0.00792487  -0.233443    -0.0970914    0.12038     0.246696     0.051037    -0.0989673    0.0423314   -0.0170621     0.166205     0.0716932   0.0564641    -0.00374539  -0.17278      0.0864596    0.0803576   -0.0382118  -0.0683769    -0.0553912   -0.0196578
  0.165212     -0.0971993    0.0267708     0.130905    -0.227091     0.0724847    0.0192062    0.0906729    0.111434     0.121359    0.0712058   -0.0778179    0.00639799  -0.107426    -0.00842695   -0.245645    -0.0748015   0.139873     -0.078566     0.137936     0.0064827   -0.114492    -0.0493703  -0.0466581     0.0995975   -0.0524233
  0.0116514     0.0428877    0.0319216     0.134435    -0.00470884   0.0170343   -0.0424419    0.0322906   -0.0335097   -0.0101316   0.0355365   -0.0191205    0.101546    -0.117685    -0.00485688    0.00717905   0.0480555   0.0396693    -0.0592431    0.0608168    0.0462139    0.0871633   -0.0190026  -0.0438956     0.0447663   -0.0417727
 -0.0432563     0.0981038    0.0447774     0.0278779   -0.00155965  -0.144529    -0.0410701   -0.050316     0.0339267    0.0399705  -0.103287    -0.0708197   -0.0650815    0.0391594   -0.117935     -0.148758    -0.102937    0.0023918    -0.0912364    0.0413179   -0.0163705   -0.057724     0.154628    0.125889     -0.0169149   -0.134321
 -0.0522587    -0.0225394   -0.0369643    -0.012192    -0.053193     0.0734629   -0.057446    -0.0368972   -0.100863    -0.099533    0.100671    -0.0123248   -0.123869    -0.0575388    0.0206336    -0.0567803    0.122611   -0.165623      0.240177    -0.191908    -0.103136    -0.0406947   -0.0851344  -0.100922      0.0517359    0.0208878
 -0.129322     -0.0809387    0.0882386     0.0139551   -0.165807    -0.0692448    0.0559176    0.0455031    0.0203456    0.104272   -0.0898288   -0.0454246    0.0773012   -0.0963677   -0.069306      0.017573    -0.0387633  -0.0501367    -0.0372314   -0.133626    -0.120855     0.0806351   -0.164323   -0.00393626    0.00253611   0.0114138
  0.128999     -0.138143    -0.0506992    -0.222691    -0.106977     0.00458067   0.0411045   -0.0128506   -0.143699     0.132785    0.31361      0.0576948   -0.0522418    0.0688938   -0.10584       0.159453     0.0899215   0.0889634    -0.00400321  -0.0945779    0.0707574   -0.0475291    0.111196    0.00461225    0.093534    -0.0727601
 -0.0505962    -0.0249812    0.0731035     0.0287261    0.0367198   -0.115147     0.127414    -0.0693959   -0.0686498   -0.0310485  -0.11318     -0.0762747   -0.119111     0.0479802    0.0421908     0.0698512    0.0360599   0.000940189   0.0483319   -2.35776e-5  -0.103143    -0.0525813    0.109252    0.111225     -0.0561898    0.137621
 -0.0828727     0.174526    -0.0338251     0.0147802    0.0474385    0.0745694   -0.082231     0.13314      0.0178376   -0.0293874  -0.0295058    0.00668242  -0.0146602    0.092054    -0.0426383     0.0744364   -0.185348   -0.119548      0.0442605    0.0510512    0.126277     0.0558598   -0.0718896  -0.0197565     0.0331773    0.101161
 -0.0164444     0.08583      0.0130497    -0.0810192   -0.102314    -0.0154322   -0.100486    -0.0324212   -0.0431962    0.0377105  -0.149072     0.023316    -0.143645     0.0370817    0.0278208     0.0686721    0.207476   -0.0359017     0.0745395   -0.0834376    0.116666    -0.0553572   -0.0637125  -0.113925      0.0509764   -0.0732838
 -0.13177       0.0370551   -0.0422263    -0.11301     -0.102904    -0.0640786    0.0920918   -0.122013     0.0804032    0.0126378  -0.0110804   -0.0316251    0.126221     0.0632763    0.0954377     0.0748752    0.0183649  -0.117929      0.0397717    0.00805252   0.0747046    0.0244781    0.0124239  -0.0248363     0.157506     0.0296064
 -0.00466798    0.100228    -0.0968414     0.0623623    0.057895     0.126404    -0.157382     0.0399156   -0.110853    -0.0690677  -0.0985704   -0.0464302    0.0140176    0.00825841   0.0728339    -0.0167244    0.0166847  -0.0135292     0.0902253   -0.0530799   -0.0218669    0.00651599   0.0263977  -0.0360206    -0.0304949    0.0562314
 -0.140588      0.0621759   -0.0285606    -0.00301651  -0.00437509   0.160415    -0.0119793    0.170546    -0.204759     0.172316    0.10779      0.0926738    0.0139501    0.0592888    0.0398864     0.0594178   -0.0613651  -0.126781      0.0598114   -0.0290048   -0.0168985    0.124065     0.130771   -0.0520347    -0.0451929    0.0182698[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      2
│      9
│     23
│     24
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 1, average log likelihood -1.049307
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      2
│      9
│     23
│     24
│     29
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 2, average log likelihood -1.038452
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      2
│      9
│     23
│     24
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 3, average log likelihood -1.041091
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      2
│      9
│     23
│     24
│     29
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.044663
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      2
│      9
│     23
│     24
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.042878
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      2
│      9
│     23
│     24
│     29
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.036456
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      2
│      9
│     23
│     24
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.049242
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      2
│      9
│     23
│     24
│     29
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.038440
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      2
│      9
│     23
│     24
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.041079
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      2
│      9
│     23
│     24
│     29
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.044662
┌ Info: EM with 100000 data points 10 iterations avll -1.044662
└ 59.0 data points per parameter
kind diag, method kmeans
[ Info: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.215553e+05
      1       6.524690e+05      -1.690862e+05 |       32
      2       6.282940e+05      -2.417505e+04 |       32
      3       6.150678e+05      -1.322619e+04 |       32
      4       6.070401e+05      -8.027666e+03 |       32
      5       6.019265e+05      -5.113588e+03 |       32
      6       5.979069e+05      -4.019623e+03 |       32
      7       5.948234e+05      -3.083526e+03 |       32
      8       5.918819e+05      -2.941516e+03 |       32
      9       5.891079e+05      -2.773920e+03 |       32
     10       5.868217e+05      -2.286221e+03 |       32
     11       5.849794e+05      -1.842283e+03 |       32
     12       5.837039e+05      -1.275557e+03 |       32
     13       5.831562e+05      -5.476961e+02 |       32
     14       5.829750e+05      -1.811389e+02 |       32
     15       5.828752e+05      -9.986255e+01 |       32
     16       5.827899e+05      -8.531066e+01 |       32
     17       5.827180e+05      -7.190095e+01 |       32
     18       5.826354e+05      -8.256184e+01 |       32
     19       5.825351e+05      -1.002971e+02 |       32
     20       5.823953e+05      -1.398199e+02 |       32
     21       5.821535e+05      -2.417650e+02 |       32
     22       5.818876e+05      -2.659460e+02 |       32
     23       5.816892e+05      -1.983299e+02 |       32
     24       5.814875e+05      -2.017735e+02 |       32
     25       5.810802e+05      -4.072380e+02 |       32
     26       5.801791e+05      -9.011273e+02 |       32
     27       5.790719e+05      -1.107238e+03 |       32
     28       5.785821e+05      -4.897449e+02 |       32
     29       5.784625e+05      -1.196522e+02 |       32
     30       5.784281e+05      -3.441447e+01 |       32
     31       5.784167e+05      -1.139391e+01 |       31
     32       5.784106e+05      -6.055119e+00 |       29
     33       5.784075e+05      -3.067667e+00 |       24
     34       5.784046e+05      -2.954121e+00 |       20
     35       5.784025e+05      -2.076260e+00 |       16
     36       5.783995e+05      -3.060103e+00 |       20
     37       5.783964e+05      -3.006591e+00 |       19
     38       5.783931e+05      -3.347859e+00 |       22
     39       5.783897e+05      -3.356557e+00 |       23
     40       5.783856e+05      -4.116032e+00 |       22
     41       5.783811e+05      -4.567480e+00 |       23
     42       5.783770e+05      -4.054461e+00 |       18
     43       5.783732e+05      -3.779030e+00 |       19
     44       5.783697e+05      -3.508480e+00 |       21
     45       5.783629e+05      -6.774909e+00 |       25
     46       5.783506e+05      -1.237021e+01 |       28
     47       5.783346e+05      -1.599687e+01 |       26
     48       5.783152e+05      -1.935097e+01 |       27
     49       5.782940e+05      -2.123057e+01 |       30
     50       5.782521e+05      -4.184928e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 578252.1441284415)
┌ Info: K-means with 32000 data points using 50 iterations
└ 37.0 data points per parameter
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.299255
[ Info: iteration 2, average log likelihood -1.265202
[ Info: iteration 3, average log likelihood -1.231359
[ Info: iteration 4, average log likelihood -1.202308
[ Info: iteration 5, average log likelihood -1.168886
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     13
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.123812
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     12
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.093771
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      4
│     20
│     22
│     23
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.071623
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     16
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.077306
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     13
│     24
│     26
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.054817
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      4
│     12
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 11, average log likelihood -1.070182
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     20
│     23
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 12, average log likelihood -1.076561
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 13, average log likelihood -1.073149
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     13
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 14, average log likelihood -1.063506
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      4
│     12
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 15, average log likelihood -1.059691
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     20
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 16, average log likelihood -1.061523
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      9
│     13
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 17, average log likelihood -1.059848
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     12
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 18, average log likelihood -1.074817
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      4
│     23
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 19, average log likelihood -1.060690
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      9
│     13
│     24
│     27
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 20, average log likelihood -1.039886
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     12
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 21, average log likelihood -1.078901
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      4
│     20
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 22, average log likelihood -1.066934
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 23, average log likelihood -1.070137
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      9
│     13
│     23
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 24, average log likelihood -1.040675
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     12
│     22
│     27
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 25, average log likelihood -1.052245
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      4
│     20
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 26, average log likelihood -1.072733
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     9
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 27, average log likelihood -1.078830
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     13
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 28, average log likelihood -1.065037
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      4
│     12
│     21
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 29, average log likelihood -1.044492
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      9
│     20
│     22
│     23
│     27
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 30, average log likelihood -1.052127
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 31, average log likelihood -1.084236
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     13
│     21
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 32, average log likelihood -1.055271
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      4
│      9
│     12
│     23
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 33, average log likelihood -1.059067
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     20
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 34, average log likelihood -1.077246
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     13
│     21
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 35, average log likelihood -1.059874
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      9
│     12
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 36, average log likelihood -1.066047
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      4
│     24
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 37, average log likelihood -1.052227
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     13
│     21
│     23
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 38, average log likelihood -1.047268
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      9
│     20
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 39, average log likelihood -1.073555
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      4
│     12
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 40, average log likelihood -1.078216
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     21
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 41, average log likelihood -1.058824
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      9
│     22
│     23
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 42, average log likelihood -1.052792
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      4
│     13
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 43, average log likelihood -1.070300
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     12
│     20
│     21
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 44, average log likelihood -1.064620
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     9
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 45, average log likelihood -1.071890
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      4
│     13
│     24
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 46, average log likelihood -1.044741
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│     12
│     20
│     21
│     22
│     23
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 47, average log likelihood -1.048404
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     9
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 48, average log likelihood -1.088056
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 49, average log likelihood -1.065766
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      4
│     12
│     13
│     21
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 50, average log likelihood -1.035472
┌ Info: EM with 100000 data points 50 iterations avll -1.035472
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
  0.128454     -0.138286     -0.0512345   -0.224012    -0.107368     0.00346284   0.0420606    -0.0108466   -0.14387       0.132649     0.314789     0.0583071   -0.0522856   0.0691329   -0.10697      0.161307     0.0901539    0.0894987   -0.00242283  -0.0951879    0.072746    -0.0474212    0.114251     0.00204473   0.0918729   -0.0731625
 -0.0144891     0.00686883    0.109258    -0.0367979   -0.138031     0.0023989    0.115106     -0.24281     -0.146077     -0.0869217   -0.125593    -0.00566768  -0.133056    0.125665     0.126002    -0.0379085    0.0367178   -0.0672126    0.0862971    0.0280229   -0.015324    -0.204584     0.130262     0.199516     0.16722      0.131236
 -0.209028     -0.0210955    -0.0598631   -0.206533    -0.120995     0.00873764   0.166399     -0.858635     0.0510338     0.063357     0.194378    -0.00752242   0.105753   -0.0861913    0.189252     0.0654328    0.144674    -0.0368272   -0.0338846   -0.161089    -0.111595    -0.170159    -0.110898    -0.0201224    0.0553934   -0.0581415
 -0.125438      0.132696      0.0224495    0.12079     -0.0164848   -0.0289823    0.0130026    -0.0929051    0.140265      0.163462     0.0934454   -0.0482368    0.0521455  -0.0323886    0.0668229    0.0486514   -0.0489766   -0.0410169   -0.0787549    0.20622     -0.0621373    0.178384    -0.14097     -0.0457216    0.148832    -0.0842154
  0.1648       -0.0971351     0.0242314    0.130977    -0.227338     0.0725615    0.019165      0.0893787    0.111603      0.121166     0.0720636   -0.0776401    0.008094   -0.106911    -0.00802516  -0.246048    -0.0720193    0.140106    -0.0768325    0.137648     0.00838748  -0.114614    -0.0487827   -0.0472406    0.0984266   -0.0520891
 -0.0686486    -0.0715281     0.177852    -0.149478    -0.150026     0.135505     0.00404198    0.0891467    0.234113     -0.120889     0.134416     0.073775     0.116601   -0.0746637    0.0608888   -0.13395     -0.0555918   -0.0904991   -0.0377332   -0.0837833   -0.0617038   -0.0266033   -0.0600224    0.0048214    0.14248     -0.156126
 -0.0211346     0.124528      0.0580148    0.090692     0.109745     0.0702418   -0.0313384    -0.0932347    0.101244     -0.0383428    0.0418438    0.0379523    0.110933   -0.127388    -0.217473    -0.0532658    0.0949307   -0.0934406   -0.0712482   -0.0426685   -0.0963607   -0.0412526    0.0749732    0.129838    -0.00900374   0.0545387
 -0.0850856     0.0225847     0.0225816    0.0342319   -0.0931053   -0.164342    -0.000841409   0.0955295    0.173168      0.0318912    0.02036     -0.0358168   -0.112572    0.0159123    0.00125151   0.0167804    0.0391078   -0.0352628    0.0964633   -0.127245    -0.00121676  -0.185322    -0.031544    -0.0298119    0.0233255    0.0773778
 -0.192459      0.0210155     0.0897903    0.0986369   -0.046526    -0.0620112    0.0697709     0.158301     0.0155585    -0.234179    -0.0808097    0.212035     0.113484   -0.01349      0.0454596    0.090437    -0.0230689   -0.117306    -0.0907069   -0.0658478   -0.194618     0.105888     0.0414997   -0.0222873    0.0117548   -0.0870882
 -0.255803     -0.000490455  -0.204843     0.0751893   -0.0754869    0.062075     0.223738      1.51522      0.0543639    -0.130362     0.204069     0.0187707    0.236438   -0.101193     0.212808     0.0806405    0.19796     -0.016515    -0.028202    -0.0356343   -0.119061     0.157546    -0.146587    -0.0858124    0.124878    -0.0132476
  0.147496      0.104555      0.00193023  -0.0289215    0.0337306    0.0102212    0.0937315     0.218295    -0.0808017    -0.189072    -0.0182029    0.103221     0.0218055   0.0350093    0.0322331   -0.0041647   -0.0150355   -0.16061     -0.06138     -0.170393     0.169103    -0.0566085    0.0911272    0.0636719    0.0943603    0.0431769
 -0.0992566     0.199704     -0.056776     0.0419144    0.0443599    0.0361981   -0.0146069     0.00367783   0.0880083    -0.030622    -0.01824      0.128421     0.0358471   0.0382285   -0.0804275    0.132183    -0.233321    -0.0321782    0.0592959   -0.0412805    0.228975     0.288466    -0.0393026    0.00207133   0.120584     0.0610897
 -0.000830707   0.0946327    -0.0970501    0.066216     0.0548621    0.125721    -0.155699      0.0319393   -0.110466     -0.0727212   -0.0988835   -0.0402597    0.0145551   0.00534402   0.0757559   -0.0184946    0.0170466   -0.00577583   0.0931836   -0.0632894   -0.0222747    0.0175724    0.0259818   -0.0270913   -0.0271472    0.0507382
  0.132572     -0.0518332     0.0586752    0.148985     0.00436701   0.0492496   -0.0919613     0.148829    -0.199672     -0.170444    -0.0171636   -0.0147805    0.140858   -0.180668    -0.0661875   -0.0376968    0.145594     0.105371    -0.046013    -0.0723615    0.13323      0.00682011   0.0816438   -0.0394449   -0.0469068   -0.00965961
 -0.0368072    -0.0646274     0.130645     0.0958259   -0.149846    -0.0476083    0.0657108    -0.0291454   -0.0167291     0.0645744   -0.0159891   -0.0492626    0.0735008  -0.135565     0.0525546    0.00749958   0.0445607    0.0185949   -0.101418    -0.0228504   -0.161907    -0.12193     -0.159196     0.0294267    0.0261103    0.0427157
 -0.0329437     0.0514975     0.0559454    0.111525    -0.0692753    0.0116885   -0.0524147    -0.0170598   -0.0740531    -0.0324845   -0.0915866    0.0536404   -0.0134867   0.0743372    0.124492    -0.0412291    0.0845332   -0.114676    -0.0923679    0.0641206    0.0878945   -0.218194     0.30096     -0.0868278   -0.0653631   -0.0293834
 -0.122084      0.0393365    -0.042051    -0.113426    -0.100351    -0.0601676    0.0815999    -0.12143      0.0774015     0.00924976  -0.0164853   -0.0303123    0.114705    0.062518     0.0994223    0.0714496    0.0265273   -0.128012     0.0392577    0.00976365   0.0758077    0.019221     0.00776775  -0.0284803    0.155444     0.0245707
  0.0134513     0.120896      0.0584023   -0.100734     0.0805771    0.159863     0.0686268     0.0168493   -0.00968103   -0.0286445    0.0417583    0.107915    -0.142436   -0.0450446   -0.0793826   -0.153158    -0.0404555   -0.188167     0.074079    -0.0182682    0.0787636   -0.145025    -0.099248     0.0687933    0.00781294  -0.086965
  0.0576375    -0.001502      0.143322     0.139932     0.00781382   0.058854    -0.24534       0.0183598   -0.0834368    -0.0645759    0.0810269    0.0158851    0.103459    0.0800059   -0.0435452   -0.0454248    0.0248474    0.0317753    0.0858681   -0.0394274   -0.226681     0.0862201   -0.0494392    0.108832     0.0390626    0.108467
 -0.0101039     0.0571536     0.0198109   -0.0775185   -0.113243    -0.0640219   -0.0809719    -0.0324897   -0.0213478     0.0576363   -0.153571     0.0215433   -0.142652    0.0332852    0.0418559    0.0993414    0.215744    -0.0218062    0.0664087   -0.0815868    0.0961116   -0.055297    -0.0640847   -0.103886     0.0405977   -0.0597462
 -0.0683034     0.0430413    -0.00166338   0.089468    -0.0227714   -0.115015    -0.344361      0.0218018    0.0747723     0.0356903   -0.0559289    0.0173982   -0.0451464  -0.0935086   -0.111211    -0.048243    -0.0277928   -0.0428964   -0.0448082   -0.195806     0.0611515    0.122564    -0.0240244   -0.0105808   -0.14315      0.0478467
 -0.0273398     0.123263     -0.0114817   -0.0638936   -0.162927     0.0656587   -0.123015      0.0335991   -0.179536     -0.0394984   -0.15592      0.026432    -0.131975    0.0300355   -0.0590096   -0.0121952    0.232414    -0.0295168    0.108397    -0.17349      0.148452    -0.0333145   -0.0555045   -0.118126     0.0449165   -0.0487114
 -0.142263      0.0611211    -0.0287077   -0.003006    -0.00441906   0.158511    -0.0120802     0.169931    -0.207433      0.175091     0.108896     0.0949049    0.0130892   0.0585297    0.0389637    0.0551287   -0.0614631   -0.124232     0.0621034   -0.0291598   -0.0180385    0.124735     0.130697    -0.0520772   -0.0436906    0.0167843
 -0.203989     -0.109427     -0.00155047  -0.096709    -0.14453     -0.0522908    0.0273347     0.0679063   -0.0412207     0.0333777   -0.10516      0.047586     0.0108388  -0.0909166   -0.165344     0.0243102   -0.0675828   -0.0579489    0.0932017   -0.0857353   -0.0177622    0.207394     0.0124579   -0.067751    -0.0303436   -0.0609965
  0.0540773    -0.0452274     0.0521538    0.0733226   -0.146556    -0.0265584    0.0230707    -0.0785201   -0.0182636     0.103943     0.119822     0.0467877   -0.103303    0.0338533    0.0103825    0.0515805    0.00721525   0.0578864   -0.00677147  -0.0130519    0.160333     0.0113714   -0.0152288   -0.0140158   -0.0369849   -0.0195686
 -0.0121217     9.48649e-5    0.0329189   -0.00581966  -0.0461805    0.0247146    0.0142424     0.120556    -0.0780619    -0.0588908   -0.0251708    0.0154194   -0.0471373   0.0659745    0.0915187   -0.0343854    0.0415291    0.0287686    0.0181096   -0.0546445   -0.0394769   -0.0618086   -0.093771    -0.174946     0.121649    -0.0540799
 -0.0145081     0.130027     -0.0411427   -0.0332194   -0.0771885    0.0615322   -0.00932032   -0.0390592   -0.0654738    -0.11804      0.0159497    0.0253108   -0.176237   -0.144449     0.0752566    0.0911043    0.0456687   -0.0082287    0.0141536    0.0180125   -0.0715964   -0.0217708   -0.0472164   -0.256142    -0.00208086   0.0200574
 -0.137416      0.020979      0.0204525    0.117305    -0.0820276   -0.01417      0.0662596     0.0133422   -0.0260986    -0.0386398    0.188207    -0.00712373  -0.024983    0.0700538   -0.113131     0.212784    -0.154555     0.0697775    0.0522557   -0.160387    -0.0794354    0.128118     0.0465689   -0.0323549   -0.108446    -0.289608
 -0.0664051     0.144862     -0.0190848   -0.00990568   0.055462     0.122006    -0.124472      0.263999    -0.0526828    -0.0328253   -0.0441099   -0.113767    -0.0668948   0.122036    -0.0238202    0.0398915   -0.115067    -0.187915     0.012861     0.120011     0.0121232   -0.178049    -0.0962226   -0.0639999   -0.0836876    0.139726
 -0.0928       -0.0700453     0.0515376    0.104295     0.273572    -0.242353     0.134807      0.0867936    0.0165127     0.0339036   -0.0871048   -0.167982    -0.106739   -0.0381138   -0.0313341    0.170586     0.0263227    0.100561     0.0117926   -0.0233853   -0.212577     0.139736     0.109554     0.0385424   -0.300147     0.135545
 -0.0428905     0.0437771     0.00709425   0.0065967   -0.0253227   -0.0392246   -0.0487382    -0.0446714   -0.0307118    -0.0272922   -0.00735985  -0.0449866   -0.0924269  -0.0069679   -0.0489434   -0.104937     0.0062866   -0.0784713    0.0682871   -0.0698761   -0.0582695   -0.0502506    0.0382248    0.0144824    0.0233552   -0.0614475
 -0.0284425    -0.0299071     0.154458     0.00598176   0.0723073    0.170371    -0.19917       0.0037986    0.000953717  -0.00823394   0.0353283   -0.0527762   -0.020761   -0.0862616    0.124399    -0.130036     0.149915    -0.169532    -0.0517823    0.0732198    0.124598    -0.190689    -0.192497     0.0662576   -0.0484427    0.0745043[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      9
│     20
│     23
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 1, average log likelihood -1.071954
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      9
│     20
│     23
│     27
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 2, average log likelihood -1.036351
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      9
│     12
│     13
│     20
│     21
│     23
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 3, average log likelihood -1.021474
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      9
│     23
│     27
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.048693
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      4
│      9
│     20
│     22
│     23
│     24
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.027249
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      9
│     12
│     13
│     20
│     21
│     23
│     27
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.025440
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      9
│     23
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.060143
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      9
│     20
│     23
│     27
│     29
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.024942
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      9
│     12
│     13
│     20
│     21
│     23
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.017632
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      4
│      9
│     23
│     27
│     29
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.055294
┌ Info: EM with 100000 data points 10 iterations avll -1.055294
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.157371   -0.0301883   -0.0742349   -0.0213248   0.0622205    0.0632073    0.0779672   -0.0529425    -0.085531     0.0299837   -0.0412361    0.0842986   -0.0717691   -0.0752644    0.0719326  -0.0548541  -0.0795164    0.0699527    0.0175903    0.198076    -0.0790108   -0.12176     -0.00295567   0.0187625   -0.0529709     0.0046301
 -0.0410454   0.0096779    0.0018135   -0.182837   -0.00437119  -0.0747781   -0.00038274  -0.102469     -0.0521423   -0.165896    -0.0995238    0.0518896    0.0534666    0.0485739   -0.0575937   0.0859791  -0.0510726   -0.00203357  -0.00143512  -0.117809    -0.137494    -0.0616414   -0.0764953    0.0440887    0.0748584     0.0827265
  0.101924   -0.0519626    0.0219543    0.0840899   0.0503899   -0.148935    -0.19398     -0.0902899     0.0817121    0.151122     0.151037    -0.0423589    0.135554     0.189318    -0.0292978   0.173874    0.166849    -0.0681455   -0.0584669   -0.234173     0.0551319   -0.0765798   -0.0319284   -0.0248285    0.0179513    -0.157596
  0.0774383  -0.0872455   -0.085661    -0.241835    0.0777791   -0.0322114    0.0637317   -0.0176416     0.0835119    0.0895059   -0.0384982   -0.0455306    0.0550905    0.0112517   -0.0533927  -0.0276245  -0.132593     0.136157    -0.15351     -0.00472368  -0.188479     0.253861     0.213018    -0.0548119   -0.150265      0.0686559
  0.0194251   0.0277139   -0.08495     -0.0864136   0.0821079   -0.0670871   -0.0187025    0.00513895   -0.128966     0.0453158    0.140621    -0.0910429   -0.0768539   -0.118621     0.0991122   0.201198   -0.0104399   -0.00530536  -0.0744381    0.0119832   -0.180031    -0.0988631    0.0967109    0.00408171  -0.0192293    -0.0895381
  0.0527562   0.0409338    0.125101    -0.0883164   0.0194289    0.16149      0.248868     0.0909361    -0.0132815   -0.00398787  -0.0559964    0.0588823    0.0412668   -0.096583    -0.130925   -0.0409733  -0.0154537    0.242366    -0.205998     0.0651288    0.194702    -0.0594282    0.0429735    0.209148     0.0295813    -0.132417
  0.0720893  -0.0751102    0.082573    -0.0253045  -0.0392396   -0.0151177   -0.148492     0.031993      0.0845799   -0.10799      0.00155412  -0.10457     -0.0740165    0.112784    -0.154108    0.0256537   0.0386156   -0.0536369   -0.132724    -0.146539    -0.00837431   0.0861129   -0.0929771   -0.0105891   -0.0592724     0.124403
  0.0809136   0.107009     0.122717     0.0599102   0.0516541   -0.125376     0.0353149    0.172269      0.00627164   0.0285657   -0.119308    -0.139767     0.177224    -0.031498     0.108526   -0.132623    0.143149    -0.101472     0.0225408    0.0151664   -0.0528764    0.0766896    0.0274256   -0.0667079   -0.00509859    0.0555239
 -0.024269   -0.158126    -0.0815747   -0.162113    0.0233225   -0.130312    -0.00924095   0.135406      0.0329922    0.0963979    0.234927     0.0854571    0.11661     -0.0374025    0.043324    0.0215412   0.121355     0.063392     0.157901    -0.197765    -0.0427433   -0.0180023   -0.0892283    0.131797     0.0240645     0.0564217
 -0.136639    0.180582    -0.143942    -0.0626303   0.0308078    0.0920593    0.0440606    0.0336172    -0.188011    -0.20885      0.0144828   -0.0188871   -0.232271     0.0951083    0.0644755   0.0730951  -0.0018916    0.081577    -0.0272841    0.107618    -0.12097      0.201975    -0.117947     0.157379     0.039467     -0.0513527
 -0.145867    0.0638112    0.213624     0.154919   -0.0101287   -0.0354312   -0.0809293    0.000269543  -0.0728855   -0.0575243    0.0463326    0.032166     0.0484914   -0.103005     0.0190675   0.150834    0.00574187  -0.0183817    0.0213929   -0.186724    -0.0390572    0.011995    -0.0865456    0.0958102   -0.0248426    -0.146174
  0.0572885   0.0563451   -0.122881     0.0153735   0.107985     0.0795822    0.00776662   0.276198     -0.0641647   -0.0102238   -0.179534     0.026116     0.0447845    0.0461239    0.137764   -0.0277683  -0.152911    -0.118518    -0.208976    -0.00467361   0.135091     0.0237391    0.186744    -0.0364998    0.0327403     0.0516305
  0.0685494   0.108449    -0.0849955   -0.141395    0.0423703   -0.176671     0.0193099    0.0564996     0.10688      0.0906467    0.0929286   -0.0634225    0.0277172   -0.0998942    0.114359    0.104251   -0.0885861    0.0259534   -0.00165547   0.0361477   -0.0364169    0.0821866   -0.0178036    0.0972094   -0.0399344    -0.0815893
 -0.0794355   0.0863464    0.095547     0.0641592  -0.0669383    0.0890628   -0.0307391   -0.0906025    -0.0941308   -0.0394214   -0.232641    -0.0662232   -0.0617814    0.0266368    0.101746   -0.064873    0.157587     0.0435196   -0.0804045   -0.0570598    0.157156    -0.00917368  -0.0826147   -0.018913    -0.0393126     0.0359581
  0.0425245  -0.118756     0.0828201   -0.115114    0.123864    -0.0825456   -0.0817691   -0.0241804    -0.0457149    0.0253546    0.325016     0.0316285   -0.141092     0.0409187    0.168527    0.0673376  -0.0692945   -0.24908      0.00725564   0.113594    -0.077908     0.12128     -0.106445    -0.0415874    0.0739544    -0.083125
 -0.134969    0.127058    -0.147038    -0.0136813   0.141252     0.122439     0.196686    -0.101271      0.148629    -0.174152    -0.0211261   -0.0348559   -0.0957295   -0.0583064   -0.0274597   0.0495758   0.019318     0.181117     0.0883506    0.0155803    0.112777    -0.00861259  -0.0233403   -0.0673027   -0.161294      0.113696
 -0.0124837  -0.0214245   -0.213163     0.136045   -0.0763245    0.137828     0.143514     0.189233     -0.0198675    0.0459163    0.0163941    0.00882598   0.0802814    0.0155159    0.151166   -0.209458   -0.102264    -0.0839304   -0.138488    -0.0810745   -0.0719029   -0.0142836   -0.0647858   -0.1642       0.108274      0.0754122
 -0.095344    0.0113398   -0.0861068    0.10008    -0.0954505   -0.00903142   0.141769    -0.0634438     0.0693887    0.0118211   -0.0322491    0.131337    -0.00116399  -0.128967     0.0169064  -0.240347    0.0573319   -0.042868    -0.039798    -0.0046947    0.0916303    0.120664     0.167778    -0.0798131   -0.0606568    -0.0384694
 -0.111038    0.0112915   -0.0911778   -0.104041    0.187357     0.064106     0.128968    -0.0206897     0.0798596   -0.0318916   -0.0510588    0.282719     0.0442256    0.0385042   -0.232629    0.124545    0.0107787   -0.204767     0.00763333   0.0942849    0.0407255   -0.14019      0.137055     0.0987036    0.000452464  -0.0058935
 -0.0595397   0.0311439    0.108905    -0.0194174  -0.211839    -0.0687944    0.208622     0.13094       0.0769802   -0.120056    -0.00253287   0.226626     0.0833932    0.128688    -0.104686   -0.112906   -0.275397    -0.0273244   -0.0584369   -0.00136309  -0.028675     0.0525749    0.110872    -0.323575    -0.0293851     0.12145
  0.0214425  -0.0273553   -0.106528    -0.0536922   0.0126114    0.130736     0.0228043    0.0926394     0.00277876   0.185333     0.0958907    0.122636     0.0351132   -0.00731539   0.134993    0.148858    0.0220852   -0.133139    -0.0109475    0.0113877   -0.141084     0.136426    -0.0285144    0.231505    -0.0544964    -0.208172
  0.0163611  -0.0555988    0.17727      0.107711    0.122893    -0.0840326   -0.0520517    0.0844883     0.0346805    0.0607919   -0.0146934    0.139669     0.00156918   0.00272728   0.189487    0.0551687   0.00375694  -0.103096    -0.0773485    0.120762    -0.105516    -0.0357075   -0.0802178   -0.0842966   -0.0182028     0.0173582
  0.0503438   0.0417489   -0.0290778    0.0106023   0.00176937  -0.116051    -0.170913    -0.0839607    -0.00415303   0.189579     0.0617731    0.0422422    0.0685046    0.0294839    0.117112    0.0787182  -0.252211     0.0362625    0.368255     0.0164228   -0.117473    -0.10484     -0.00538107   0.0703615   -0.009105      0.0929924
 -0.029724   -0.00663387  -0.00577649   0.0164576   0.262139    -0.0653815    0.0470429   -0.0930071     0.0612941   -0.0572991    0.0621174    0.134932    -0.0411551   -0.0144472   -0.0104304   0.0269978  -0.048427     0.0376788   -0.00838185   0.0447642   -0.108555     0.0676803    0.116511     0.0117223   -0.156281     -0.0571146
  0.10793     0.0827346   -0.0881252    0.0598673  -0.0522487   -0.0314881   -0.0232339    0.0428972    -0.0717952    0.0568682    0.108345     0.00461228  -0.0805287   -0.076358     0.0265746  -0.0236215  -0.0701939   -0.030682     0.279279    -0.0631196    0.0395538   -0.169836     0.181465     0.0703672    0.132511      0.00936246
 -0.103915    0.0531763   -0.163931     0.0441701   0.0397028   -0.198642     0.088142     0.159731      0.210216     0.0535909   -0.186025    -0.0278294    0.138481     0.0027067   -0.149403   -0.0114399   0.00433563  -0.0853765   -0.0973119   -0.0095915    0.144247     0.109398     0.0335191   -0.0146767    0.251766     -0.111263
  0.0655046   0.0615582   -0.0294215    0.0140951   0.00604132  -0.153224     0.0189073    0.0376795     0.00350281   0.254956    -0.101465    -0.0666543   -0.00493752   0.0395127    0.0363785  -0.25155    -0.0750896    0.0200772    0.094743    -0.140387     0.164547    -0.08728      0.0983947    0.0432657    0.0202018    -0.130322
  0.120414    0.0442807    0.0595723    0.0725562   0.0323497   -0.00319741   0.0789363   -0.0842611     0.0101008   -0.225104    -0.0270287   -0.140566     0.0760723   -0.0663948    0.152542   -0.191681   -0.0741271   -0.014453     0.15091     -0.145677    -0.00970282   0.126138     0.189891    -0.0028195   -0.0331485    -0.0617794
 -0.0701847  -0.108855    -0.122051    -0.0741883   0.165308     0.0781754    0.0246058   -0.102382     -0.0497469   -0.0718378    0.0381431    0.176045     0.0665261   -0.0711471   -0.0122966   0.0664411  -0.116842    -0.012931    -0.0288486    0.0921619    0.0407173    0.141405    -0.0482836    0.0614469   -0.0175772     0.0315581
 -0.116798   -0.00118649  -0.019645    -0.0600895   0.159075    -0.0182958   -0.0759767   -0.00969961   -0.14943     -0.050798    -0.197972    -0.0814179    0.128107    -0.062011     0.0605901   0.0445456   0.210293     0.214766    -0.21755     -0.211264    -0.0126351    0.118313     0.0397578   -0.0981201    0.0291352     0.08089
 -0.103042    0.168369    -0.0100336    0.0522857   0.14597     -0.10266     -0.00655749   0.0562916     0.0196322   -0.103524    -0.0733717    0.0496205    0.0766264   -0.0227678   -0.0382313   0.0337565  -0.0612105    0.0540776   -0.00722369  -0.112238     0.153554    -0.117345     0.0635453   -0.0466888   -0.0222947    -0.103398
  0.0815693  -0.0659881    0.0848077   -0.0337297   0.168457     0.0101027    0.0708233    0.102743     -0.0756772    0.0499822    0.145096    -0.0484746    0.161789     0.0609824   -0.159431   -0.198985   -0.0921668   -0.142506     0.0116047    0.101606    -0.00792483   0.051963    -0.0526221    0.113506    -0.0677069     0.0791971kind full, method split
┌ Info: 0: avll = 
└   tll[1] = -1.4297478179341807
[ Info: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.429766
[ Info: iteration 2, average log likelihood -1.429702
[ Info: iteration 3, average log likelihood -1.429654
[ Info: iteration 4, average log likelihood -1.429597
[ Info: iteration 5, average log likelihood -1.429530
[ Info: iteration 6, average log likelihood -1.429450
[ Info: iteration 7, average log likelihood -1.429356
[ Info: iteration 8, average log likelihood -1.429237
[ Info: iteration 9, average log likelihood -1.429061
[ Info: iteration 10, average log likelihood -1.428751
[ Info: iteration 11, average log likelihood -1.428185
[ Info: iteration 12, average log likelihood -1.427276
[ Info: iteration 13, average log likelihood -1.426173
[ Info: iteration 14, average log likelihood -1.425258
[ Info: iteration 15, average log likelihood -1.424736
[ Info: iteration 16, average log likelihood -1.424504
[ Info: iteration 17, average log likelihood -1.424410
[ Info: iteration 18, average log likelihood -1.424373
[ Info: iteration 19, average log likelihood -1.424358
[ Info: iteration 20, average log likelihood -1.424351
[ Info: iteration 21, average log likelihood -1.424348
[ Info: iteration 22, average log likelihood -1.424347
[ Info: iteration 23, average log likelihood -1.424346
[ Info: iteration 24, average log likelihood -1.424345
[ Info: iteration 25, average log likelihood -1.424345
[ Info: iteration 26, average log likelihood -1.424344
[ Info: iteration 27, average log likelihood -1.424344
[ Info: iteration 28, average log likelihood -1.424344
[ Info: iteration 29, average log likelihood -1.424344
[ Info: iteration 30, average log likelihood -1.424343
[ Info: iteration 31, average log likelihood -1.424343
[ Info: iteration 32, average log likelihood -1.424343
[ Info: iteration 33, average log likelihood -1.424343
[ Info: iteration 34, average log likelihood -1.424343
[ Info: iteration 35, average log likelihood -1.424343
[ Info: iteration 36, average log likelihood -1.424343
[ Info: iteration 37, average log likelihood -1.424342
[ Info: iteration 38, average log likelihood -1.424342
[ Info: iteration 39, average log likelihood -1.424342
[ Info: iteration 40, average log likelihood -1.424342
[ Info: iteration 41, average log likelihood -1.424342
[ Info: iteration 42, average log likelihood -1.424342
[ Info: iteration 43, average log likelihood -1.424342
[ Info: iteration 44, average log likelihood -1.424342
[ Info: iteration 45, average log likelihood -1.424342
[ Info: iteration 46, average log likelihood -1.424342
[ Info: iteration 47, average log likelihood -1.424342
[ Info: iteration 48, average log likelihood -1.424342
[ Info: iteration 49, average log likelihood -1.424342
[ Info: iteration 50, average log likelihood -1.424342
┌ Info: EM with 100000 data points 50 iterations avll -1.424342
└ 952.4 data points per parameter
┌ Info: 1
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4297663017210192
│     -1.4297022873064724
│      ⋮
└     -1.4243418363301907
[ Info: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.424356
[ Info: iteration 2, average log likelihood -1.424290
[ Info: iteration 3, average log likelihood -1.424239
[ Info: iteration 4, average log likelihood -1.424181
[ Info: iteration 5, average log likelihood -1.424114
[ Info: iteration 6, average log likelihood -1.424037
[ Info: iteration 7, average log likelihood -1.423952
[ Info: iteration 8, average log likelihood -1.423862
[ Info: iteration 9, average log likelihood -1.423772
[ Info: iteration 10, average log likelihood -1.423686
[ Info: iteration 11, average log likelihood -1.423610
[ Info: iteration 12, average log likelihood -1.423548
[ Info: iteration 13, average log likelihood -1.423501
[ Info: iteration 14, average log likelihood -1.423466
[ Info: iteration 15, average log likelihood -1.423441
[ Info: iteration 16, average log likelihood -1.423423
[ Info: iteration 17, average log likelihood -1.423410
[ Info: iteration 18, average log likelihood -1.423399
[ Info: iteration 19, average log likelihood -1.423391
[ Info: iteration 20, average log likelihood -1.423383
[ Info: iteration 21, average log likelihood -1.423377
[ Info: iteration 22, average log likelihood -1.423371
[ Info: iteration 23, average log likelihood -1.423367
[ Info: iteration 24, average log likelihood -1.423362
[ Info: iteration 25, average log likelihood -1.423358
[ Info: iteration 26, average log likelihood -1.423355
[ Info: iteration 27, average log likelihood -1.423352
[ Info: iteration 28, average log likelihood -1.423349
[ Info: iteration 29, average log likelihood -1.423347
[ Info: iteration 30, average log likelihood -1.423345
[ Info: iteration 31, average log likelihood -1.423343
[ Info: iteration 32, average log likelihood -1.423342
[ Info: iteration 33, average log likelihood -1.423340
[ Info: iteration 34, average log likelihood -1.423339
[ Info: iteration 35, average log likelihood -1.423338
[ Info: iteration 36, average log likelihood -1.423337
[ Info: iteration 37, average log likelihood -1.423336
[ Info: iteration 38, average log likelihood -1.423335
[ Info: iteration 39, average log likelihood -1.423335
[ Info: iteration 40, average log likelihood -1.423334
[ Info: iteration 41, average log likelihood -1.423333
[ Info: iteration 42, average log likelihood -1.423333
[ Info: iteration 43, average log likelihood -1.423332
[ Info: iteration 44, average log likelihood -1.423332
[ Info: iteration 45, average log likelihood -1.423332
[ Info: iteration 46, average log likelihood -1.423331
[ Info: iteration 47, average log likelihood -1.423331
[ Info: iteration 48, average log likelihood -1.423331
[ Info: iteration 49, average log likelihood -1.423330
[ Info: iteration 50, average log likelihood -1.423330
┌ Info: EM with 100000 data points 50 iterations avll -1.423330
└ 473.9 data points per parameter
┌ Info: 2
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4243564486508735
│     -1.4242904074379967
│      ⋮
└     -1.423330236630549
[ Info: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.423341
[ Info: iteration 2, average log likelihood -1.423280
[ Info: iteration 3, average log likelihood -1.423223
[ Info: iteration 4, average log likelihood -1.423154
[ Info: iteration 5, average log likelihood -1.423067
[ Info: iteration 6, average log likelihood -1.422962
[ Info: iteration 7, average log likelihood -1.422845
[ Info: iteration 8, average log likelihood -1.422727
[ Info: iteration 9, average log likelihood -1.422617
[ Info: iteration 10, average log likelihood -1.422519
[ Info: iteration 11, average log likelihood -1.422434
[ Info: iteration 12, average log likelihood -1.422362
[ Info: iteration 13, average log likelihood -1.422302
[ Info: iteration 14, average log likelihood -1.422252
[ Info: iteration 15, average log likelihood -1.422210
[ Info: iteration 16, average log likelihood -1.422177
[ Info: iteration 17, average log likelihood -1.422150
[ Info: iteration 18, average log likelihood -1.422127
[ Info: iteration 19, average log likelihood -1.422109
[ Info: iteration 20, average log likelihood -1.422093
[ Info: iteration 21, average log likelihood -1.422080
[ Info: iteration 22, average log likelihood -1.422069
[ Info: iteration 23, average log likelihood -1.422059
[ Info: iteration 24, average log likelihood -1.422049
[ Info: iteration 25, average log likelihood -1.422041
[ Info: iteration 26, average log likelihood -1.422033
[ Info: iteration 27, average log likelihood -1.422026
[ Info: iteration 28, average log likelihood -1.422020
[ Info: iteration 29, average log likelihood -1.422013
[ Info: iteration 30, average log likelihood -1.422007
[ Info: iteration 31, average log likelihood -1.422001
[ Info: iteration 32, average log likelihood -1.421996
[ Info: iteration 33, average log likelihood -1.421990
[ Info: iteration 34, average log likelihood -1.421985
[ Info: iteration 35, average log likelihood -1.421980
[ Info: iteration 36, average log likelihood -1.421975
[ Info: iteration 37, average log likelihood -1.421970
[ Info: iteration 38, average log likelihood -1.421965
[ Info: iteration 39, average log likelihood -1.421960
[ Info: iteration 40, average log likelihood -1.421956
[ Info: iteration 41, average log likelihood -1.421951
[ Info: iteration 42, average log likelihood -1.421946
[ Info: iteration 43, average log likelihood -1.421942
[ Info: iteration 44, average log likelihood -1.421937
[ Info: iteration 45, average log likelihood -1.421933
[ Info: iteration 46, average log likelihood -1.421928
[ Info: iteration 47, average log likelihood -1.421924
[ Info: iteration 48, average log likelihood -1.421920
[ Info: iteration 49, average log likelihood -1.421915
[ Info: iteration 50, average log likelihood -1.421911
┌ Info: EM with 100000 data points 50 iterations avll -1.421911
└ 236.4 data points per parameter
┌ Info: 3
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4233414178267056
│     -1.4232798834886675
│      ⋮
└     -1.4219109066164164
[ Info: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.421915
[ Info: iteration 2, average log likelihood -1.421851
[ Info: iteration 3, average log likelihood -1.421791
[ Info: iteration 4, average log likelihood -1.421722
[ Info: iteration 5, average log likelihood -1.421638
[ Info: iteration 6, average log likelihood -1.421537
[ Info: iteration 7, average log likelihood -1.421421
[ Info: iteration 8, average log likelihood -1.421295
[ Info: iteration 9, average log likelihood -1.421167
[ Info: iteration 10, average log likelihood -1.421044
[ Info: iteration 11, average log likelihood -1.420930
[ Info: iteration 12, average log likelihood -1.420828
[ Info: iteration 13, average log likelihood -1.420736
[ Info: iteration 14, average log likelihood -1.420656
[ Info: iteration 15, average log likelihood -1.420585
[ Info: iteration 16, average log likelihood -1.420524
[ Info: iteration 17, average log likelihood -1.420470
[ Info: iteration 18, average log likelihood -1.420424
[ Info: iteration 19, average log likelihood -1.420384
[ Info: iteration 20, average log likelihood -1.420349
[ Info: iteration 21, average log likelihood -1.420318
[ Info: iteration 22, average log likelihood -1.420291
[ Info: iteration 23, average log likelihood -1.420266
[ Info: iteration 24, average log likelihood -1.420244
[ Info: iteration 25, average log likelihood -1.420223
[ Info: iteration 26, average log likelihood -1.420204
[ Info: iteration 27, average log likelihood -1.420186
[ Info: iteration 28, average log likelihood -1.420169
[ Info: iteration 29, average log likelihood -1.420153
[ Info: iteration 30, average log likelihood -1.420138
[ Info: iteration 31, average log likelihood -1.420123
[ Info: iteration 32, average log likelihood -1.420110
[ Info: iteration 33, average log likelihood -1.420096
[ Info: iteration 34, average log likelihood -1.420084
[ Info: iteration 35, average log likelihood -1.420071
[ Info: iteration 36, average log likelihood -1.420060
[ Info: iteration 37, average log likelihood -1.420048
[ Info: iteration 38, average log likelihood -1.420037
[ Info: iteration 39, average log likelihood -1.420027
[ Info: iteration 40, average log likelihood -1.420017
[ Info: iteration 41, average log likelihood -1.420007
[ Info: iteration 42, average log likelihood -1.419998
[ Info: iteration 43, average log likelihood -1.419988
[ Info: iteration 44, average log likelihood -1.419980
[ Info: iteration 45, average log likelihood -1.419971
[ Info: iteration 46, average log likelihood -1.419963
[ Info: iteration 47, average log likelihood -1.419955
[ Info: iteration 48, average log likelihood -1.419947
[ Info: iteration 49, average log likelihood -1.419939
[ Info: iteration 50, average log likelihood -1.419932
┌ Info: EM with 100000 data points 50 iterations avll -1.419932
└ 118.1 data points per parameter
┌ Info: 4
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.421915317562202
│     -1.4218512195481876
│      ⋮
└     -1.419932020841387
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.419934
[ Info: iteration 2, average log likelihood -1.419864
[ Info: iteration 3, average log likelihood -1.419797
[ Info: iteration 4, average log likelihood -1.419717
[ Info: iteration 5, average log likelihood -1.419617
[ Info: iteration 6, average log likelihood -1.419492
[ Info: iteration 7, average log likelihood -1.419343
[ Info: iteration 8, average log likelihood -1.419176
[ Info: iteration 9, average log likelihood -1.419000
[ Info: iteration 10, average log likelihood -1.418823
[ Info: iteration 11, average log likelihood -1.418653
[ Info: iteration 12, average log likelihood -1.418495
[ Info: iteration 13, average log likelihood -1.418352
[ Info: iteration 14, average log likelihood -1.418225
[ Info: iteration 15, average log likelihood -1.418112
[ Info: iteration 16, average log likelihood -1.418013
[ Info: iteration 17, average log likelihood -1.417925
[ Info: iteration 18, average log likelihood -1.417847
[ Info: iteration 19, average log likelihood -1.417778
[ Info: iteration 20, average log likelihood -1.417715
[ Info: iteration 21, average log likelihood -1.417658
[ Info: iteration 22, average log likelihood -1.417607
[ Info: iteration 23, average log likelihood -1.417559
[ Info: iteration 24, average log likelihood -1.417515
[ Info: iteration 25, average log likelihood -1.417475
[ Info: iteration 26, average log likelihood -1.417438
[ Info: iteration 27, average log likelihood -1.417403
[ Info: iteration 28, average log likelihood -1.417370
[ Info: iteration 29, average log likelihood -1.417340
[ Info: iteration 30, average log likelihood -1.417312
[ Info: iteration 31, average log likelihood -1.417285
[ Info: iteration 32, average log likelihood -1.417260
[ Info: iteration 33, average log likelihood -1.417236
[ Info: iteration 34, average log likelihood -1.417214
[ Info: iteration 35, average log likelihood -1.417193
[ Info: iteration 36, average log likelihood -1.417173
[ Info: iteration 37, average log likelihood -1.417154
[ Info: iteration 38, average log likelihood -1.417136
[ Info: iteration 39, average log likelihood -1.417119
[ Info: iteration 40, average log likelihood -1.417102
[ Info: iteration 41, average log likelihood -1.417086
[ Info: iteration 42, average log likelihood -1.417071
[ Info: iteration 43, average log likelihood -1.417056
[ Info: iteration 44, average log likelihood -1.417042
[ Info: iteration 45, average log likelihood -1.417028
[ Info: iteration 46, average log likelihood -1.417015
[ Info: iteration 47, average log likelihood -1.417002
[ Info: iteration 48, average log likelihood -1.416989
[ Info: iteration 49, average log likelihood -1.416977
[ Info: iteration 50, average log likelihood -1.416965
┌ Info: EM with 100000 data points 50 iterations avll -1.416965
└ 59.0 data points per parameter
┌ Info: 5
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4199338332603073
│     -1.419863782591393
│      ⋮
└     -1.4169652132965087
┌ Info: Total log likelihood: 
│   tll =
│    251-element Array{Float64,1}:
│     -1.4297478179341807
│     -1.4297663017210192
│     -1.4297022873064724
│     -1.4296536636545136
│      ⋮
│     -1.4169893455797777
│     -1.416977117107458
└     -1.4169652132965087
32×26 Array{Float64,2}:
 -0.0376875    0.59421     0.00283902  -0.67946    -0.439795   -0.267341   -0.0816478  -0.209561    0.150022    -0.00893382    0.0112605    0.189433   -0.0557444     0.270313    -0.363205    0.620478   -0.154517     0.508384   -0.22125     -0.0459662   -0.114428    -1.14644     0.183045     0.351858    -0.31532      0.378066
  0.665945     0.46762    -0.134316    -0.590095   -0.458239    0.466995   -0.414937   -0.316382   -0.285571     0.152284      0.463304     0.112268    0.160027      0.226106    -0.291747    0.4639     -0.128074     0.213964    0.0756041   -0.190843     0.0582999    0.100319   -0.426477     0.111558     0.369411    -0.381011
  0.272954    -0.393465   -0.964384    -0.0195243  -0.641919   -0.55619    -0.50955     0.442091    0.791291    -0.358219      0.0914914    0.21734    -0.286362      0.423883     0.370757    0.381659   -0.430218     0.344547   -0.10307     -0.105173    -0.170237    -0.0817746   0.145258    -0.401241     0.0256646   -0.141762
  0.131512     0.149056   -0.187154     0.0688792  -0.0157467  -0.43979    -0.0193893   0.5238      0.803349     0.776197     -0.304866    -0.788716   -0.139504      0.197935    -0.165268   -0.0651456   0.391229     0.343182   -0.109902    -0.140651    -0.219172     0.073613   -0.562396    -0.356367     0.0307475    0.0179276
 -0.439537    -0.16251    -0.529445     0.523426   -0.254749   -0.0372549   0.481292    0.133049   -0.455543    -0.466432     -0.235192    -0.112783   -0.290745     -0.347831     0.342844   -0.394093    0.247432    -0.190626   -0.447186    -0.0446715   -0.29379      0.373574    0.655045    -0.154547     0.121582     0.127425
 -0.110586    -0.327227   -0.25221     -0.390288   -0.176783    1.16277     0.358738   -0.10687    -0.628266    -0.97195      -0.0452508    0.641449   -0.241778     -0.252078     0.173972    0.0796093  -0.395099    -0.0490854   0.249257     0.573892     0.23673     -0.119714    0.128853     0.347753     0.0314061    0.369349
 -0.367157     0.800413    0.458336     0.751694   -0.325228   -0.338438   -0.477295    0.164954   -0.630489     0.204288      0.240063     0.258951    0.484457     -0.0160523    0.163405   -0.247878   -0.0867355   -0.243385   -0.877011    -0.469264    -0.297085     0.135754    0.354879     0.203658     0.367666    -0.205007
  0.913945     0.509729   -0.103249     0.319574   -0.112101   -0.100792    0.376067   -0.138626   -0.350931    -0.0489474     0.105288     0.290201    0.580901      0.0317171    0.125934   -0.197253   -0.27253      0.0467147   0.0886824    0.132863    -0.526941     0.910383    0.375958    -0.722728    -0.0961961    0.107547
 -0.702919     0.208954   -0.374898    -0.106544    0.571349    0.362551   -0.26903    -0.239389    0.377752    -0.172421      0.198765     0.172355   -0.420726     -0.771859    -0.551291    0.282337   -0.0261888    0.0928558  -0.18855     -0.17292      0.2637       0.323202   -0.0768752    0.284148     0.734087    -0.316163
 -0.358263     0.131224   -0.146762    -0.729793    0.37506     0.161849    0.456984   -0.719005    0.81094     -0.524065     -0.363283    -0.322988   -0.239507      0.0591414    0.518764    0.0492175   0.115374     0.0734325   0.314682    -0.473006     0.181869    -0.018735    0.255808     0.243028    -0.149977    -0.0780487
 -0.427434    -0.486654    0.0353336    0.466991    0.314574   -0.154656   -0.163663    0.476714    0.309944    -0.0592614    -0.127978     0.26845    -0.0723261    -0.408034     0.266861   -0.272097    0.2712       0.308423    0.0980968   -0.224171     0.203539    -0.477738   -0.0640003    0.13668     -0.340812     0.952388
 -0.297333    -0.724821    0.14522     -0.0806549   0.50748    -0.216152   -0.223481    0.429894    0.577752    -0.280683     -0.319859    -0.314884   -0.601119      0.256148    -0.230115    0.485638    0.18523     -0.6369      0.0390182   -0.206559     0.171352    -0.492071   -0.339535     0.406462    -0.0276724    0.0322219
  0.339934     0.21848     0.0207632   -0.633491   -0.0918164  -0.227553   -0.407268   -0.641975   -0.150432     0.314058      0.350863    -0.11434    -0.30833       0.214321     0.0212181  -0.474518    0.361867    -0.25722     0.516734     0.0641865   -0.0408793   -0.0527326   0.199248    -0.972977    -0.384045    -0.0528555
  0.611992    -0.565286   -0.13842      0.745711    0.608127    0.900339   -0.183044    0.0715731   0.058566    -0.289253      0.379102    -0.439231    0.260108      0.0407822   -0.0228826  -0.140149   -0.00749257  -0.19508     0.356713     0.0658416    0.226449     0.522931   -0.166758    -0.477703     0.0802212    0.144948
 -0.168273     0.730619    1.0615       0.173649    0.577075    0.1066     -0.178325   -0.574751   -0.495308     0.615169      0.0737114   -0.732441    0.719316     -0.575846    -0.17552     0.0803363  -0.144931    -0.0622618   0.78268      0.494079    -0.412228    -0.220995   -0.354818     0.457359    -0.0912988   -0.0173654
  0.0948365   -0.0776784   0.293645    -0.236785    0.265319   -0.243698   -0.19247     0.160253    0.24732      0.271905      0.14522      0.173582    0.231258      0.179217    -0.110593    0.118112   -0.269075     0.148871    0.162456     0.053747     0.0600375   -0.323169   -0.197291     0.251425    -0.347262    -0.0212084
  0.186532     0.161582    0.139954     0.301708    0.162276   -0.364654   -0.316163    0.464649   -0.0302015    0.507933      0.074516    -0.0890224   0.0140761    -0.0198377   -0.254685   -0.061777    0.023905     0.18425    -0.392993    -0.0133756   -0.180609     0.0024846  -0.194336    -0.116241    -0.00634772   0.20468
  0.295231     0.250678    0.524835    -0.0735519   0.210776   -0.0944023  -0.185672   -0.0335306   0.294028     0.1598        0.178377     0.124723    0.394553      0.209149     0.0777601  -0.0502213  -0.0870501    0.104344    0.333524    -0.154463    -0.124339    -0.156641   -0.00548715  -0.130797    -0.473341     0.23203
 -0.087473    -0.217254    0.238877    -0.132556   -0.363188   -0.147728    0.381678   -0.399889   -0.745767     0.208369     -0.0411826   -0.283143    0.296577      0.421416     0.741737   -0.0814114   0.08857     -0.547818   -0.305297     0.16925     -0.198145    -0.437039   -0.10201     -0.284345    -0.251925     0.0099436
  0.3587      -0.247254    0.465138     0.0434192  -0.557666   -0.109063   -0.244324    0.692981   -0.275166     0.205359     -0.03592      0.0963343  -0.0142499     0.161422     0.32903     0.0947898   0.270827    -0.399422    0.583829     0.164423    -0.8283       0.155617   -0.503916     0.0796867   -0.6249      -0.124567
  0.205512    -0.403294   -0.179421    -0.0371989   0.353245   -0.828501    0.23919     0.327035    0.302557     0.187368      0.611934    -0.221829    0.265925      0.223801    -0.0587481  -0.775883   -0.530209    -0.0769289   0.239903     0.148207     0.00452817   0.0308101  -0.203808     0.0363692    0.128518    -0.58948
  0.214594    -0.153193    0.122331    -0.589569   -0.0403901  -0.121481    0.430416    0.11495     0.00924802   0.354892     -0.388199    -0.0529054   0.363318      0.52565     -0.182297    0.312307   -0.927081     0.195372   -0.125323     0.226005     0.457205    -0.0386616  -0.249589     0.270787     0.254035    -0.814586
  0.124865    -0.97978    -0.034846     0.473894   -0.0593744   0.23408     0.140595    0.768211   -0.472464     0.672041     -0.288779     0.3514      0.125246      0.00526984  -0.0323947  -0.0860739   0.0162446   -0.453983   -0.12143      0.128307     0.559087     0.308754   -0.299424    -0.258917     0.438513    -0.314435
 -0.0683792    0.289504   -0.161855     0.596899   -0.0937903   0.0172601  -0.0465522   0.473933    0.221648     0.274472      0.3427       0.166633    0.232531     -0.647203    -0.0786216  -0.0859099   0.0656192    0.310683    0.360973    -0.00983564   0.497019    -0.135265    0.127446    -0.522042     0.51897     -0.386495
  7.01916e-5   0.240722   -0.400051     0.0189464  -0.205555   -0.0399926   0.627222    0.227937   -0.305612     0.000324369  -0.265764     0.0706087   0.179572      0.0799927    0.435451   -0.230231    0.374941     0.291053   -0.787205    -0.246701     0.336907     0.172231    0.476676    -0.270374     0.379686     0.144435
 -0.934881     0.160915   -0.135959     0.0692662  -0.217263   -0.154778    0.428311    0.0604603  -0.0983883   -0.280216     -0.467237     0.059668   -0.00380131    0.0444525   -0.0146869  -0.0247165  -0.0363966   -0.122182   -0.279844    -0.279702    -0.055849    -0.0397632   0.311417     0.822185     0.120049    -0.234863
 -0.052409     0.403602    0.424777    -0.223578    0.798123    0.28476     0.548212   -0.302477   -0.279728     0.120281     -0.528185     0.137111    8.67771e-5    0.377445    -0.409645    0.0804934  -0.0163106   -0.428227   -0.393427     0.025775     0.321695     0.503525    0.308959     0.662082    -0.0966225    0.299771
 -0.651766    -0.182588    0.190625     0.198844    0.799144    0.0456476   0.387089   -0.161807   -0.0655829   -0.167538     -0.00833133  -0.105474   -0.0247106    -0.506847     0.0510194  -0.247627   -0.0327667   -0.391259    0.0447733    0.303586    -0.192075     0.296561    0.275439     0.00699858  -0.162225    -0.184798
 -0.147577     0.207923   -0.35085     -0.184191   -0.151206   -0.214612   -0.237715   -0.369175   -0.0100268   -0.451274      0.317659    -0.13179    -0.28227      -0.582655     0.0755587   0.0984242   0.391229     0.199645    0.11742     -0.00250261  -0.66777     -0.0816445   0.0717136    0.0135893   -0.0343217    0.0110977
  0.00928347   0.245152    0.037278    -0.265125   -0.231334    0.451686   -0.131081   -0.317532   -0.121043    -0.2253       -0.227884    -0.0536718  -0.269125     -0.00345844  -0.0161866   0.234267    0.595983    -0.117002    0.130601    -0.403495     0.275748     0.221803    0.0751949   -0.111959     0.199569     0.13573
  0.009241    -0.109598   -0.1354      -0.122607    0.0147074   0.0319893   0.0962809   0.0213783   0.0555979   -0.0183461    -0.0808427   -0.065076   -0.0231763     0.00868001   0.0448285   0.0201785  -0.103087    -0.0699849  -0.00176299   0.0224423    0.0472992    0.0324396  -0.0229443    0.0112698    0.0822857   -0.136506
  0.124262    -0.166951   -0.261989    -0.0804725   0.0677519   0.0303843  -0.110443   -0.189426   -0.0907494   -0.343164      0.137015     0.702783   -0.000477302   0.0859315   -0.0366353   0.133299   -0.601296     0.152086   -0.0529462    0.192703     0.198034     0.0341568   0.190988     0.359513    -0.105027     0.0405238[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.416954
[ Info: iteration 2, average log likelihood -1.416942
[ Info: iteration 3, average log likelihood -1.416931
[ Info: iteration 4, average log likelihood -1.416921
[ Info: iteration 5, average log likelihood -1.416910
[ Info: iteration 6, average log likelihood -1.416900
[ Info: iteration 7, average log likelihood -1.416890
[ Info: iteration 8, average log likelihood -1.416880
[ Info: iteration 9, average log likelihood -1.416871
[ Info: iteration 10, average log likelihood -1.416861
┌ Info: EM with 100000 data points 10 iterations avll -1.416861
└ 59.0 data points per parameter
kind full, method kmeans
[ Info: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.327392e+05
      1       7.160838e+05      -2.166554e+05 |       32
      2       7.019305e+05      -1.415322e+04 |       32
      3       6.957739e+05      -6.156604e+03 |       32
      4       6.925459e+05      -3.228002e+03 |       32
      5       6.906545e+05      -1.891481e+03 |       32
      6       6.893790e+05      -1.275440e+03 |       32
      7       6.884313e+05      -9.476871e+02 |       32
      8       6.877028e+05      -7.285781e+02 |       32
      9       6.871467e+05      -5.560108e+02 |       32
     10       6.867245e+05      -4.221973e+02 |       32
     11       6.863714e+05      -3.531674e+02 |       32
     12       6.860587e+05      -3.126582e+02 |       32
     13       6.857855e+05      -2.732114e+02 |       32
     14       6.855687e+05      -2.168172e+02 |       32
     15       6.853670e+05      -2.016618e+02 |       32
     16       6.851874e+05      -1.795992e+02 |       32
     17       6.850380e+05      -1.493984e+02 |       32
     18       6.849013e+05      -1.367672e+02 |       32
     19       6.847688e+05      -1.324475e+02 |       32
     20       6.846337e+05      -1.351346e+02 |       32
     21       6.845050e+05      -1.286487e+02 |       32
     22       6.843751e+05      -1.299676e+02 |       32
     23       6.842441e+05      -1.309237e+02 |       32
     24       6.841017e+05      -1.424348e+02 |       32
     25       6.839776e+05      -1.241160e+02 |       32
     26       6.838600e+05      -1.175869e+02 |       32
     27       6.837425e+05      -1.174779e+02 |       32
     28       6.836287e+05      -1.138252e+02 |       32
     29       6.835217e+05      -1.070106e+02 |       32
     30       6.834284e+05      -9.327099e+01 |       32
     31       6.833458e+05      -8.263819e+01 |       32
     32       6.832740e+05      -7.182926e+01 |       32
     33       6.832121e+05      -6.188675e+01 |       32
     34       6.831548e+05      -5.727318e+01 |       32
     35       6.830939e+05      -6.090007e+01 |       32
     36       6.830315e+05      -6.239927e+01 |       32
     37       6.829688e+05      -6.265611e+01 |       32
     38       6.829144e+05      -5.448398e+01 |       32
     39       6.828678e+05      -4.657286e+01 |       32
     40       6.828267e+05      -4.113073e+01 |       32
     41       6.827898e+05      -3.680240e+01 |       32
     42       6.827584e+05      -3.145807e+01 |       32
     43       6.827249e+05      -3.352451e+01 |       32
     44       6.826913e+05      -3.361483e+01 |       32
     45       6.826611e+05      -3.012409e+01 |       32
     46       6.826304e+05      -3.077639e+01 |       32
     47       6.825938e+05      -3.656656e+01 |       32
     48       6.825545e+05      -3.924912e+01 |       32
     49       6.825156e+05      -3.898195e+01 |       32
     50       6.824772e+05      -3.831015e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 682477.2431285288)
┌ Info: K-means with 32000 data points using 50 iterations
└ 37.0 data points per parameter
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.429043
[ Info: iteration 2, average log likelihood -1.423938
[ Info: iteration 3, average log likelihood -1.422557
[ Info: iteration 4, average log likelihood -1.421586
[ Info: iteration 5, average log likelihood -1.420618
[ Info: iteration 6, average log likelihood -1.419710
[ Info: iteration 7, average log likelihood -1.419024
[ Info: iteration 8, average log likelihood -1.418585
[ Info: iteration 9, average log likelihood -1.418308
[ Info: iteration 10, average log likelihood -1.418119
[ Info: iteration 11, average log likelihood -1.417977
[ Info: iteration 12, average log likelihood -1.417863
[ Info: iteration 13, average log likelihood -1.417768
[ Info: iteration 14, average log likelihood -1.417687
[ Info: iteration 15, average log likelihood -1.417615
[ Info: iteration 16, average log likelihood -1.417551
[ Info: iteration 17, average log likelihood -1.417494
[ Info: iteration 18, average log likelihood -1.417442
[ Info: iteration 19, average log likelihood -1.417394
[ Info: iteration 20, average log likelihood -1.417351
[ Info: iteration 21, average log likelihood -1.417311
[ Info: iteration 22, average log likelihood -1.417275
[ Info: iteration 23, average log likelihood -1.417241
[ Info: iteration 24, average log likelihood -1.417210
[ Info: iteration 25, average log likelihood -1.417180
[ Info: iteration 26, average log likelihood -1.417153
[ Info: iteration 27, average log likelihood -1.417128
[ Info: iteration 28, average log likelihood -1.417104
[ Info: iteration 29, average log likelihood -1.417081
[ Info: iteration 30, average log likelihood -1.417060
[ Info: iteration 31, average log likelihood -1.417039
[ Info: iteration 32, average log likelihood -1.417020
[ Info: iteration 33, average log likelihood -1.417002
[ Info: iteration 34, average log likelihood -1.416985
[ Info: iteration 35, average log likelihood -1.416968
[ Info: iteration 36, average log likelihood -1.416952
[ Info: iteration 37, average log likelihood -1.416937
[ Info: iteration 38, average log likelihood -1.416923
[ Info: iteration 39, average log likelihood -1.416909
[ Info: iteration 40, average log likelihood -1.416896
[ Info: iteration 41, average log likelihood -1.416883
[ Info: iteration 42, average log likelihood -1.416871
[ Info: iteration 43, average log likelihood -1.416860
[ Info: iteration 44, average log likelihood -1.416849
[ Info: iteration 45, average log likelihood -1.416839
[ Info: iteration 46, average log likelihood -1.416829
[ Info: iteration 47, average log likelihood -1.416819
[ Info: iteration 48, average log likelihood -1.416810
[ Info: iteration 49, average log likelihood -1.416801
[ Info: iteration 50, average log likelihood -1.416793
┌ Info: EM with 100000 data points 50 iterations avll -1.416793
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.233158   -0.00425921  -0.177674     0.230954     0.194905    0.22808    -0.230355     0.237868    0.220362    -0.184682    0.080493     0.22892     0.0022827   -0.828453    -0.0678648    0.305176     0.624174     0.804527     0.202254   -0.575224     0.00261665  -0.275891    -0.225378    0.167157    0.163172      0.61005
  1.085      -0.12389     -0.191119     0.109807     0.343661    0.108917   -0.266753    -0.0153068   0.189513     0.504099    0.375884     0.138851    0.0814241    0.10504      0.0870802   -0.17422     -0.222215     0.0427156    0.221902   -0.0857476    0.618557     0.298456    -0.154451   -1.30512     0.0379563    -0.154397
 -0.730893    0.540582    -0.137354    -0.255295     0.763939    0.102141    0.0582133   -0.604573    0.312882     0.249547   -0.170493    -0.120545   -0.476348    -0.159458    -0.116025    -0.208389     0.166736    -0.406385     0.0358924  -0.418641     0.190443     0.645874    -0.0628973   0.190077    0.298002     -0.192189
 -0.344945    0.0764226    0.0554105   -0.47668     -0.242072   -0.272649    0.401961    -0.566223   -0.555527    -0.347748   -0.0382947    0.0545432   0.167211     0.290184     0.210844     0.311366     0.0704964   -0.213242    -0.097187    0.0810509   -0.309958    -0.256525     0.354548    0.404412   -0.432443     -0.0120765
  0.221213    0.0554894   -0.0632764   -0.202578    -0.0649518   0.14102     0.0344772   -0.151228   -0.23366      0.0808493  -0.0707669   -0.0760072   0.00206257   0.201429     0.179672     0.0679386    0.0735312    0.00724964  -0.0780895  -0.0175055   -0.0159584    0.195264    -0.0438577  -0.077688    0.0866552     0.0740551
 -0.285069    0.317584    -0.564441    -0.315156    -0.38445     0.509158   -0.297763    -0.404797   -0.374986     0.0612266   0.356597     0.940731    0.0394344   -0.273377     0.0815228    0.208883    -0.600035     0.433443    -0.130857   -0.180853     0.0888922   -0.00465521   0.17048     0.635886    0.362273     -0.489789
  0.163035   -0.47237     -0.116429    -0.409105     0.0888656  -0.222675    0.527222     0.0515491  -0.146904     0.154158   -0.0395108   -0.0914591   0.234269     0.543697     0.126036    -0.179225    -0.829601    -0.193175    -0.0460004   0.150069     0.280467     0.0900528   -0.0915174   0.340383    0.129639     -0.725542
  0.180626   -0.0666488   -0.570159    -0.2748      -1.04027    -0.491757   -0.313122     0.295747    0.235436     0.101542   -0.215968    -0.033182   -0.415363     0.460236     0.246289     0.268453     0.0366732    0.367996    -0.257012   -0.155913    -0.381715    -0.451412    -0.310277   -0.226044   -0.238472      0.0704433
 -0.213764    0.014606     0.0321413    0.279907     0.261268    0.0334562   0.131078     0.139569    0.0943927   -0.0892725  -0.0826179    0.0756161   0.0668068   -0.235504    -0.0960907   -0.0426251   -0.100224    -0.0711268   -0.0508874   0.0214853    0.0846962    0.130797     0.100358    0.0633035   0.0579158    -0.0185361
 -0.346215   -0.378732    -0.103267     0.382752     0.570782    0.157626    0.203467    -0.0984584  -0.0730376   -0.448872    0.445189    -0.0725243   0.115364    -0.831769    -0.0159768   -0.350287    -0.0244085   -0.283054     0.496066    0.574227    -0.176228     0.372525     0.177515   -0.185555   -0.0914405    -0.449158
  0.0515143  -0.0867344   -0.533575    -0.307629    -0.0257379   0.105489    0.139611    -0.216162    0.527612    -0.966477   -0.188931     0.252922   -0.240564    -0.025292     0.363528     0.00011229   0.102998     0.320625     0.0603166  -0.505344     0.0709779    0.33286      0.359908   -0.0589975   0.141196      0.160695
  0.305681    0.307791     0.359464    -0.117935     0.385954    0.316467    0.253821    -0.289185   -0.149842     0.250464   -0.277845    -0.0461704   0.126858     0.904561    -0.458986     0.450454    -0.289205    -0.0191532   -0.802939    0.00641516   0.662387    -0.0434216    0.0603631   0.431231    0.254032      0.376178
 -0.350411    1.09882      0.389829    -0.00235632   0.302325   -0.191091   -0.00895695  -0.670475    0.597756    -0.350741    0.263982    -0.177013    0.139593    -0.369582     0.045798    -0.160913    -0.0967678    0.787518     0.221841    0.210315    -0.335591    -0.475054     0.664224    0.0580655  -0.554042      0.321288
  0.231503   -0.344731    -0.0902209    0.343263    -0.222432    0.109844    0.158491     0.868156   -0.565114     0.503628    0.0141947    0.721211    0.508488    -0.0563946   -0.532346     0.09192      0.151794    -0.272342    -0.188502    0.00668534   0.134894     0.602711    -0.209085   -0.164116    0.396986     -0.543054
  0.0174225  -0.566768     0.358652     0.501945     0.524018   -0.575749    0.279066     0.836343    0.771348     0.147541   -0.121314    -0.638591    0.154517     0.0548299    0.124733    -0.301671     0.460593    -0.237549     0.397506   -0.0307746   -0.199203     0.0453014   -0.156486   -0.199683   -0.303355      0.0314865
  0.513937   -0.437592     0.302634     0.0235817   -0.0741256   0.620955    0.311992    -0.172378   -0.633268    -0.0670462  -0.0248151   -0.478451    0.119524     0.101151     0.816125    -0.0726148    0.243334    -0.395902     0.338211    0.216431    -0.534123    -0.0359548   -0.669498   -0.615221    0.0382044     0.211007
  0.57789     0.337025     0.472646    -0.734794     0.17067     0.129298   -0.459168    -0.174402    0.457434     0.193965    0.196638     0.14383     0.270331     0.438127    -0.175116     0.479423    -0.325091     0.250691     0.60084    -0.0862393    0.0208575   -0.267119    -0.535857    0.449626   -0.394201     -0.133864
  0.750867    0.801114    -0.0429683    0.0928185   -0.387458   -0.243488    0.282204    -0.20335    -0.342532     0.0225163   0.256188     0.127243    0.282822     0.0441258    0.0311547   -0.348739    -0.237719    -0.0871664    0.207221    0.191895    -0.885709     0.735012     0.328203   -0.521535   -0.130847     -0.036481
 -0.144523    0.503324     0.35918      0.102411     0.154543   -0.258134    0.155636     0.374536    0.0176163    0.518218   -0.423031     0.186274    0.306182     0.0691476   -0.238686    -0.220309     0.0462616    0.123286    -0.46428    -0.39701     -0.0217687    0.00129823   0.211578    0.186309   -0.266416      0.335146
  0.515412   -0.219297     0.0833717    0.317011    -0.179108   -0.0835002  -0.793825     0.379989    0.0569138   -0.120879    0.790523     0.128607    0.17952      0.0430908   -0.130469    -0.031318     0.0134653    0.0883622    0.115239   -0.0818561   -0.194606    -0.183351    -0.210469   -0.152943   -0.186702      0.141007
 -0.15779    -0.0451572   -0.155352    -0.119558    -0.0126009  -0.0732343   0.0301547    0.0743193  -0.00991896  -0.0467048   0.0288257    0.0584923  -0.0697188    0.031746     0.00965637  -0.110142    -0.0284711   -0.0583102   -0.0233678   0.00814706  -0.0487536    0.0381366    0.192464    0.0438576  -0.0761143    -0.0638036
 -0.681381   -0.534705     0.070906    -0.2041       0.292312   -0.0254998   0.00515728   0.233624    0.415529    -0.436401   -0.508099    -0.248792   -0.690136     0.103643    -0.202954     0.40302      0.349862    -0.681372    -0.0886956  -0.246476     0.178017    -0.44517     -0.112714    0.620315    0.000452436   0.0333211
  0.327712   -0.247545    -0.474585    -0.226437     0.501681   -0.0925535  -0.296795     0.157877    0.468385     0.541621   -0.147502    -0.563704   -0.365507     0.271179    -0.658445    -0.134859    -0.181389     0.127232     0.200664    0.887201    -0.392577     0.199942    -0.0926847  -0.200548   -0.607283      0.248629
 -0.368043    0.735415     1.02188      0.180188    -0.268388   -0.219859    0.0352707   -0.132717   -0.582393     0.0514733  -0.329722     0.165825    0.66941      0.309515     0.603422    -0.134676     0.326487    -0.853669    -0.486321   -0.655915    -0.127212     0.176087     0.377366   -0.18141     0.10903      -0.367143
  0.0107911   0.395452    -0.318344     0.45516     -0.491162   -0.656952    0.118824     0.817686    0.385806     0.562769    0.234632    -0.3485      0.528886     0.230295     0.33776     -0.155076     0.165906     0.410239    -0.0467626  -0.281047     0.302228     0.00886661   0.0314706  -0.322261    0.909764     -0.291634
 -0.0917116   0.450971     1.01124      0.339824     0.427341   -0.0496363  -0.192228    -0.153708   -0.623857     0.95323     0.119935    -0.610043    0.539706    -0.459726    -0.105656     0.0543724   -0.00995275  -0.171149     0.310434    0.478913    -0.384829    -0.193333    -0.495476    0.213418   -0.0342081    -0.0945177
 -0.35572    -0.058484    -0.619262     0.675777    -0.125321   -0.119634    0.565002     0.215696   -0.705855    -0.362756   -0.264793    -0.10994    -0.179254    -0.279889     0.422485    -0.4381       0.160717    -0.145059    -0.927328   -0.105172    -0.185971     0.367064     0.802482   -0.170083    0.158448      0.332903
 -0.341004   -0.292122    -0.28683      0.0487029    0.212322   -0.375103    0.0614426    0.242029    0.543204    -0.0120384  -0.00527027   0.206484   -0.169444    -0.35484     -0.223476     0.517678    -1.04851      0.437874    -0.453127    0.32405      0.304358    -0.205489    -0.157626    0.211571    0.52934      -0.498061
 -0.379228   -0.751949     0.00287123   0.491277     0.159076    0.0783055  -0.144438     0.105993   -0.167029     0.0271946  -0.249837     0.452675   -0.0603077   -0.0506086    0.66264     -0.439781    -0.178043    -0.173323    -0.117923    0.173802     0.392813    -0.316745     0.0226033   0.0639519  -0.303161      0.619729
 -0.0842796   0.0131368    0.0398747   -0.494899    -0.0902192  -0.360423   -0.122687    -0.0196664   0.269621     0.125413    0.0563811   -0.063837   -0.15955     -0.00834207  -0.0143271    0.0369671    0.104467     0.0221646    0.252346   -0.192122    -0.0948563   -0.492603    -0.0729047  -0.0124662  -0.0846711    -0.187811
  0.0291409  -0.0984522    0.13227     -0.502874     0.329267    1.02223     0.263104    -0.237615   -0.70543     -0.812925   -0.0424667    0.827042   -0.0273684   -0.169098    -0.204665     0.125397    -0.273508    -0.331776     0.252854    0.626085     0.38356      0.103591     0.523239    0.510586   -0.169404      0.498424
 -0.064999    0.539588    -0.426918    -0.0771365   -0.539596    0.425506   -0.0108228   -0.429722   -0.501196    -0.286102    0.0525146   -0.43352    -0.203188    -0.289102    -0.367704     0.306969     0.711386    -0.0271029   -0.103343   -0.0612326    0.252576     0.211494     0.122284   -0.213482    0.699216     -0.358776[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.416784
[ Info: iteration 2, average log likelihood -1.416776
[ Info: iteration 3, average log likelihood -1.416769
[ Info: iteration 4, average log likelihood -1.416761
[ Info: iteration 5, average log likelihood -1.416754
[ Info: iteration 6, average log likelihood -1.416746
[ Info: iteration 7, average log likelihood -1.416739
[ Info: iteration 8, average log likelihood -1.416732
[ Info: iteration 9, average log likelihood -1.416726
[ Info: iteration 10, average log likelihood -1.416719
┌ Info: EM with 100000 data points 10 iterations avll -1.416719
└ 59.0 data points per parameter
[ Info: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.678561e+05
      1       2.230230e+04      -1.455538e+05 |        2
      2       7.823675e+03      -1.447862e+04 |        0
      3       7.823675e+03       0.000000e+00 |        0
K-means converged with 3 iterations (objv = 7823.67549422947)
┌ Info: K-means with 900 data points using 3 iterations
└ 150.0 data points per parameter
[ Info: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
[ Info: iteration 1, average log likelihood -2.043155
[ Info: iteration 2, average log likelihood -2.043154
[ Info: iteration 3, average log likelihood -2.043154
[ Info: iteration 4, average log likelihood -2.043154
[ Info: iteration 5, average log likelihood -2.043154
[ Info: iteration 6, average log likelihood -2.043154
[ Info: iteration 7, average log likelihood -2.043154
[ Info: iteration 8, average log likelihood -2.043154
[ Info: iteration 9, average log likelihood -2.043154
[ Info: iteration 10, average log likelihood -2.043154
┌ Info: EM with 900 data points 10 iterations avll -2.043154
└ 81.8 data points per parameter
   Testing GaussianMixtures tests passed 
