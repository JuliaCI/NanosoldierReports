Julia Version 1.5.0-DEV.45
Commit eb5410a703 (2020-01-10 02:37 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-8.0.1 (ORCJIT, skylake)
Environment:
  JULIA_DEPOT_PATH = ::/usr/local/share/julia

 Resolving package versions...
 Installed GaussianMixtures ─── v0.3.0
 Installed Missings ─────────── v0.4.3
 Installed Blosc ────────────── v0.5.1
 Installed Compat ───────────── v2.2.0
 Installed OrderedCollections ─ v1.1.0
 Installed StatsFuns ────────── v0.9.3
 Installed StaticArrays ─────── v0.12.1
 Installed Distributions ────── v0.22.0
 Installed CMakeWrapper ─────── v0.2.3
 Installed ScikitLearnBase ──── v0.5.0
 Installed URIParser ────────── v0.4.0
 Installed Parameters ───────── v0.12.0
 Installed DataAPI ──────────── v1.1.0
 Installed StatsBase ────────── v0.32.0
 Installed HDF5 ─────────────── v0.12.5
 Installed Distances ────────── v0.8.2
 Installed OpenBLAS_jll ─────── v0.3.7+4
 Installed JLD ──────────────── v0.9.1
 Installed FillArrays ───────── v0.8.2
 Installed BinaryProvider ───── v0.5.8
 Installed NearestNeighbors ─── v0.4.4
 Installed Clustering ───────── v0.13.3
 Installed PDMats ───────────── v0.9.10
 Installed DataStructures ───── v0.17.7
 Installed SortingAlgorithms ── v0.3.1
 Installed OpenSpecFun_jll ──── v0.5.3+1
 Installed CMake ────────────── v1.1.2
 Installed BinDeps ──────────── v1.0.0
 Installed QuadGK ───────────── v2.3.1
 Installed Rmath ────────────── v0.6.0
 Installed Arpack_jll ───────── v3.5.0+2
 Installed FileIO ───────────── v1.2.1
 Installed Arpack ───────────── v0.4.0
 Installed SpecialFunctions ─── v0.9.0
 Installed LegacyStrings ────── v0.4.1
  Updating `~/.julia/environments/v1.5/Project.toml`
  [cc18c42c] + GaussianMixtures v0.3.0
  Updating `~/.julia/environments/v1.5/Manifest.toml`
  [7d9fca2a] + Arpack v0.4.0
  [68821587] + Arpack_jll v3.5.0+2
  [9e28174c] + BinDeps v1.0.0
  [b99e7846] + BinaryProvider v0.5.8
  [a74b3585] + Blosc v0.5.1
  [631607c0] + CMake v1.1.2
  [d5fb7624] + CMakeWrapper v0.2.3
  [aaaa29a8] + Clustering v0.13.3
  [34da2185] + Compat v2.2.0
  [9a962f9c] + DataAPI v1.1.0
  [864edb3b] + DataStructures v0.17.7
  [b4f34e82] + Distances v0.8.2
  [31c24e10] + Distributions v0.22.0
  [5789e2e9] + FileIO v1.2.1
  [1a297f60] + FillArrays v0.8.2
  [cc18c42c] + GaussianMixtures v0.3.0
  [f67ccb44] + HDF5 v0.12.5
  [4138dd39] + JLD v0.9.1
  [1b4a561d] + LegacyStrings v0.4.1
  [e1d29d7a] + Missings v0.4.3
  [b8a86587] + NearestNeighbors v0.4.4
  [4536629a] + OpenBLAS_jll v0.3.7+4
  [efe28fd5] + OpenSpecFun_jll v0.5.3+1
  [bac558e1] + OrderedCollections v1.1.0
  [90014a1f] + PDMats v0.9.10
  [d96e819e] + Parameters v0.12.0
  [1fd47b50] + QuadGK v2.3.1
  [79098fc4] + Rmath v0.6.0
  [6e75b9c4] + ScikitLearnBase v0.5.0
  [a2af1166] + SortingAlgorithms v0.3.1
  [276daf66] + SpecialFunctions v0.9.0
  [90137ffa] + StaticArrays v0.12.1
  [2913bbd2] + StatsBase v0.32.0
  [4c63d2b9] + StatsFuns v0.9.3
  [30578b45] + URIParser v0.4.0
  [2a0f44e3] + Base64 
  [ade2ca70] + Dates 
  [8bb1440f] + DelimitedFiles 
  [8ba89e20] + Distributed 
  [b77e0a4c] + InteractiveUtils 
  [76f85450] + LibGit2 
  [8f399da3] + Libdl 
  [37e2e46d] + LinearAlgebra 
  [56ddb016] + Logging 
  [d6f4376e] + Markdown 
  [a63ad114] + Mmap 
  [44cfe95a] + Pkg 
  [de0858da] + Printf 
  [9abbd945] + Profile 
  [3fa0cd96] + REPL 
  [9a3f8284] + Random 
  [ea8e919c] + SHA 
  [9e88b42a] + Serialization 
  [1a1011a3] + SharedArrays 
  [6462fe0b] + Sockets 
  [2f01184e] + SparseArrays 
  [10745b16] + Statistics 
  [4607b0f0] + SuiteSparse 
  [8dfed614] + Test 
  [cf7118a7] + UUIDs 
  [4ec0a83e] + Unicode 
  Building CMake → `~/.julia/packages/CMake/nSK2r/deps/build.log`
  Updating `/tmp/jl_30D0Uf/Project.toml`
 [no changes]
  Updating `/tmp/jl_30D0Uf/Manifest.toml`
 [no changes]
  Building Blosc → `~/.julia/packages/Blosc/lzFr0/deps/build.log`
  Updating `/tmp/jl_yfxsmK/Project.toml`
 [no changes]
  Updating `/tmp/jl_yfxsmK/Manifest.toml`
 [no changes]
  Building HDF5 ─→ `~/.julia/packages/HDF5/Zh9on/deps/build.log`
  Updating `/tmp/jl_dSk46n/Project.toml`
 [no changes]
  Updating `/tmp/jl_dSk46n/Manifest.toml`
 [no changes]
  Building Rmath → `~/.julia/packages/Rmath/BoBag/deps/build.log`
  Updating `/tmp/jl_KA05Pn/Project.toml`
 [no changes]
  Updating `/tmp/jl_KA05Pn/Manifest.toml`
 [no changes]
   Testing GaussianMixtures
  Updating `/tmp/jl_lbPTFK/Project.toml`
 [no changes]
  Updating `/tmp/jl_lbPTFK/Manifest.toml`
 [no changes]
Running sandbox
Status `/tmp/jl_lbPTFK/Project.toml`
  [aaaa29a8] Clustering v0.13.3
  [34da2185] Compat v2.2.0
  [31c24e10] Distributions v0.22.0
  [5789e2e9] FileIO v1.2.1
  [cc18c42c] GaussianMixtures v0.3.0
  [4138dd39] JLD v0.9.1
  [90014a1f] PDMats v0.9.10
  [6e75b9c4] ScikitLearnBase v0.5.0
  [276daf66] SpecialFunctions v0.9.0
  [2913bbd2] StatsBase v0.32.0
  [8bb1440f] DelimitedFiles 
  [8ba89e20] Distributed 
  [37e2e46d] LinearAlgebra 
  [56ddb016] Logging 
  [de0858da] Printf 
  [9a3f8284] Random 
  [10745b16] Statistics 
[ Info: Testing Data
(100000, -1.1589874865495719e6, [3159.551778027675, 96840.44822197233], [161.91088310782698 -4314.2634129801 -2985.0365225469845; -83.45571582434471 4528.784559743785 2648.032193401585], [[2952.9444962583525 318.0217512003256 -92.09399422576448; 318.0217512003256 6470.67507017714 3041.693636389454; -92.0939942257645 3041.693636389454 5309.120828198702], [97227.41627163636 -60.45385713255159 -98.53093823230743; -60.45385713255153 94286.21823650174 -2976.866742735031; -98.53093823230748 -2976.866742735031 93780.39860250954]])
┌ Warning: rmprocs: process 1 not removed
└ @ Distributed /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Distributed/src/cluster.jl:1015
[ Info: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.280721e+03
      1       8.626603e+02      -4.180610e+02 |        5
      2       8.249169e+02      -3.774333e+01 |        0
      3       8.249169e+02       0.000000e+00 |        0
K-means converged with 3 iterations (objv = 824.9169427989386)
┌ Info: K-means with 272 data points using 3 iterations
└ 11.3 data points per parameter
[ Info: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
┌ Info: EM with 272 data points 0 iterations avll -2.053669
└ 5.8 data points per parameter
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:221
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:221
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:221
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:221
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:631 [inlined]
└ @ Core ./broadcast.jl:631
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:230
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:230
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:631 [inlined]
└ @ Core ./broadcast.jl:631
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:631 [inlined]
└ @ Core ./broadcast.jl:631
[ Info: iteration 1, lowerbound -3.807768
[ Info: iteration 2, lowerbound -3.682283
[ Info: iteration 3, lowerbound -3.533972
[ Info: iteration 4, lowerbound -3.355095
[ Info: iteration 5, lowerbound -3.173346
[ Info: iteration 6, lowerbound -3.019838
[ Info: dropping number of Gaussions to 6
[ Info: iteration 7, lowerbound -2.900560
[ Info: iteration 8, lowerbound -2.820951
[ Info: dropping number of Gaussions to 5
[ Info: iteration 9, lowerbound -2.792345
[ Info: dropping number of Gaussions to 3
[ Info: iteration 10, lowerbound -2.774633
[ Info: iteration 11, lowerbound -2.758638
[ Info: iteration 12, lowerbound -2.746592
[ Info: iteration 13, lowerbound -2.729986
[ Info: iteration 14, lowerbound -2.707812
[ Info: iteration 15, lowerbound -2.679414
[ Info: iteration 16, lowerbound -2.644792
[ Info: iteration 17, lowerbound -2.604840
[ Info: iteration 18, lowerbound -2.561408
[ Info: iteration 19, lowerbound -2.517065
[ Info: iteration 20, lowerbound -2.474413
[ Info: iteration 21, lowerbound -2.435157
[ Info: iteration 22, lowerbound -2.399596
[ Info: iteration 23, lowerbound -2.367199
[ Info: iteration 24, lowerbound -2.338460
[ Info: iteration 25, lowerbound -2.316878
[ Info: iteration 26, lowerbound -2.307578
[ Info: dropping number of Gaussions to 2
[ Info: iteration 27, lowerbound -2.302985
[ Info: iteration 28, lowerbound -2.299261
[ Info: iteration 29, lowerbound -2.299257
[ Info: iteration 30, lowerbound -2.299255
[ Info: iteration 31, lowerbound -2.299254
[ Info: iteration 32, lowerbound -2.299253
[ Info: iteration 33, lowerbound -2.299253
[ Info: iteration 34, lowerbound -2.299253
[ Info: iteration 35, lowerbound -2.299253
[ Info: iteration 36, lowerbound -2.299253
[ Info: iteration 37, lowerbound -2.299253
[ Info: iteration 38, lowerbound -2.299253
[ Info: iteration 39, lowerbound -2.299253
[ Info: iteration 40, lowerbound -2.299253
[ Info: iteration 41, lowerbound -2.299253
[ Info: iteration 42, lowerbound -2.299253
[ Info: iteration 43, lowerbound -2.299253
[ Info: iteration 44, lowerbound -2.299253
[ Info: iteration 45, lowerbound -2.299253
[ Info: iteration 46, lowerbound -2.299253
[ Info: iteration 47, lowerbound -2.299253
[ Info: iteration 48, lowerbound -2.299253
[ Info: iteration 49, lowerbound -2.299253
[ Info: iteration 50, lowerbound -2.299253
[ Info: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
History[Fri Jan 10 22:43:36 2020: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
, Fri Jan 10 22:43:44 2020: K-means with 272 data points using 3 iterations
11.3 data points per parameter
, Fri Jan 10 22:43:47 2020: EM with 272 data points 0 iterations avll -2.053669
5.8 data points per parameter
, Fri Jan 10 22:43:49 2020: GMM converted to Variational GMM
, Fri Jan 10 22:43:58 2020: iteration 1, lowerbound -3.807768
, Fri Jan 10 22:43:58 2020: iteration 2, lowerbound -3.682283
, Fri Jan 10 22:43:58 2020: iteration 3, lowerbound -3.533972
, Fri Jan 10 22:43:58 2020: iteration 4, lowerbound -3.355095
, Fri Jan 10 22:43:58 2020: iteration 5, lowerbound -3.173346
, Fri Jan 10 22:43:58 2020: iteration 6, lowerbound -3.019838
, Fri Jan 10 22:43:59 2020: dropping number of Gaussions to 6
, Fri Jan 10 22:43:59 2020: iteration 7, lowerbound -2.900560
, Fri Jan 10 22:43:59 2020: iteration 8, lowerbound -2.820951
, Fri Jan 10 22:43:59 2020: dropping number of Gaussions to 5
, Fri Jan 10 22:43:59 2020: iteration 9, lowerbound -2.792345
, Fri Jan 10 22:43:59 2020: dropping number of Gaussions to 3
, Fri Jan 10 22:43:59 2020: iteration 10, lowerbound -2.774633
, Fri Jan 10 22:43:59 2020: iteration 11, lowerbound -2.758638
, Fri Jan 10 22:43:59 2020: iteration 12, lowerbound -2.746592
, Fri Jan 10 22:43:59 2020: iteration 13, lowerbound -2.729986
, Fri Jan 10 22:43:59 2020: iteration 14, lowerbound -2.707812
, Fri Jan 10 22:43:59 2020: iteration 15, lowerbound -2.679414
, Fri Jan 10 22:43:59 2020: iteration 16, lowerbound -2.644792
, Fri Jan 10 22:43:59 2020: iteration 17, lowerbound -2.604840
, Fri Jan 10 22:43:59 2020: iteration 18, lowerbound -2.561408
, Fri Jan 10 22:43:59 2020: iteration 19, lowerbound -2.517065
, Fri Jan 10 22:43:59 2020: iteration 20, lowerbound -2.474413
, Fri Jan 10 22:43:59 2020: iteration 21, lowerbound -2.435157
, Fri Jan 10 22:43:59 2020: iteration 22, lowerbound -2.399596
, Fri Jan 10 22:43:59 2020: iteration 23, lowerbound -2.367199
, Fri Jan 10 22:43:59 2020: iteration 24, lowerbound -2.338460
, Fri Jan 10 22:43:59 2020: iteration 25, lowerbound -2.316878
, Fri Jan 10 22:43:59 2020: iteration 26, lowerbound -2.307578
, Fri Jan 10 22:43:59 2020: dropping number of Gaussions to 2
, Fri Jan 10 22:43:59 2020: iteration 27, lowerbound -2.302985
, Fri Jan 10 22:43:59 2020: iteration 28, lowerbound -2.299261
, Fri Jan 10 22:43:59 2020: iteration 29, lowerbound -2.299257
, Fri Jan 10 22:43:59 2020: iteration 30, lowerbound -2.299255
, Fri Jan 10 22:43:59 2020: iteration 31, lowerbound -2.299254
, Fri Jan 10 22:43:59 2020: iteration 32, lowerbound -2.299253
, Fri Jan 10 22:43:59 2020: iteration 33, lowerbound -2.299253
, Fri Jan 10 22:43:59 2020: iteration 34, lowerbound -2.299253
, Fri Jan 10 22:43:59 2020: iteration 35, lowerbound -2.299253
, Fri Jan 10 22:43:59 2020: iteration 36, lowerbound -2.299253
, Fri Jan 10 22:43:59 2020: iteration 37, lowerbound -2.299253
, Fri Jan 10 22:43:59 2020: iteration 38, lowerbound -2.299253
, Fri Jan 10 22:43:59 2020: iteration 39, lowerbound -2.299253
, Fri Jan 10 22:43:59 2020: iteration 40, lowerbound -2.299253
, Fri Jan 10 22:43:59 2020: iteration 41, lowerbound -2.299253
, Fri Jan 10 22:43:59 2020: iteration 42, lowerbound -2.299253
, Fri Jan 10 22:43:59 2020: iteration 43, lowerbound -2.299253
, Fri Jan 10 22:43:59 2020: iteration 44, lowerbound -2.299253
, Fri Jan 10 22:43:59 2020: iteration 45, lowerbound -2.299253
, Fri Jan 10 22:43:59 2020: iteration 46, lowerbound -2.299253
, Fri Jan 10 22:43:59 2020: iteration 47, lowerbound -2.299253
, Fri Jan 10 22:43:59 2020: iteration 48, lowerbound -2.299253
, Fri Jan 10 22:43:59 2020: iteration 49, lowerbound -2.299253
, Fri Jan 10 22:43:59 2020: iteration 50, lowerbound -2.299253
, Fri Jan 10 22:43:59 2020: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [178.04509222620683, 95.9549077737932]
β = [178.04509222620683, 95.9549077737932]
m = [4.250300733268344 79.28686694433881; 2.0002292577737495 53.85198717245284]
ν = [180.04509222620683, 97.9549077737932]
W = LinearAlgebra.UpperTriangular{Float64,Array{Float64,2}}[[0.18404155547463094 -0.0076440490423476636; 0.0 0.0085817051663043], [0.37587636119753715 -0.008953123827377972; 0.0 0.0127486647774175]]
Kind: diag, size256
nx: 100000 sum(zeroth order stats): 99999.99999999996
avll from stats: -0.9867772286946044
avll from llpg:  -0.9867772286946092
avll direct:     -0.9867772286946089
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 100000.0
avll from stats: -1.0047303124835278
avll from llpg:  -1.0047303124835278
avll direct:     -1.0047303124835278
sum posterior: 100000.0
32×26 Array{Float64,2}:
  0.0923144     0.00755195  -0.0580717    0.0240363    0.126915      0.0399172   -0.0113279     0.02155      -0.018971     0.0370865   -0.103906     0.0826566    0.0167064    0.0327098    0.123506     0.0653844    0.0846062     0.0126546    0.106719     0.0480214   0.149573     0.0474676   -0.119071     0.0471451   -0.0203953   -0.0580468
 -0.0143114     0.0336484   -0.0453783   -0.151527    -0.0930963     0.0919644    0.0296524     0.0577378    -0.00965691   0.283845     0.0296104    0.0293471   -0.0523429   -0.194194     0.0573817   -0.0196512    0.122141      0.06374      0.0164845   -0.134965   -0.0885577    0.149242     0.126033    -0.105066    -0.26676      0.184488
  0.0422162    -0.120313     0.0899419    0.162903     0.108351     -0.0357353    0.0437531     0.11629       0.0686138   -0.0235417   -0.00999322  -0.00594713  -0.0395618    0.00341254  -0.114961     0.0578619   -0.151477      0.101984    -0.0116276    0.0676312   0.00601751   0.0549247    0.040911     0.0654088    0.0482693    0.0258965
 -0.14859       0.0242554   -0.183327    -0.0958216    0.19112      -0.103841     0.00873206   -0.178234     -0.198545     0.062292    -0.0220998   -0.232435    -0.0145697    0.129606    -0.0773673    0.253248    -0.108095     -0.113087     0.0844631    0.0229718   0.00618807  -0.113093    -0.0539657   -0.22197      0.0920611   -0.0301333
  0.0137614     0.0587651    0.0618539   -0.053515    -0.11444      -0.0122679   -0.182745     -0.0191168     0.135018     0.132289    -0.0181168    0.0206208    0.126916     0.223579     0.0490148   -0.0884672   -0.0559379     0.0290861   -0.132522     0.106345    0.0688911   -0.0316284    0.0676337   -0.290277     0.064891    -0.0876223
  0.025546      0.0463879   -0.0742075    0.164699    -0.0191819    -0.142411     0.0219788    -0.0953233    -0.0967453    0.0194044    0.095419    -0.0311212    0.0254946   -0.0222098   -0.00536087   0.0536433   -0.0315304     0.00905131  -0.0192675    0.115036   -0.0540399   -0.0836561    0.0741084    0.0073719   -0.020074     0.1374
 -0.166601     -0.101728     0.0434745   -0.0450777    0.0355631    -0.0231812    0.0997871     0.0114489    -0.0473516   -0.0492124   -0.00132807   0.0951616   -0.0542969    0.0311747    0.102223     0.0311949   -0.0705621    -0.0612875   -0.0244184    0.0842897  -0.0430648   -0.0294119    0.0501901    0.0455551    0.0608344   -0.0150202
 -0.141232     -0.0968143    0.00130378  -0.147899     0.0666479     0.117399     0.156523      0.138498     -0.0629465    0.169936     0.0210665    0.0179723    0.144581     0.0552151   -0.0735191   -0.00122279  -0.179805      0.0196485   -0.144678     0.0459304  -0.0206682   -0.177015    -0.097242     0.141817     0.009583     0.00802988
 -0.00521575    0.0514405   -0.0786527   -0.0610943   -0.0835855     0.00582086  -0.00285143   -0.0106908     0.155657    -0.0194407   -0.0369538   -0.0175165    0.178704    -0.160128     0.11014     -0.0369218   -0.0581924     0.0144033    0.00636127  -0.0348977   0.0806071    0.0697747    0.0328083    0.0855297   -0.0751455   -0.114932
 -0.0861597    -0.0836284   -0.0763922    0.0104619   -0.12831       0.0251938   -0.0286953     0.022819      0.0887272   -0.0623536    0.150032     0.0603354   -0.168052     0.0184861    0.0302141   -0.117271    -0.000981705   0.139805     0.136565     0.128403   -0.0428286   -0.231753    -0.129831    -0.0987225   -0.19602     -0.00359993
 -0.000685837   0.00600035  -0.167206     0.0556737    0.146175      0.193797    -0.00779796   -0.142006     -0.191093     0.0273474    0.0565397   -0.00071305   0.128375    -0.0686942   -0.0198728   -0.0014738   -0.0164847     0.213559     0.0126498   -0.0468544  -0.0862667   -0.0232726    0.00179076  -0.168982    -0.0452084    0.0992376
 -0.0093287     0.0998392    0.0485814    0.0918171   -0.0857079     0.0834853    0.0356248     0.132247      0.0218815    0.0544206   -0.0827169   -0.205533     0.0829502   -0.0427837    0.0476697   -0.095271    -0.156258     -0.0445768    0.0305528    0.0371935  -0.0883889   -0.0364914   -0.00860736  -0.0108655    0.120185     0.136437
  0.0367095     0.153165     0.0394206    0.0435564    0.00172718   -0.0316747    0.187406      0.0452779     0.157015    -0.0415918   -0.0137542    0.114103     0.0384304    0.0857953   -0.0112186    0.149813    -0.085315     -0.131377    -0.0929774    0.233899   -0.0568202    0.143785    -0.0264999   -0.0333257    0.0368538    0.0317427
  0.185054     -0.0625418    0.0236731    0.00319207  -0.00974625   -0.0403351   -0.062363     -0.0825307    -0.0225166    0.0646315   -0.149086    -0.0379247   -0.056405    -0.0727886    0.0785959   -0.0678606    0.120601     -0.0431897    0.0633583    0.0616986   0.023482    -0.0854038    0.327602    -0.077436    -0.00562632   0.112694
  0.0402031     0.0172263    0.0015448    0.0311194    0.218163      0.00752811  -0.123306     -0.110116     -0.102701     0.0833806   -0.0591815    0.147601     0.0657783    0.0623629   -0.0607631    0.205865    -0.127641     -0.0401685   -0.102067    -0.149596   -0.0301792   -0.155778     0.165638     0.262718     0.0706853   -0.0846327
 -0.0404685    -0.0509219   -0.0549886    0.105842    -0.0550303    -0.215883     0.010337     -0.0347744     0.123758     0.194419     0.0909071   -0.0800918   -0.170444     0.101647     0.120521    -0.192538     0.0477809     0.185672    -0.0780648   -0.0212296  -0.120961     0.036845    -0.125185    -0.198578     0.0859266   -0.151688
 -0.0699377    -0.134065    -0.0416386   -0.00338201  -0.0365585    -0.0886694    0.122904     -0.178213      0.0308616    0.108045    -0.0403467   -0.1279      -0.052949    -0.0817093    0.164393     0.00259169   0.0131417    -0.0183376    0.0153441   -0.0754637   0.0537465   -0.145262    -0.0112674   -0.0278569   -0.277144    -0.00991518
 -0.0464235    -0.123381    -0.0325607   -0.00347713  -0.18256      -0.0564153    0.0898372     0.0294936    -0.0352499    0.00886947   0.0873145    0.241117     0.0980892   -0.026105    -0.0668138    0.166799     0.00868392    0.0259173    0.127196     0.0176955  -0.111241     0.107286     0.0733222   -0.0962686   -0.0861689   -0.0384857
  0.0701031    -0.0226362   -0.0485131   -0.123746    -0.0944352    -0.048069     0.112253      0.0419039    -0.129755    -0.0540061   -0.0678633    0.115994    -0.0199931   -0.0852195   -0.0728766   -0.0785931    0.243817     -0.167602    -0.0151655   -0.083186    0.0487599    0.12109      0.0119296    0.0849113   -0.148248     0.0306004
  0.108035      0.0900793    0.0103666    0.0853318   -0.0639051    -0.0319573   -0.0402597     0.0095839    -0.103912    -0.152394     0.0414661    0.127896    -0.0120246   -0.154641     0.137893     0.0982064   -0.000297563  -0.0238452   -0.146053    -0.12241     0.0866311    0.0976642    0.0483589   -0.0906044   -0.146548     0.184801
  0.0945207     0.125548     0.0973654   -0.0286917   -0.0578617     0.0286842    0.0739962    -0.015043     -0.036671    -0.126892    -0.109049     0.039845    -0.140781    -0.0328548   -0.0122542   -0.0912863   -0.141718      0.0613904   -0.0120234   -0.161292   -0.0677976   -0.0797821    0.0150901   -0.0677438   -0.149009    -0.125534
 -0.0750693     0.025826     0.0348026   -0.0519      -0.124984      0.00579085  -0.205911      0.101606      0.0777031    0.138851    -0.0167077    0.0544587    0.092074     0.0262195    0.00683118   0.125958     0.2584       -0.192667     0.0140268    0.0923751  -0.0411532    0.00298079  -0.0974255   -0.0954139    0.0380344    0.00543406
  0.0891999    -0.086511     0.123236    -0.0371717   -0.0879238    -0.0383819    0.000248264  -0.107103      0.0238208   -0.00572871  -0.0362417   -0.102143    -0.0887326    0.021814     0.19489      0.0113236    0.114005     -0.0191263   -0.117029     0.0412458  -0.053967     0.166232     0.104372     0.0790404   -0.0712053    0.0676006
  0.148828      0.222064    -0.110248    -0.0225007    0.0511905     0.0568314    0.0680811     0.184402     -0.0630539   -0.20396     -0.00306818  -0.0950683    0.0201297   -0.0137532    0.154166    -0.00663552   0.138117     -0.0220014   -0.0383912   -0.11348    -0.0434155   -0.0118618    0.116798     0.0977022    0.0134247   -0.0920194
 -0.118521      0.0459641   -0.0217693   -0.118551     0.0413929    -0.172953     0.148182     -0.0142323     0.0134523   -0.119552    -0.0585701   -0.0397806    0.0309897   -0.0361448    0.127479    -0.0765485    0.0879344    -0.0327318    0.145283     0.02763    -0.0125128    0.0604882   -0.00624943   0.0684353   -0.103739     0.0824641
 -0.0230568     0.0647342   -0.195411     0.0552875    0.0504192     0.0306277   -0.125685      0.140415      0.307381     0.0187493    0.144435    -0.0256336    0.00829021   0.0642949    0.0824643   -0.175577    -0.0868077     0.00318233  -0.0396369    0.0212299   0.041608     0.135597    -0.173615     0.0704559   -0.172537    -0.116683
  0.102205      0.0524409   -0.117677    -0.0603771   -0.023921      0.0092927    0.148116     -0.131505      0.0208757   -0.0531946   -0.124191     0.0151523   -0.0539504   -0.126445     0.0910429   -0.091334    -0.05399      -0.106016     0.0513412    0.0431554   0.0609138   -0.0876725   -0.188017    -0.0125531    0.0540154   -0.0155762
  0.0760114    -0.164718     0.0859886    0.0261407   -0.0802192     0.0511506   -0.0684422    -0.0184449    -0.184109     0.0677545   -0.196967    -0.0381846    0.0921091   -0.184195     0.0610964    0.11937     -0.017139     -0.0706562    0.00862729   0.057517    0.108727     0.0151276    0.123816    -0.00209416  -0.108571     0.0482723
 -0.184704      0.0528865   -0.0954537    0.0441644   -0.0987494     0.0338565   -0.00245179   -0.0865687     0.183591    -0.0251417    0.0196692    0.183307     0.028797    -0.19628     -0.228604     0.238113     0.157774     -0.0812431    0.0327233   -0.0661563   0.0503763    0.023989     0.0487951   -0.134964    -0.191984     0.0158651
  0.0951407     0.0443228   -0.0330148    0.0496319    0.000486421  -0.0795592   -0.0638079     0.000954873  -0.0870398   -0.0493312   -0.095593     0.059949    -0.116726    -0.122782     0.00928324  -0.0686542   -0.0143058     0.107721    -0.059498    -0.145522   -0.0517395    0.0509615    0.012832    -0.0965803   -0.107402    -0.00110369
 -0.0225456     0.00426188   0.0136473    0.151803    -0.032549     -0.0221653    0.0484262    -0.0338162    -0.0509033   -0.0391216   -0.0909244    0.0506255   -0.0489592   -0.142042    -0.0949244   -0.0111049   -0.183436      0.305463     0.0564755    0.11911    -0.00982615  -0.075972    -0.0615803    0.124207    -0.097144    -0.0292281
  0.0894286    -0.118701     0.0648483    0.159009     0.00518762    0.0182445   -0.127171     -0.0427445     0.0471994   -0.19542      0.0493914   -0.0566007    0.151916     0.0336976    0.0966886    0.0617964   -0.0662904     0.114242     0.0749389    0.120734    0.0384862   -0.0732098   -0.269725     0.11292      0.00991732  -0.0951494kind diag, method split
┌ Info: 0: avll = 
└   tll[1] = -1.403286126611891
[ Info: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.403383
[ Info: iteration 2, average log likelihood -1.403297
[ Info: iteration 3, average log likelihood -1.402795
[ Info: iteration 4, average log likelihood -1.396662
[ Info: iteration 5, average log likelihood -1.379042
[ Info: iteration 6, average log likelihood -1.370556
[ Info: iteration 7, average log likelihood -1.368854
[ Info: iteration 8, average log likelihood -1.368219
[ Info: iteration 9, average log likelihood -1.367868
[ Info: iteration 10, average log likelihood -1.367628
[ Info: iteration 11, average log likelihood -1.367448
[ Info: iteration 12, average log likelihood -1.367313
[ Info: iteration 13, average log likelihood -1.367211
[ Info: iteration 14, average log likelihood -1.367131
[ Info: iteration 15, average log likelihood -1.367066
[ Info: iteration 16, average log likelihood -1.367014
[ Info: iteration 17, average log likelihood -1.366970
[ Info: iteration 18, average log likelihood -1.366932
[ Info: iteration 19, average log likelihood -1.366898
[ Info: iteration 20, average log likelihood -1.366868
[ Info: iteration 21, average log likelihood -1.366840
[ Info: iteration 22, average log likelihood -1.366815
[ Info: iteration 23, average log likelihood -1.366793
[ Info: iteration 24, average log likelihood -1.366774
[ Info: iteration 25, average log likelihood -1.366756
[ Info: iteration 26, average log likelihood -1.366739
[ Info: iteration 27, average log likelihood -1.366722
[ Info: iteration 28, average log likelihood -1.366704
[ Info: iteration 29, average log likelihood -1.366684
[ Info: iteration 30, average log likelihood -1.366663
[ Info: iteration 31, average log likelihood -1.366639
[ Info: iteration 32, average log likelihood -1.366614
[ Info: iteration 33, average log likelihood -1.366586
[ Info: iteration 34, average log likelihood -1.366555
[ Info: iteration 35, average log likelihood -1.366521
[ Info: iteration 36, average log likelihood -1.366489
[ Info: iteration 37, average log likelihood -1.366462
[ Info: iteration 38, average log likelihood -1.366439
[ Info: iteration 39, average log likelihood -1.366421
[ Info: iteration 40, average log likelihood -1.366407
[ Info: iteration 41, average log likelihood -1.366396
[ Info: iteration 42, average log likelihood -1.366387
[ Info: iteration 43, average log likelihood -1.366380
[ Info: iteration 44, average log likelihood -1.366375
[ Info: iteration 45, average log likelihood -1.366370
[ Info: iteration 46, average log likelihood -1.366368
[ Info: iteration 47, average log likelihood -1.366365
[ Info: iteration 48, average log likelihood -1.366364
[ Info: iteration 49, average log likelihood -1.366363
[ Info: iteration 50, average log likelihood -1.366362
┌ Info: EM with 100000 data points 50 iterations avll -1.366362
└ 952.4 data points per parameter
┌ Info: 1
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4033825713172
│     -1.4032971191859447
│      ⋮
└     -1.3663623463443944
[ Info: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.366540
[ Info: iteration 2, average log likelihood -1.366372
[ Info: iteration 3, average log likelihood -1.365617
[ Info: iteration 4, average log likelihood -1.358026
[ Info: iteration 5, average log likelihood -1.339863
[ Info: iteration 6, average log likelihood -1.330412
[ Info: iteration 7, average log likelihood -1.327331
[ Info: iteration 8, average log likelihood -1.325687
[ Info: iteration 9, average log likelihood -1.324479
[ Info: iteration 10, average log likelihood -1.323483
[ Info: iteration 11, average log likelihood -1.322708
[ Info: iteration 12, average log likelihood -1.322134
[ Info: iteration 13, average log likelihood -1.321660
[ Info: iteration 14, average log likelihood -1.321226
[ Info: iteration 15, average log likelihood -1.320809
[ Info: iteration 16, average log likelihood -1.320396
[ Info: iteration 17, average log likelihood -1.319982
[ Info: iteration 18, average log likelihood -1.319573
[ Info: iteration 19, average log likelihood -1.319163
[ Info: iteration 20, average log likelihood -1.318693
[ Info: iteration 21, average log likelihood -1.318054
[ Info: iteration 22, average log likelihood -1.317023
[ Info: iteration 23, average log likelihood -1.315699
[ Info: iteration 24, average log likelihood -1.314443
[ Info: iteration 25, average log likelihood -1.313357
[ Info: iteration 26, average log likelihood -1.312474
[ Info: iteration 27, average log likelihood -1.311816
[ Info: iteration 28, average log likelihood -1.311367
[ Info: iteration 29, average log likelihood -1.311092
[ Info: iteration 30, average log likelihood -1.310943
[ Info: iteration 31, average log likelihood -1.310866
[ Info: iteration 32, average log likelihood -1.310826
[ Info: iteration 33, average log likelihood -1.310803
[ Info: iteration 34, average log likelihood -1.310789
[ Info: iteration 35, average log likelihood -1.310779
[ Info: iteration 36, average log likelihood -1.310772
[ Info: iteration 37, average log likelihood -1.310766
[ Info: iteration 38, average log likelihood -1.310761
[ Info: iteration 39, average log likelihood -1.310756
[ Info: iteration 40, average log likelihood -1.310753
[ Info: iteration 41, average log likelihood -1.310749
[ Info: iteration 42, average log likelihood -1.310747
[ Info: iteration 43, average log likelihood -1.310744
[ Info: iteration 44, average log likelihood -1.310742
[ Info: iteration 45, average log likelihood -1.310740
[ Info: iteration 46, average log likelihood -1.310739
[ Info: iteration 47, average log likelihood -1.310737
[ Info: iteration 48, average log likelihood -1.310736
[ Info: iteration 49, average log likelihood -1.310735
[ Info: iteration 50, average log likelihood -1.310734
┌ Info: EM with 100000 data points 50 iterations avll -1.310734
└ 473.9 data points per parameter
┌ Info: 2
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.3665395605716677
│     -1.3663722899493453
│      ⋮
└     -1.3107340059881947
[ Info: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.310985
[ Info: iteration 2, average log likelihood -1.310759
[ Info: iteration 3, average log likelihood -1.310163
[ Info: iteration 4, average log likelihood -1.304331
[ Info: iteration 5, average log likelihood -1.284507
[ Info: iteration 6, average log likelihood -1.270595
[ Info: iteration 7, average log likelihood -1.266168
[ Info: iteration 8, average log likelihood -1.263959
[ Info: iteration 9, average log likelihood -1.262276
[ Info: iteration 10, average log likelihood -1.260833
[ Info: iteration 11, average log likelihood -1.259655
[ Info: iteration 12, average log likelihood -1.258725
[ Info: iteration 13, average log likelihood -1.258048
[ Info: iteration 14, average log likelihood -1.257523
[ Info: iteration 15, average log likelihood -1.257086
[ Info: iteration 16, average log likelihood -1.256730
[ Info: iteration 17, average log likelihood -1.256468
[ Info: iteration 18, average log likelihood -1.256303
[ Info: iteration 19, average log likelihood -1.256196
[ Info: iteration 20, average log likelihood -1.256120
[ Info: iteration 21, average log likelihood -1.256059
[ Info: iteration 22, average log likelihood -1.256006
[ Info: iteration 23, average log likelihood -1.255956
[ Info: iteration 24, average log likelihood -1.255906
[ Info: iteration 25, average log likelihood -1.255858
[ Info: iteration 26, average log likelihood -1.255810
[ Info: iteration 27, average log likelihood -1.255763
[ Info: iteration 28, average log likelihood -1.255715
[ Info: iteration 29, average log likelihood -1.255668
[ Info: iteration 30, average log likelihood -1.255620
[ Info: iteration 31, average log likelihood -1.255571
[ Info: iteration 32, average log likelihood -1.255521
[ Info: iteration 33, average log likelihood -1.255468
[ Info: iteration 34, average log likelihood -1.255413
[ Info: iteration 35, average log likelihood -1.255355
[ Info: iteration 36, average log likelihood -1.255292
[ Info: iteration 37, average log likelihood -1.255221
[ Info: iteration 38, average log likelihood -1.255143
[ Info: iteration 39, average log likelihood -1.255053
[ Info: iteration 40, average log likelihood -1.254950
[ Info: iteration 41, average log likelihood -1.254830
[ Info: iteration 42, average log likelihood -1.254688
[ Info: iteration 43, average log likelihood -1.254525
[ Info: iteration 44, average log likelihood -1.254338
[ Info: iteration 45, average log likelihood -1.254115
[ Info: iteration 46, average log likelihood -1.253844
[ Info: iteration 47, average log likelihood -1.253517
[ Info: iteration 48, average log likelihood -1.253155
[ Info: iteration 49, average log likelihood -1.252797
[ Info: iteration 50, average log likelihood -1.252473
┌ Info: EM with 100000 data points 50 iterations avll -1.252473
└ 236.4 data points per parameter
┌ Info: 3
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.3109853887543563
│     -1.3107591397304565
│      ⋮
└     -1.2524727426248012
[ Info: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.252491
[ Info: iteration 2, average log likelihood -1.251959
[ Info: iteration 3, average log likelihood -1.250983
[ Info: iteration 4, average log likelihood -1.241868
[ Info: iteration 5, average log likelihood -1.211901
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.190845
[ Info: iteration 7, average log likelihood -1.191395
[ Info: iteration 8, average log likelihood -1.180628
[ Info: iteration 9, average log likelihood -1.173966
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      3
│     15
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.167401
[ Info: iteration 11, average log likelihood -1.190672
[ Info: iteration 12, average log likelihood -1.178623
[ Info: iteration 13, average log likelihood -1.174224
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 14, average log likelihood -1.169754
[ Info: iteration 15, average log likelihood -1.177647
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     15
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 16, average log likelihood -1.169005
[ Info: iteration 17, average log likelihood -1.180359
[ Info: iteration 18, average log likelihood -1.173587
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 19, average log likelihood -1.168607
[ Info: iteration 20, average log likelihood -1.178429
[ Info: iteration 21, average log likelihood -1.171248
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     15
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 22, average log likelihood -1.168086
[ Info: iteration 23, average log likelihood -1.178626
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 24, average log likelihood -1.170626
[ Info: iteration 25, average log likelihood -1.179154
[ Info: iteration 26, average log likelihood -1.172353
[ Info: iteration 27, average log likelihood -1.170142
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     15
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 28, average log likelihood -1.167721
[ Info: iteration 29, average log likelihood -1.177248
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 30, average log likelihood -1.168500
[ Info: iteration 31, average log likelihood -1.178385
[ Info: iteration 32, average log likelihood -1.171195
[ Info: iteration 33, average log likelihood -1.168580
[ Info: iteration 34, average log likelihood -1.165767
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      3
│     15
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 35, average log likelihood -1.160806
[ Info: iteration 36, average log likelihood -1.184336
[ Info: iteration 37, average log likelihood -1.173020
[ Info: iteration 38, average log likelihood -1.170195
[ Info: iteration 39, average log likelihood -1.168379
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 40, average log likelihood -1.165028
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     15
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 41, average log likelihood -1.172735
[ Info: iteration 42, average log likelihood -1.178524
[ Info: iteration 43, average log likelihood -1.172184
[ Info: iteration 44, average log likelihood -1.170154
[ Info: iteration 45, average log likelihood -1.168510
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 46, average log likelihood -1.165306
[ Info: iteration 47, average log likelihood -1.173116
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     15
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 48, average log likelihood -1.165040
[ Info: iteration 49, average log likelihood -1.177744
[ Info: iteration 50, average log likelihood -1.171800
┌ Info: EM with 100000 data points 50 iterations avll -1.171800
└ 118.1 data points per parameter
┌ Info: 4
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.2524905948806966
│     -1.2519590219768297
│      ⋮
└     -1.1718001460677345
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     7
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 1, average log likelihood -1.169556
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     7
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 2, average log likelihood -1.167533
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     7
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 3, average log likelihood -1.164883
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      7
│      8
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.152371
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      5
│      7
│      8
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.111829
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     6
│     7
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.089777
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      5
│      7
│      8
│     13
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.084990
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     6
│     7
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.088464
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      5
│      7
│      8
│     30
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.079587
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     6
│     7
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.084077
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      5
│      7
│      8
│     13
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 11, average log likelihood -1.077889
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      6
│      7
│      8
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 12, average log likelihood -1.084475
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      5
│      7
│      8
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 13, average log likelihood -1.083855
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     6
│     7
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 14, average log likelihood -1.080442
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      5
│      7
│      8
│     13
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 15, average log likelihood -1.075418
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     6
│     7
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 16, average log likelihood -1.083759
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      5
│      7
│      8
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 17, average log likelihood -1.075555
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      6
│      7
│      8
│     27
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 18, average log likelihood -1.071631
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      5
│      7
│      8
│     13
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 19, average log likelihood -1.086850
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     6
│     7
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 20, average log likelihood -1.079570
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      5
│      7
│      8
│     30
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 21, average log likelihood -1.072507
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     6
│     7
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 22, average log likelihood -1.080883
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      5
│      7
│      8
│     13
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 23, average log likelihood -1.075205
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      6
│      7
│      8
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 24, average log likelihood -1.072768
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      5
│      7
│      8
│     27
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 25, average log likelihood -1.076355
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     6
│     7
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 26, average log likelihood -1.083931
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      5
│      7
│      8
│     13
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 27, average log likelihood -1.074123
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     6
│     7
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 28, average log likelihood -1.082498
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      5
│      7
│      8
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 29, average log likelihood -1.075473
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      6
│      7
│      8
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 30, average log likelihood -1.072950
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      5
│      7
│      8
│     13
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 31, average log likelihood -1.078097
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      6
│      7
│      8
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 32, average log likelihood -1.075014
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      5
│      7
│      8
│     30
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 33, average log likelihood -1.078956
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     6
│     7
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 34, average log likelihood -1.082564
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      5
│      7
│      8
│     13
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 35, average log likelihood -1.076427
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      6
│      7
│      8
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 36, average log likelihood -1.074398
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      5
│      7
│      8
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 37, average log likelihood -1.078502
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     6
│     7
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 38, average log likelihood -1.075231
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      5
│      7
│      8
│     13
│     27
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 39, average log likelihood -1.069765
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     6
│     7
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 40, average log likelihood -1.089006
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      5
│      7
│      8
│     30
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 41, average log likelihood -1.076944
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     6
│     7
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 42, average log likelihood -1.081062
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      5
│      7
│      8
│     13
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 43, average log likelihood -1.075340
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      6
│      7
│      8
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 44, average log likelihood -1.072022
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      5
│      7
│      8
│     27
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 45, average log likelihood -1.070723
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      5
│      6
│      7
│      8
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 46, average log likelihood -1.074416
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      7
│      8
│     13
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 47, average log likelihood -1.079156
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      5
│      7
│      8
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 48, average log likelihood -1.072350
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      5
│      6
│      7
│      8
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 49, average log likelihood -1.060754
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      7
│      8
│     13
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 50, average log likelihood -1.079104
┌ Info: EM with 100000 data points 50 iterations avll -1.079104
└ 59.0 data points per parameter
┌ Info: 5
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.1695557008675819
│     -1.1675326516597029
│      ⋮
└     -1.0791040861407388
┌ Info: Total log likelihood: 
│   tll =
│    251-element Array{Float64,1}:
│     -1.403286126611891
│     -1.4033825713172
│     -1.4032971191859447
│     -1.4027951070656481
│      ⋮
│     -1.0723495409419224
│     -1.0607543372344526
└     -1.0791040861407388
32×26 Array{Float64,2}:
 -0.210583     0.11869     -0.0981988     0.0616813   -0.0929367    0.0288744     0.0016598   -0.0961467    0.184731    -0.0247101    0.0259941   0.179075     0.0074968   -0.254564    -0.238191    0.213604     0.192333    -0.0795084    0.0485333   -0.0569272   0.0456804    0.0307918    0.0324566   -0.142887    -0.181929     -0.0259016
  0.0706586   -0.0156846   -0.0653504    -0.113409    -0.0974649   -0.0448336     0.118443     0.0548555   -0.123378    -0.0552166   -0.0654591   0.125004    -0.0180751   -0.119188    -0.0786206  -0.0504956    0.240074    -0.168614    -0.027201    -0.0824456   0.0482297    0.119831    -0.00939335   0.0846658   -0.145834      0.0143597
 -0.0342988   -0.0143095    0.000448729   0.170962    -0.0359128   -0.0523853     0.0428304   -0.0491314   -0.0537515   -0.0346029   -0.0815204   0.0513694   -0.0487085   -0.132695    -0.0636199   0.00669019  -0.18351      0.29269      0.0973715    0.125705   -0.00204602  -0.070586    -0.0630734    0.144722    -0.107402     -0.0244233
 -0.162111     0.0356426   -0.175819     -0.107276     0.186489    -0.0669525     0.00843362  -0.17946     -0.209801     0.0622959   -0.0297272  -0.225761    -0.00832695   0.0868956   -0.0894434   0.269471    -0.107202    -0.0926392    0.100899     0.012713    0.00507317  -0.116393    -0.0533873   -0.225157     0.107981     -0.0395451
  0.0721475    0.0114545   -0.0526821     0.00871139   0.113058     0.0357631    -0.0148805    0.0225462   -0.0260748    0.0325345   -0.11822     0.0520673    0.00451958   0.0429734    0.116496    0.0841715    0.0720904    0.0199015    0.137452     0.0585753   0.165996     0.0456553   -0.161756     0.0498791   -0.00723573   -0.0474069
 -0.0280034   -0.0507745   -0.0412825     0.104359    -0.040698    -0.219613      0.0234761   -0.0474619    0.102187     0.204458     0.105645   -0.0795384   -0.171191     0.0943928    0.120643   -0.237182     0.0715352    0.201292    -0.0905736   -0.0405242  -0.128579     0.0344073   -0.0960372   -0.246038     0.0938177    -0.150125
 -0.0284785    0.338012    -0.166667      0.0946925    0.0489266    0.0828225    -0.0094359    0.129675     0.304903     0.0272927    0.182785   -0.0257543   -0.733499     0.0625873    0.0178714  -0.187503    -0.0834219    0.050075    -0.0345362    0.0247635   0.0501189    0.131394    -0.132132    -0.0302447   -0.0418504    -0.0598954
 -0.0548335   -0.247567    -0.19071      -0.00733136   0.0314173   -0.0351166    -0.0979957    0.137886     0.304908     0.0259339    0.150301   -0.0603288    0.799383     0.0626412    0.154055   -0.20062     -0.081266     0.00937268  -0.0364041    0.0133484   0.0321464    0.131274    -0.216966     0.0821913   -0.241795     -0.196114
 -0.00627084   0.0331396   -0.0746776    -0.147519    -0.109877     0.0641104     0.0187968    0.0621925   -0.00621536   0.287555     0.0304756  -0.0151243   -0.0568308   -0.166462     0.0576727  -0.0191369    0.123004     0.0701989   -0.0067894   -0.151327   -0.0669931    0.15776      0.125097    -0.099053    -0.268673      0.182422
  0.167017     0.22058     -0.100772     -0.0817129    0.0972029    0.0665464     0.047235     0.1854      -0.0715317   -0.203378     0.0448686  -0.0998766   -0.0178772   -0.00585967   0.158763   -0.0231495    0.129661    -0.022097    -0.0565957   -0.10297    -0.0330289   -0.0134526    0.115638     0.0967967    0.0126582    -0.0988471
 -0.0202449    0.108147    -0.551221      0.0692669   -0.145623     0.0957321    -0.0814269    0.294989     0.0342391    0.0636833    0.0276926  -0.190442     0.101194    -0.029429     0.0223737  -0.0944549   -0.262731    -0.045701     0.0337772   -0.206712   -0.0921451   -0.0705902   -0.00288995   0.0705407    0.137869      0.194421
 -0.0098629    0.0800406    0.724645      0.0686587   -0.0114308    0.0639403     0.172473    -0.0235372    0.0712713    0.067893    -0.170082   -0.229524     0.0746007   -0.0590462    0.149664   -0.108434    -0.0954709   -0.0388253    0.028732     0.352609   -0.0704679   -0.0163295   -0.0136689   -0.0618829    0.109175      0.0155644
  0.0173504   -0.111115     0.0703159     0.156863     0.105407    -0.00518767    0.0498118    0.127359     0.0705891   -0.0309348   -0.0102456  -0.0420137   -0.0748825   -0.00627988  -0.118705    0.0650525   -0.147623     0.0974168   -0.0112892    0.0658849   0.0192565    0.0568451    0.0446341    0.0665397   -0.000718232   0.025315
 -0.118168    -0.0988376    0.00178259   -0.136533     0.0678372    0.105606      0.15572      0.171631    -0.0326823    0.155901     0.022638    0.0268022    0.149499     0.0535452   -0.0651932   0.012052    -0.181535     0.0200391   -0.0983451    0.0482909  -0.0212548   -0.16591     -0.0839323    0.176656    -0.00772922    0.0150222
 -0.0729302   -0.10868      0.0487224    -0.0113985   -0.00575682   0.00383428    0.00738277  -0.00296476  -0.114667    -0.00664062  -0.0961146   0.0400573   -0.00341175  -0.0675445    0.0848193   0.0296509   -0.0403019   -0.0433296   -0.0102737    0.0512476   0.0142199    0.00188454   0.0862264    0.0158706   -0.0127228     0.00841924
  0.165738     0.0738103   -0.108812     -0.0520214   -0.0404529    0.0187054     0.152392    -0.165823     0.0399088   -0.0447413   -0.121799    0.0173285   -0.0132317   -0.120373     0.0936294  -0.0935172   -0.0791851   -0.0975114    0.0477418    0.0456043   0.0641429   -0.0693442   -0.171847    -0.007162     0.0489925    -0.014612
  0.0935095    0.09387     -0.129051      0.124194    -0.0893352    0.040448     -0.0752563   -0.0278148   -0.0685457   -0.0569446   -0.0986533  -0.0209566   -0.232075    -0.0565301   -0.0154238  -0.100345    -2.2796       0.117191     0.0561203   -0.157567   -0.167415     0.32965     -0.141768     0.121086    -0.322405     -0.108969
  0.11732     -0.0975233    0.0326386    -0.0106102   -0.079507     0.0348703    -0.0212541   -0.033504    -0.0507124   -0.175795    -0.0997872   0.0155722   -0.0927198    0.0154887   -0.074041   -0.131488    -0.0562624    0.051314     0.0334135   -0.165453    0.198113    -0.276265     0.0872239   -0.176838     0.00320399    0.0511996
  0.0739631    0.249267     0.286213     -0.106038    -0.0472656    0.0299703     0.245549     0.0145026   -0.0119277   -0.118596    -0.126288    0.0775326   -0.178542    -0.104907     0.186545   -0.121455    -1.4876       0.0289197   -0.0716666   -0.105258   -0.250187    -0.175864     0.0349658   -0.145779    -0.14968      -0.08675
  0.0860738    0.243128     0.198098     -0.112554    -0.0828633    0.014448      0.230427    -0.016603    -0.0251077   -0.101777    -0.103661    0.0624331   -0.11851     -0.00184652  -0.159837   -0.0548839    2.96392      0.0725698    0.00153464  -0.200659   -0.134075    -0.140293     0.00429167   0.010665    -0.217334     -0.381579
 -0.0917422   -0.118262     0.0146387    -0.15842     -0.0492227   -0.079709      0.141092    -0.0563857    0.032913     0.0936174   -0.0415735  -0.126194    -0.0319667   -0.0977313    0.127477    0.0196222    0.0127469   -0.00107514   0.100171    -0.105754   -0.789586    -0.164696     0.135105    -0.0220506   -0.228328     -0.0599835
 -0.0529682   -0.135849    -0.0412945     0.118512    -0.0181243   -0.102796      0.109464    -0.338515     0.0282681    0.117518    -0.0404724  -0.133474    -0.110673    -0.071724     0.190468   -0.00756456   0.0415543   -0.031144    -0.129324    -0.0458202   0.66997     -0.142593    -0.0117061   -0.0283426   -0.307077      0.0605298
  0.058755     0.0584389    0.0296876     0.174645    -0.00465798  -0.137409      0.0310665   -0.937956    -0.0968708    0.0183676    0.0947343  -0.0308338   -0.0846963   -0.0260684   -0.118294    0.0829368   -0.0333831   -0.0151828    0.0745871    0.0782988  -0.191652    -0.042455     0.0494291   -0.00259351  -0.0368529     0.186343
 -0.00262856   0.027294    -0.0948959     0.15999     -0.0459382   -0.148337      0.0197039    0.820788    -0.0960933    0.0181224    0.0933895  -0.0291045    0.224206    -0.0307471    0.0979367   0.0361321   -0.0271913    0.0298524   -0.11005      0.14501     0.131708    -0.107948     0.13635      0.0390326    0.0290171     0.08334
 -0.0307635   -0.0150119    0.022021     -0.0210943   -0.109607     0.00980567   -0.111639    -0.002919     0.103973     0.0621048    0.0703445   0.040862    -0.0313928    0.121309     0.0376809  -0.100262    -0.0127716    0.0593892   -0.0111533    0.116996    0.00265274  -0.133358    -0.0181342   -0.193047    -0.0792773    -0.0398342
  0.144992    -0.0677292    0.0591257    -0.0117791   -0.0589075   -0.0374597    -0.0294321   -0.0887811    0.014395     0.0287608   -0.092637   -0.0447234   -0.0660163   -0.0263415    0.114628   -0.0536784    0.104592    -0.00327463  -0.0495645    0.0371911  -0.011046    -0.00417047   0.215624    -0.0269487   -0.0367239     0.0894492
 -0.00231696   0.0596571   -0.105615     -0.043445    -0.0607321    0.000595124  -0.0128277   -0.00832957   0.136836    -0.0290344   -0.0474758  -0.00306023   0.12897     -0.16222      0.0965525  -0.0599036   -0.0509612    0.0294696   -0.00457662  -0.0486892   0.0707901    0.108703     0.0358524    0.0533397   -0.0991398    -0.109737
  0.0392085    0.00419239  -0.0417514     0.046783    -0.140581    -0.0501331     0.0272926    0.0293177   -0.0759212   -0.0540208    0.0660821   0.172741     0.0137413   -0.099614     0.0352094   0.10972      0.00265927   0.0137899   -0.0166492   -0.0610985  -0.0334602    0.0977914    0.0495761   -0.0816234   -0.104783      0.05654
  0.0294887   -0.0656863   -0.0439242     0.0992644    0.0586809    0.0955046    -0.059032    -0.0873177   -0.0488436   -0.0833492    0.063465   -0.0321393    0.122042    -0.00241063   0.04064     0.0253596   -0.0502139    0.142583     0.0433191    0.0387546  -0.0281609   -0.042017    -0.136712    -0.00556099  -0.0200942     0.00141627
 -0.126785     0.0456697   -0.0454269    -0.114098     0.0444358   -0.17142       0.185135     0.0021139    0.0112509   -0.121757    -0.104459   -0.0398075    0.0266916   -0.0393478    0.121125   -0.0664731    0.0925285   -0.020201     0.140979     0.0252738  -0.016574     0.0595959   -0.00434128   0.064453    -0.0730843     0.0559889
 -0.0103684    0.00584561   0.0107096    -0.0024122    0.05608      0.0216826    -0.184748    -0.00182151  -0.0266913    0.111176    -0.0160862   0.113286     0.0616755    0.016496    -0.0149436   0.152105     0.0694728   -0.0821261   -0.0761112   -0.0471197  -0.0528813   -0.0604097    0.0240921    0.0659256    0.0556368    -0.0436811
  0.0569826    0.123642     0.0352607     0.0394099    0.0191642   -0.0343258     0.156858     0.040233     0.140029    -0.0270371   -0.0389685   0.11796      0.040084     0.055       -0.033096    0.141569    -0.0895392   -0.105882    -0.0935453    0.205079   -0.0876906    0.112152    -0.0282163    0.0292927    0.0794182     0.0244903[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     5
│     7
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 1, average log likelihood -1.077227
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      5
│      6
│      7
│      8
│     27
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 2, average log likelihood -1.055304
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      5
│      7
│      8
│     13
│     27
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 3, average log likelihood -1.061323
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      5
│      6
│      7
│      8
│     27
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.065741
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      5
│      7
│      8
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.067316
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      5
│      6
│      7
│      8
│     13
│     27
│     30
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.049606
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      5
│      7
│      8
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.077086
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      5
│      6
│      7
│      8
│     27
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.055877
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      5
│      7
│      8
│     13
│     27
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.060859
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      5
│      6
│      7
│      8
│     27
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.065719
┌ Info: EM with 100000 data points 10 iterations avll -1.065719
└ 59.0 data points per parameter
kind diag, method kmeans
[ Info: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.827679e+05
      1       6.893995e+05      -1.933684e+05 |       32
      2       6.495367e+05      -3.986281e+04 |       32
      3       6.309412e+05      -1.859549e+04 |       32
      4       6.219306e+05      -9.010600e+03 |       32
      5       6.169317e+05      -4.998891e+03 |       32
      6       6.133472e+05      -3.584545e+03 |       32
      7       6.100009e+05      -3.346310e+03 |       32
      8       6.077328e+05      -2.268130e+03 |       32
      9       6.065670e+05      -1.165719e+03 |       32
     10       6.058741e+05      -6.929667e+02 |       32
     11       6.054125e+05      -4.615807e+02 |       32
     12       6.050973e+05      -3.152398e+02 |       32
     13       6.047997e+05      -2.975874e+02 |       32
     14       6.044733e+05      -3.263443e+02 |       32
     15       6.041909e+05      -2.824720e+02 |       32
     16       6.039605e+05      -2.303685e+02 |       32
     17       6.037648e+05      -1.956854e+02 |       32
     18       6.036334e+05      -1.314499e+02 |       31
     19       6.035294e+05      -1.039268e+02 |       32
     20       6.034343e+05      -9.510180e+01 |       31
     21       6.033370e+05      -9.731593e+01 |       32
     22       6.032395e+05      -9.747995e+01 |       32
     23       6.031414e+05      -9.812815e+01 |       32
     24       6.030187e+05      -1.226768e+02 |       31
     25       6.027914e+05      -2.273556e+02 |       32
     26       6.024378e+05      -3.535227e+02 |       32
     27       6.018449e+05      -5.929536e+02 |       32
     28       6.008934e+05      -9.514980e+02 |       32
     29       6.000322e+05      -8.611739e+02 |       32
     30       5.997298e+05      -3.023934e+02 |       32
     31       5.996651e+05      -6.474426e+01 |       32
     32       5.996396e+05      -2.544237e+01 |       31
     33       5.996233e+05      -1.631356e+01 |       30
     34       5.996143e+05      -8.987695e+00 |       27
     35       5.996086e+05      -5.742856e+00 |       27
     36       5.996039e+05      -4.734593e+00 |       27
     37       5.996001e+05      -3.738562e+00 |       23
     38       5.995965e+05      -3.654051e+00 |       22
     39       5.995925e+05      -3.943752e+00 |       26
     40       5.995875e+05      -4.989598e+00 |       24
     41       5.995834e+05      -4.120507e+00 |       24
     42       5.995804e+05      -3.026931e+00 |       17
     43       5.995779e+05      -2.526119e+00 |       22
     44       5.995754e+05      -2.455042e+00 |       19
     45       5.995738e+05      -1.636091e+00 |       19
     46       5.995724e+05      -1.380255e+00 |       19
     47       5.995710e+05      -1.412767e+00 |       13
     48       5.995701e+05      -8.773312e-01 |       15
     49       5.995695e+05      -6.356195e-01 |       14
     50       5.995690e+05      -4.599500e-01 |        9
K-means terminated without convergence after 50 iterations (objv = 599569.0017653676)
┌ Info: K-means with 32000 data points using 50 iterations
└ 37.0 data points per parameter
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.317786
[ Info: iteration 2, average log likelihood -1.284667
[ Info: iteration 3, average log likelihood -1.246842
[ Info: iteration 4, average log likelihood -1.204340
[ Info: iteration 5, average log likelihood -1.152298
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     26
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.087876
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      8
│     22
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.077947
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     12
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.094914
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      2
│      3
│     26
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.058029
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.082922
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      8
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 11, average log likelihood -1.052018
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     12
│     26
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 12, average log likelihood -1.053227
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      2
│      3
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 13, average log likelihood -1.063006
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      5
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 14, average log likelihood -1.064298
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      8
│     26
│     27
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 15, average log likelihood -1.045733
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     12
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 16, average log likelihood -1.070767
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 17, average log likelihood -1.044668
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      2
│      5
│      8
│     26
│     27
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 18, average log likelihood -1.011876
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 19, average log likelihood -1.091079
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     12
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 20, average log likelihood -1.057452
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      3
│     26
│     27
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 21, average log likelihood -1.020921
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      2
│      8
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 22, average log likelihood -1.043362
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      5
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 23, average log likelihood -1.055758
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     12
│     26
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 24, average log likelihood -1.040184
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      3
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 25, average log likelihood -1.048825
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     2
│     5
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 26, average log likelihood -1.034811
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     26
│     27
│     29
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 27, average log likelihood -1.041086
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     12
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 28, average log likelihood -1.074310
[ Info: iteration 29, average log likelihood -1.055544
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      2
│      3
│      5
│      8
│     26
│     27
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 30, average log likelihood -1.006473
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     22
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 31, average log likelihood -1.082219
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     12
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 32, average log likelihood -1.073427
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     26
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 33, average log likelihood -1.042335
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      8
│     22
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 34, average log likelihood -1.044739
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 35, average log likelihood -1.051806
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      5
│     12
│     26
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 36, average log likelihood -1.031031
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      3
│      8
│     22
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 37, average log likelihood -1.051812
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 38, average log likelihood -1.066894
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     26
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 39, average log likelihood -1.042445
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     12
│     22
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 40, average log likelihood -1.042044
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     3
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 41, average log likelihood -1.041826
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     26
│     27
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 42, average log likelihood -1.044430
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 43, average log likelihood -1.066430
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     12
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 44, average log likelihood -1.039040
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      3
│      5
│      8
│     26
│     27
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 45, average log likelihood -1.014735
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     22
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 46, average log likelihood -1.080615
[ Info: iteration 47, average log likelihood -1.059166
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      8
│     12
│     14
│     22
│     26
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 48, average log likelihood -1.012181
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 49, average log likelihood -1.083656
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 50, average log likelihood -1.048778
┌ Info: EM with 100000 data points 50 iterations avll -1.048778
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.191539    -0.0957282   0.028035    -0.0434168    0.0502434    -0.0277907    0.0767193    0.0334088   -0.0630775   -0.0557201  -0.0052358     0.0972826   -0.0558103    0.0393549    0.103387     0.025337    -0.0706994   -0.0401877    -0.0189431    0.062205    -0.0403495   -0.0172066    0.0601813    0.0203361    0.0596805    -0.0122235
  0.0453581    0.0794446  -0.061057     0.0179041   -0.0866234    -0.0121101   -0.023966    -0.003906     0.0339451   -0.0855522   0.00720063    0.0500735    0.0717236   -0.171196     0.128748     0.013431    -0.0326141    0.00144505   -0.0726132   -0.0775706    0.0656829    0.111501     0.0318949    0.00455535  -0.118638      0.0221044
  0.108977     0.0629433  -0.0306643    0.0424178    0.000596481  -0.0697082   -0.0848291    0.00852233  -0.111568    -0.057846   -0.109115      0.0745955   -0.107055    -0.133932    -0.00843398  -0.0668768    0.00990041   0.0985435    -0.0990237   -0.16449     -0.0599812    0.0666746    0.0128665   -0.099205    -0.0922138    -0.00436958
 -0.215002     0.117348   -0.0970613    0.0626876   -0.099009      0.0286455    0.00240166  -0.0967974    0.183781    -0.0242848   0.0241819     0.182158     0.00900867  -0.251167    -0.237652     0.215462     0.198873    -0.0821414     0.0507769   -0.0566965    0.0459244    0.0324417    0.0317654   -0.139658    -0.184899     -0.025671
  0.0141262   -0.110088    0.0733306    0.155666     0.105806     -0.00130924   0.0473022    0.130717     0.0712756   -0.0312984  -0.0101047    -0.0400359   -0.0741722   -0.00548663  -0.120412     0.0653193   -0.149016     0.0979977    -0.0108291    0.0657293    0.0204968    0.0560076    0.0440167    0.0680702    0.0015811     0.0252669
 -0.124358    -0.0994333   0.00235124  -0.142153     0.0701495     0.10954      0.152816     0.190242    -0.0333695    0.151935    0.0197766     0.0279413    0.156501     0.0535067   -0.0673798    0.0108383   -0.182367     0.0200786    -0.0994528    0.0467904   -0.0165523   -0.170739    -0.0813914    0.180978    -0.00989075    0.0184577
  0.092581     0.121911    0.109693    -0.036771    -0.0739694     0.0297161    0.105517    -0.0145785   -0.0374545   -0.121435   -0.107812      0.0386504   -0.150109    -0.0351801   -0.0162319   -0.103416    -0.0827127    0.0639091     0.00106149  -0.157117    -0.0794478   -0.0988717    0.00742325  -0.063247    -0.159726     -0.130626
  0.07056      0.131645    0.0363463    0.0425638    0.0237656    -0.0345763    0.167375     0.0421011    0.146348    -0.0355939  -0.0413027     0.140365     0.0227027    0.0559629   -0.0223033    0.141204    -0.0833304   -0.109813     -0.104961     0.226027    -0.0734041    0.123822    -0.0216507    0.00880638   0.0736779     0.032043
  0.159787     0.0766984  -0.111585    -0.0532313   -0.0435366     0.0187467    0.152104    -0.16381      0.0370572   -0.0464553  -0.12367       0.0165757   -0.0136539   -0.121502     0.0925862   -0.0915349   -0.0805701   -0.100028      0.0468762    0.0484705    0.0584069   -0.067704    -0.170573    -0.00647461   0.0490246    -0.0155678
 -0.158513     0.0360348  -0.174983    -0.106357     0.184161     -0.0641476    0.00768052  -0.177825    -0.207765     0.0616329  -0.0300379    -0.224439    -0.00890846   0.0849837   -0.0854122    0.268244    -0.106083    -0.090871      0.102938     0.0086948    0.00510468  -0.115045    -0.0531669   -0.22453      0.10823      -0.036258
  0.0288906    0.0445246  -0.0299193    0.167917    -0.0242734    -0.142749     0.0254018   -0.0972411   -0.0965821    0.0184519   0.0940358    -0.0306016    0.0649136   -0.0282736   -0.0158817    0.0604094   -0.0302674    0.00669523   -0.0115774    0.110188    -0.0376999   -0.074394     0.0905842    0.0168558   -0.0052938     0.136542
  0.103312    -0.0946141   0.117096    -0.0345355   -0.089098     -0.0303642    0.00935332  -0.133823     0.0336803   -0.0151546  -0.0113264    -0.0759028   -0.11818      0.018384     0.171614     0.0176803    0.10903      0.00691808   -0.346214     0.0391094   -0.0729029    0.171035     0.0993865    0.0486657   -0.0617998     0.109968
  0.18359     -0.0631248   0.0231164    0.00357305  -0.0520917    -0.0286282   -0.0503893   -0.0744217    0.00367184   0.0650532  -0.155633     -0.0385334   -0.0338155   -0.065649     0.0804705   -0.0993912    0.109339    -0.0289379     0.0510211    0.0312163    0.0247194   -0.113003     0.341555    -0.075303    -0.0159002     0.0990131
  0.038054    -0.0146701   0.0366401   -0.0140331   -0.0534123    -0.138882     0.104633    -0.0490616    0.0165461   -0.0693836  -0.0922895    -0.0156876   -0.0266837   -0.0351086    0.11736     -0.0772666    0.0874356   -0.000250156   0.434572     0.0377708   -0.0463391    0.0669653    0.0324331    0.00098765  -0.0791444     0.0575228
 -0.00803443   0.033211   -0.0737647   -0.147795    -0.106823      0.0643965    0.0188552    0.061318    -0.00425702   0.288016    0.027641     -0.0142158   -0.0584464   -0.168326     0.0569631   -0.0185782    0.12139      0.0704311    -0.00609672  -0.151682    -0.066335     0.16049      0.125067    -0.0981673   -0.268629      0.182488
  0.0169861    0.0708858   0.0506467   -0.0506643   -0.0798283    -0.0108614   -0.206894    -0.0165453    0.137825     0.168381   -0.000131864   0.0217454    0.0932866    0.223621     0.0399019   -0.0864876   -0.0313551   -0.00324506   -0.134015     0.115685     0.0648561   -0.0691973    0.0832222   -0.285044     0.0223434    -0.0794552
  0.103722    -0.128817    0.0644229    0.154907    -0.00337563   -0.00023797  -0.123005    -0.0451538    0.0785662   -0.192323    0.0643705    -0.0609879    0.145451     0.0497887    0.0961022    0.0543736   -0.0657442    0.0861856     0.0762689    0.120507     0.0347074   -0.0800928   -0.275324     0.109384     0.00849747   -0.103084
 -0.0728881   -0.126685   -0.0119872   -0.0244781   -0.0346383    -0.0907884    0.125015    -0.19251      0.0306149    0.105427   -0.0411561    -0.129678    -0.0694085   -0.0846803    0.157999     0.00640002   0.0269215   -0.0159026    -0.0100529   -0.0766709   -0.0778093   -0.154452     0.0639577   -0.0251351   -0.266828     -0.00210906
  0.0497649    0.0105822  -0.0037334    0.0321244    0.223358      0.032723    -0.1325      -0.0930822   -0.103481     0.0802897  -0.0310677     0.148767     0.0650561    0.0615685   -0.0506934    0.199777    -0.1295      -0.0477809    -0.108241    -0.149327    -0.0724339   -0.130505     0.15802      0.24655      0.072055     -0.0670363
  0.0171454   -0.0990443   0.0110919    0.0668363   -0.0618741    -0.0885987   -0.0200101   -0.0437086   -0.028373     0.142569   -0.0472711    -0.0599513   -0.0587725   -0.0312294    0.0962523   -0.0865424    0.0288055    0.0671642    -0.0474738    0.0270198   -0.00677411   0.0219019    0.00333933  -0.106511     0.000547404  -0.0688989
 -0.0153702    0.0944595   0.0586533    0.0687409   -0.0804137     0.0807182    0.0411734    0.142372     0.0514619    0.0656403  -0.0662002    -0.209462     0.088394    -0.0440467    0.0835828   -0.101       -0.181993    -0.0424859     0.0313566    0.0601014   -0.0815077   -0.0449963   -0.00789913   0.00697386   0.12447       0.109255
 -0.0327644   -0.0706411  -0.0970105    0.190215     0.0529351     0.022275     0.14794      0.141588     0.258486     0.022391    0.0283582    -0.0447793   -0.237414     0.0396983    0.00180216  -0.0878579   -0.0669864    0.0923614    -0.0165142    0.0752028    0.0243083    0.121854    -0.0287179    0.0724991    0.0270342     0.0474072
 -0.0376437   -0.014362    0.00178513   0.168577    -0.0338033    -0.0485307    0.0352283   -0.0454529   -0.0557197   -0.039179   -0.0836147     0.053108    -0.0442563   -0.137566    -0.0700523    0.00366464  -0.185166     0.297845      0.112672     0.133726    -0.00324802  -0.0770893   -0.0649565    0.148242    -0.106445     -0.0211535
 -0.0418372   -0.104687   -0.0649782   -0.00231467  -0.196311     -0.0554107    0.117457     0.0383697   -0.0386949    0.030325    0.128066      0.237077     0.0874651   -0.0297567   -0.0593959    0.160193     0.00858678   0.025258      0.129934     0.0310538   -0.0917092    0.101323     0.0758207   -0.0908431   -0.0749233    -0.0390307
 -0.0437267    0.0114583  -0.171178     0.0452328    0.124825      0.196048     0.00191614  -0.128045    -0.187196     0.0245632   0.0833804     0.00150552   0.111253    -0.071802    -0.0248922   -0.00692132  -0.057535     0.214359      0.00885198  -0.0462485   -0.0839789   -0.0174635   -0.00121159  -0.125879    -0.0498069     0.100266
  0.064968     0.0134214  -0.0449539    0.00870673   0.121782      0.0366765   -0.00857102   0.0212191   -0.0202344    0.0363301  -0.103062      0.057156     0.00490156   0.0403181    0.115861     0.0709256    0.0629961    0.0245764     0.135606     0.0465346    0.143919     0.0491393   -0.156466     0.031733    -0.0111457    -0.0471189
 -0.0289796    0.0984282  -0.190119     0.012492     0.041894      0.048773    -0.0870799    0.13175      0.306092     0.0202685   0.187606     -0.039426     0.0131584    0.0612612    0.0871953   -0.194481    -0.0818459    0.0148126    -0.0367059   -0.00384181   0.0573241    0.134632    -0.189652     0.0349471   -0.169944     -0.145186
 -0.00730646  -0.0607169  -0.0383039   -0.0428588   -0.124413     -0.00492268   0.0601668    0.0361501   -0.0364467   -0.0515704   0.0420068     0.100497    -0.0966321   -0.0543347   -0.0191205   -0.0834599    0.124393    -0.011596      0.048774     0.0147979   -0.0114703   -0.0459832   -0.0625299   -0.00578206  -0.172988      0.00959842
 -0.0636297   -0.034479   -0.00918281  -0.112682     0.163261     -0.170617     0.167727    -0.109575     0.00580148  -0.161134   -0.255833     -0.0310415    0.0237916   -0.0301693    0.135659    -0.0996124   -0.0589697    0.035275      0.0646469    0.0274545   -0.0464394    0.0122643   -0.036019     0.0665779   -0.093294      0.0313521
 -0.0659454    0.0103292   0.0266616   -0.044137    -0.113199     -0.00161577  -0.190263     0.104964     0.0838226    0.125258   -0.0188591     0.0689403    0.0760219   -0.00604375  -0.00288074   0.115956     0.222838    -0.14277      -0.0152108    0.0941488   -0.0415546    0.00938733  -0.123691    -0.0898053    0.0640795    -0.0184782
  0.162464     0.220686   -0.102715    -0.0819999    0.0994027     0.0669978    0.0447891    0.185089    -0.0713592   -0.204518    0.0441962    -0.100109    -0.0160075   -0.00651788   0.159452    -0.0234644    0.129405    -0.0225615    -0.055428    -0.103898    -0.032887    -0.0136242    0.115518     0.0982595    0.0128814    -0.0999279
 -0.137229     0.0523506  -0.0506092   -0.119967     0.0265051    -0.172433     0.197642     0.0118636    0.0129254   -0.120083   -0.0968587    -0.0391357    0.025788    -0.0426557    0.119111    -0.065416     0.122945    -0.0311923     0.157138     0.026737    -0.0100901    0.0703533   -0.0023575    0.0656437   -0.0766863     0.0604325[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     26
│     27
│     29
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 1, average log likelihood -1.027829
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      3
│      8
│     12
│     22
│     26
│     27
│     29
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 2, average log likelihood -1.005128
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      5
│     26
│     27
│     29
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 3, average log likelihood -1.010876
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      8
│     12
│     26
│     27
│     29
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.010034
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      3
│     20
│     22
│     26
│     27
│     29
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -0.988280
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      5
│      8
│     12
│     26
│     27
│     29
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -0.994737
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      3
│     20
│     26
│     27
│     29
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.000158
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      8
│     12
│     22
│     26
│     27
│     29
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -0.998658
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      3
│      5
│     20
│     26
│     27
│     29
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -0.983951
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      8
│     12
│     26
│     27
│     29
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.011108
┌ Info: EM with 100000 data points 10 iterations avll -1.011108
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.0436745   -0.0747104    -0.135563    -0.0716696   -0.155216    -0.0821692  -0.0770541     0.0447519   -0.101177     0.011623    -0.0620004    0.138551     0.0299805   -0.0306442   -0.227789    -0.0628294   0.00738608   0.00145069   0.131282     0.113181    -0.0462563     0.0522343    0.107089     0.139027    -0.0680589    0.377376
 -0.190906    -0.113149      0.0473708    0.0948785   -0.213085    -0.0855878  -0.0946332     0.0790407   -0.104143     0.163376     0.119292    -0.0432332   -0.0296183    0.157458    -0.0256112   -0.136248    0.0232709    0.1045      -0.0735702    0.0691894   -0.0383257    -0.0997572    0.00256603   0.152269     0.129297     0.1467
 -0.038828     0.159946     -0.158087     0.0342526   -0.0116275   -0.0823179   0.0300105     0.0814943   -0.108266     0.139107     0.075727     0.00143083   0.014296    -0.0620648    0.1295      -0.133594   -0.035202    -0.158447    -0.00345199  -0.0849926    0.0177194     0.154343     0.133641     0.00591766   0.0915895    0.116619
 -0.0012848   -0.0239407    -0.0175384    0.188971     0.0786274    0.106827   -0.0513093    -0.0206704    0.136699     0.0165713   -0.16471     -0.164939     0.0842237    0.00125664   0.00611076   0.153211    0.0992325   -0.028671    -0.00967132   0.216396    -0.0620371     0.0429672   -0.068557    -0.148019     0.0162835    0.0680161
 -0.0756474    0.060259      0.0595964   -0.0684873   -0.0128041   -0.0371384  -0.0683838     0.0468854    0.0537647    0.0642727    0.0149755   -0.07011     -0.0889537   -0.00341196   0.0408843   -0.0712064   0.0433224    0.0538878    0.0434978   -0.0214284   -0.068331     -0.0789168    0.161773    -0.011523     0.0819494   -0.0832916
 -0.00557548   0.114362     -0.0488553    0.00257615  -0.128333    -0.141801   -0.0254876    -0.179545     0.0734893   -0.0924582   -0.0172629    0.146681     0.0176861    0.0211011   -0.0265512    0.0754739  -0.0300182   -0.190882    -0.140929     0.0473163   -0.150593     -0.0740951   -0.123794    -0.074352     0.0283815    0.149278
  0.199577    -0.0761602     0.173494    -0.12873     -0.0365192    0.0146963  -0.0630856    -0.122744     0.0487496    0.0931284   -0.0545036    0.0935203    0.0403314    0.0708877    0.0191546    0.0818688   0.0731926   -0.0670024   -0.161521    -0.083082    -0.0765726     0.0101415   -0.164118     0.0119755    0.0975628   -0.131737
  0.0140628    0.148093     -0.0545069   -0.0109874   -0.0463301   -0.16661     0.0253913    -0.189452    -0.1026      -0.0130777   -0.111514    -0.132847    -0.0189256    0.10505     -0.0682598   -0.0184194  -0.0745073    0.133897     0.127023     0.0329552    0.00834603   -0.126836     0.0785907   -0.0679093    0.0169258   -0.0112816
 -0.00675406  -0.0281402     0.051326     0.125155    -0.102458     0.0153753  -0.130842      0.0371248    0.156975     0.0496813   -0.0155187   -0.14998     -0.165108    -0.0476239   -0.0242853   -0.130175   -0.0603793    0.0961679   -0.0991337   -0.0179871   -0.139546     -0.0197189    0.0815808    0.042093    -0.12138      0.110483
  0.0699233   -0.116617     -0.0530293    0.0904478   -0.0916543   -0.0219694  -0.221911     -0.149689    -0.0662437   -0.0827528   -0.162907     0.0646623    0.0544092    0.0226925   -0.17104     -0.046333   -0.0185983   -0.0103886    0.0689337   -0.0692368   -0.00342922   -0.0438056    0.00973847   0.045857    -0.0348979    0.0178773
  0.023247    -0.000511124   0.0285026   -0.112125    -0.00199902   0.0377962  -0.121393     -0.123895    -0.025478    -0.0011155   -0.104447    -0.0138594   -0.124744     0.191709     0.118155    -0.0301728   0.193992    -0.0306434    0.126802    -0.0593592   -0.000204859  -0.0899835    0.0341614    0.00296834  -0.102459    -0.0228731
 -0.0198155    0.000647335  -0.0191154   -0.0982368   -0.208786    -0.0564664   0.0562417    -0.162438    -0.16193      0.0581484   -0.0214637   -0.0810336   -0.0295248    0.0724988   -0.0536482   -0.0558737   0.0187612    0.0404324    0.22534     -0.0679702   -0.18515       0.0159241   -0.0825549    0.0972788    0.0232967    0.0523967
  0.0401873    0.13036       0.0853535   -0.103759    -0.0940518   -0.220371   -0.146153      0.0228355   -0.269907     0.0448802   -0.022187     0.0795508    0.0448068    0.0134463    0.066301    -0.0708722   0.0817535   -0.0764215    0.260377     0.111028     0.109721      0.0839066   -0.0323991    0.0727953   -0.0847969    0.203049
 -0.152729     0.115141     -0.0187672    0.0322753    0.00378071  -0.0877771  -0.0667844    -0.0810891   -0.0777308   -0.134009    -0.276249     0.117713    -0.0272596    0.0944013    0.0252556   -0.0606713  -0.086791     0.00515013   0.0377203   -0.0527375   -0.0912188     0.190378    -0.0151147    0.0432689    0.00352689   0.00416654
  0.00504222   0.133462      0.00959242   0.0303609   -0.101281    -0.0754876   0.0901965    -0.041528     0.0527179   -0.145186    -0.0209066    0.0838617   -0.246714    -0.0679026   -0.0436693    0.113813   -0.0179442   -0.0606051   -0.0604173    9.75304e-5  -0.0116711    -0.0870583   -0.0550957    0.170477    -0.034864    -0.00120938
 -0.16926     -0.0950939     0.277715     0.0753674   -0.00752139   0.170505   -0.000871145   0.0499085    0.0953064   -0.164666     0.0530183    0.00507854   0.0201518   -0.00398333  -0.101716    -0.0601442   0.0453994    0.0209189   -0.0115766    0.0504808   -0.0886702     0.0920509    0.0171998    0.115037    -0.0527451   -0.0813529
  0.129672    -0.0127097    -0.0679447    0.017022    -0.178234    -0.0774327  -0.0915712    -0.070883    -0.100332    -0.0454378   -0.0214195   -0.126642    -0.193832     0.0847185    0.00170185   0.0386588  -0.0474587   -0.160815    -0.151       -0.0206394    0.0024225    -0.137776    -0.100785    -0.056144     0.0799851    0.0267489
 -0.0827157   -0.105163     -0.0595623   -0.113313    -0.0858      -0.0310234   0.0277604    -0.0563052   -0.0588524    0.0398219    0.00507478  -0.17379     -0.0458249    0.0619139    0.223462     0.084543   -0.0871979   -0.0354612    0.12295     -0.0618925    0.124339     -0.168118     0.0134274    0.0432576   -0.00399663   0.0150605
  0.108333    -0.0580172     0.0101582    0.0423785   -0.150924    -0.0733232   0.17614      -0.052584     0.0697926   -0.0190677    0.0385274   -0.127418    -0.100667     0.0435334   -0.0479797    0.0673584   0.14433      0.168335    -0.0730038   -0.0308944   -0.0468429     0.124591     0.0298521   -0.0691628   -0.0978764    0.081752
  0.00332352   0.13237       0.0940909   -0.156048     0.0206496    0.0541076   0.0116938    -0.0673179    0.1416       0.0500861   -0.0480532   -0.023165    -0.0617364   -0.118235    -0.151955     0.0237709  -0.057567    -0.0471681    0.109716     0.0180677   -0.022179      0.192838     0.0544224    0.0267541   -0.0502367    0.00616298
  0.101735    -0.065836      0.060919    -0.136896     0.00739951  -0.0474175  -0.000966747  -0.033944    -0.00396958  -0.0302354   -0.143535     0.0809517   -0.0843301   -0.00332919  -0.0919176   -0.0625828   0.174081     0.00293968   0.0344285   -0.0888139   -0.00122058   -0.0106263    0.0528708    0.0701864   -0.0115589    0.00165242
 -0.00968344   0.0966738     0.228841    -0.057736     0.0702819    0.14921    -0.055025      0.0165842   -0.0126817    0.144265    -0.116712     0.101318    -0.0493893   -0.0807079   -0.0247095    0.0216957  -0.106806    -0.0469364    0.100298    -0.0607049   -0.107132      0.0692549    0.0607119   -0.0955039   -0.0397621    5.69925e-6
  0.0985651    0.0327031    -0.0692023   -0.00463877  -0.176732    -0.010783   -0.03911       0.00350617   0.106897    -0.117931     0.233553    -0.245376    -0.0315038   -0.234543     0.036014    -0.0449058  -0.104157     0.00957772  -0.133059     0.113148    -0.0634483    -0.0703958   -0.121837    -0.0642948   -0.229076     0.0581949
  0.0313999   -0.105882     -0.0799571    0.159959    -0.0571147   -0.112758   -0.016063     -0.0243896    0.161519    -0.12481     -0.0376309    0.01371     -0.117421     0.0559314    0.0508744   -0.101762    0.0522       0.0436935   -0.049405    -0.0332262    0.00561881    0.0373812    0.0515136   -0.0273928   -0.225719     0.135793
  0.15897      0.201092     -0.129128    -0.055752     0.00944613   0.105073   -0.109633      0.0130641   -0.186198     0.0208177   -0.192251     0.203165    -0.00934208   0.0648907   -0.102611     0.0879073   0.0723994   -0.0301071   -0.0495553    0.0154134    0.0524894     0.0862235    0.080481    -0.0545432   -0.0839327   -0.096876
 -0.128398     0.14439      -0.064027    -0.0364541    0.116669    -0.0920672  -0.0599036    -0.0725941    0.155581     0.00856854   0.112863    -0.0558696   -0.0231145   -0.0603429    0.165799     0.020406    0.24648     -0.167642    -0.00796951  -0.154953     0.106334     -0.0629365   -0.0892106   -0.0855758   -0.0622898    0.0127764
  0.0160521    0.10017       0.0963801   -0.0541029    0.0211786    0.0130593   0.064755      0.0735112   -0.00236795  -0.0597553    0.0364075    0.0864664    0.0189611    0.136863     0.00853639   0.0229687  -0.00756703  -0.0780929   -0.0516912    0.0925064    0.0648528    -0.00824769  -0.0362699    0.0196079    0.0709265   -0.0363384
 -0.0710667    0.107082     -0.117591     0.0532703   -0.0163527    0.0709052  -0.0181315     0.0881247    0.0385954    0.0898828   -0.0849044    0.185273     0.0476006   -0.0799466    0.0137343   -0.0749673   0.0248962    0.164668     0.0286746   -0.0565599   -0.0726716     0.0538676   -0.0467394   -0.135631     0.166165    -0.00672008
  0.08352      0.0488144     0.028364    -0.0817154    0.0466139   -0.0532683  -0.176015     -0.161176     0.0518965    0.174997     0.120997    -0.0293252    0.0466451   -0.0149546   -0.0309812   -0.0677538   0.00751476   0.00455113  -0.156471     0.203706    -0.0377241    -0.061406     0.0257821   -0.133394     0.0188971   -0.199584
 -0.0293403   -0.052208      0.130758    -0.0445149   -0.0444634   -0.0343651  -0.0270868    -0.0043138    0.0671158   -0.0224262   -0.110902     0.0903742   -0.153538     0.0553415    0.130759    -0.163104    0.0349748    0.0497954   -0.0601013   -0.143728     0.0962559     0.0100592   -0.156741    -0.121826    -0.0874496   -0.0165605
 -0.0959609    0.113448     -0.131114    -0.0624893   -0.149165     0.112052    0.0506539     0.0513663    0.00237842   0.0381626    0.0964004    0.150285    -0.111287    -0.00637308   0.0152091   -0.102884   -0.0138091    0.074607    -0.0136375   -0.0213756   -0.0287856     0.0658996   -0.0271703    0.076553     0.0705611    0.00753589
 -0.0863317   -0.0644011    -0.00151373  -0.0372892    0.094683    -0.126143    0.128021      0.0039582   -0.120016    -0.102537     0.0401054   -0.104129     0.0639616   -0.0319185    0.0665528   -0.0012278   0.076728     0.0548673    0.1474       0.0996207    0.168441      0.0894479    0.0795654   -0.117349     0.0123058   -0.0324852kind full, method split
┌ Info: 0: avll = 
└   tll[1] = -1.4199666551781327
[ Info: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.419985
[ Info: iteration 2, average log likelihood -1.419884
[ Info: iteration 3, average log likelihood -1.419799
[ Info: iteration 4, average log likelihood -1.419699
[ Info: iteration 5, average log likelihood -1.419579
[ Info: iteration 6, average log likelihood -1.419439
[ Info: iteration 7, average log likelihood -1.419280
[ Info: iteration 8, average log likelihood -1.419087
[ Info: iteration 9, average log likelihood -1.418812
[ Info: iteration 10, average log likelihood -1.418380
[ Info: iteration 11, average log likelihood -1.417714
[ Info: iteration 12, average log likelihood -1.416826
[ Info: iteration 13, average log likelihood -1.415913
[ Info: iteration 14, average log likelihood -1.415235
[ Info: iteration 15, average log likelihood -1.414859
[ Info: iteration 16, average log likelihood -1.414687
[ Info: iteration 17, average log likelihood -1.414615
[ Info: iteration 18, average log likelihood -1.414585
[ Info: iteration 19, average log likelihood -1.414572
[ Info: iteration 20, average log likelihood -1.414567
[ Info: iteration 21, average log likelihood -1.414564
[ Info: iteration 22, average log likelihood -1.414563
[ Info: iteration 23, average log likelihood -1.414562
[ Info: iteration 24, average log likelihood -1.414562
[ Info: iteration 25, average log likelihood -1.414561
[ Info: iteration 26, average log likelihood -1.414561
[ Info: iteration 27, average log likelihood -1.414561
[ Info: iteration 28, average log likelihood -1.414560
[ Info: iteration 29, average log likelihood -1.414560
[ Info: iteration 30, average log likelihood -1.414560
[ Info: iteration 31, average log likelihood -1.414560
[ Info: iteration 32, average log likelihood -1.414560
[ Info: iteration 33, average log likelihood -1.414559
[ Info: iteration 34, average log likelihood -1.414559
[ Info: iteration 35, average log likelihood -1.414559
[ Info: iteration 36, average log likelihood -1.414559
[ Info: iteration 37, average log likelihood -1.414559
[ Info: iteration 38, average log likelihood -1.414559
[ Info: iteration 39, average log likelihood -1.414559
[ Info: iteration 40, average log likelihood -1.414559
[ Info: iteration 41, average log likelihood -1.414559
[ Info: iteration 42, average log likelihood -1.414559
[ Info: iteration 43, average log likelihood -1.414559
[ Info: iteration 44, average log likelihood -1.414559
[ Info: iteration 45, average log likelihood -1.414559
[ Info: iteration 46, average log likelihood -1.414559
[ Info: iteration 47, average log likelihood -1.414559
[ Info: iteration 48, average log likelihood -1.414559
[ Info: iteration 49, average log likelihood -1.414559
[ Info: iteration 50, average log likelihood -1.414559
┌ Info: EM with 100000 data points 50 iterations avll -1.414559
└ 952.4 data points per parameter
┌ Info: 1
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4199851116983073
│     -1.4198841445846975
│      ⋮
└     -1.414558506319509
[ Info: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.414573
[ Info: iteration 2, average log likelihood -1.414488
[ Info: iteration 3, average log likelihood -1.414412
[ Info: iteration 4, average log likelihood -1.414320
[ Info: iteration 5, average log likelihood -1.414209
[ Info: iteration 6, average log likelihood -1.414089
[ Info: iteration 7, average log likelihood -1.413976
[ Info: iteration 8, average log likelihood -1.413880
[ Info: iteration 9, average log likelihood -1.413804
[ Info: iteration 10, average log likelihood -1.413743
[ Info: iteration 11, average log likelihood -1.413691
[ Info: iteration 12, average log likelihood -1.413643
[ Info: iteration 13, average log likelihood -1.413597
[ Info: iteration 14, average log likelihood -1.413556
[ Info: iteration 15, average log likelihood -1.413519
[ Info: iteration 16, average log likelihood -1.413488
[ Info: iteration 17, average log likelihood -1.413461
[ Info: iteration 18, average log likelihood -1.413439
[ Info: iteration 19, average log likelihood -1.413420
[ Info: iteration 20, average log likelihood -1.413403
[ Info: iteration 21, average log likelihood -1.413389
[ Info: iteration 22, average log likelihood -1.413375
[ Info: iteration 23, average log likelihood -1.413364
[ Info: iteration 24, average log likelihood -1.413353
[ Info: iteration 25, average log likelihood -1.413343
[ Info: iteration 26, average log likelihood -1.413335
[ Info: iteration 27, average log likelihood -1.413327
[ Info: iteration 28, average log likelihood -1.413320
[ Info: iteration 29, average log likelihood -1.413313
[ Info: iteration 30, average log likelihood -1.413308
[ Info: iteration 31, average log likelihood -1.413303
[ Info: iteration 32, average log likelihood -1.413298
[ Info: iteration 33, average log likelihood -1.413294
[ Info: iteration 34, average log likelihood -1.413291
[ Info: iteration 35, average log likelihood -1.413287
[ Info: iteration 36, average log likelihood -1.413284
[ Info: iteration 37, average log likelihood -1.413282
[ Info: iteration 38, average log likelihood -1.413280
[ Info: iteration 39, average log likelihood -1.413278
[ Info: iteration 40, average log likelihood -1.413276
[ Info: iteration 41, average log likelihood -1.413274
[ Info: iteration 42, average log likelihood -1.413272
[ Info: iteration 43, average log likelihood -1.413271
[ Info: iteration 44, average log likelihood -1.413270
[ Info: iteration 45, average log likelihood -1.413269
[ Info: iteration 46, average log likelihood -1.413268
[ Info: iteration 47, average log likelihood -1.413267
[ Info: iteration 48, average log likelihood -1.413266
[ Info: iteration 49, average log likelihood -1.413265
[ Info: iteration 50, average log likelihood -1.413264
┌ Info: EM with 100000 data points 50 iterations avll -1.413264
└ 473.9 data points per parameter
┌ Info: 2
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4145732046614068
│     -1.4144881528539193
│      ⋮
└     -1.4132641844711478
[ Info: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.413274
[ Info: iteration 2, average log likelihood -1.413219
[ Info: iteration 3, average log likelihood -1.413170
[ Info: iteration 4, average log likelihood -1.413114
[ Info: iteration 5, average log likelihood -1.413045
[ Info: iteration 6, average log likelihood -1.412963
[ Info: iteration 7, average log likelihood -1.412868
[ Info: iteration 8, average log likelihood -1.412766
[ Info: iteration 9, average log likelihood -1.412665
[ Info: iteration 10, average log likelihood -1.412570
[ Info: iteration 11, average log likelihood -1.412485
[ Info: iteration 12, average log likelihood -1.412410
[ Info: iteration 13, average log likelihood -1.412345
[ Info: iteration 14, average log likelihood -1.412289
[ Info: iteration 15, average log likelihood -1.412241
[ Info: iteration 16, average log likelihood -1.412199
[ Info: iteration 17, average log likelihood -1.412164
[ Info: iteration 18, average log likelihood -1.412135
[ Info: iteration 19, average log likelihood -1.412109
[ Info: iteration 20, average log likelihood -1.412088
[ Info: iteration 21, average log likelihood -1.412069
[ Info: iteration 22, average log likelihood -1.412053
[ Info: iteration 23, average log likelihood -1.412038
[ Info: iteration 24, average log likelihood -1.412025
[ Info: iteration 25, average log likelihood -1.412013
[ Info: iteration 26, average log likelihood -1.412002
[ Info: iteration 27, average log likelihood -1.411992
[ Info: iteration 28, average log likelihood -1.411983
[ Info: iteration 29, average log likelihood -1.411974
[ Info: iteration 30, average log likelihood -1.411966
[ Info: iteration 31, average log likelihood -1.411958
[ Info: iteration 32, average log likelihood -1.411950
[ Info: iteration 33, average log likelihood -1.411943
[ Info: iteration 34, average log likelihood -1.411936
[ Info: iteration 35, average log likelihood -1.411930
[ Info: iteration 36, average log likelihood -1.411924
[ Info: iteration 37, average log likelihood -1.411918
[ Info: iteration 38, average log likelihood -1.411912
[ Info: iteration 39, average log likelihood -1.411907
[ Info: iteration 40, average log likelihood -1.411901
[ Info: iteration 41, average log likelihood -1.411896
[ Info: iteration 42, average log likelihood -1.411892
[ Info: iteration 43, average log likelihood -1.411887
[ Info: iteration 44, average log likelihood -1.411882
[ Info: iteration 45, average log likelihood -1.411878
[ Info: iteration 46, average log likelihood -1.411874
[ Info: iteration 47, average log likelihood -1.411870
[ Info: iteration 48, average log likelihood -1.411866
[ Info: iteration 49, average log likelihood -1.411862
[ Info: iteration 50, average log likelihood -1.411859
┌ Info: EM with 100000 data points 50 iterations avll -1.411859
└ 236.4 data points per parameter
┌ Info: 3
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4132736355753375
│     -1.4132190793831303
│      ⋮
└     -1.4118585911451227
[ Info: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.411864
[ Info: iteration 2, average log likelihood -1.411819
[ Info: iteration 3, average log likelihood -1.411780
[ Info: iteration 4, average log likelihood -1.411736
[ Info: iteration 5, average log likelihood -1.411683
[ Info: iteration 6, average log likelihood -1.411619
[ Info: iteration 7, average log likelihood -1.411542
[ Info: iteration 8, average log likelihood -1.411452
[ Info: iteration 9, average log likelihood -1.411355
[ Info: iteration 10, average log likelihood -1.411253
[ Info: iteration 11, average log likelihood -1.411152
[ Info: iteration 12, average log likelihood -1.411055
[ Info: iteration 13, average log likelihood -1.410964
[ Info: iteration 14, average log likelihood -1.410880
[ Info: iteration 15, average log likelihood -1.410802
[ Info: iteration 16, average log likelihood -1.410732
[ Info: iteration 17, average log likelihood -1.410668
[ Info: iteration 18, average log likelihood -1.410610
[ Info: iteration 19, average log likelihood -1.410557
[ Info: iteration 20, average log likelihood -1.410509
[ Info: iteration 21, average log likelihood -1.410465
[ Info: iteration 22, average log likelihood -1.410425
[ Info: iteration 23, average log likelihood -1.410388
[ Info: iteration 24, average log likelihood -1.410352
[ Info: iteration 25, average log likelihood -1.410319
[ Info: iteration 26, average log likelihood -1.410288
[ Info: iteration 27, average log likelihood -1.410258
[ Info: iteration 28, average log likelihood -1.410230
[ Info: iteration 29, average log likelihood -1.410203
[ Info: iteration 30, average log likelihood -1.410178
[ Info: iteration 31, average log likelihood -1.410153
[ Info: iteration 32, average log likelihood -1.410130
[ Info: iteration 33, average log likelihood -1.410108
[ Info: iteration 34, average log likelihood -1.410088
[ Info: iteration 35, average log likelihood -1.410068
[ Info: iteration 36, average log likelihood -1.410050
[ Info: iteration 37, average log likelihood -1.410032
[ Info: iteration 38, average log likelihood -1.410016
[ Info: iteration 39, average log likelihood -1.410001
[ Info: iteration 40, average log likelihood -1.409986
[ Info: iteration 41, average log likelihood -1.409973
[ Info: iteration 42, average log likelihood -1.409960
[ Info: iteration 43, average log likelihood -1.409949
[ Info: iteration 44, average log likelihood -1.409937
[ Info: iteration 45, average log likelihood -1.409927
[ Info: iteration 46, average log likelihood -1.409917
[ Info: iteration 47, average log likelihood -1.409908
[ Info: iteration 48, average log likelihood -1.409899
[ Info: iteration 49, average log likelihood -1.409890
[ Info: iteration 50, average log likelihood -1.409883
┌ Info: EM with 100000 data points 50 iterations avll -1.409883
└ 118.1 data points per parameter
┌ Info: 4
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4118643409861595
│     -1.41181856447946
│      ⋮
└     -1.4098825631311038
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.409884
[ Info: iteration 2, average log likelihood -1.409819
[ Info: iteration 3, average log likelihood -1.409757
[ Info: iteration 4, average log likelihood -1.409683
[ Info: iteration 5, average log likelihood -1.409592
[ Info: iteration 6, average log likelihood -1.409478
[ Info: iteration 7, average log likelihood -1.409343
[ Info: iteration 8, average log likelihood -1.409192
[ Info: iteration 9, average log likelihood -1.409031
[ Info: iteration 10, average log likelihood -1.408871
[ Info: iteration 11, average log likelihood -1.408718
[ Info: iteration 12, average log likelihood -1.408577
[ Info: iteration 13, average log likelihood -1.408450
[ Info: iteration 14, average log likelihood -1.408338
[ Info: iteration 15, average log likelihood -1.408238
[ Info: iteration 16, average log likelihood -1.408150
[ Info: iteration 17, average log likelihood -1.408071
[ Info: iteration 18, average log likelihood -1.408001
[ Info: iteration 19, average log likelihood -1.407938
[ Info: iteration 20, average log likelihood -1.407882
[ Info: iteration 21, average log likelihood -1.407831
[ Info: iteration 22, average log likelihood -1.407784
[ Info: iteration 23, average log likelihood -1.407742
[ Info: iteration 24, average log likelihood -1.407703
[ Info: iteration 25, average log likelihood -1.407667
[ Info: iteration 26, average log likelihood -1.407633
[ Info: iteration 27, average log likelihood -1.407601
[ Info: iteration 28, average log likelihood -1.407571
[ Info: iteration 29, average log likelihood -1.407543
[ Info: iteration 30, average log likelihood -1.407517
[ Info: iteration 31, average log likelihood -1.407491
[ Info: iteration 32, average log likelihood -1.407467
[ Info: iteration 33, average log likelihood -1.407443
[ Info: iteration 34, average log likelihood -1.407421
[ Info: iteration 35, average log likelihood -1.407399
[ Info: iteration 36, average log likelihood -1.407379
[ Info: iteration 37, average log likelihood -1.407358
[ Info: iteration 38, average log likelihood -1.407339
[ Info: iteration 39, average log likelihood -1.407320
[ Info: iteration 40, average log likelihood -1.407302
[ Info: iteration 41, average log likelihood -1.407284
[ Info: iteration 42, average log likelihood -1.407267
[ Info: iteration 43, average log likelihood -1.407250
[ Info: iteration 44, average log likelihood -1.407234
[ Info: iteration 45, average log likelihood -1.407217
[ Info: iteration 46, average log likelihood -1.407202
[ Info: iteration 47, average log likelihood -1.407186
[ Info: iteration 48, average log likelihood -1.407171
[ Info: iteration 49, average log likelihood -1.407156
[ Info: iteration 50, average log likelihood -1.407141
┌ Info: EM with 100000 data points 50 iterations avll -1.407141
└ 59.0 data points per parameter
┌ Info: 5
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4098837000675064
│     -1.4098190765789302
│      ⋮
└     -1.407141445676463
┌ Info: Total log likelihood: 
│   tll =
│    251-element Array{Float64,1}:
│     -1.4199666551781327
│     -1.4199851116983073
│     -1.4198841445846975
│     -1.4197991529309668
│      ⋮
│     -1.4071711039369783
│     -1.4071561591219848
└     -1.407141445676463
32×26 Array{Float64,2}:
  0.256661     0.585394      0.105525   -0.574429    -0.203572     0.16115    -0.272211     0.350814    -0.167713    -0.254439   -1.07699     -0.334517   -0.0882313   0.591922    0.229046    0.0243226  -0.42998    -0.248255     0.300863     -0.380538    0.34054    -0.26562      0.454399   -0.73855     -0.161201    -0.168138
 -0.145739     0.26394      -0.431743    0.0384869    0.221555     0.0364714  -0.205786    -0.330295    -0.0450809    0.166177   -0.225849    -0.0986762  -0.473435    0.393373   -0.152644   -0.0755057  -0.106358    0.170392    -0.164156     -0.149956    0.175217   -0.278822    -0.296572   -0.613768     0.111155    -0.233002
  0.125809     0.0355517    -0.229451   -0.321149     0.0900422    0.114265   -0.211071     0.70802     -0.0191832   -0.435162    0.0800605   -0.266305   -0.483226    0.251812   -0.0948623   0.9197      0.327088    0.355487     0.0523313     0.153307   -0.0949783   0.168844     0.0270648  -0.276564    -0.252064     0.134569
  0.00646157   0.0904228    -0.168419    0.91171     -0.587713    -0.113847   -0.120048     0.553716     0.00482562  -0.288612    0.00755982   0.162395    0.0930837  -0.43416    -0.318394   -0.0294359   0.117982   -0.00369274   0.513831     -0.0765902  -0.0483739  -0.0639882    0.661717   -0.699796    -0.276304    -0.263193
 -0.608585     0.369925     -0.255089    0.239244     0.468791     0.0293275   0.149737    -0.280023    -0.276035    -0.179387   -0.41505      0.715881   -0.551045   -0.371045   -0.649441   -0.0228717  -0.0673353  -0.04171     -0.401023      0.142755   -0.457802   -0.250359    -0.187929   -0.617264    -0.174806    -0.134649
  0.139489     0.108992      0.663006   -0.498891    -0.169575    -0.36025     0.63679      0.0867169    0.2385       0.624232    0.353698     0.479852   -0.311252    0.403032   -0.530215    0.327109   -0.310266   -0.683127     0.027973      0.500047   -0.0707834   0.00464367  -0.109491   -0.753055    -0.198199    -0.299839
  0.323032    -0.716801      0.154346    0.126191     0.382899    -0.163459    0.261563    -0.188406     0.325149    -0.223435    0.207001     0.479737   -0.355109   -0.345715   -0.618972    0.0497255   0.273683    0.241173     0.255369     -0.457138   -0.770063   -0.46226     -0.174458    0.00475808  -0.166492    -0.0202185
  0.450373    -0.0839776    -0.0755488   0.00275273   0.705319    -0.0353891  -0.413074    -0.583513     0.550759     0.352192    0.0102463   -0.18609     0.14637    -0.213873   -1.02213     0.425866    0.267972    0.450634    -0.763967      0.404908   -0.169668   -0.713826    -0.017207   -0.696413     0.426188     0.282869
 -0.329163    -0.577788     -0.145645    0.529199    -0.00632962  -0.533612    0.507188    -0.538763     0.126802     0.761459    0.677976    -0.737582   -0.0782273  -0.557238    0.182402   -0.952793    0.0737642  -0.321837    -0.893478     -0.703925   -0.316618   -0.336212    -0.226747    0.0863906    0.00902045   0.33333
  0.0780248   -0.337401     -0.369388    0.638697    -0.530937    -0.475731   -0.406202     0.0291006    0.20198      0.388749    0.62811     -0.449473    0.566242   -0.138743    0.434383   -0.0196647  -0.0322685   0.0465551   -0.669367      0.973034   -0.341774    0.345102    -0.525506   -0.438926     0.0978328    0.522113
 -0.280025    -0.0436736     0.376157    0.190217     0.295323    -0.100412   -0.411593    -0.63604     -0.0826252    0.492637   -0.0726888    0.31369     0.53202    -0.459782    0.165955   -0.429887   -0.552035   -0.316261     0.298084      0.136994    0.235883    0.0148488    0.021523    0.306029    -0.107734     0.223036
  0.302067    -0.0231422     0.360202   -0.0490126   -0.285263     0.25194     0.264879    -0.00122833  -0.0293298   -0.189225    0.0281691    0.207444    0.454047   -0.034348   -0.0400472  -0.470474    0.115716   -0.15138      0.364308      0.0508808   0.104624    0.119253     0.0321894   0.840411     0.191807    -0.0408983
  0.567677     0.115269      0.111457    0.0845775   -0.495096     0.345958   -0.223842     0.48365      0.396589    -0.0121915   0.539201    -0.446476    0.231187    0.362359   -0.0406613   0.276919    0.308031   -0.309862    -0.145589      0.405828    0.336705    0.215305     0.247507    0.75839      0.354608     0.73668
  0.217495     0.119958     -0.0530795  -0.281329     0.224816    -0.202723   -0.076418     0.17872      0.0255663    0.17278     0.225594     0.0784699   0.67335    -0.43284    -0.301303    0.548038    0.0629132  -0.24323      0.317393      0.522725   -0.291807    0.64448      0.206374    0.428573    -0.155171    -0.0432961
 -0.167466    -0.222816     -0.1191     -0.0444912    0.00962793   0.0116576   0.554314    -0.335763     0.336381    -0.432103   -0.15516      0.211714   -0.735287    0.404242    0.693116    0.644247    0.644211    0.305056     0.14318       0.189049    0.485599   -0.169043    -0.517138    0.101416     0.0451374    0.70498
 -1.08185      0.303431      0.407695   -0.461421    -0.449884    -0.61165     0.245428    -0.172424     0.0990037    0.0434736  -0.129711     0.303858   -0.375746    0.0429133   0.935126    0.29193     0.115717   -0.367389    -0.168198      0.158977    0.581854    0.871754     0.246322    0.64585     -0.171033     0.543659
 -0.0531687   -0.227041      0.0810073  -0.0553316   -0.107944     0.188805   -0.651698     0.206698    -0.495572     0.202674   -0.179932    -0.212778    0.310575   -0.128681   -0.504627   -0.265021    0.076755   -0.0119382   -0.937666     -0.426996   -0.511569   -0.238637     0.820873   -0.103535    -0.439207    -0.341596
  0.458336     0.362891     -0.313907    0.0030786    0.184624     0.164829   -0.217411     0.2866      -0.46182     -0.163023   -0.738035    -0.705761    0.44325     0.0732708  -0.420502   -0.363989   -0.463969    0.0268423    0.421432     -0.297453   -0.350245   -0.038939     0.148772    0.155681     0.298016    -0.437937
 -0.147889     0.334018      0.303003    0.0699776    0.183711     0.0501446   0.0604675    0.251369    -0.181907    -0.044981   -0.399632     0.431804   -0.0617328  -0.152771   -0.586113   -0.400241    0.225363   -0.160211     0.177602     -0.0842608   0.330519   -0.0949298    0.529089    0.0963866   -0.393883     0.144545
  0.0230588    0.141326      0.0988844   0.0146539   -0.188355     0.0159439  -0.0986584   -0.0996217   -0.131847     0.283359   -0.0277372   -0.0643304   0.095319   -0.0714689  -0.0380001  -0.419445   -0.215315   -0.114716    -0.0642429    -0.194725   -0.0982442  -0.0105868    0.12469     0.0308616   -0.0126592   -0.120308
 -0.142206    -0.0601985     0.0845022   0.439203    -0.467592    -0.343168    0.291264     0.133906    -0.365547    -0.35237     0.446989     0.069831   -0.106798   -0.510422    0.651096   -0.233149   -0.0884441  -0.156281     0.269589      0.221858   -0.495626    0.211476     0.221192    0.408614    -0.272165    -0.113992
 -0.276369     0.256026      0.258905    0.0706571   -0.943386    -0.283422    0.31399      0.311586    -0.212107     0.0984635   0.0946505   -0.461737    0.139437    0.398209    0.880443   -0.363078   -0.249913   -0.195052     0.40407      -0.166938    0.633876    0.436568     0.289129    0.575937    -0.407984    -0.281366
 -0.267078    -0.265143     -0.0806669  -0.303881    -0.21376      0.336401   -0.356534    -0.424144    -0.121915    -0.720456   -0.558042    -0.118686    0.265634    0.0199439   0.705994   -0.207443    0.25566     0.0662289   -0.0700057    -0.130843   -0.150422   -0.0270818    0.116381    0.506749     0.146474     0.0554864
 -0.309253    -0.0525012     0.166172    0.317568    -0.399321     0.0317216  -0.20587      0.0669802   -0.0840588   -0.319204   -0.0804063    0.0455077  -0.569761    0.083767    0.400204   -0.811928    0.0213406   0.307969    -0.229307     -0.579977    0.363026   -0.238514    -0.185327   -0.181667     0.23419      0.0928634
  0.275554     0.140769     -0.186766   -0.201669     0.336697     0.286219   -0.118604    -0.788524    -0.0910983    0.268397   -0.0936999   -0.0390354   0.0160794   0.442329   -0.103958   -0.202454    0.0142165  -0.594786    -0.500679     -0.0269986   0.179758   -0.27593     -0.667635   -0.0451542    0.287074     0.313355
 -0.216916     0.339087     -0.375654   -0.0792813    0.376048    -0.117146    0.201582     0.0698153   -0.110211     0.173861   -0.135956    -0.0487824   0.0432593   0.0945819  -0.282784   -0.133506   -0.0499362   0.227173    -0.157609      0.158549    0.27592     0.691656    -0.649541    0.209397     0.174808     0.448461
  0.0637982   -0.0192857    -0.631346   -0.455439    -0.344294    -0.0800834   0.21376     -0.499549     0.30858      0.190764    0.357474    -0.204461   -0.214716    0.144835    0.658454   -0.0278383  -0.459034    0.180303    -0.0554201    -0.415578   -0.242664    0.327197    -0.431385    0.196214     0.1825      -0.489537
  0.115819    -0.289313      0.66867    -0.614985    -0.173915    -0.153121   -0.00314593  -0.273614     0.0718934    0.622572    0.0953141   -0.275329    0.0718678   0.676388    0.0866347  -0.19867     0.198691    0.539372    -0.173883     -0.182548   -0.257133   -0.0344775   -0.091672    0.189743     0.637638    -0.147501
  0.00868906   0.281919     -0.102628   -0.0864324   -0.0549688    0.146731   -0.420137     0.269044    -0.0199304   -0.286257   -0.0221589   -0.0113578  -0.276183   -0.200913    0.0918168   0.535805    0.157259   -0.0681339    0.15201       0.105933    0.0705516  -0.230899     0.210457   -0.221238    -0.355886     0.301283
  0.0817319    0.0257595    -0.122224   -0.10871      0.0215646    0.126054    0.00983985   0.00335842  -0.081179    -0.273644   -0.19402     -0.110091   -0.141264    0.229333   -0.0152302   0.226079    0.15786     0.0918223    0.000267089   0.157437   -0.17623    -0.0219475   -0.0232349  -0.108999     0.0986425   -0.102324
  0.133297    -0.000269013  -0.19544     0.329562     0.00460716  -0.477843    0.016637     0.139382     0.292834     0.225693    0.493387     0.108535   -0.310775    0.121351   -0.143816    0.425237    0.0437775   0.280018     0.168398      0.150508   -0.020559    0.0029408   -0.20518    -0.776829    -0.0530054   -0.127751
 -0.115713    -0.473012      0.0464884  -0.0433932   -0.0359125   -0.270159    0.186544    -0.382532     0.333524     0.194355    0.273534     0.222657    0.0404213  -0.102828    0.181951    0.0572054   0.114505    0.022544     0.16804       0.111102    0.117783    0.122211    -0.315489    0.364497    -0.0811124    0.177214[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.407127
[ Info: iteration 2, average log likelihood -1.407113
[ Info: iteration 3, average log likelihood -1.407099
[ Info: iteration 4, average log likelihood -1.407085
[ Info: iteration 5, average log likelihood -1.407071
[ Info: iteration 6, average log likelihood -1.407058
[ Info: iteration 7, average log likelihood -1.407045
[ Info: iteration 8, average log likelihood -1.407033
[ Info: iteration 9, average log likelihood -1.407020
[ Info: iteration 10, average log likelihood -1.407008
┌ Info: EM with 100000 data points 10 iterations avll -1.407008
└ 59.0 data points per parameter
kind full, method kmeans
[ Info: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.590287e+05
      1       7.058128e+05      -1.532158e+05 |       32
      2       6.891194e+05      -1.669344e+04 |       32
      3       6.828971e+05      -6.222345e+03 |       32
      4       6.797184e+05      -3.178636e+03 |       32
      5       6.776787e+05      -2.039701e+03 |       32
      6       6.762194e+05      -1.459366e+03 |       32
      7       6.751383e+05      -1.081021e+03 |       32
      8       6.742745e+05      -8.638525e+02 |       32
      9       6.736543e+05      -6.202233e+02 |       32
     10       6.731408e+05      -5.134198e+02 |       32
     11       6.726680e+05      -4.728545e+02 |       32
     12       6.722694e+05      -3.985854e+02 |       32
     13       6.719188e+05      -3.506486e+02 |       32
     14       6.715957e+05      -3.230418e+02 |       32
     15       6.713089e+05      -2.867983e+02 |       32
     16       6.710476e+05      -2.612887e+02 |       32
     17       6.708100e+05      -2.375826e+02 |       32
     18       6.705759e+05      -2.341580e+02 |       32
     19       6.703627e+05      -2.131854e+02 |       32
     20       6.701763e+05      -1.863528e+02 |       32
     21       6.699941e+05      -1.822932e+02 |       32
     22       6.698188e+05      -1.752591e+02 |       32
     23       6.696550e+05      -1.638167e+02 |       32
     24       6.695071e+05      -1.479037e+02 |       32
     25       6.693669e+05      -1.401281e+02 |       32
     26       6.692328e+05      -1.341204e+02 |       32
     27       6.691050e+05      -1.277988e+02 |       32
     28       6.689842e+05      -1.208488e+02 |       32
     29       6.688782e+05      -1.059990e+02 |       32
     30       6.687777e+05      -1.004860e+02 |       32
     31       6.686937e+05      -8.404007e+01 |       32
     32       6.686134e+05      -8.029991e+01 |       32
     33       6.685414e+05      -7.192977e+01 |       32
     34       6.684776e+05      -6.380639e+01 |       32
     35       6.684147e+05      -6.292618e+01 |       32
     36       6.683552e+05      -5.948680e+01 |       32
     37       6.682968e+05      -5.837817e+01 |       32
     38       6.682460e+05      -5.078342e+01 |       32
     39       6.681968e+05      -4.924097e+01 |       32
     40       6.681415e+05      -5.534423e+01 |       32
     41       6.680941e+05      -4.738460e+01 |       32
     42       6.680436e+05      -5.046427e+01 |       32
     43       6.679943e+05      -4.932541e+01 |       32
     44       6.679421e+05      -5.222602e+01 |       32
     45       6.678897e+05      -5.231544e+01 |       32
     46       6.678484e+05      -4.138837e+01 |       32
     47       6.678073e+05      -4.107168e+01 |       32
     48       6.677696e+05      -3.769775e+01 |       32
     49       6.677314e+05      -3.817123e+01 |       32
     50       6.677005e+05      -3.092752e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 667700.4820416558)
┌ Info: K-means with 32000 data points using 50 iterations
└ 37.0 data points per parameter
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.418401
[ Info: iteration 2, average log likelihood -1.413421
[ Info: iteration 3, average log likelihood -1.412040
[ Info: iteration 4, average log likelihood -1.410974
[ Info: iteration 5, average log likelihood -1.409899
[ Info: iteration 6, average log likelihood -1.408999
[ Info: iteration 7, average log likelihood -1.408428
[ Info: iteration 8, average log likelihood -1.408115
[ Info: iteration 9, average log likelihood -1.407934
[ Info: iteration 10, average log likelihood -1.407815
[ Info: iteration 11, average log likelihood -1.407726
[ Info: iteration 12, average log likelihood -1.407654
[ Info: iteration 13, average log likelihood -1.407593
[ Info: iteration 14, average log likelihood -1.407540
[ Info: iteration 15, average log likelihood -1.407494
[ Info: iteration 16, average log likelihood -1.407452
[ Info: iteration 17, average log likelihood -1.407414
[ Info: iteration 18, average log likelihood -1.407380
[ Info: iteration 19, average log likelihood -1.407348
[ Info: iteration 20, average log likelihood -1.407320
[ Info: iteration 21, average log likelihood -1.407293
[ Info: iteration 22, average log likelihood -1.407268
[ Info: iteration 23, average log likelihood -1.407246
[ Info: iteration 24, average log likelihood -1.407224
[ Info: iteration 25, average log likelihood -1.407204
[ Info: iteration 26, average log likelihood -1.407186
[ Info: iteration 27, average log likelihood -1.407168
[ Info: iteration 28, average log likelihood -1.407151
[ Info: iteration 29, average log likelihood -1.407136
[ Info: iteration 30, average log likelihood -1.407121
[ Info: iteration 31, average log likelihood -1.407107
[ Info: iteration 32, average log likelihood -1.407093
[ Info: iteration 33, average log likelihood -1.407080
[ Info: iteration 34, average log likelihood -1.407068
[ Info: iteration 35, average log likelihood -1.407056
[ Info: iteration 36, average log likelihood -1.407045
[ Info: iteration 37, average log likelihood -1.407034
[ Info: iteration 38, average log likelihood -1.407023
[ Info: iteration 39, average log likelihood -1.407013
[ Info: iteration 40, average log likelihood -1.407003
[ Info: iteration 41, average log likelihood -1.406993
[ Info: iteration 42, average log likelihood -1.406984
[ Info: iteration 43, average log likelihood -1.406975
[ Info: iteration 44, average log likelihood -1.406966
[ Info: iteration 45, average log likelihood -1.406957
[ Info: iteration 46, average log likelihood -1.406949
[ Info: iteration 47, average log likelihood -1.406940
[ Info: iteration 48, average log likelihood -1.406932
[ Info: iteration 49, average log likelihood -1.406924
[ Info: iteration 50, average log likelihood -1.406917
┌ Info: EM with 100000 data points 50 iterations avll -1.406917
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
  0.0403703   -0.169292   -0.229175    -0.330715     0.377923     0.134863      0.337793     0.11721      0.027466   -0.525482    -0.00539114   0.0923434  -0.976772     0.767419     0.162472    0.671227    0.833679    0.484438    -0.0619274  -0.00692143   0.206979   -0.259998   -0.264789    0.0377917  -0.321895     0.524351
 -0.235202     0.295287   -0.221535    -0.172849     0.653735     0.406167     -0.346529    -0.474631    -0.0458783   0.135073    -0.239884     0.233149   -0.00155056  -0.494548    -0.756675    0.37977     0.0760826  -0.0987579   -0.725254    0.283184    -0.181592   -0.37716     0.103418   -0.253372   -0.367488     0.506715
  0.204824    -0.0818481   0.272665    -0.598921     0.253812     0.123041      0.0325483   -0.867695     0.233632    0.44479     -0.189677    -0.114619    0.0292094    0.764883    -0.233948   -0.0502077   0.0587645   0.15521     -0.480489   -0.0224133   -0.113633   -0.262859   -0.42577    -0.032338    0.817922    -0.017751
  0.172705     0.282065    0.297457    -0.19849      0.112277     0.280897      0.0799105    0.378956    -0.0561758  -0.231446    -0.456348     0.327955    0.16954      0.0988961   -0.640169    0.0917044   0.254156   -0.00932708   0.429735    0.0448887    0.225646    0.0275805   0.315766    0.375709   -0.0817919    0.0960477
  0.0953894    0.348097   -0.365453     0.641238    -0.158802     0.059487     -0.0495839    0.823941    -0.3623     -0.542854     0.0919035    0.370249   -0.188312    -0.674828    -0.286352   -0.073688    0.0779353  -0.216548     0.392748    0.110392     0.0517965  -0.0119085   0.283001   -0.274772   -0.622775     0.173389
 -0.00759448   0.216889    0.293786    -0.222164    -0.0226308    0.0903371    -0.142114     0.0971158   -0.250901    0.0908406   -0.249749     0.0380549   0.0788168    0.0508198   -0.160718   -0.296355    0.0716735  -0.0328999   -0.0228969  -0.167439     0.0960097  -0.0529808   0.315799    0.254794   -0.0631239    0.0862518
  0.357675     0.613705   -0.234364     0.111579    -0.385868    -0.0267855    -0.186938     0.549848    -0.411126   -0.211573    -0.313639    -0.408387   -0.0398811    0.696136     0.328619    0.103123   -0.1008      0.132944    -0.0420142   0.0431966    0.210674    0.127787   -0.134607   -0.794239    0.525931    -0.330603
  0.00394262  -0.111542   -0.0831112   -0.149378    -0.462889    -0.161365      0.104086    -0.187525     0.174882    0.219738     0.288673    -0.247062    0.001452     0.154538     0.519142   -0.201474   -0.228604    0.0504882    0.0490958  -0.211789    -0.125896    0.250698   -0.0606785   0.279005    0.00377734  -0.344813
  0.0483728    0.019777   -0.123593    -0.00617739   0.110049    -0.000506282  -0.00596895  -0.123765     0.0392741  -0.00203418   0.073047     0.0127034  -0.123731     0.0193306   -0.0377785   0.149073    0.0694965  -0.0303841   -0.0430719   0.186563    -0.0132375   0.0152905  -0.290901   -0.111357    0.0112932    0.19932
 -0.245204    -0.0315997   0.203149     0.407461    -0.144481    -0.659138      0.0163691    0.420353     0.115465   -0.0590417    0.896739     0.314912    0.38384     -0.269663     0.278908    0.270185    0.0657177  -0.0382136    0.406455    0.480333    -0.539432    0.835438    0.0769423   0.29104     0.00811122   0.0404872
  0.0923081   -0.409511    0.41975      0.153788    -0.418955    -0.20509       0.767777    -0.394371    -0.62583    -0.174405     0.22092      0.295321   -0.258578    -0.244344     0.531196   -0.647386   -0.427847   -0.761464    -0.0410266  -0.284291    -0.689314   -0.246263   -0.117754    0.382986   -0.517325    -0.0103367
 -0.917036     0.148285    0.741758    -0.727739    -0.270447    -0.788613      0.526294    -0.337517     0.0399512   0.296598    -0.227789     0.890208   -0.233001     0.159708     1.06474     0.383258    0.13414    -0.651767    -0.187268    0.317142     0.457935    0.501224    0.224693    0.568516   -0.497194     1.20737
  0.122259     0.198263    0.391092    -0.0961807   -0.020793    -0.478462      0.316976     0.192272     0.232829    0.545551     0.242396     0.444771   -0.259428     0.43536     -0.711791    0.356267   -0.136753   -0.361395    -0.0504316   0.298687    -0.0953931  -0.108535   -0.123894   -1.06904    -0.150296    -0.222719
  0.212101     0.135019   -0.297745    -0.572109     0.00686248  -0.0373355    -0.128765     0.366197     0.120763   -0.134143    -0.0071893   -0.294213    0.178415    -0.161398    -0.214248    0.929654    0.100218    0.0402395    0.311529    0.491411    -0.194174    0.413647    0.32163    -0.0116429  -0.251187    -0.188288
  0.0258201   -0.403591   -0.035845     0.455285    -0.480613    -0.100361      0.230627    -0.17464      0.554669   -0.181971    -0.0302601    0.247245   -0.3481       0.0790086    0.791846    0.501569    0.0618062   0.133902     0.521737    0.260923     0.490958   -0.429099   -0.474622   -0.230591    0.293018     0.605339
  0.35123     -0.567124    0.0111329    0.347658     0.133587    -0.265708      0.251062    -0.279412     0.462759    0.434882     0.558504     0.0989673  -0.262211    -0.644845    -0.548705    0.0903503   0.296321    0.293542    -0.218828   -0.0222856   -0.33425    -0.257962   -0.290543    0.0169239   0.0802667    0.300926
  0.144186    -0.075006   -0.386523    -0.206801    -0.140058     0.51323      -0.485166    -0.340802    -0.187744   -0.751337    -0.543546    -0.325947    0.331966    -0.00099732   0.668621   -0.113593    0.180091    0.0512151    0.163707   -0.105262    -0.132835   -0.0873821  -0.0552052   0.631157    0.343962     0.104371
 -0.685344     0.147485   -0.00940952   0.0382368   -0.45694     -0.173955      0.12859      0.126694     0.148577   -0.862417    -0.152346     0.136547   -0.480039    -0.100303     0.665535   -0.0985235   0.716699    0.282653    -0.294621   -0.213434     0.359725    0.828646   -0.18209     0.410292    0.148483     0.151351
 -0.330488     0.0213971   0.0622271    0.230095    -0.167433     0.339257     -0.284791    -0.36071     -0.221439    0.068635    -0.139429    -0.0756094  -0.413158     0.266015     0.295932   -1.04948    -0.237238    0.147098    -0.499888   -0.565821     0.446956   -0.448082   -0.453683   -0.144439    0.278012     0.219602
 -0.129604    -0.0179259   0.177175    -0.220993    -0.843605    -0.10683       0.10505     -0.0140282   -0.19151     0.0486548    0.153649    -0.581252    0.198172     0.0660107    1.0902     -0.247996   -0.345011   -0.0578036    0.299968   -0.0329129    0.287305    0.407975    0.318169    0.920265   -0.137057    -0.15092
 -0.31652     -0.149582   -0.0819676    0.250386    -0.105929    -0.109117     -0.0964035   -0.164713     0.0831714  -0.408246    -0.341069     0.321047   -0.192244    -0.0253177   -0.24921    -0.032845    0.149426    0.228928     0.118746   -0.283399    -0.253765   -0.248268    0.415852   -0.339971   -0.19964     -0.312586
 -0.187503    -0.0491236  -0.36519      0.301025     0.178613    -0.124699      0.0249903   -0.198477     0.0306629  -0.0876332   -0.0222526    0.141225   -0.622308     0.0293384   -0.304621   -0.0243418   0.0569935   0.176198    -0.208906   -0.195653    -0.332546   -0.315578   -0.170118   -0.700308    0.0102633   -0.236206
 -0.313048     0.39677    -0.651666    -0.174022     0.213569     0.0768392     0.00125263  -0.348608     0.466048    0.463866     0.41078     -0.0440143  -0.534732     0.287007     0.14487     0.38029    -0.46778    -0.038654     0.300687    0.172544     0.54776     0.493839   -0.561858   -0.0310338  -0.140198    -0.188914
  0.0882035   -0.307335    0.0895545    0.101041     0.0912112   -0.117989      0.0564898   -0.697916     0.209416    0.294607     0.174129     0.379064    0.891877    -0.283245    -0.145141   -0.626817   -0.131853   -0.309493     0.108916    0.165783     0.090693    0.415581   -0.387117    0.770979    0.389196     0.066935
  0.188415    -0.264311    1.12679     -0.0654502   -0.423587     0.20204      -0.116663     0.496968    -0.0187581   0.104478     0.361748    -0.229182    0.228341    -0.0195655   -0.0308459  -0.122656    0.714274    0.114257    -0.0258825  -0.15857     -0.0858271  -0.288085    0.509595    0.0324177  -0.147752    -0.182603
  0.506019    -0.0243998  -0.235921     0.292308     0.056777     0.296021     -0.429755     0.109409    -0.28969     0.246555    -0.142009    -0.301891    0.560017    -0.018784    -1.01939    -0.528794   -0.405781    0.0332693   -0.0477076  -0.294401    -0.641911   -0.140868    0.365415   -0.133096   -0.0322894   -0.843085
 -0.504832     0.304882    0.346558     0.646861    -0.0757228   -0.0360257    -0.225809    -0.131585    -0.205898    0.458463    -0.34253     -0.111999    0.301823    -0.455685     0.139138   -0.47189    -0.34786    -0.587372     0.142577    0.121621     0.40767     0.0183404   0.575614   -0.0177189  -0.171242     0.129512
 -0.0731998   -0.144156    0.462651    -0.253796     0.281201    -0.545634     -0.215142    -0.31515     -0.189307    0.0748992   -0.0487491    0.814719   -0.165983    -0.493269     0.529546   -0.253604   -0.257376    0.356635     0.767929   -0.00157545  -0.11975    -0.450216   -0.609961   -0.136226    0.0173456   -0.0247465
 -0.0319696   -0.473924   -0.446902     0.236296    -0.166704    -0.642586     -0.45409      0.00884177   0.169172    0.176182     0.441623    -0.956752    0.355934    -0.0671493    0.474267    0.036877    0.136694   -0.015136    -0.950109    0.333038    -0.322019    0.107776   -0.145331   -0.674687   -0.0705882    0.370988
  0.18098      0.621759    0.191898    -0.583634    -0.248029     0.284568     -0.317497     0.364265    -0.0822745  -0.277634    -0.988331    -0.346299   -0.318829     0.356708     0.226227    0.0137672  -0.468381   -0.220754     0.421383   -0.524955     0.289723   -0.555781    0.698947   -0.816102   -0.491597    -0.101246
  0.567392     0.177975   -0.00561103   0.0503834   -0.379225     0.348331     -0.262848     0.567872     0.313363    0.0870297    0.553111    -0.392876    0.221406     0.404796    -0.12089     0.279228    0.228092   -0.32983     -0.269724    0.495296     0.32832     0.311297    0.0827493   0.839074    0.417141     0.9437
 -0.120752     0.296567   -0.474422    -0.435882     0.526671    -0.42432       0.21393      0.23563     -0.829088   -0.0305164   -0.698676    -0.377945   -0.0810255    0.143333    -0.157373   -0.317554   -0.27216     0.0146566   -0.142959   -0.294836    -0.174097    0.449933   -0.248146    0.232678    0.0817452    0.0774353[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.406909
[ Info: iteration 2, average log likelihood -1.406901
[ Info: iteration 3, average log likelihood -1.406894
[ Info: iteration 4, average log likelihood -1.406887
[ Info: iteration 5, average log likelihood -1.406880
[ Info: iteration 6, average log likelihood -1.406873
[ Info: iteration 7, average log likelihood -1.406866
[ Info: iteration 8, average log likelihood -1.406859
[ Info: iteration 9, average log likelihood -1.406852
[ Info: iteration 10, average log likelihood -1.406845
┌ Info: EM with 100000 data points 10 iterations avll -1.406845
└ 59.0 data points per parameter
[ Info: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.678561e+05
      1       2.230230e+04      -1.455538e+05 |        2
      2       7.823675e+03      -1.447862e+04 |        0
      3       7.823675e+03       0.000000e+00 |        0
K-means converged with 3 iterations (objv = 7823.67549422947)
┌ Info: K-means with 900 data points using 3 iterations
└ 150.0 data points per parameter
[ Info: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
[ Info: iteration 1, average log likelihood -2.043155
[ Info: iteration 2, average log likelihood -2.043154
[ Info: iteration 3, average log likelihood -2.043154
[ Info: iteration 4, average log likelihood -2.043154
[ Info: iteration 5, average log likelihood -2.043154
[ Info: iteration 6, average log likelihood -2.043154
[ Info: iteration 7, average log likelihood -2.043154
[ Info: iteration 8, average log likelihood -2.043154
[ Info: iteration 9, average log likelihood -2.043154
[ Info: iteration 10, average log likelihood -2.043154
┌ Info: EM with 900 data points 10 iterations avll -2.043154
└ 81.8 data points per parameter
   Testing GaussianMixtures tests passed 
