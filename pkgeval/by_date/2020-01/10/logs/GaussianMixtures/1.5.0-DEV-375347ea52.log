Julia Version 1.5.0-DEV.35
Commit 375347ea52 (2020-01-09 03:35 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-8.0.1 (ORCJIT, skylake)
Environment:
  JULIA_DEPOT_PATH = ::/usr/local/share/julia

 Resolving package versions...
 Installed GaussianMixtures ─── v0.3.0
 Installed LegacyStrings ────── v0.4.1
 Installed OpenBLAS_jll ─────── v0.3.7+4
 Installed Rmath ────────────── v0.6.0
 Installed ScikitLearnBase ──── v0.5.0
 Installed HDF5 ─────────────── v0.12.5
 Installed StaticArrays ─────── v0.12.1
 Installed DataStructures ───── v0.17.7
 Installed OpenSpecFun_jll ──── v0.5.3+1
 Installed Parameters ───────── v0.12.0
 Installed Missings ─────────── v0.4.3
 Installed Arpack ───────────── v0.4.0
 Installed SortingAlgorithms ── v0.3.1
 Installed CMakeWrapper ─────── v0.2.3
 Installed JLD ──────────────── v0.9.1
 Installed Clustering ───────── v0.13.3
 Installed BinaryProvider ───── v0.5.8
 Installed SpecialFunctions ─── v0.9.0
 Installed Arpack_jll ───────── v3.5.0+2
 Installed FillArrays ───────── v0.8.2
 Installed Blosc ────────────── v0.5.1
 Installed BinDeps ──────────── v1.0.0
 Installed URIParser ────────── v0.4.0
 Installed StatsBase ────────── v0.32.0
 Installed Distances ────────── v0.8.2
 Installed QuadGK ───────────── v2.3.1
 Installed DataAPI ──────────── v1.1.0
 Installed CMake ────────────── v1.1.2
 Installed StatsFuns ────────── v0.9.3
 Installed NearestNeighbors ─── v0.4.4
 Installed OrderedCollections ─ v1.1.0
 Installed PDMats ───────────── v0.9.10
 Installed Compat ───────────── v2.2.0
 Installed Distributions ────── v0.22.0
 Installed FileIO ───────────── v1.2.1
  Updating `~/.julia/environments/v1.5/Project.toml`
  [cc18c42c] + GaussianMixtures v0.3.0
  Updating `~/.julia/environments/v1.5/Manifest.toml`
  [7d9fca2a] + Arpack v0.4.0
  [68821587] + Arpack_jll v3.5.0+2
  [9e28174c] + BinDeps v1.0.0
  [b99e7846] + BinaryProvider v0.5.8
  [a74b3585] + Blosc v0.5.1
  [631607c0] + CMake v1.1.2
  [d5fb7624] + CMakeWrapper v0.2.3
  [aaaa29a8] + Clustering v0.13.3
  [34da2185] + Compat v2.2.0
  [9a962f9c] + DataAPI v1.1.0
  [864edb3b] + DataStructures v0.17.7
  [b4f34e82] + Distances v0.8.2
  [31c24e10] + Distributions v0.22.0
  [5789e2e9] + FileIO v1.2.1
  [1a297f60] + FillArrays v0.8.2
  [cc18c42c] + GaussianMixtures v0.3.0
  [f67ccb44] + HDF5 v0.12.5
  [4138dd39] + JLD v0.9.1
  [1b4a561d] + LegacyStrings v0.4.1
  [e1d29d7a] + Missings v0.4.3
  [b8a86587] + NearestNeighbors v0.4.4
  [4536629a] + OpenBLAS_jll v0.3.7+4
  [efe28fd5] + OpenSpecFun_jll v0.5.3+1
  [bac558e1] + OrderedCollections v1.1.0
  [90014a1f] + PDMats v0.9.10
  [d96e819e] + Parameters v0.12.0
  [1fd47b50] + QuadGK v2.3.1
  [79098fc4] + Rmath v0.6.0
  [6e75b9c4] + ScikitLearnBase v0.5.0
  [a2af1166] + SortingAlgorithms v0.3.1
  [276daf66] + SpecialFunctions v0.9.0
  [90137ffa] + StaticArrays v0.12.1
  [2913bbd2] + StatsBase v0.32.0
  [4c63d2b9] + StatsFuns v0.9.3
  [30578b45] + URIParser v0.4.0
  [2a0f44e3] + Base64 
  [ade2ca70] + Dates 
  [8bb1440f] + DelimitedFiles 
  [8ba89e20] + Distributed 
  [b77e0a4c] + InteractiveUtils 
  [76f85450] + LibGit2 
  [8f399da3] + Libdl 
  [37e2e46d] + LinearAlgebra 
  [56ddb016] + Logging 
  [d6f4376e] + Markdown 
  [a63ad114] + Mmap 
  [44cfe95a] + Pkg 
  [de0858da] + Printf 
  [9abbd945] + Profile 
  [3fa0cd96] + REPL 
  [9a3f8284] + Random 
  [ea8e919c] + SHA 
  [9e88b42a] + Serialization 
  [1a1011a3] + SharedArrays 
  [6462fe0b] + Sockets 
  [2f01184e] + SparseArrays 
  [10745b16] + Statistics 
  [4607b0f0] + SuiteSparse 
  [8dfed614] + Test 
  [cf7118a7] + UUIDs 
  [4ec0a83e] + Unicode 
  Building Rmath → `~/.julia/packages/Rmath/BoBag/deps/build.log`
  Updating `/tmp/jl_niErK3/Project.toml`
 [no changes]
  Updating `/tmp/jl_niErK3/Manifest.toml`
 [no changes]
  Building CMake → `~/.julia/packages/CMake/nSK2r/deps/build.log`
  Updating `/tmp/jl_o43RQ5/Project.toml`
 [no changes]
  Updating `/tmp/jl_o43RQ5/Manifest.toml`
 [no changes]
  Building Blosc → `~/.julia/packages/Blosc/lzFr0/deps/build.log`
  Updating `/tmp/jl_RQVeLA/Project.toml`
 [no changes]
  Updating `/tmp/jl_RQVeLA/Manifest.toml`
 [no changes]
  Building HDF5 ─→ `~/.julia/packages/HDF5/Zh9on/deps/build.log`
  Updating `/tmp/jl_WQcYFO/Project.toml`
 [no changes]
  Updating `/tmp/jl_WQcYFO/Manifest.toml`
 [no changes]
   Testing GaussianMixtures
  Updating `/tmp/jl_TMyHbw/Project.toml`
 [no changes]
  Updating `/tmp/jl_TMyHbw/Manifest.toml`
 [no changes]
Running sandbox
Status `/tmp/jl_TMyHbw/Project.toml`
  [aaaa29a8] Clustering v0.13.3
  [34da2185] Compat v2.2.0
  [31c24e10] Distributions v0.22.0
  [5789e2e9] FileIO v1.2.1
  [cc18c42c] GaussianMixtures v0.3.0
  [4138dd39] JLD v0.9.1
  [90014a1f] PDMats v0.9.10
  [6e75b9c4] ScikitLearnBase v0.5.0
  [276daf66] SpecialFunctions v0.9.0
  [2913bbd2] StatsBase v0.32.0
  [8bb1440f] DelimitedFiles 
  [8ba89e20] Distributed 
  [37e2e46d] LinearAlgebra 
  [56ddb016] Logging 
  [de0858da] Printf 
  [9a3f8284] Random 
  [10745b16] Statistics 
[ Info: Testing Data
(100000, -1.288454015420614e6, [99999.16192935262, 0.8380706473846147], [71.50770873759086 -304.44983100101604 351.40970320769844; 0.027917147565549867 2.7045297780894857 -3.181487454516665], [[100442.00814534488 -181.71310719635093 -161.41704557937044; -181.71310719635093 99437.00760507135 -268.82095467015006; -161.41704557937044 -268.8209546701501 100015.25959336926], [0.3421016716258697 0.09037968707836047 0.2039446965836243; 0.09037968707836047 8.727802134805907 -10.266432395346785; 0.20394469658362432 -10.266432395346785 12.360752486885122]])
┌ Warning: rmprocs: process 1 not removed
└ @ Distributed /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Distributed/src/cluster.jl:1015
[ Info: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.690944e+03
      1       8.853743e+02      -8.055701e+02 |        5
      2       8.630127e+02      -2.236168e+01 |        2
      3       8.578110e+02      -5.201614e+00 |        0
      4       8.578110e+02       0.000000e+00 |        0
K-means converged with 4 iterations (objv = 857.8110389721696)
┌ Info: K-means with 272 data points using 4 iterations
└ 11.3 data points per parameter
[ Info: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
┌ Info: EM with 272 data points 0 iterations avll -2.066459
└ 5.8 data points per parameter
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:221
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:221
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:221
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:221
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:631 [inlined]
└ @ Core ./broadcast.jl:631
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:230
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:230
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:631 [inlined]
└ @ Core ./broadcast.jl:631
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:631 [inlined]
└ @ Core ./broadcast.jl:631
[ Info: iteration 1, lowerbound -3.769772
[ Info: iteration 2, lowerbound -3.617232
[ Info: iteration 3, lowerbound -3.452726
[ Info: iteration 4, lowerbound -3.275702
[ Info: iteration 5, lowerbound -3.111628
[ Info: dropping number of Gaussions to 7
[ Info: iteration 6, lowerbound -2.978269
[ Info: dropping number of Gaussions to 6
[ Info: iteration 7, lowerbound -2.875333
[ Info: iteration 8, lowerbound -2.814433
[ Info: dropping number of Gaussions to 5
[ Info: iteration 9, lowerbound -2.783446
[ Info: dropping number of Gaussions to 4
[ Info: iteration 10, lowerbound -2.762057
[ Info: dropping number of Gaussions to 3
[ Info: iteration 11, lowerbound -2.732105
[ Info: iteration 12, lowerbound -2.696634
[ Info: iteration 13, lowerbound -2.650809
[ Info: iteration 14, lowerbound -2.590878
[ Info: iteration 15, lowerbound -2.522616
[ Info: iteration 16, lowerbound -2.456856
[ Info: iteration 17, lowerbound -2.402506
[ Info: iteration 18, lowerbound -2.361327
[ Info: iteration 19, lowerbound -2.331277
[ Info: iteration 20, lowerbound -2.312545
[ Info: iteration 21, lowerbound -2.307530
[ Info: dropping number of Gaussions to 2
[ Info: iteration 22, lowerbound -2.302923
[ Info: iteration 23, lowerbound -2.299260
[ Info: iteration 24, lowerbound -2.299256
[ Info: iteration 25, lowerbound -2.299254
[ Info: iteration 26, lowerbound -2.299254
[ Info: iteration 27, lowerbound -2.299253
[ Info: iteration 28, lowerbound -2.299253
[ Info: iteration 29, lowerbound -2.299253
[ Info: iteration 30, lowerbound -2.299253
[ Info: iteration 31, lowerbound -2.299253
[ Info: iteration 32, lowerbound -2.299253
[ Info: iteration 33, lowerbound -2.299253
[ Info: iteration 34, lowerbound -2.299253
[ Info: iteration 35, lowerbound -2.299253
[ Info: iteration 36, lowerbound -2.299253
[ Info: iteration 37, lowerbound -2.299253
[ Info: iteration 38, lowerbound -2.299253
[ Info: iteration 39, lowerbound -2.299253
[ Info: iteration 40, lowerbound -2.299253
[ Info: iteration 41, lowerbound -2.299253
[ Info: iteration 42, lowerbound -2.299253
[ Info: iteration 43, lowerbound -2.299253
[ Info: iteration 44, lowerbound -2.299253
[ Info: iteration 45, lowerbound -2.299253
[ Info: iteration 46, lowerbound -2.299253
[ Info: iteration 47, lowerbound -2.299253
[ Info: iteration 48, lowerbound -2.299253
[ Info: iteration 49, lowerbound -2.299253
[ Info: iteration 50, lowerbound -2.299253
[ Info: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
History[Fri Jan 10 09:33:47 2020: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
, Fri Jan 10 09:33:56 2020: K-means with 272 data points using 4 iterations
11.3 data points per parameter
, Fri Jan 10 09:33:58 2020: EM with 272 data points 0 iterations avll -2.066459
5.8 data points per parameter
, Fri Jan 10 09:34:00 2020: GMM converted to Variational GMM
, Fri Jan 10 09:34:08 2020: iteration 1, lowerbound -3.769772
, Fri Jan 10 09:34:08 2020: iteration 2, lowerbound -3.617232
, Fri Jan 10 09:34:08 2020: iteration 3, lowerbound -3.452726
, Fri Jan 10 09:34:08 2020: iteration 4, lowerbound -3.275702
, Fri Jan 10 09:34:08 2020: iteration 5, lowerbound -3.111628
, Fri Jan 10 09:34:09 2020: dropping number of Gaussions to 7
, Fri Jan 10 09:34:09 2020: iteration 6, lowerbound -2.978269
, Fri Jan 10 09:34:09 2020: dropping number of Gaussions to 6
, Fri Jan 10 09:34:09 2020: iteration 7, lowerbound -2.875333
, Fri Jan 10 09:34:09 2020: iteration 8, lowerbound -2.814433
, Fri Jan 10 09:34:09 2020: dropping number of Gaussions to 5
, Fri Jan 10 09:34:09 2020: iteration 9, lowerbound -2.783446
, Fri Jan 10 09:34:09 2020: dropping number of Gaussions to 4
, Fri Jan 10 09:34:09 2020: iteration 10, lowerbound -2.762057
, Fri Jan 10 09:34:09 2020: dropping number of Gaussions to 3
, Fri Jan 10 09:34:09 2020: iteration 11, lowerbound -2.732105
, Fri Jan 10 09:34:09 2020: iteration 12, lowerbound -2.696634
, Fri Jan 10 09:34:09 2020: iteration 13, lowerbound -2.650809
, Fri Jan 10 09:34:09 2020: iteration 14, lowerbound -2.590878
, Fri Jan 10 09:34:09 2020: iteration 15, lowerbound -2.522616
, Fri Jan 10 09:34:09 2020: iteration 16, lowerbound -2.456856
, Fri Jan 10 09:34:09 2020: iteration 17, lowerbound -2.402506
, Fri Jan 10 09:34:09 2020: iteration 18, lowerbound -2.361327
, Fri Jan 10 09:34:09 2020: iteration 19, lowerbound -2.331277
, Fri Jan 10 09:34:09 2020: iteration 20, lowerbound -2.312545
, Fri Jan 10 09:34:09 2020: iteration 21, lowerbound -2.307530
, Fri Jan 10 09:34:09 2020: dropping number of Gaussions to 2
, Fri Jan 10 09:34:09 2020: iteration 22, lowerbound -2.302923
, Fri Jan 10 09:34:09 2020: iteration 23, lowerbound -2.299260
, Fri Jan 10 09:34:09 2020: iteration 24, lowerbound -2.299256
, Fri Jan 10 09:34:09 2020: iteration 25, lowerbound -2.299254
, Fri Jan 10 09:34:09 2020: iteration 26, lowerbound -2.299254
, Fri Jan 10 09:34:09 2020: iteration 27, lowerbound -2.299253
, Fri Jan 10 09:34:09 2020: iteration 28, lowerbound -2.299253
, Fri Jan 10 09:34:09 2020: iteration 29, lowerbound -2.299253
, Fri Jan 10 09:34:09 2020: iteration 30, lowerbound -2.299253
, Fri Jan 10 09:34:09 2020: iteration 31, lowerbound -2.299253
, Fri Jan 10 09:34:09 2020: iteration 32, lowerbound -2.299253
, Fri Jan 10 09:34:09 2020: iteration 33, lowerbound -2.299253
, Fri Jan 10 09:34:09 2020: iteration 34, lowerbound -2.299253
, Fri Jan 10 09:34:09 2020: iteration 35, lowerbound -2.299253
, Fri Jan 10 09:34:09 2020: iteration 36, lowerbound -2.299253
, Fri Jan 10 09:34:09 2020: iteration 37, lowerbound -2.299253
, Fri Jan 10 09:34:09 2020: iteration 38, lowerbound -2.299253
, Fri Jan 10 09:34:09 2020: iteration 39, lowerbound -2.299253
, Fri Jan 10 09:34:09 2020: iteration 40, lowerbound -2.299253
, Fri Jan 10 09:34:09 2020: iteration 41, lowerbound -2.299253
, Fri Jan 10 09:34:09 2020: iteration 42, lowerbound -2.299253
, Fri Jan 10 09:34:09 2020: iteration 43, lowerbound -2.299253
, Fri Jan 10 09:34:09 2020: iteration 44, lowerbound -2.299253
, Fri Jan 10 09:34:09 2020: iteration 45, lowerbound -2.299253
, Fri Jan 10 09:34:09 2020: iteration 46, lowerbound -2.299253
, Fri Jan 10 09:34:09 2020: iteration 47, lowerbound -2.299253
, Fri Jan 10 09:34:09 2020: iteration 48, lowerbound -2.299253
, Fri Jan 10 09:34:09 2020: iteration 49, lowerbound -2.299253
, Fri Jan 10 09:34:09 2020: iteration 50, lowerbound -2.299253
, Fri Jan 10 09:34:09 2020: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [95.95490777398331, 178.04509222601664]
β = [95.95490777398331, 178.04509222601664]
m = [2.0002292577753473 53.851987172461186; 4.250300733269888 79.28686694436152]
ν = [97.95490777398331, 180.04509222601664]
W = LinearAlgebra.UpperTriangular{Float64,Array{Float64,2}}[[0.3758763611948778 -0.008953123827346499; 0.0 0.012748664777409716], [0.1840415554748454 -0.007644049042327738; 0.0 0.008581705166333135]]
Kind: diag, size256
nx: 100000 sum(zeroth order stats): 100000.0
avll from stats: -1.0225417617840822
avll from llpg:  -1.0225417617840824
avll direct:     -1.0225417617840824
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 99999.99999999999
avll from stats: -1.0240726831987603
avll from llpg:  -1.02407268319876
avll direct:     -1.02407268319876
sum posterior: 100000.0
32×26 Array{Float64,2}:
 -0.0137023   -0.065312    0.014897    -0.146498     0.185681    -0.0689671    -0.00901233   -0.349084    0.141948    -0.00966429   0.021597     0.0316376  -0.199357     0.0636669    0.00105727  -0.0109779    0.0402217    0.0247146     0.122787   -0.00238163   -0.127822      0.0025503    0.127468    0.213383     -0.227315     0.0999495
 -0.0458216   -0.0752282   0.152251    -0.0856657   -0.011017     0.000392859  -0.0072032     0.0384955  -0.0842849   -0.0362461   -0.0538087    0.0157746   0.34574      0.0276588   -0.205368     0.0366295   -0.245684    -0.0475145     0.211864    0.0187774     0.026481     -0.00177425   0.040436    0.0598317     0.063546     0.116383
  0.0154425   -0.0502318  -0.0966076   -0.00853255  -0.184529    -0.0298736     0.159333      0.114723   -0.0840267    0.00866788   0.0261756    0.0515682   0.189957     0.0764528   -0.0570831    0.0590057   -0.249072     0.139033      0.0627409  -0.0567097    -0.0684378     0.0118883   -0.0662027   0.00644143    0.00775091  -0.051808
 -0.0645956    0.166256    0.0670655    0.111668     0.1594      -0.154851      0.182621     -0.0813424   0.0531108    0.0341088    0.026356    -0.0984374   0.035039    -0.193396     0.134044     0.102576    -0.224618     0.0856702     0.0714635  -0.00587424   -0.00273732    0.117783     0.0419176   0.129247      0.0704261   -0.00928879
 -0.020817    -0.0811219  -0.0639189    0.0545602   -0.088303     0.166898     -0.122833     -0.171611   -0.180916     0.0916498    0.0347405   -0.0114321   0.130456    -0.0345666   -0.0613247    0.0299839   -0.147927     0.0319757     0.1827      0.0192735    -0.185638     -0.192997    -0.105485   -0.0716965     0.0255293    0.0926724
 -0.0531654   -0.097928   -0.125195     0.0656409   -0.0425874   -0.0578296    -0.122604      0.0894299   0.00310725   0.0962052    0.0277556   -0.142307   -0.102617    -0.0232819    0.102185     0.00472738   0.070557     0.0110283     0.142507   -0.153741      0.0363316     0.110846    -0.115169   -0.0883924    -0.0388414    0.0324791
  0.0917472    0.0715794  -0.0517795   -0.0360308   -0.14129      0.19343       0.22107      -0.0819765  -0.120973     0.0158273    0.0285159    0.0407404  -0.171492     0.101679    -0.0608737    0.0982006    0.070043     0.00835139   -0.0236192  -0.101918     -0.134248     -0.175299    -0.083286    0.164476     -0.115063    -0.186104
 -0.100045    -0.112817    0.0888231    0.0572878    0.0585796   -0.134774      0.0506358    -0.0708813  -0.0155506   -0.0331871    0.0294079    0.137419   -0.0265838    0.00899545   0.00377097  -0.0236859    0.0278012    0.000720198  -0.0460817   0.0564943    -0.156975      0.230589    -0.0156683  -0.113471     -0.0987262   -0.0442133
  0.00593024  -0.0176579  -0.0878117   -0.0904858   -0.185874    -0.00367772   -0.10262      -0.090279   -0.137862     0.0388102   -0.0224077   -0.151812   -0.0514008    0.0432036   -0.0656715   -0.0144937   -0.0539854    0.100707      0.111819    0.00241059   -0.0722744     0.00838993   0.100142    0.153554      0.0186259    0.0319974
  0.118362    -0.122176   -0.0851291    0.0918658    0.0126961    0.0470562     0.00160859    0.0676777  -0.194214    -0.0270442   -0.00573301   0.0115878  -0.0529956   -0.0646246   -0.0245955   -0.177575     0.00344853  -0.080383     -0.0797156   0.118413     -0.00772365   -0.131368    -0.07957     0.0182099    -0.105674    -0.046843
  0.0680081   -0.123626    0.0404758   -0.0991462   -0.0863028    0.0181126    -0.126099      0.0187144  -0.0442325   -0.179593     0.0602631   -0.0329196   0.113808    -0.0809691   -0.124924    -0.260163     0.0495916    0.10071      -0.244367    0.000209292   0.174658      0.0758412   -0.172173   -0.0634526    -0.0369713    0.129283
  0.123596     0.195915   -0.0671329    0.0464715   -0.0509325   -0.114791      0.136914     -0.0306177   0.089631    -0.0638669   -0.010327    -0.048435    0.00671846  -0.203577    -0.0272282   -0.0321614   -0.0820504   -0.00831561    0.0602843  -0.111385     -0.146373     -0.0532137    0.0548423   0.000244714  -0.0457719   -0.2263
 -0.0461149   -0.0415884   0.0383142   -0.187666    -0.125835    -0.0249924     0.0372418     0.0726631   0.102224    -0.0298313   -0.022918    -0.0419293   0.0163466   -0.0707488   -0.0137543    0.017801     0.200241     0.108193      0.0457088   0.142336     -0.177535     -0.134585    -0.0674793  -0.019873     -0.00922189   0.0362699
 -0.00809044   0.0578415   0.0154225    0.0403728    0.0420756    0.262866     -0.093273      0.194096    0.0961297   -0.0583895    0.043093    -0.118861   -0.00528697   0.156982     0.122895     0.0853644   -0.0676013    0.0368436     0.0246648   0.0290752    -0.0145824     0.0130139    0.0239796   0.0193595    -0.170457    -0.160334
 -0.0486928    0.0569979  -0.0369913    0.0586999    0.0510101    0.0813274    -0.170387      0.150887   -0.11058      0.164969     0.0592485    0.0247224  -0.0423963    0.11809      0.17067      0.00294903   0.0118208   -0.000632833  -0.0548384   0.0441399     0.00832496   -0.0562115   -0.0394112   0.0636741    -0.194436    -0.0397319
  0.0746105   -0.0356632  -0.0223629    0.00678267   0.00288994   0.111748      0.0513794     0.165656    0.0377329   -0.0451627   -0.00559453  -0.0219122  -0.132924     0.00890328   0.0364584   -0.0160783   -0.103738    -0.13171       0.117296   -0.210686      0.0849658    -0.0646595   -0.0431161  -0.208264     -0.0225574   -0.0234693
  0.0595211   -0.0992514   0.0891857    0.0378559    0.0983524    0.0601848    -0.0290467    -0.0213445   0.012133     0.0413553   -0.130103    -0.0662664  -0.155787     0.0918911    0.0269685   -0.0482935   -0.0742285   -0.0519066     0.0224443   0.0884718     0.00121695    0.0862125    0.082375    0.232544      0.018647    -0.0581075
 -0.0514547    0.0907529   0.0942251    0.0234043    0.093121     0.13387      -0.0147164     0.109961    0.0323838   -0.117593     0.0498286    0.238544   -0.0631185    0.165035    -0.00386802   0.00894324  -0.123551    -0.00366662    0.103306   -0.126774     -0.0631839     0.0767305   -0.0358441   0.114189     -0.101027    -0.161488
 -0.130661     0.142849   -0.224185    -0.0401253   -0.100767    -0.027341     -0.000270591  -0.019306    0.0784628   -0.0573744    0.0619891   -0.0841928  -0.0094908   -0.0726181   -0.138092     0.170884     0.111581     0.125324      0.155       0.0918776    -0.124057     -0.15051      0.0815792  -0.120992     -0.0520981   -0.0301148
 -0.00127507  -0.0245052  -0.00412285   0.103091    -0.0186168   -0.00780143    0.0898084    -0.0617756  -0.00997104  -0.0747261   -0.0762776   -0.145817   -0.00130119  -0.0546672   -0.356429    -0.0516253   -0.0317112    0.0173185    -0.0237276  -0.020042     -0.000248302  -0.170352    -0.0391597  -0.0541818     0.104292    -0.120505
  0.148534     0.117873   -0.202028     0.0158653   -0.107574    -0.0221639     0.0283453     0.0183009  -0.133571     0.0921161   -0.0862267   -0.1257     -0.0689977    0.0396041   -0.191988     0.195505     0.149602    -0.0197157     0.170775   -0.143693      0.0806946     0.133455     0.0828526  -0.0354007     0.0606028    0.179791
 -0.0393919   -0.0364687   0.0568994   -0.011121    -0.079458    -0.193778     -0.0610467    -0.0601011  -0.00833567  -0.0321482    0.0343366    0.0273924   0.0852047    0.0497077   -0.0840685    0.0181698    0.133925     0.186804     -0.0820666   0.00945653    0.0339789    -0.0418463   -0.0968681  -0.0362644     0.0561425   -0.036414
 -0.0985896   -0.129862   -0.069083     0.170578    -0.0921376   -0.134997     -0.195228     -0.0392314  -0.0662033    0.0585818    0.088999     0.0758749   0.0405737   -0.208896     0.0336908    0.0583515    0.0326685   -0.0975274    -0.0166358  -0.121951     -0.156031      0.0548969   -0.207217   -0.0450205     0.0761057    0.0689944
 -0.0348525   -0.12229     0.0667299   -0.011693    -0.131531     0.172294      0.112815     -0.163747   -0.0433782    0.00180137  -0.0752745    0.145347   -0.0950291    0.321888    -0.0298944   -0.0143635    0.150504     0.0158704     0.0374374   0.072551      0.144857     -0.0718745   -0.101231    0.0980738    -0.093329    -0.0713348
  0.148994    -0.0697503   0.0539534   -0.0173892   -0.0412786    0.235728      0.0094317     0.0491049  -0.0417816   -0.00166592   0.132412     0.161351    0.0461742   -0.202366     0.0368779   -0.0848353    0.0351001    0.0504581    -0.0766295  -0.0617117     0.0449848     0.0683961    0.119795    0.163244     -0.00468036  -0.00969457
 -0.153066     0.0788669  -0.0449284    0.140468     0.0432759   -0.0235227     0.0145318     0.27669    -0.00351971   0.00734603   0.1299      -0.0299743  -0.0257728   -0.149078    -0.0747777   -0.0368487   -0.0442225   -0.0150627    -0.034548    0.160427      0.0184813     0.0308104   -0.0368159  -0.043608      0.0601586   -0.0999638
 -0.0259825   -0.0964911  -0.0410926    0.109524     0.294801     0.0318074     0.122443      0.0336744  -0.136385    -0.113907     0.0641281   -0.0726109  -0.12584      0.0436549    0.171979     0.170874     0.00533889  -0.0512718     0.102336    0.129802      0.0119099    -0.0108398    0.106026    0.199945     -0.0170016    0.0872709
 -0.0811262    0.0149745  -0.111585    -0.222726     0.0214049    0.0600252    -0.0207682    -0.0418251  -0.0325787   -0.0783393   -0.0263608    0.0816691   0.0385272   -0.0971548    0.00330394  -0.070743     0.108613     0.107249      0.0428518  -0.139309      0.272598     -0.0862921    0.173181   -0.137526     -0.150896    -0.0409395
 -0.0564825   -0.0770111  -0.093228    -0.0392338   -0.0176112   -0.0677217     0.102071      0.108903    0.0954844   -0.0937583    0.0783123    0.0779013   0.0329206   -0.285074    -0.123154     0.0914749    0.0144045   -0.0854095     0.0222465   0.0684628    -0.24812      -0.142827    -0.0377046   0.166707     -0.0929492   -0.0564352
  0.020361    -0.0627889   0.0445275    0.0234541    0.16016      0.14851      -0.0227643    -0.18901     0.0301725    0.0264634    0.174012     0.0601596  -0.144224     0.0697695    0.136752    -0.0385439    0.0815636   -0.107038      0.0704031   0.00826661   -0.0661276    -0.0402699    0.210741   -0.0739356     0.0282564    0.0278527
 -0.0112296   -0.152553    0.0762794   -0.0575102    0.040538    -0.0571356    -0.246108      0.0787815   7.10101e-5   0.12943      0.234755     0.137756   -0.103517    -0.101346     0.0118837    0.0724139    0.0792563    0.105615      0.0482782  -0.155423      0.217934      0.0561155    0.182443    0.179229      0.106092     0.059415
  0.182554     0.0279438  -0.0917938    0.0529202    0.128415    -0.142011     -0.0777091    -0.0211993   0.00783113   0.0958554   -0.179457    -0.15139     0.0400916    0.177941     0.0962014   -0.0411114    0.00397413  -0.0319672     0.0577648  -0.0366633    -0.0125971    -0.052816     0.0485763  -0.1972        0.0241621    0.13553kind diag, method split
┌ Info: 0: avll = 
└   tll[1] = -1.4004024794653644
[ Info: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.400472
[ Info: iteration 2, average log likelihood -1.400388
[ Info: iteration 3, average log likelihood -1.399698
[ Info: iteration 4, average log likelihood -1.393713
[ Info: iteration 5, average log likelihood -1.379809
[ Info: iteration 6, average log likelihood -1.372631
[ Info: iteration 7, average log likelihood -1.370832
[ Info: iteration 8, average log likelihood -1.369795
[ Info: iteration 9, average log likelihood -1.368952
[ Info: iteration 10, average log likelihood -1.368327
[ Info: iteration 11, average log likelihood -1.367877
[ Info: iteration 12, average log likelihood -1.367528
[ Info: iteration 13, average log likelihood -1.367258
[ Info: iteration 14, average log likelihood -1.367061
[ Info: iteration 15, average log likelihood -1.366920
[ Info: iteration 16, average log likelihood -1.366815
[ Info: iteration 17, average log likelihood -1.366732
[ Info: iteration 18, average log likelihood -1.366661
[ Info: iteration 19, average log likelihood -1.366584
[ Info: iteration 20, average log likelihood -1.366466
[ Info: iteration 21, average log likelihood -1.366267
[ Info: iteration 22, average log likelihood -1.366000
[ Info: iteration 23, average log likelihood -1.365767
[ Info: iteration 24, average log likelihood -1.365615
[ Info: iteration 25, average log likelihood -1.365527
[ Info: iteration 26, average log likelihood -1.365479
[ Info: iteration 27, average log likelihood -1.365453
[ Info: iteration 28, average log likelihood -1.365440
[ Info: iteration 29, average log likelihood -1.365433
[ Info: iteration 30, average log likelihood -1.365429
[ Info: iteration 31, average log likelihood -1.365427
[ Info: iteration 32, average log likelihood -1.365426
[ Info: iteration 33, average log likelihood -1.365425
[ Info: iteration 34, average log likelihood -1.365424
[ Info: iteration 35, average log likelihood -1.365424
[ Info: iteration 36, average log likelihood -1.365423
[ Info: iteration 37, average log likelihood -1.365423
[ Info: iteration 38, average log likelihood -1.365423
[ Info: iteration 39, average log likelihood -1.365423
[ Info: iteration 40, average log likelihood -1.365423
[ Info: iteration 41, average log likelihood -1.365423
[ Info: iteration 42, average log likelihood -1.365423
[ Info: iteration 43, average log likelihood -1.365423
[ Info: iteration 44, average log likelihood -1.365423
[ Info: iteration 45, average log likelihood -1.365423
[ Info: iteration 46, average log likelihood -1.365423
[ Info: iteration 47, average log likelihood -1.365423
[ Info: iteration 48, average log likelihood -1.365423
[ Info: iteration 49, average log likelihood -1.365423
[ Info: iteration 50, average log likelihood -1.365423
┌ Info: EM with 100000 data points 50 iterations avll -1.365423
└ 952.4 data points per parameter
┌ Info: 1
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4004722953267976
│     -1.4003879131684152
│      ⋮
└     -1.3654229195002143
[ Info: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.365519
[ Info: iteration 2, average log likelihood -1.365422
[ Info: iteration 3, average log likelihood -1.365025
[ Info: iteration 4, average log likelihood -1.361094
[ Info: iteration 5, average log likelihood -1.347501
[ Info: iteration 6, average log likelihood -1.336497
[ Info: iteration 7, average log likelihood -1.332410
[ Info: iteration 8, average log likelihood -1.330224
[ Info: iteration 9, average log likelihood -1.328495
[ Info: iteration 10, average log likelihood -1.327083
[ Info: iteration 11, average log likelihood -1.326131
[ Info: iteration 12, average log likelihood -1.325486
[ Info: iteration 13, average log likelihood -1.325006
[ Info: iteration 14, average log likelihood -1.324624
[ Info: iteration 15, average log likelihood -1.324299
[ Info: iteration 16, average log likelihood -1.324008
[ Info: iteration 17, average log likelihood -1.323736
[ Info: iteration 18, average log likelihood -1.323474
[ Info: iteration 19, average log likelihood -1.323216
[ Info: iteration 20, average log likelihood -1.322960
[ Info: iteration 21, average log likelihood -1.322697
[ Info: iteration 22, average log likelihood -1.322405
[ Info: iteration 23, average log likelihood -1.322091
[ Info: iteration 24, average log likelihood -1.321786
[ Info: iteration 25, average log likelihood -1.321522
[ Info: iteration 26, average log likelihood -1.321319
[ Info: iteration 27, average log likelihood -1.321180
[ Info: iteration 28, average log likelihood -1.321087
[ Info: iteration 29, average log likelihood -1.321022
[ Info: iteration 30, average log likelihood -1.320973
[ Info: iteration 31, average log likelihood -1.320933
[ Info: iteration 32, average log likelihood -1.320897
[ Info: iteration 33, average log likelihood -1.320864
[ Info: iteration 34, average log likelihood -1.320830
[ Info: iteration 35, average log likelihood -1.320796
[ Info: iteration 36, average log likelihood -1.320759
[ Info: iteration 37, average log likelihood -1.320719
[ Info: iteration 38, average log likelihood -1.320676
[ Info: iteration 39, average log likelihood -1.320630
[ Info: iteration 40, average log likelihood -1.320580
[ Info: iteration 41, average log likelihood -1.320527
[ Info: iteration 42, average log likelihood -1.320473
[ Info: iteration 43, average log likelihood -1.320417
[ Info: iteration 44, average log likelihood -1.320361
[ Info: iteration 45, average log likelihood -1.320301
[ Info: iteration 46, average log likelihood -1.320235
[ Info: iteration 47, average log likelihood -1.320160
[ Info: iteration 48, average log likelihood -1.320068
[ Info: iteration 49, average log likelihood -1.319962
[ Info: iteration 50, average log likelihood -1.319857
┌ Info: EM with 100000 data points 50 iterations avll -1.319857
└ 473.9 data points per parameter
┌ Info: 2
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.3655188558401046
│     -1.3654221211943567
│      ⋮
└     -1.3198566676767467
[ Info: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.319929
[ Info: iteration 2, average log likelihood -1.319712
[ Info: iteration 3, average log likelihood -1.319257
[ Info: iteration 4, average log likelihood -1.315453
[ Info: iteration 5, average log likelihood -1.301713
[ Info: iteration 6, average log likelihood -1.288209
[ Info: iteration 7, average log likelihood -1.282604
[ Info: iteration 8, average log likelihood -1.280136
[ Info: iteration 9, average log likelihood -1.278184
[ Info: iteration 10, average log likelihood -1.276487
[ Info: iteration 11, average log likelihood -1.275258
[ Info: iteration 12, average log likelihood -1.274331
[ Info: iteration 13, average log likelihood -1.273671
[ Info: iteration 14, average log likelihood -1.273234
[ Info: iteration 15, average log likelihood -1.272916
[ Info: iteration 16, average log likelihood -1.272668
[ Info: iteration 17, average log likelihood -1.272456
[ Info: iteration 18, average log likelihood -1.272254
[ Info: iteration 19, average log likelihood -1.272041
[ Info: iteration 20, average log likelihood -1.271791
[ Info: iteration 21, average log likelihood -1.271468
[ Info: iteration 22, average log likelihood -1.271014
[ Info: iteration 23, average log likelihood -1.270403
[ Info: iteration 24, average log likelihood -1.269694
[ Info: iteration 25, average log likelihood -1.268998
[ Info: iteration 26, average log likelihood -1.268412
[ Info: iteration 27, average log likelihood -1.267951
[ Info: iteration 28, average log likelihood -1.267546
[ Info: iteration 29, average log likelihood -1.267142
[ Info: iteration 30, average log likelihood -1.266723
[ Info: iteration 31, average log likelihood -1.266324
[ Info: iteration 32, average log likelihood -1.265998
[ Info: iteration 33, average log likelihood -1.265736
[ Info: iteration 34, average log likelihood -1.265447
[ Info: iteration 35, average log likelihood -1.265086
[ Info: iteration 36, average log likelihood -1.264692
[ Info: iteration 37, average log likelihood -1.264298
[ Info: iteration 38, average log likelihood -1.263990
[ Info: iteration 39, average log likelihood -1.263813
[ Info: iteration 40, average log likelihood -1.263721
[ Info: iteration 41, average log likelihood -1.263663
[ Info: iteration 42, average log likelihood -1.263615
[ Info: iteration 43, average log likelihood -1.263559
[ Info: iteration 44, average log likelihood -1.263470
[ Info: iteration 45, average log likelihood -1.263305
[ Info: iteration 46, average log likelihood -1.263007
[ Info: iteration 47, average log likelihood -1.262612
[ Info: iteration 48, average log likelihood -1.262271
[ Info: iteration 49, average log likelihood -1.262073
[ Info: iteration 50, average log likelihood -1.261985
┌ Info: EM with 100000 data points 50 iterations avll -1.261985
└ 236.4 data points per parameter
┌ Info: 3
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.319929187495195
│     -1.3197118296271821
│      ⋮
└     -1.2619850806813413
[ Info: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.262165
[ Info: iteration 2, average log likelihood -1.261911
[ Info: iteration 3, average log likelihood -1.260986
[ Info: iteration 4, average log likelihood -1.252453
[ Info: iteration 5, average log likelihood -1.228045
[ Info: iteration 6, average log likelihood -1.206895
[ Info: iteration 7, average log likelihood -1.195621
[ Info: iteration 8, average log likelihood -1.186443
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.179051
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     13
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.192309
[ Info: iteration 11, average log likelihood -1.194906
[ Info: iteration 12, average log likelihood -1.185753
[ Info: iteration 13, average log likelihood -1.181599
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 14, average log likelihood -1.178117
[ Info: iteration 15, average log likelihood -1.189701
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     11
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 16, average log likelihood -1.182201
[ Info: iteration 17, average log likelihood -1.187398
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     13
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 18, average log likelihood -1.180584
[ Info: iteration 19, average log likelihood -1.186169
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 20, average log likelihood -1.177110
[ Info: iteration 21, average log likelihood -1.190234
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     11
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 22, average log likelihood -1.183324
[ Info: iteration 23, average log likelihood -1.188651
[ Info: iteration 24, average log likelihood -1.181804
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 25, average log likelihood -1.177632
[ Info: iteration 26, average log likelihood -1.190467
[ Info: iteration 27, average log likelihood -1.183622
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     11
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 28, average log likelihood -1.179066
[ Info: iteration 29, average log likelihood -1.183451
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 30, average log likelihood -1.174260
[ Info: iteration 31, average log likelihood -1.186289
[ Info: iteration 32, average log likelihood -1.178363
[ Info: iteration 33, average log likelihood -1.173650
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     11
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 34, average log likelihood -1.169641
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 35, average log likelihood -1.175649
[ Info: iteration 36, average log likelihood -1.186230
[ Info: iteration 37, average log likelihood -1.178816
[ Info: iteration 38, average log likelihood -1.174211
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 39, average log likelihood -1.170126
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     11
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 40, average log likelihood -1.181685
[ Info: iteration 41, average log likelihood -1.184944
[ Info: iteration 42, average log likelihood -1.177043
[ Info: iteration 43, average log likelihood -1.172441
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 44, average log likelihood -1.168361
[ Info: iteration 45, average log likelihood -1.182719
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     11
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 46, average log likelihood -1.175783
[ Info: iteration 47, average log likelihood -1.181361
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 48, average log likelihood -1.173562
[ Info: iteration 49, average log likelihood -1.184685
[ Info: iteration 50, average log likelihood -1.177769
┌ Info: EM with 100000 data points 50 iterations avll -1.177769
└ 118.1 data points per parameter
┌ Info: 4
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.2621646300364637
│     -1.2619107435949006
│      ⋮
└     -1.1777694621321693
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     21
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 1, average log likelihood -1.173629
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      9
│     21
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 2, average log likelihood -1.170150
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     10
│     21
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 3, average log likelihood -1.165249
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     21
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.142734
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      3
│     10
│     13
│     14
│     21
│     22
│     25
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.070188
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      8
│      9
│     12
│     21
│     22
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.073460
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      3
│     10
│     14
│     21
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.077261
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│     12
│     13
│     14
│     21
│     22
│     25
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.056404
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      3
│      9
│     10
│     12
│     19
│     21
│     22
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.054878
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      8
│     14
│     21
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.082505
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│     10
│     12
│     13
│     21
│     22
│     25
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 11, average log likelihood -1.060041
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      3
│      9
│     14
│     21
│     22
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 12, average log likelihood -1.059335
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      8
│     10
│     12
│     13
│     21
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 13, average log likelihood -1.064824
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│     12
│     14
│     21
│     22
│     25
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 14, average log likelihood -1.055419
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      2
│      3
│      9
│     10
│      ⋮
│     21
│     22
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 15, average log likelihood -1.042385
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      8
│     21
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 16, average log likelihood -1.091503
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      3
│     10
│     12
│     14
│     21
│     22
│     25
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 17, average log likelihood -1.062432
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│     13
│     14
│     21
│     22
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 18, average log likelihood -1.076664
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      2
│      3
│      8
│      9
│      ⋮
│     21
│     22
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 19, average log likelihood -1.048105
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│     14
│     19
│     21
│     22
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 20, average log likelihood -1.081416
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│     10
│     12
│     13
│     14
│     21
│     22
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 21, average log likelihood -1.067068
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      3
│     21
│     22
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 22, average log likelihood -1.069119
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      2
│      8
│      9
│     10
│      ⋮
│     22
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 23, average log likelihood -1.048615
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      3
│     13
│     21
│     22
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 24, average log likelihood -1.083364
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│     10
│     12
│     14
│     19
│     21
│     22
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 25, average log likelihood -1.067034
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      3
│     12
│     13
│     14
│     21
│     22
│     26
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 26, average log likelihood -1.063647
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      2
│      8
│      9
│     10
│     21
│     22
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 27, average log likelihood -1.064855
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      3
│     12
│     14
│     21
│     22
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 28, average log likelihood -1.075438
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│     10
│     13
│     14
│     21
│     22
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 29, average log likelihood -1.071121
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      3
│     12
│     21
│     22
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 30, average log likelihood -1.063694
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      8
│      9
│     10
│     12
│      ⋮
│     22
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 31, average log likelihood -1.050247
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      2
│      3
│     13
│     21
│     22
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 32, average log likelihood -1.078095
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│     10
│     12
│     14
│     21
│     22
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 33, average log likelihood -1.074216
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      3
│     12
│     13
│     14
│     21
│     22
│     26
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 34, average log likelihood -1.053397
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      2
│      8
│      9
│     10
│     21
│     22
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 35, average log likelihood -1.056865
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      3
│     12
│     14
│     21
│     22
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 36, average log likelihood -1.072703
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│     10
│     13
│     14
│     21
│     22
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 37, average log likelihood -1.068610
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      3
│     12
│     21
│     22
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 38, average log likelihood -1.061596
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      2
│      8
│      9
│     10
│      ⋮
│     22
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 39, average log likelihood -1.050636
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      3
│     13
│     19
│     21
│     22
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 40, average log likelihood -1.078572
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│     10
│     12
│     14
│     21
│     22
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 41, average log likelihood -1.071661
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      3
│     12
│     13
│     14
│     21
│     22
│     26
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 42, average log likelihood -1.056955
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      2
│      8
│      9
│     10
│     21
│     22
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 43, average log likelihood -1.061953
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      3
│     12
│     14
│     21
│     22
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 44, average log likelihood -1.072678
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│     10
│     13
│     14
│     21
│     22
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 45, average log likelihood -1.068313
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      3
│     12
│     21
│     22
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 46, average log likelihood -1.060181
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      2
│      8
│      9
│     10
│      ⋮
│     22
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 47, average log likelihood -1.046713
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      3
│     13
│     21
│     22
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 48, average log likelihood -1.072681
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│     10
│     12
│     14
│     21
│     22
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 49, average log likelihood -1.058264
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      3
│     12
│     13
│     14
│     21
│     22
│     26
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 50, average log likelihood -1.046169
┌ Info: EM with 100000 data points 50 iterations avll -1.046169
└ 59.0 data points per parameter
┌ Info: 5
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.1736289101001218
│     -1.1701503038097072
│      ⋮
└     -1.0461686054395267
┌ Info: Total log likelihood: 
│   tll =
│    251-element Array{Float64,1}:
│     -1.4004024794653644
│     -1.4004722953267976
│     -1.4003879131684152
│     -1.3996984753128616
│      ⋮
│     -1.0726810503945643
│     -1.0582637562702735
└     -1.0461686054395267
32×26 Array{Float64,2}:
  0.017028    -0.0901145    -0.00850517   0.0827369  -0.0640724    0.00330544  -0.103748    -0.00296386  -0.0723636     0.0297847    0.102662      0.0962525    0.042833    -0.206182     0.0495957   -0.00832505   0.0503722   -0.0105111   -0.0327947   -0.10864     -0.0764103     0.0517012    -0.0469455    0.04074       0.0372878   0.0412073
  0.0178101   -0.114075      0.00736512  -0.0466885   0.032599     0.0270227   -0.0780656    0.0323903   -0.0682458    -0.153853     0.0702714    -0.0510191    0.0392583   -0.0452634   -0.0229547   -0.11662      0.0188494    0.0638331   -0.130778     0.0347793    0.131455      0.0470551    -0.0739302    0.000302496  -0.0260264   0.124324
 -0.0401311   -0.035956      0.0342927   -0.181432   -0.10278     -0.0230287    0.0306757    0.0693184    0.102452     -0.0373034   -0.00299685   -0.0272796    0.0167906   -0.0740679   -0.00761796   0.018067     0.191498     0.0803006    0.044981     0.12703     -0.17782      -0.146361     -0.0613647   -0.0144855    -0.0122348   0.040294
  0.0435087   -0.015477     -0.0968926   -0.0758097  -0.187651     0.0201822   -0.096645    -0.0903817   -0.146684      0.0369449   -0.0310653    -0.153201    -0.023707     0.0558368   -0.0652341   -0.0163904   -0.0554276    0.209842     0.137006     0.00944807  -0.073283      0.0102109     0.107553     0.157676      0.0203345   0.0329252
 -0.143686     0.149508     -0.198524    -0.0313289  -0.107107    -0.00321717  -0.0377064   -0.0256612    0.0805818    -0.0634878    0.0623158    -0.083698     0.00954551  -0.0780186   -0.125006     0.171839     0.134222     0.117538     0.173543     0.0907282   -0.107202     -0.148665      0.0701014   -0.14536      -0.057302   -0.00632251
 -0.031503    -0.000837761   0.0484119   -0.0163432   0.0220472    0.0410598   -0.0777445    0.101914    -0.100863      0.0657806    0.00800557    0.0220271    0.132375     0.0810827    0.00011068   0.00646358  -0.102784    -0.0187193    0.0741324    0.0300169    0.0191601    -0.0200639    -0.00747894   0.0421604    -0.0552748   0.0409532
  0.139557     0.10587      -0.190824     0.0128751  -0.0975785   -0.0299731   -0.0440801   -0.0209798   -0.169185      0.0909413   -0.079538     -0.123532    -0.0734824    0.0416767   -0.185923     0.179853     0.142537    -0.019026     0.158677    -0.141496     0.0833037     0.116209      0.0531606   -0.0403367     0.05444     0.177402
  0.0758965   -0.052686     -0.0189331    0.0310986  -0.0171755    0.117219     0.187695     0.145673     0.0366969    -0.0523869    0.000551288  -0.0208156   -0.121793    -0.00131303   0.0330982   -0.0238633   -0.0980929   -0.133086     0.109791    -0.209252     0.0848517    -0.0828838    -0.0252144   -0.194402     -0.0742492  -0.021541
 -0.01636     -0.0993216    -0.0248972    0.154477    0.288991     0.0380795    0.154945     0.0310193   -0.137486     -0.116511     0.0392656    -0.0483487   -0.153076     0.0455501    0.207559     0.172097    -0.00368612  -0.0499242    0.0542078    0.156       -0.0218571    -0.0407138     0.105839     0.205355     -0.0413218   0.0682881
 -0.0194361   -0.122282      0.0663945   -0.0204637  -0.126948     0.184197     0.112964    -0.162422    -0.0255406    -0.0299912   -0.0742411     0.143768    -0.101718     0.303375    -0.0339526   -0.0136758    0.161401     0.0143327    0.045941     0.0682228    0.146328     -0.0744434    -0.102828     0.116352     -0.0920496  -0.0794923
  0.164268     0.199435     -0.0706405    0.0800182  -0.0029577   -0.106455     0.140024    -0.0330881    0.0620372    -0.0750518   -0.00177309   -0.0491986    0.00415816  -0.187371    -0.0202655   -0.0149837   -0.106007    -0.00577592   0.0635948   -0.120005    -0.170914     -0.0348944     0.0568142   -0.0205408    -0.0616575  -0.229318
 -0.0396851   -0.0369939     0.0585897   -0.0121749  -0.0738229   -0.199556    -0.0663631   -0.0726535   -0.0268606    -0.0424767    0.0276167     0.020547     0.0851133    0.0410399   -0.102748     0.0356476    0.114751     0.194436    -0.0790564    0.0113553    0.0367319    -0.00615141   -0.0943426   -0.0186612     0.053354   -0.0395564
 -0.0871209   -0.000682522  -0.116661    -0.218731   -0.0374405    0.0763389    0.0093984   -0.0430804   -0.0137237    -0.0823447    0.00348137    0.0763302    0.0374902   -0.0998164    0.0119716   -0.0491547    0.128843     0.102892     0.0456764   -0.137561     0.271747     -0.111453      0.185155    -0.131792     -0.150794   -0.0452643
 -0.0975228   -0.0947176     0.12199      0.0610889   0.0636059   -0.144846     0.0511539   -0.0707582    0.0118124    -0.0377024    0.0537759     0.115396    -0.0523203    0.00923185   0.00462117  -0.0218762    0.0362524    0.00388885  -0.0494683    0.0547184   -0.1584        0.204803     -0.0383152   -0.117878     -0.108624   -0.0469564
 -0.141757     0.0603255    -0.0410056    0.142592    0.0368084   -0.0240911    0.0222956    0.245981     0.0204538     0.031147     0.133218     -0.0238524   -0.0225103   -0.1527      -0.0737883   -8.18802e-5  -0.0429066    0.0288411   -0.0249862    0.192479     0.0449045     0.0424311    -0.108372    -0.0355845     0.0518331  -0.0859836
 -0.0133503   -0.148173      0.106857    -0.0311695   0.0305675   -0.0678508   -0.254769     0.0837456    0.0270493     0.123193     0.245471      0.133793    -0.0945829   -0.0838334   -0.0388006    0.112779     0.0766481    0.0866209    0.0375821   -0.129029     0.223494      0.0564233     0.161449     0.158816      0.125273    0.0496744
 -0.00965744  -0.0704301    -0.133675    -0.0162005  -0.18027     -0.0267566    0.14294      0.158699    -0.100402      0.00471497  -0.0568266    -0.169617     0.11314      0.0233746   -0.0577406    0.0410787   -0.249129     0.166201     0.128096    -0.0710942   -0.0171543    -0.0207121    -0.0604757   -0.00134493   -0.460939   -0.0353913
  0.0411371   -0.0913012    -0.109885     0.0085428  -0.18308     -0.030739     0.185019     0.0569385   -0.0749331     0.055235     0.0956026     0.311947     0.255133     0.0887656   -0.0551588    0.0677406   -0.237847     0.174517    -0.0779308   -0.037067    -0.0823363     0.0532472    -0.0719666   -0.00207518    0.536162   -0.101759
  0.0271003   -0.0861814     0.07855      0.0702227   0.0393226    0.0265501    0.0170598   -0.0312436   -0.000363374  -0.00962515  -0.0940782    -0.0893859   -0.0371852    0.0212947   -0.172508    -0.0503753   -0.0589054   -0.0270445   -0.00902244   0.0236115    0.000527211  -0.0337394     0.023158     0.101026      0.0429651  -0.0958303
  0.190229     0.010737     -0.0910779    0.0533657   0.130596    -0.144387    -0.0868939   -0.0202396    0.0279498     0.0899306   -0.183103     -0.145154     0.0557193    0.180643     0.0705633   -0.0390376    0.00346109  -0.0587216    0.0502153   -0.0350894    0.0103946    -0.0746015     0.0502146   -0.197036      0.0214939   0.168429
  0.0795495   -0.101944     -0.0652742    0.0348483  -0.117269     0.182886    -0.1089      -0.188487    -0.115682      0.0911284    0.178116      0.034422     0.168992    -0.479224    -0.0689083   -0.135917    -0.0186626    0.0367155    0.1825      -0.145336    -0.0402583    -0.154483     -0.149094    -0.0709008    -0.0200225   0.00747368
 -0.115245    -0.0667133    -0.0583004    0.0671215  -0.0483406    0.163996    -0.134289    -0.162255    -0.225141      0.0801681   -0.0183726    -0.0283958    0.0715585    0.354149    -0.0622877    0.146948    -0.259835    -0.0154231    0.182387     0.147454    -0.302752     -0.21791      -0.058687    -0.0767734     0.0848138   0.229525
  0.0237506   -0.0360785     0.0548857    0.0842296   0.179799     0.147373    -0.0344907   -0.189652     0.0327685     0.0265983    0.196073      0.100016    -0.127047     0.0781509    0.145037    -0.0433999    0.089858    -0.125051     0.0701349    0.00789084  -0.0221202    -0.0172483     0.211649    -0.0757429     0.0235895   0.0326686
  0.0788609    0.0435682    -0.0272599   -0.03157    -0.141937     0.193325     0.215333    -0.0911409   -0.120659      0.0173505    0.0340264     0.00179347  -0.146993     0.108809    -0.063317     0.00919154   0.0500765    0.00795699  -0.026595    -0.0931332   -0.125699     -0.156469     -0.0864567    0.154113     -0.119576   -0.190363
 -0.0496652    0.098451      0.0949066    0.0194735   0.117076     0.166914    -0.0313494    0.111866     0.03021      -0.113562     0.0625357     0.231388    -0.0627038    0.163233    -0.00543779   0.00160256  -0.123522     0.0185728    0.124454    -0.132922    -0.0669335     0.0750454    -0.102357     0.115217     -0.0941851  -0.218528
 -0.0515001   -0.0918289    -0.126085     0.0712586  -0.0460543   -0.0607381   -0.126505     0.109713     0.0237177     0.035519     0.0248572    -0.115342    -0.102323    -0.0244691    0.100753     0.0083803    0.0737058    0.0049825    0.118623    -0.159869     0.0418358     0.109411     -0.117137    -0.0889026    -0.0387906   0.0268186
 -0.00507742  -0.073256     -0.0778193   -0.18001     0.189423    -0.0629582    0.0272724   -0.302028     0.126436     -0.19934      0.146133      0.0596017   -0.19721      0.0698545    0.0308456    0.120373    -0.0403878    0.00282618   0.00793304   0.0520541   -0.112456     -0.00597857    0.107428     0.226346     -0.248996    0.0337935
 -0.0639751   -0.0625275     0.0406862   -0.125696    0.187951    -0.131452    -0.00959432  -0.365158     0.149775     -0.0132032   -0.019309      0.0310738   -0.193072     0.0637599   -0.0317224   -0.053997     0.10516      0.0453881    0.138514    -0.0332615   -0.18377      -0.000674864   0.176836     0.200943     -0.238053    0.136017
 -0.00860287   0.0377409    -0.00256665   0.0284098   0.0339496    0.264464    -0.102541     0.188044     0.102439     -0.098074     0.0389855    -0.0808184   -0.00228665   0.147276     0.121187     0.072718    -0.0679478    0.0352868    0.0249164    0.020173    -0.0170632     0.0151241     0.0208902    0.0364127    -0.184139   -0.166718
  0.109057    -0.118807     -0.0916065    0.097921    0.00802582   0.0493605    8.82614e-5   0.0670607   -0.203524     -0.0175491   -0.00717724    0.0512821   -0.0366313   -0.06294     -0.0260175   -0.199008     0.00345684  -0.0775342   -0.0846549    0.145288    -0.0113871    -0.128166     -0.0766825   -0.00876383   -0.0987614  -0.0468981
 -0.0565089   -0.0433598    -0.0875693   -0.0500077  -0.0190136   -0.0703273    0.110617     0.110294     0.112224     -0.0628007    0.0802914     0.0634637    0.030135    -0.2938      -0.120503     0.0990641    0.0289398   -0.0908854    0.00872845   0.0700079   -0.245161     -0.137011     -0.0455029    0.184295     -0.104211   -0.0702277
 -0.0722753    0.196339      0.0640948    0.115589    0.153876    -0.150938     0.206042    -0.0878262    0.0614897     0.0222424    0.0305449    -0.116173     0.00575699  -0.179588     0.126416     0.104404    -0.183336     0.146256     0.0780493   -0.00647702   0.00425299    0.118291      0.00282463   0.127474      0.104079   -0.00845755[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      2
│      8
│      9
│     10
│     12
│     21
│     22
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 1, average log likelihood -1.052752
┌ Warning: Variances had to be floored 
│   ind =
│    12-element Array{Int64,1}:
│      2
│      3
│      8
│      9
│      ⋮
│     25
│     26
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 2, average log likelihood -1.026328
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      2
│      8
│      9
│     10
│      ⋮
│     21
│     22
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 3, average log likelihood -1.046066
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      2
│      3
│      8
│      9
│      ⋮
│     25
│     26
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.030083
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      2
│      8
│      9
│     10
│      ⋮
│     21
│     22
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.048030
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      2
│      3
│      8
│      9
│      ⋮
│     25
│     26
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.024179
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      2
│      8
│      9
│     10
│     12
│     21
│     22
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.052110
┌ Warning: Variances had to be floored 
│   ind =
│    12-element Array{Int64,1}:
│      2
│      3
│      8
│      9
│      ⋮
│     25
│     26
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.026320
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      2
│      8
│      9
│     10
│      ⋮
│     21
│     22
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.046060
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      2
│      3
│      8
│      9
│      ⋮
│     25
│     26
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.030080
┌ Info: EM with 100000 data points 10 iterations avll -1.030080
└ 59.0 data points per parameter
kind diag, method kmeans
[ Info: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.506247e+05
      1       6.716953e+05      -1.789293e+05 |       32
      2       6.410881e+05      -3.060723e+04 |       32
      3       6.242884e+05      -1.679975e+04 |       32
      4       6.133093e+05      -1.097911e+04 |       32
      5       6.070520e+05      -6.257213e+03 |       32
      6       6.040833e+05      -2.968758e+03 |       32
      7       6.025211e+05      -1.562167e+03 |       32
      8       6.012885e+05      -1.232616e+03 |       32
      9       6.003833e+05      -9.052514e+02 |       32
     10       5.996232e+05      -7.600166e+02 |       32
     11       5.988993e+05      -7.239668e+02 |       32
     12       5.981103e+05      -7.889909e+02 |       32
     13       5.973617e+05      -7.485337e+02 |       32
     14       5.967818e+05      -5.799610e+02 |       32
     15       5.963816e+05      -4.001600e+02 |       32
     16       5.961138e+05      -2.678383e+02 |       32
     17       5.959410e+05      -1.727555e+02 |       32
     18       5.958308e+05      -1.102785e+02 |       32
     19       5.957252e+05      -1.055407e+02 |       32
     20       5.956616e+05      -6.360417e+01 |       32
     21       5.956075e+05      -5.414468e+01 |       32
     22       5.955597e+05      -4.779059e+01 |       32
     23       5.955221e+05      -3.760544e+01 |       31
     24       5.954992e+05      -2.282276e+01 |       32
     25       5.954792e+05      -2.009567e+01 |       30
     26       5.954623e+05      -1.689759e+01 |       29
     27       5.954501e+05      -1.213848e+01 |       30
     28       5.954401e+05      -9.970756e+00 |       30
     29       5.954313e+05      -8.816349e+00 |       29
     30       5.954236e+05      -7.731902e+00 |       27
     31       5.954163e+05      -7.283105e+00 |       26
     32       5.954103e+05      -6.001564e+00 |       27
     33       5.954051e+05      -5.206312e+00 |       26
     34       5.954003e+05      -4.775000e+00 |       27
     35       5.953953e+05      -5.036343e+00 |       32
     36       5.953909e+05      -4.413813e+00 |       24
     37       5.953869e+05      -3.950259e+00 |       27
     38       5.953817e+05      -5.184796e+00 |       26
     39       5.953757e+05      -6.054053e+00 |       26
     40       5.953708e+05      -4.882722e+00 |       28
     41       5.953672e+05      -3.585280e+00 |       23
     42       5.953641e+05      -3.074932e+00 |       22
     43       5.953602e+05      -3.905728e+00 |       23
     44       5.953563e+05      -3.956136e+00 |       21
     45       5.953544e+05      -1.912811e+00 |       23
     46       5.953524e+05      -1.971731e+00 |       24
     47       5.953498e+05      -2.579268e+00 |       19
     48       5.953473e+05      -2.550513e+00 |       19
     49       5.953442e+05      -3.042076e+00 |       24
     50       5.953407e+05      -3.487089e+00 |       22
K-means terminated without convergence after 50 iterations (objv = 595340.7417971303)
┌ Info: K-means with 32000 data points using 50 iterations
└ 37.0 data points per parameter
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.316423
[ Info: iteration 2, average log likelihood -1.287230
[ Info: iteration 3, average log likelihood -1.258743
[ Info: iteration 4, average log likelihood -1.224752
[ Info: iteration 5, average log likelihood -1.181502
[ Info: iteration 6, average log likelihood -1.135773
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      2
│      8
│     11
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.092211
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      5
│     10
│     12
│     15
│     22
│     26
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.085164
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     4
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.134694
[ Info: iteration 10, average log likelihood -1.104155
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      2
│      7
│      8
│     11
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 11, average log likelihood -1.062621
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      6
│     10
│     22
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 12, average log likelihood -1.074043
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      4
│      5
│     12
│     15
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 13, average log likelihood -1.087214
[ Info: iteration 14, average log likelihood -1.106648
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      7
│      8
│     11
│     18
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 15, average log likelihood -1.052653
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      2
│     10
│     22
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 16, average log likelihood -1.074922
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      4
│      5
│     12
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 17, average log likelihood -1.082144
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      6
│     15
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 18, average log likelihood -1.086335
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      7
│     11
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 19, average log likelihood -1.074549
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      2
│      8
│     10
│     18
│     22
│     26
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 20, average log likelihood -1.041318
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      4
│      5
│     12
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 21, average log likelihood -1.099846
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     15
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 22, average log likelihood -1.097265
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      6
│      7
│     11
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 23, average log likelihood -1.062013
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      2
│      8
│     10
│     18
│     22
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 24, average log likelihood -1.055377
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      4
│      5
│     12
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 25, average log likelihood -1.097258
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     15
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 26, average log likelihood -1.098752
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      6
│      7
│     11
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 27, average log likelihood -1.064294
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      2
│      8
│     10
│     18
│     22
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 28, average log likelihood -1.055153
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      4
│      5
│     12
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 29, average log likelihood -1.096504
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     15
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 30, average log likelihood -1.094950
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      7
│     11
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 31, average log likelihood -1.070976
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      2
│      6
│      8
│     10
│     18
│     22
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 32, average log likelihood -1.038433
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      4
│      5
│     12
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 33, average log likelihood -1.100743
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     15
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 34, average log likelihood -1.103069
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      7
│     11
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 35, average log likelihood -1.069038
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      2
│      8
│     10
│     18
│     22
│     26
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 36, average log likelihood -1.039826
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      4
│      5
│      6
│     12
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 37, average log likelihood -1.085183
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     15
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 38, average log likelihood -1.107589
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      7
│     11
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 39, average log likelihood -1.074462
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      2
│      8
│     10
│     18
│     22
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 40, average log likelihood -1.045223
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      4
│      5
│      6
│     12
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 41, average log likelihood -1.085468
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     15
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 42, average log likelihood -1.107029
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      7
│     11
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 43, average log likelihood -1.074188
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      2
│      8
│     10
│     18
│     22
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 44, average log likelihood -1.045648
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      4
│      5
│      6
│     12
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 45, average log likelihood -1.084914
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     15
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 46, average log likelihood -1.103123
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      7
│     11
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 47, average log likelihood -1.062333
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      2
│      8
│     10
│     18
│     22
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 48, average log likelihood -1.038268
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      4
│      5
│      6
│     12
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 49, average log likelihood -1.081924
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     15
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 50, average log likelihood -1.097745
┌ Info: EM with 100000 data points 50 iterations avll -1.097745
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
  0.118031     0.108276    -0.177249     0.00194118    0.0112715    0.000125224  -0.0340863   -0.0579632  -0.183924      0.0650125   -0.0721261   -0.111473    -0.049582     0.0406483   -0.16513      0.160019     0.0942448    -0.180951     0.110665   -0.131319      0.0761981    0.0891753    0.079912     -0.0278303    0.0593509   0.10449
 -0.0403102   -0.0487902    0.0488389    0.000221165  -0.078185    -0.151381     -0.069587    -0.0823714  -0.0315202    -0.044979     0.0167126    0.0259747    0.0724846    0.0852563   -0.0911757    0.0190768    0.107048      0.164876    -0.0523144   0.0143665     0.0248832   -0.035539    -0.0911724    -0.0340621    0.0346602  -0.0454369
 -0.0974325   -0.120107    -0.0507058    0.171702     -0.0878551   -0.189008     -0.178837    -0.0252982  -0.0741415     0.0538256    0.0888716    0.0401335    0.0261438   -0.208328     0.0577383    0.0653714    0.0355047    -0.0603522   -0.0174835  -0.126007     -0.159019     0.0596449   -0.202593     -0.0597067    0.0785251   0.0750281
 -0.0388432   -0.0897822   -0.0554616    0.172261      0.310692    -0.0233414     0.0382075    0.0232836  -0.132779     -0.0926934    0.0783993   -0.0881916   -0.118801     0.0132298    0.283996     0.142524    -0.0102865    -0.0787632    0.125683    0.0999331     0.0227476   -0.0112241    0.072279      0.183089    -0.0190179   0.118703
 -0.0624455   -0.172576    -0.113892     0.0719511    -0.0360848   -0.0719153    -0.127383     0.102003    0.0259333     0.031692     0.0230233   -0.0925212   -0.0867463   -0.0485993    0.0866371    0.00861862   0.0687592     0.0113341    0.122402   -0.162254      0.0263679    0.0968331   -0.103464     -0.077689    -0.0385457   0.0546773
 -0.00484336  -0.110355     0.0435222   -0.0113929    -0.079262     0.229997      0.122935    -0.133874   -0.0530388    -0.0383332   -0.0844269    0.131505    -0.117111     0.32452      0.00866985   0.0274578    0.11733       0.00148553   0.0583119   0.0587646     0.207197    -0.0761755   -0.0526657     0.103906    -0.0766554  -0.0827623
 -0.0860884   -0.00869994  -0.109477    -0.192622     -0.0366927    0.0741786     0.00895386  -0.0365696  -0.0117435    -0.0721231    0.00687113   0.0747421    0.0184871   -0.0943026    0.00890084  -0.0394291    0.120965      0.100963     0.0412581  -0.124545      0.267544    -0.09461      0.16041      -0.12795     -0.144616   -0.0472679
 -0.059043     0.0791747    0.0590718    0.0347454     0.111029     0.151915     -0.0549179    0.0786305   0.00795533   -0.0905843    0.0719499    0.177988    -0.0493637    0.14012     -0.00372567   0.0205858   -0.108699      0.00746044   0.136656   -0.108017     -0.0629871    0.0687912   -0.0882319     0.0875585   -0.0762961  -0.15381
  0.14982      0.183161    -0.0673431    0.0798957     0.00971546  -0.105977      0.125637    -0.0327646   0.0530207    -0.0761729   -0.00348406  -0.0483119    0.00325985  -0.168726    -0.0190619   -0.00539333  -0.0992339    -0.00739079   0.0645004  -0.113602     -0.160026    -0.0363274    0.0558878    -0.0239464   -0.0594514  -0.212478
 -0.0443436   -0.0477096   -0.0827964   -0.037064      0.00271015  -0.065096      0.138814     0.105911    0.107107     -0.0787755    0.0710209    0.0457807    0.0214424   -0.253095    -0.114504     0.111239     0.0230459    -0.104342     0.0242667   0.0537381    -0.245051    -0.138404    -0.026685      0.197861    -0.110267   -0.0637244
  0.0434895   -0.123686     0.036429    -0.115042     -0.0786011    0.0268585    -0.0939074    0.0169369  -0.0407892    -0.170294     0.0585829   -0.0271151    0.0877309   -0.0567567   -0.113231    -0.245336     0.0254483     0.113142    -0.211856    0.00135727    0.176047     0.0637477   -0.165402     -0.0963937   -0.0453522   0.103888
  0.0832846   -0.0601291   -0.0282554    0.0585113    -0.0158263    0.121578      0.324288     0.222281    0.0326463    -0.0538241   -0.00362688  -0.0263764   -0.113408    -0.0201818    0.0385303   -0.00592867  -0.100586     -0.123099     0.122017   -0.169855      0.137841    -0.0901233   -0.00269769   -0.187329    -0.0731774  -0.0141976
 -0.012115     0.0333719   -0.0254802    0.034984      0.0176023    0.232083     -0.101017     0.174032    0.0829528    -0.0837799    0.0407593   -0.0821966   -0.0174319    0.10618      0.111899     0.0527383   -0.053389      0.020136     0.0336109   0.000594564  -0.0106699    0.0199245   -0.00330119    0.0127843   -0.150466   -0.153016
  0.0398692   -0.015153    -0.0958136   -0.077313     -0.183335     0.023072     -0.0960558   -0.0860846  -0.143642      0.0360533   -0.0333296   -0.153859    -0.0274083    0.0558321   -0.0649325   -0.015682    -0.0544742     0.208141     0.131777    0.00870632   -0.0732962    0.0100052    0.105117      0.156693     0.0195421   0.0329144
 -0.040574    -0.0360059    0.034546    -0.182704     -0.102643    -0.0245326     0.0298539    0.068863    0.102132     -0.0378299   -0.00687015  -0.0263284    0.0157255   -0.0748031   -0.00730303   0.0181965    0.192027      0.0800111    0.0418839   0.128568     -0.178511    -0.147068    -0.0617592    -0.016214    -0.0123321   0.0400619
  0.0789174   -0.0581056    0.0600323    0.0434655    -0.0237493    0.115641      0.0231289   -0.0113175  -0.0349705    -0.0255711    0.0254194   -0.00389762   0.0293648   -0.101508    -0.145895    -0.0686744   -0.000606503   0.0181519   -0.0310397  -0.0480125     0.0117237   -0.0558166    0.0398657     0.0691492    0.0570571  -0.0576927
 -0.144579     0.0545929   -0.0445279    0.141409      0.0342242   -0.0228058     0.0177479    0.253527    0.0270867     0.0277543    0.136486    -0.0267241   -0.0168416   -0.150864    -0.0727504    0.00106918  -0.0430829     0.0250187   -0.0234137   0.1888        0.0422643    0.0359693   -0.11082      -0.0360907    0.0504162  -0.0883096
 -0.024748    -0.102873    -0.0550134    0.057486     -0.084247     0.172821     -0.108111    -0.177644   -0.210873      0.0921708    0.0681255   -0.0118461    0.121237    -0.116678    -0.0650024    0.038619    -0.140474     -0.00273684   0.170741    0.0233113    -0.217225    -0.231713    -0.101627     -0.0677365    0.0286714   0.23393
 -0.035316    -0.0754273    0.158579    -0.085749     -0.0093723    0.000384708  -0.00740741   0.0259164  -0.0848107    -0.0366541   -0.0448429    0.0249504    0.332747     0.0211116   -0.204182     0.0158137   -0.20453      -0.0398373    0.20788     0.0191516     0.0174245   -0.00223215   0.0473004     0.0416821    0.0712778   0.127206
 -0.0333013   -0.0661315    0.00702427  -0.139751      0.188215    -0.107074      0.00154623  -0.339325    0.143109     -0.0655868    0.0321909    0.0367165   -0.192561     0.0632656   -0.0142992   -0.00141638   0.0617619     0.0317229    0.0946025  -0.00361337   -0.161329    -0.00290908   0.156811      0.206727    -0.236317    0.106511
  0.158542     0.0912162   -0.195838     0.0322673    -0.198452    -0.0453932    -0.0248755    0.0417551  -0.13274       0.100782    -0.089646    -0.12321     -0.106207     0.0516057   -0.179526     0.173152     0.1726        0.130446     0.204224   -0.15922       0.0875488    0.131191     0.024808     -0.0620049    0.0507054   0.218033
  0.00400614  -0.103275     0.148068     0.0560015     0.143528    -0.0190796    -0.0129079   -0.0365145   0.0106741     0.0105587   -0.0767862   -0.0290334   -0.0821407    0.0474072    0.022455    -0.0348089   -0.0460275    -0.0374053   -0.0171689   0.0570312    -0.0566769    0.114597     0.0511626     0.0511055   -0.0547654  -0.0597115
 -0.0334101    0.0616213   -0.0619127    0.0486445     0.0547316    0.0825127    -0.14155      0.180804   -0.118279      0.16376      0.0528307    0.0228135   -0.0666599    0.131222     0.200767     0.00327867   0.00366562   -0.00161517  -0.0436807   0.0402904     0.0162652   -0.037875    -0.0565462     0.043029    -0.180849   -0.0560138
 -0.0721805    0.188836     0.0593637    0.111839      0.152666    -0.152644      0.221783    -0.082833    0.0590803     0.0266737    0.0299555   -0.111057     0.00172605  -0.176685     0.124095     0.105153    -0.178249      0.133957     0.0749216  -0.00901493    0.00129825   0.111547     0.000389467   0.11913      0.0963116  -0.0075695
  0.0944769   -0.0323872   -0.0625269    0.0344555    -0.0753612    0.130033      0.105073    -0.014719   -0.16127       0.00177195   0.0142839    0.02223     -0.0853011    0.031338    -0.0427283   -0.0919634    0.0275477    -0.0329634   -0.0507176   0.0251763    -0.0725804   -0.138281    -0.0796684     0.0791337   -0.108701   -0.124024
 -0.0959737   -0.0926521    0.109333     0.0535736     0.0582173   -0.131606      0.0482647   -0.0714958   0.0113442    -0.0364181    0.0513559    0.111787    -0.0505116    0.00846964   0.00513219  -0.0263555    0.0324805    -0.00193486  -0.0442096   0.0408418    -0.148115     0.184247    -0.0312686    -0.117535    -0.102287   -0.0455716
  0.0121722   -0.0825874   -0.125658    -0.0054931    -0.172163    -0.0281968     0.154157     0.113872   -0.0896373     0.0316722    0.00525713   0.0315235    0.16568      0.0493933   -0.0553311    0.0489132   -0.232321      0.162239     0.0458435  -0.056367     -0.0461379    0.00665806  -0.0628488    -0.00343287  -0.0496354  -0.0629021
  0.0644238   -0.12095      0.0552667    0.0318908     0.0645726    0.089809     -0.00505407  -0.064369   -0.000920891   0.0336209   -0.126715    -0.0178837   -0.205534     0.0792261    0.0117133   -0.00833247  -0.0278371    -0.049921     0.0212666   0.112134     -0.0023212    0.0496257    0.0228034     0.390943    -0.0332455  -0.0714293
  0.181562     0.012       -0.0926691    0.055025      0.120997    -0.145041     -0.0885615   -0.0155994   0.0224622     0.0858475   -0.174735    -0.148111     0.0481976    0.179251     0.0720699   -0.0376817    0.00831703   -0.0517506    0.0474393  -0.0354449     0.0137688   -0.0665145    0.0468812    -0.195138     0.0187226   0.161275
 -0.0104958   -0.138583     0.0960206   -0.0316254     0.0235129   -0.0569419    -0.227164     0.057383    0.0112557     0.111613     0.232428     0.121578    -0.092877    -0.0576965   -0.0262455    0.0965957    0.07055       0.0755059    0.0377555  -0.120662      0.205125     0.0373501    0.137808      0.148371     0.111983    0.0511749
 -0.142208     0.149449    -0.196792    -0.0326029    -0.108378    -0.00358415   -0.0453648   -0.0242517   0.0815652    -0.0632514    0.0620979   -0.0832958    0.0108048   -0.0767109   -0.128613     0.17187      0.130552      0.116159     0.177104    0.0892021    -0.108809    -0.147581     0.0697459    -0.144628    -0.0561716  -0.0066485
  0.024444    -0.0374759    0.0518529    0.0840009     0.177561     0.147378     -0.0347148   -0.190006    0.0328752     0.0265958    0.193862     0.101631    -0.125289     0.0751766    0.143938    -0.0425341    0.0893947    -0.125071     0.0706262   0.00790477   -0.0231574   -0.0181694    0.211121     -0.0755035    0.0228115   0.0323927[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      7
│     11
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 1, average log likelihood -1.077811
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      2
│      4
│      7
│      8
│     10
│     11
│     18
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 2, average log likelihood -1.023602
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      5
│      6
│      7
│     11
│     12
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 3, average log likelihood -1.027723
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      2
│      7
│      8
│     10
│     11
│     15
│     18
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.035070
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      4
│      6
│      7
│     11
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.035818
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      2
│      5
│      7
│      8
│      ⋮
│     18
│     22
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.018571
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      7
│     11
│     15
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.046038
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      2
│      4
│      6
│      7
│      ⋮
│     11
│     18
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.006659
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      5
│      7
│     11
│     12
│     16
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.032880
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      2
│      7
│      8
│     10
│     11
│     15
│     18
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.023613
┌ Info: EM with 100000 data points 10 iterations avll -1.023613
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
  0.0187929    0.0492313   -0.145359     0.0805554    0.0264092  -0.0757668   -0.00199619   0.0102747     0.0340627   -0.0127406  -0.0298173   -0.169367    -0.0648296   -0.0425908   -0.127823     0.202606     0.106522    -0.14593      0.148147     -0.0200066   -0.0226059   0.0185523    0.18086     -0.0654704    0.136324    -0.11783
  0.199014     0.0575391   -0.0354592   -0.123303     0.122046    0.105008    -0.0767658   -0.0887924     0.0472182    0.0778378   0.0565571   -0.0498986   -0.027055     0.0203162   -0.0615105    0.0702954   -0.042732    -0.0639379    0.0420057     0.00606468  -0.0349139  -0.0787182   -0.103948     0.077576     0.0417791   -0.129526
 -0.0538895   -0.158938     0.108766     0.00975541   0.127873   -0.0103676   -0.0230196   -0.0240496    -0.00596089  -0.137422   -0.0768406    0.042533    -0.024001     0.162364    -0.0188573    0.114401    -0.00352844   0.0273828    0.000714065   0.100615     0.0329259   0.0989052    0.0300233    0.189426     0.0799837    0.0419995
  0.133064     0.0380422    0.0241802    0.0365532   -0.0824261   0.0455099    0.139917    -0.10937      -0.0936368    0.0896103  -0.062168     0.116289    -0.0586347    0.225572    -0.0536467    0.071822    -0.0817538   -0.0322285    0.0190935    -0.0299401   -0.0141447   0.0403216    0.112103    -0.0515027    0.0449837    0.163974
 -0.104131    -0.0771371   -0.00413622  -0.123046     0.0926515  -0.0803948   -0.197237     0.0391328     0.03022     -0.120416    0.133746    -0.0484445   -0.00714187  -0.0633063    0.205895     0.0079361    0.112169    -0.15606      0.0362659    -0.0650331    0.0406075  -0.0153964    0.0407995   -0.0747372   -0.103563    -0.104455
 -0.125176    -0.118834     0.0242989    0.0468447    0.0586925   0.0307272    0.0581303    0.0853855    -0.155269     0.0269059  -0.119565     0.0145873   -0.0872981   -0.0624316    0.0626078    0.123311    -0.316289    -0.0118948   -0.048639      0.0756325    0.120836   -0.043461     0.0936839   -0.00136229  -0.024507    -0.0493489
  0.0373583   -0.186061    -0.101046    -0.0408732    0.178059   -0.146584    -0.158202    -0.101329      0.105372     0.044166   -0.0255148   -0.0179497    0.126624     0.039423    -0.00456809  -0.0392435    0.0902152   -0.154021     0.00545283    0.0446075   -0.090286    0.05589     -0.10694     -0.09987     -0.181748     0.0991579
  0.0437334    0.00446813  -0.0231716    0.0489925   -0.0596627   0.0943445    0.0391609    0.0167047    -0.0609471    0.185837   -0.0471219   -0.210942    -0.0150854    0.0521783   -0.113218     0.00151336   0.0466302    0.0104674   -0.0381758     0.0285552    0.0720409   0.0115789   -0.0283865    0.0906874    0.0481527   -0.00829024
 -0.147695     0.134337    -0.266199    -0.108866    -0.0965346  -0.0760756   -0.106132     0.147123     -0.130973     0.0541152  -0.0593694   -0.135845    -0.0733524   -0.0188142   -0.0393883   -0.00411408  -0.0446988   -0.0368552   -0.183058     -0.179942    -0.0412148  -0.223342    -0.0780894    0.0131325   -0.0909033    0.0902327
 -0.116862     0.034486    -0.0680459   -0.0282756   -0.0111826  -0.119828    -0.0126728    0.0137369    -0.0617617   -0.108188   -0.0581839   -0.0709614    0.137878     0.00788545   0.101926     0.122039     0.0139012   -0.0109683    0.0498416    -0.0326982    0.320655   -0.0929975    0.0387466    0.0773078   -0.0737314    0.0731999
 -0.0372801   -0.0669532   -0.0774255   -0.131229     0.038343    0.0607997   -0.104416     0.121031     -0.0757518    0.106053   -0.00332865   0.20288      0.0987996    0.0384024   -0.139334     0.221655     0.0474149   -0.0761813   -0.113514     -0.0484016    0.0735299  -0.0687032    0.139319    -0.141091    -0.0610781   -0.0200071
 -0.118584    -0.158564    -0.0206727   -0.0677543    0.0908193  -0.00668024   0.0714226   -0.0125935    -0.132007     0.0314547   0.0285068    0.107234     0.126561     0.0961485    0.0689133   -0.12584      0.0552557    0.146638    -0.0874673     0.026849     0.155751   -0.0223546    0.101774    -0.104386     0.0193154   -0.00798569
  0.139018     0.14913      0.161194     0.04254      0.0415599   0.0659258   -0.0269478   -0.0642112     0.062156    -0.0623461  -0.0296413   -0.0047644    0.0127841   -0.113602    -0.0685362    0.00630751   0.0781412    0.0779612    0.0687945     0.0553211    0.187659   -0.20744      0.0330858   -0.0100687    0.0118722   -0.046811
  0.0427705   -0.0159536    0.0420275   -0.0606662   -0.0603664   0.0815016    0.0174819   -0.181698      0.139462     0.114585    0.155359     0.103336     0.133087     0.0921537    0.0924438   -0.101123     0.106854     0.056411    -0.0220113    -0.0955003    0.154043    0.0269734    0.0513082    0.296134    -0.126856    -0.0798184
  0.240508    -0.0222687    0.0069993   -0.0489708   -0.0605976   0.120283    -0.223807    -0.0608134    -0.0255481    0.0529765   0.123295     0.055052    -0.121459    -0.0300272   -0.113687    -0.0254003    0.160374    -0.0802625    0.0994726    -0.083474     0.155931   -0.0206374    0.0229499    0.267006    -0.0344405   -0.0981308
  0.16463      0.0459824   -0.0472391    0.0971915    0.222736   -0.110165     0.0676549   -0.000742404   0.0586612    0.0305773   0.035466    -0.0117014    0.025078     0.0500644    0.135312    -0.0931568    0.0213747   -0.0619156    0.0643393     0.00187062   0.0279938   0.0886454   -0.1799       0.0787581   -0.17035     -0.115194
  0.0389109    0.100975    -0.110646    -0.220255     0.179289    0.0309009    0.00793094   0.0961242     0.0727924   -0.223644    0.0176256   -0.205177     0.0534795    0.0425649    0.0455638   -0.117151    -0.127491     0.09692     -0.0256323     0.0331624    0.0865459  -0.0362197   -0.0797638    0.176335    -0.0714709    0.0222106
 -0.04737     -0.0807938   -0.0251873   -0.128615    -0.0262358  -0.0326905   -0.117156    -0.0505606    -0.245416    -0.0232693  -0.0734062   -0.018396     0.108498    -0.145481     0.0289065   -0.0390161   -0.00267614  -0.134849    -0.0377664     0.129094     0.086292   -0.0668569   -0.0723705    0.142583     0.166245     0.072951
 -0.00430542   0.0473383   -0.157725     0.0383492   -0.0677691  -0.185892     0.0069487    0.0217032    -0.0821091    0.0370541  -0.0499181   -0.039161    -0.0272627   -0.0942872   -0.12682     -0.253219     0.00202774  -0.111629     0.0483904    -0.0196486   -0.0139737   0.225211    -0.138688     0.124645    -0.0338159   -0.0789971
 -0.152691    -0.0793292   -0.113636    -0.00098752  -0.105152    0.0264965   -0.00454657  -0.0272887    -0.14706      0.105206    0.00738196  -0.0988308    0.0977036    0.0745193   -0.0460461    0.0204221   -0.314446     0.0426165   -0.108338      0.113259     0.151548    0.0609041    0.121252    -0.0615457    0.095933    -0.0464752
  0.125603     0.139283    -0.063844     0.0321843   -0.0233612   0.156122    -0.0134788   -0.0129577     0.15282     -0.0687955   0.0027528    0.126401     0.103639     0.0501218   -0.127322    -0.0460103    0.0496299    0.0672545    0.0336259     0.010531     0.0746584   0.0373981    0.149263     0.240863    -0.0466008   -0.0384058
  0.054622     0.0228575    0.00848239   0.156558     0.0735739  -0.0965513   -0.0131312    0.129598      0.062671     0.0985837  -0.0133818    0.0965572   -0.130928     0.112949     0.0947014   -0.0387991    0.0605316    0.102883    -0.0800472     0.0927028   -0.0286091  -0.147569     0.0422504   -0.190794    -0.140455    -0.00406381
  0.103206    -0.0531741    0.16034     -0.00616823   0.0953272   0.00183587  -0.018316    -0.111318      0.155425     0.0175231   0.0229409    0.096475    -0.0476617    0.0324823   -0.149526    -0.233182    -0.121404    -0.0137872   -0.102693      0.0849011   -0.0629075  -0.248461    -0.0185715   -0.045091    -0.0883236   -0.194334
  0.0449553   -0.0319744    0.193529     0.0396027    0.115948    0.192233    -0.0937991   -0.110345      0.0439679   -0.0742806  -0.203221    -0.144587     0.0058738    0.081242    -0.0641676    0.143879     0.111071    -0.12965     -0.107517      0.15179     -0.0619214   9.39615e-5   0.0999353    0.11566     -0.184475     0.143915
 -0.00751173   0.0281071    0.0671235    0.124668    -0.158114    0.0318106   -0.0953424   -0.0826756    -0.0692826    0.122175   -0.180853     0.0892214   -0.146967     0.0763343   -0.0706869    0.00520392  -0.146937     0.0714211   -0.154339      0.0526561   -0.150665    0.00294211  -0.0699914    0.0541792    0.0540994    0.109325
  0.0189744    0.126015    -0.00546707   0.192126    -0.0614034   0.0277297    0.0293894   -0.0345213     0.00211674   0.066048   -0.0130081   -0.0989568    0.130779     0.0165965    0.0715295   -0.153482    -0.0309664    0.00984057  -0.0462667     0.0175141    0.0321569  -0.190807    -0.0237353   -0.0297319   -0.0577632   -0.0223834
 -0.214278    -0.202157     0.134484    -0.0312618   -0.141145    0.0548567    0.0528598    0.0766803    -0.0363761   -0.086805    0.029656    -0.0206068    0.052918     0.0873321    0.0154414   -0.054679     0.123622     0.279834     0.0197449    -0.0333933   -0.0687931   0.00721547   0.0730714    0.0792353    0.0883202   -0.0336038
  0.0201878    0.0577626   -0.0255735   -0.0678005   -0.212624   -0.0523608   -0.0977625   -0.111633     -0.0892807    0.130907   -0.0338151    0.0369588   -0.14286     -0.0924669    0.126024    -0.0357862   -0.0290095    0.0109069    0.122812      0.13632     -0.188571    0.0179692   -0.0983171    0.262991     0.00824712   0.0525244
  0.0571913   -0.0492238    0.0599807   -0.0601144   -0.128882    0.0868061   -0.00018072   0.0116749     0.0932033   -0.0818845  -0.0264312    0.00929742  -0.102915    -0.0185301   -0.0574992   -0.0838288    0.0271084    0.02127      0.0346576     0.00923146  -0.0396914  -0.0444721   -0.1305      -0.00277322  -0.0929642   -0.0725122
  0.236726     0.120525    -0.0323006    0.0472963   -0.232565   -0.0485043   -0.0237289   -0.0508438     0.032101     0.0663794   0.239002     0.0257373    0.112318     0.00347704   0.0996103   -0.0898279    0.0261411    0.0244076   -0.0422922     0.0561839    0.12242     0.0474143   -0.0361983    0.00730349   0.188785     0.204544
 -0.097096     0.0939068   -0.00838567  -0.0659319   -0.0960314  -0.0381213    0.153427     0.0388196     0.0979418    0.0510894   0.0129537    0.0355613    0.0964723   -0.125573     0.0965809    0.0680841   -0.0282522   -0.0133844   -0.112564      0.176733    -0.101434   -0.139759     0.00393279   0.093747    -0.050367    -0.0169801
 -0.066322    -0.177448    -0.0129779    0.147531    -0.0139831  -0.200458     0.0871291    0.0391536    -0.120018     0.0447987  -0.0399969    0.138877    -0.092907    -0.087175    -0.0812671    0.0975015    0.0817125    0.0356436   -0.187449      0.0497637   -0.102845    0.211323     0.14798     -0.0719452    0.0565067    0.0626041kind full, method split
┌ Info: 0: avll = 
└   tll[1] = -1.4234428541568418
[ Info: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.423461
[ Info: iteration 2, average log likelihood -1.423401
[ Info: iteration 3, average log likelihood -1.423353
[ Info: iteration 4, average log likelihood -1.423294
[ Info: iteration 5, average log likelihood -1.423217
[ Info: iteration 6, average log likelihood -1.423119
[ Info: iteration 7, average log likelihood -1.422989
[ Info: iteration 8, average log likelihood -1.422803
[ Info: iteration 9, average log likelihood -1.422504
[ Info: iteration 10, average log likelihood -1.421993
[ Info: iteration 11, average log likelihood -1.421190
[ Info: iteration 12, average log likelihood -1.420182
[ Info: iteration 13, average log likelihood -1.419268
[ Info: iteration 14, average log likelihood -1.418678
[ Info: iteration 15, average log likelihood -1.418382
[ Info: iteration 16, average log likelihood -1.418252
[ Info: iteration 17, average log likelihood -1.418196
[ Info: iteration 18, average log likelihood -1.418172
[ Info: iteration 19, average log likelihood -1.418162
[ Info: iteration 20, average log likelihood -1.418158
[ Info: iteration 21, average log likelihood -1.418155
[ Info: iteration 22, average log likelihood -1.418154
[ Info: iteration 23, average log likelihood -1.418154
[ Info: iteration 24, average log likelihood -1.418154
[ Info: iteration 25, average log likelihood -1.418153
[ Info: iteration 26, average log likelihood -1.418153
[ Info: iteration 27, average log likelihood -1.418153
[ Info: iteration 28, average log likelihood -1.418153
[ Info: iteration 29, average log likelihood -1.418153
[ Info: iteration 30, average log likelihood -1.418153
[ Info: iteration 31, average log likelihood -1.418153
[ Info: iteration 32, average log likelihood -1.418153
[ Info: iteration 33, average log likelihood -1.418153
[ Info: iteration 34, average log likelihood -1.418153
[ Info: iteration 35, average log likelihood -1.418152
[ Info: iteration 36, average log likelihood -1.418152
[ Info: iteration 37, average log likelihood -1.418152
[ Info: iteration 38, average log likelihood -1.418152
[ Info: iteration 39, average log likelihood -1.418152
[ Info: iteration 40, average log likelihood -1.418152
[ Info: iteration 41, average log likelihood -1.418152
[ Info: iteration 42, average log likelihood -1.418152
[ Info: iteration 43, average log likelihood -1.418152
[ Info: iteration 44, average log likelihood -1.418152
[ Info: iteration 45, average log likelihood -1.418152
[ Info: iteration 46, average log likelihood -1.418152
[ Info: iteration 47, average log likelihood -1.418152
[ Info: iteration 48, average log likelihood -1.418152
[ Info: iteration 49, average log likelihood -1.418152
[ Info: iteration 50, average log likelihood -1.418152
┌ Info: EM with 100000 data points 50 iterations avll -1.418152
└ 952.4 data points per parameter
┌ Info: 1
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4234610519053985
│     -1.4234006608736942
│      ⋮
└     -1.4181520562403176
[ Info: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.418167
[ Info: iteration 2, average log likelihood -1.418101
[ Info: iteration 3, average log likelihood -1.418044
[ Info: iteration 4, average log likelihood -1.417974
[ Info: iteration 5, average log likelihood -1.417885
[ Info: iteration 6, average log likelihood -1.417776
[ Info: iteration 7, average log likelihood -1.417656
[ Info: iteration 8, average log likelihood -1.417536
[ Info: iteration 9, average log likelihood -1.417428
[ Info: iteration 10, average log likelihood -1.417340
[ Info: iteration 11, average log likelihood -1.417271
[ Info: iteration 12, average log likelihood -1.417218
[ Info: iteration 13, average log likelihood -1.417177
[ Info: iteration 14, average log likelihood -1.417144
[ Info: iteration 15, average log likelihood -1.417118
[ Info: iteration 16, average log likelihood -1.417096
[ Info: iteration 17, average log likelihood -1.417077
[ Info: iteration 18, average log likelihood -1.417061
[ Info: iteration 19, average log likelihood -1.417046
[ Info: iteration 20, average log likelihood -1.417034
[ Info: iteration 21, average log likelihood -1.417022
[ Info: iteration 22, average log likelihood -1.417012
[ Info: iteration 23, average log likelihood -1.417004
[ Info: iteration 24, average log likelihood -1.416996
[ Info: iteration 25, average log likelihood -1.416988
[ Info: iteration 26, average log likelihood -1.416982
[ Info: iteration 27, average log likelihood -1.416976
[ Info: iteration 28, average log likelihood -1.416971
[ Info: iteration 29, average log likelihood -1.416966
[ Info: iteration 30, average log likelihood -1.416962
[ Info: iteration 31, average log likelihood -1.416958
[ Info: iteration 32, average log likelihood -1.416954
[ Info: iteration 33, average log likelihood -1.416951
[ Info: iteration 34, average log likelihood -1.416948
[ Info: iteration 35, average log likelihood -1.416945
[ Info: iteration 36, average log likelihood -1.416942
[ Info: iteration 37, average log likelihood -1.416940
[ Info: iteration 38, average log likelihood -1.416937
[ Info: iteration 39, average log likelihood -1.416935
[ Info: iteration 40, average log likelihood -1.416933
[ Info: iteration 41, average log likelihood -1.416931
[ Info: iteration 42, average log likelihood -1.416929
[ Info: iteration 43, average log likelihood -1.416927
[ Info: iteration 44, average log likelihood -1.416926
[ Info: iteration 45, average log likelihood -1.416924
[ Info: iteration 46, average log likelihood -1.416922
[ Info: iteration 47, average log likelihood -1.416921
[ Info: iteration 48, average log likelihood -1.416920
[ Info: iteration 49, average log likelihood -1.416918
[ Info: iteration 50, average log likelihood -1.416917
┌ Info: EM with 100000 data points 50 iterations avll -1.416917
└ 473.9 data points per parameter
┌ Info: 2
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.418166747006478
│     -1.418101181986424
│      ⋮
└     -1.4169172425921288
[ Info: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.416929
[ Info: iteration 2, average log likelihood -1.416879
[ Info: iteration 3, average log likelihood -1.416839
[ Info: iteration 4, average log likelihood -1.416792
[ Info: iteration 5, average log likelihood -1.416734
[ Info: iteration 6, average log likelihood -1.416662
[ Info: iteration 7, average log likelihood -1.416579
[ Info: iteration 8, average log likelihood -1.416489
[ Info: iteration 9, average log likelihood -1.416397
[ Info: iteration 10, average log likelihood -1.416310
[ Info: iteration 11, average log likelihood -1.416230
[ Info: iteration 12, average log likelihood -1.416158
[ Info: iteration 13, average log likelihood -1.416094
[ Info: iteration 14, average log likelihood -1.416037
[ Info: iteration 15, average log likelihood -1.415987
[ Info: iteration 16, average log likelihood -1.415942
[ Info: iteration 17, average log likelihood -1.415903
[ Info: iteration 18, average log likelihood -1.415869
[ Info: iteration 19, average log likelihood -1.415839
[ Info: iteration 20, average log likelihood -1.415812
[ Info: iteration 21, average log likelihood -1.415787
[ Info: iteration 22, average log likelihood -1.415765
[ Info: iteration 23, average log likelihood -1.415744
[ Info: iteration 24, average log likelihood -1.415724
[ Info: iteration 25, average log likelihood -1.415705
[ Info: iteration 26, average log likelihood -1.415688
[ Info: iteration 27, average log likelihood -1.415671
[ Info: iteration 28, average log likelihood -1.415655
[ Info: iteration 29, average log likelihood -1.415639
[ Info: iteration 30, average log likelihood -1.415624
[ Info: iteration 31, average log likelihood -1.415610
[ Info: iteration 32, average log likelihood -1.415597
[ Info: iteration 33, average log likelihood -1.415584
[ Info: iteration 34, average log likelihood -1.415571
[ Info: iteration 35, average log likelihood -1.415560
[ Info: iteration 36, average log likelihood -1.415549
[ Info: iteration 37, average log likelihood -1.415538
[ Info: iteration 38, average log likelihood -1.415529
[ Info: iteration 39, average log likelihood -1.415519
[ Info: iteration 40, average log likelihood -1.415511
[ Info: iteration 41, average log likelihood -1.415503
[ Info: iteration 42, average log likelihood -1.415495
[ Info: iteration 43, average log likelihood -1.415488
[ Info: iteration 44, average log likelihood -1.415481
[ Info: iteration 45, average log likelihood -1.415474
[ Info: iteration 46, average log likelihood -1.415468
[ Info: iteration 47, average log likelihood -1.415462
[ Info: iteration 48, average log likelihood -1.415456
[ Info: iteration 49, average log likelihood -1.415451
[ Info: iteration 50, average log likelihood -1.415446
┌ Info: EM with 100000 data points 50 iterations avll -1.415446
└ 236.4 data points per parameter
┌ Info: 3
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4169286361414968
│     -1.4168792533778778
│      ⋮
└     -1.415445618864175
[ Info: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.415450
[ Info: iteration 2, average log likelihood -1.415387
[ Info: iteration 3, average log likelihood -1.415330
[ Info: iteration 4, average log likelihood -1.415264
[ Info: iteration 5, average log likelihood -1.415182
[ Info: iteration 6, average log likelihood -1.415080
[ Info: iteration 7, average log likelihood -1.414960
[ Info: iteration 8, average log likelihood -1.414826
[ Info: iteration 9, average log likelihood -1.414685
[ Info: iteration 10, average log likelihood -1.414547
[ Info: iteration 11, average log likelihood -1.414418
[ Info: iteration 12, average log likelihood -1.414301
[ Info: iteration 13, average log likelihood -1.414196
[ Info: iteration 14, average log likelihood -1.414105
[ Info: iteration 15, average log likelihood -1.414025
[ Info: iteration 16, average log likelihood -1.413956
[ Info: iteration 17, average log likelihood -1.413896
[ Info: iteration 18, average log likelihood -1.413845
[ Info: iteration 19, average log likelihood -1.413801
[ Info: iteration 20, average log likelihood -1.413764
[ Info: iteration 21, average log likelihood -1.413731
[ Info: iteration 22, average log likelihood -1.413701
[ Info: iteration 23, average log likelihood -1.413676
[ Info: iteration 24, average log likelihood -1.413652
[ Info: iteration 25, average log likelihood -1.413631
[ Info: iteration 26, average log likelihood -1.413611
[ Info: iteration 27, average log likelihood -1.413593
[ Info: iteration 28, average log likelihood -1.413576
[ Info: iteration 29, average log likelihood -1.413560
[ Info: iteration 30, average log likelihood -1.413545
[ Info: iteration 31, average log likelihood -1.413531
[ Info: iteration 32, average log likelihood -1.413517
[ Info: iteration 33, average log likelihood -1.413503
[ Info: iteration 34, average log likelihood -1.413491
[ Info: iteration 35, average log likelihood -1.413478
[ Info: iteration 36, average log likelihood -1.413466
[ Info: iteration 37, average log likelihood -1.413454
[ Info: iteration 38, average log likelihood -1.413443
[ Info: iteration 39, average log likelihood -1.413432
[ Info: iteration 40, average log likelihood -1.413421
[ Info: iteration 41, average log likelihood -1.413410
[ Info: iteration 42, average log likelihood -1.413400
[ Info: iteration 43, average log likelihood -1.413389
[ Info: iteration 44, average log likelihood -1.413379
[ Info: iteration 45, average log likelihood -1.413370
[ Info: iteration 46, average log likelihood -1.413360
[ Info: iteration 47, average log likelihood -1.413350
[ Info: iteration 48, average log likelihood -1.413341
[ Info: iteration 49, average log likelihood -1.413332
[ Info: iteration 50, average log likelihood -1.413323
┌ Info: EM with 100000 data points 50 iterations avll -1.413323
└ 118.1 data points per parameter
┌ Info: 4
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4154497907881982
│     -1.4153874482826416
│      ⋮
└     -1.4133231235404806
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.413325
[ Info: iteration 2, average log likelihood -1.413243
[ Info: iteration 3, average log likelihood -1.413161
[ Info: iteration 4, average log likelihood -1.413063
[ Info: iteration 5, average log likelihood -1.412940
[ Info: iteration 6, average log likelihood -1.412792
[ Info: iteration 7, average log likelihood -1.412625
[ Info: iteration 8, average log likelihood -1.412448
[ Info: iteration 9, average log likelihood -1.412269
[ Info: iteration 10, average log likelihood -1.412096
[ Info: iteration 11, average log likelihood -1.411933
[ Info: iteration 12, average log likelihood -1.411785
[ Info: iteration 13, average log likelihood -1.411652
[ Info: iteration 14, average log likelihood -1.411534
[ Info: iteration 15, average log likelihood -1.411429
[ Info: iteration 16, average log likelihood -1.411336
[ Info: iteration 17, average log likelihood -1.411254
[ Info: iteration 18, average log likelihood -1.411181
[ Info: iteration 19, average log likelihood -1.411116
[ Info: iteration 20, average log likelihood -1.411057
[ Info: iteration 21, average log likelihood -1.411005
[ Info: iteration 22, average log likelihood -1.410957
[ Info: iteration 23, average log likelihood -1.410914
[ Info: iteration 24, average log likelihood -1.410874
[ Info: iteration 25, average log likelihood -1.410837
[ Info: iteration 26, average log likelihood -1.410803
[ Info: iteration 27, average log likelihood -1.410771
[ Info: iteration 28, average log likelihood -1.410741
[ Info: iteration 29, average log likelihood -1.410713
[ Info: iteration 30, average log likelihood -1.410686
[ Info: iteration 31, average log likelihood -1.410661
[ Info: iteration 32, average log likelihood -1.410637
[ Info: iteration 33, average log likelihood -1.410614
[ Info: iteration 34, average log likelihood -1.410592
[ Info: iteration 35, average log likelihood -1.410571
[ Info: iteration 36, average log likelihood -1.410551
[ Info: iteration 37, average log likelihood -1.410531
[ Info: iteration 38, average log likelihood -1.410513
[ Info: iteration 39, average log likelihood -1.410495
[ Info: iteration 40, average log likelihood -1.410477
[ Info: iteration 41, average log likelihood -1.410460
[ Info: iteration 42, average log likelihood -1.410444
[ Info: iteration 43, average log likelihood -1.410428
[ Info: iteration 44, average log likelihood -1.410413
[ Info: iteration 45, average log likelihood -1.410398
[ Info: iteration 46, average log likelihood -1.410384
[ Info: iteration 47, average log likelihood -1.410370
[ Info: iteration 48, average log likelihood -1.410356
[ Info: iteration 49, average log likelihood -1.410343
[ Info: iteration 50, average log likelihood -1.410330
┌ Info: EM with 100000 data points 50 iterations avll -1.410330
└ 59.0 data points per parameter
┌ Info: 5
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4133251661801327
│     -1.4132426817908479
│      ⋮
└     -1.4103298422126356
┌ Info: Total log likelihood: 
│   tll =
│    251-element Array{Float64,1}:
│     -1.4234428541568418
│     -1.4234610519053985
│     -1.4234006608736942
│     -1.4233529534822862
│      ⋮
│     -1.4103561079153204
│     -1.410342808055893
└     -1.4103298422126356
32×26 Array{Float64,2}:
 -0.112564     0.108112      0.486802   -0.0233327   0.20684    -0.020812   -0.736375   -0.428257     0.0944961   -0.283414    -0.186991     0.000826052   0.351535    0.19304      0.229952    0.28876     0.0226191   0.154364   -0.171232    0.147355   -0.228641    -0.811794   -0.102289     0.502239      0.0196414   0.0837404
  0.346116    -0.660698      0.488495    0.328585   -0.243727   -0.0230384  -0.476031    0.00705928  -0.0880448    0.58736     -0.0943538    0.063825      0.479303   -0.327556    -0.204218    0.121752   -0.479078    0.324061    0.246606    0.423111    0.577819    -0.228754    0.461673     0.3946       -0.415825    0.123964
  0.71516     -0.309359      0.490386   -0.42355     0.041828    0.343056   -0.330074   -0.112296    -0.300582     0.0278778   -0.0598163    0.48565       0.01268     0.543849    -0.619668   -0.324176    0.0884444  -0.270075    0.383065   -0.250156   -0.125069    -0.478374   -0.68316     -0.016381     -0.137514    0.279389
 -0.0103441   -0.30488      -0.131029   -0.466899    0.459111    0.62361    -0.34434    -0.0147066   -0.436687     0.431548     0.58917      0.491582      1.17956     0.397101    -0.401236    0.103231    0.579415    0.275215    0.087035   -0.127163    0.743004    -0.218594    0.0974347   -0.48892      -0.441779    0.465145
 -0.755891    -0.458739      0.616862   -0.0385344  -0.4515     -0.208038   -0.191006    0.35281      0.133733    -0.194172     0.0859535    0.332312      0.0101239   0.770346     0.772936   -0.193395    0.113287    0.239172   -0.438756    0.774948    0.171585    -0.198707    0.0977959   -0.18549      -0.729425    0.0478795
  0.2108       0.0921693    -0.157504   -0.0272201  -0.493626   -0.0498143  -0.251822    0.636911     0.282153     0.1012       1.0025       0.0198926    -0.0826559   0.492404     0.538126    0.397227    0.215408    0.657847    0.277721    0.486355    0.177505    -0.113903    0.121015    -0.174797      0.336091   -0.154887
  0.267642    -0.239678      0.088975    0.0212238   0.640711   -0.0276235   0.221631    0.210176     0.0810692   -0.0641837    0.29639     -0.10162       0.154545   -0.0571495    0.113448   -0.379429    0.317786    0.678316   -0.0289168  -0.197186    0.329847     0.324737   -0.73584      0.332735     -0.171822   -0.132502
 -0.0803552   -0.0841928     0.263302   -0.390844    0.354265    0.082505   -0.439741    0.387826    -0.121409    -0.0902782    0.487394     0.092942      0.18246    -0.33378      0.213821   -0.300553   -0.0633893   0.831023   -0.147076   -0.507886   -0.272003    -0.134698    0.549754    -0.269867     -0.474609   -0.094302
 -0.332488    -0.167209     -0.0350204  -0.175155   -0.172648    0.291292    0.402478   -0.00554208  -0.082294     0.248356    -0.355441    -0.0485646     0.263527   -0.321631    -0.43608     0.0329385   0.0625161  -0.546649    0.611138    0.258571   -0.323255     0.382749    0.0308307    0.383455      0.0639394  -0.263172
 -0.384032    -0.312061     -0.0849112  -0.135784   -0.0759531  -0.0714877   0.0480749  -0.265129    -0.00482967  -0.113787     0.239075    -0.0200216    -0.130602    0.281639     0.0539687  -0.0540141  -0.0281909   0.105596    0.222853    0.261263   -0.0648036    0.213889   -0.194415     0.407595     -0.167853   -0.198107
 -0.529778    -0.0683516    -0.28725     0.231573   -0.285538    0.243876    0.0361016   0.238952     0.0672318   -0.0912754    0.167639    -0.479327     -0.0920342   0.104398     0.0229838   0.154349    0.134511    0.141983   -0.0932829   0.490108    0.168633     0.67169     0.122774    -0.0778278    -0.668545    0.3588
 -0.163106     0.23098       0.0494932   0.344632   -0.367487   -0.111619    0.25582    -0.153493     0.157036     0.188114    -0.324426    -0.386298     -0.201984   -0.23259      0.264016   -0.0090853   0.0244785   0.172528   -0.547557    0.528746   -0.0221268    0.525175    0.344946     0.249573      0.627919   -0.132334
 -0.242439     0.170782     -0.263657    0.0259894  -0.248031   -0.238058    0.161552    0.0341435    0.24668     -0.109573    -0.126536     0.102299     -0.037446    0.136541     0.0179806   0.0399496  -0.327752   -0.630274   -0.0496172   0.16541    -0.309554    -0.551147    0.372125    -0.401366      0.2161     -0.0166419
  0.517262    -0.0924825    -0.124726    0.36999    -0.185315   -0.580896    0.234207    0.0433809   -0.130885     0.0977546    0.390795     0.214904     -0.266093    0.295093    -0.0185813  -0.229237    0.101551   -0.0893584  -0.286637    0.0876224   0.442173    -0.312907    0.00946849  -0.435396      0.391132   -0.0177724
  0.0286945   -0.0531354     0.106978    0.100141    0.0192253   0.0244671  -0.0437133   0.170654    -0.0655505   -0.134649    -0.00327586  -0.0886114     0.0417501   0.0174993   -0.0692273  -0.0169661  -0.0325358   0.150055   -0.119012   -0.0396762   0.131805    -0.0261674   0.068901    -0.183222     -0.216773    0.138614
  0.255349     0.292156     -0.0758572  -0.181541    0.127813    0.0253107  -0.139331   -0.165592     0.0287825    0.288393    -0.159138     0.107803      0.032995   -0.156545     0.0107109   0.175639    0.0839863  -0.0738224  -0.086665   -0.191461    0.00552016  -0.129725   -0.0946061    0.255242      0.402621   -0.0830951
  0.146274     0.212442     -0.0906546   0.320582    0.540364    0.401017   -0.621286   -0.148502    -0.645316     0.0574799    0.0559433    0.236896      0.260512   -0.351925    -0.159497    0.547546    0.29424     0.414025   -0.0621311   0.569801   -0.319644     0.666164   -0.188704     0.424487      0.319071   -0.769004
  0.186755     0.208116     -0.439874    0.289535    0.65601     0.0103435   0.34441    -0.43668     -0.494066     0.268662    -0.571216    -0.00498678    0.390761   -0.775641    -0.618415   -0.315076    0.168046   -0.281474   -0.544704   -0.20401     0.258988     0.283394   -0.0247687    0.262705      0.231015    0.256685
  0.6584       0.131487     -0.229063   -0.144091    0.139662    0.357738   -0.0721206  -0.268614     0.0252028   -0.00640769   0.153719    -0.594055      0.0160185  -0.570112    -1.04667    -0.08815    -0.257956   -0.101902    0.73225    -1.00711     0.0804153   -0.0228355   0.0948048    0.0530456     0.656508   -0.447938
  0.422446     0.334733     -0.14834    -0.277388    0.416362    0.104714    0.430136    0.366611     0.772616     0.222408    -0.0592493    0.204789      0.0234993   0.160237     0.076573    0.0361316  -0.166021   -0.216171    0.510105   -0.545991   -0.395339     0.036304   -0.262203     0.43458      -0.259604   -0.0128482
  0.102156    -0.0237187     0.141457    0.353526   -0.0551835  -0.0549057  -0.0767989  -0.46523     -0.364739    -0.289951    -1.14383     -0.222093      0.0427021  -0.175723     0.0311309   0.208056   -0.705273   -0.191039   -0.166613   -0.607363   -0.0908997   -0.281707    0.810419    -0.000102971  -0.591452    0.793168
 -0.295139    -0.136127     -0.100009   -0.682708   -0.539007   -0.253272    0.0467326  -0.288855     0.179143     0.178697    -0.171693    -0.458479     -0.440361    0.217164     0.183449   -0.408489   -0.241094   -0.563347    0.117537   -0.567883    0.276467    -0.740895    0.170262    -0.211227     -0.652629    1.01789
  0.326922     0.340343      0.0418434  -0.0463232  -0.344752   -0.272194   -0.369285   -0.470902    -0.421791     0.197708    -0.363348     0.166774      0.141245    0.0715454   -0.309107    0.219261   -0.0418355  -0.182033   -0.407274   -0.104703    0.348441    -0.598831    0.476425    -0.162151      0.85823    -0.101956
  0.578082     0.0996539     0.0622968   0.273016   -0.169666    0.733516   -0.0689219   0.415381    -0.434598     0.0526006   -0.522846     0.100665     -0.101532    0.074073    -0.196034    0.247072   -0.421152   -0.215086    0.107641   -0.207438    0.250812    -0.03636     0.468134    -0.449092     -0.0162951  -0.0418779
  0.00322373  -0.0341672    -0.984825    0.647624    0.0824776  -0.733102    0.232581    0.300435    -0.174533    -0.492997     0.186194     0.44373      -0.452946   -0.302726     0.290317    0.718256   -0.360348    0.055776   -0.0543081   0.305209   -0.0572457    0.354698    0.256911     0.0470608    -0.0376083   0.125967
  0.5093      -0.185864     -0.343897    0.213622    0.477097   -0.052091   -0.16745     1.04589      0.652172    -0.00679402   0.568477     1.07544       1.11332     0.118008    -0.192645    0.393764   -0.294331    0.244899   -0.197265    0.12629    -0.199782     0.324122    0.698483    -0.05445       0.0900535  -0.0704417
 -0.572245    -0.121878      0.159277   -0.264761   -0.128115    0.166939   -0.603505    0.0345671    0.192145     0.0556923    0.33145     -0.106791      0.132566   -0.0822221    0.212794   -0.0758752   0.0897519   0.236411   -0.0179417   0.435456   -0.233528    -0.0789368   0.0974314    0.201713     -0.254033   -0.197315
  0.3798      -0.160479      0.209116   -0.0510849   0.126386    0.308348    0.712183    0.444978     0.228604     0.267785    -0.167961    -0.460358      0.063639   -0.0657063   -0.141619   -0.337898   -0.0382568   0.0692133   0.188546   -0.266366   -0.0873597    0.357339   -0.142884     0.178203     -0.264046    0.271788
 -0.657612    -0.280582      0.296008   -0.397749    0.12308    -0.90661     0.20917    -0.429142     0.549807    -0.278014    -0.0960405    0.153728     -0.165895    0.149729     0.170073   -0.31629    -0.0232231  -0.384119   -0.0638697   0.15082    -0.322982    -0.0748355  -0.368033     0.695132      0.145147   -0.328378
 -0.376772     0.659178     -0.359136   -0.899368    0.019979    0.276102    0.231884   -0.0676352    0.141879    -0.194863     0.269419    -0.169283     -0.373807    0.0246439    0.266177   -0.384957    0.478564   -0.351606   -0.417431   -0.460894   -0.330086    -0.0919591  -0.552928    -0.215042      0.148482   -0.20952
 -0.0897083    0.000923665  -0.0231662   0.326945   -0.113629   -0.178668   -0.56978    -0.667792    -0.808311    -0.40492     -0.00843653  -0.0799155    -0.555828   -0.00912453   0.345232   -0.12734     0.383448    0.21217    -0.162812    0.204154    0.442836    -0.343099   -0.617254    -0.307803     -0.187949   -0.0145292
 -0.0936756   -0.0908261    -0.418488    0.431536   -0.0809439  -0.199268    0.861342   -0.521769     0.236439     0.200671    -0.507058    -0.281551     -0.430072    0.372876    -0.288768    0.0208499   0.0350532  -1.19549     0.145924    0.381884    0.288249     0.104961   -0.668912    -0.0648057     0.247273    0.027974[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.410317
[ Info: iteration 2, average log likelihood -1.410305
[ Info: iteration 3, average log likelihood -1.410293
[ Info: iteration 4, average log likelihood -1.410281
[ Info: iteration 5, average log likelihood -1.410270
[ Info: iteration 6, average log likelihood -1.410258
[ Info: iteration 7, average log likelihood -1.410247
[ Info: iteration 8, average log likelihood -1.410237
[ Info: iteration 9, average log likelihood -1.410226
[ Info: iteration 10, average log likelihood -1.410216
┌ Info: EM with 100000 data points 10 iterations avll -1.410216
└ 59.0 data points per parameter
kind full, method kmeans
[ Info: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.043207e+05
      1       7.075616e+05      -1.967591e+05 |       32
      2       6.904204e+05      -1.714113e+04 |       32
      3       6.845999e+05      -5.820487e+03 |       32
      4       6.813993e+05      -3.200595e+03 |       32
      5       6.794072e+05      -1.992126e+03 |       32
      6       6.780558e+05      -1.351434e+03 |       32
      7       6.770301e+05      -1.025663e+03 |       32
      8       6.762268e+05      -8.032856e+02 |       32
      9       6.755861e+05      -6.407077e+02 |       32
     10       6.750371e+05      -5.490072e+02 |       32
     11       6.745837e+05      -4.533863e+02 |       32
     12       6.742053e+05      -3.783986e+02 |       32
     13       6.738921e+05      -3.132139e+02 |       32
     14       6.736214e+05      -2.707178e+02 |       32
     15       6.734001e+05      -2.213059e+02 |       32
     16       6.731971e+05      -2.030244e+02 |       32
     17       6.730145e+05      -1.825626e+02 |       32
     18       6.728381e+05      -1.764235e+02 |       32
     19       6.726687e+05      -1.693389e+02 |       32
     20       6.725051e+05      -1.636386e+02 |       32
     21       6.723598e+05      -1.453044e+02 |       32
     22       6.722404e+05      -1.193791e+02 |       32
     23       6.721387e+05      -1.016775e+02 |       32
     24       6.720387e+05      -1.000885e+02 |       32
     25       6.719390e+05      -9.962535e+01 |       32
     26       6.718509e+05      -8.812056e+01 |       32
     27       6.717738e+05      -7.714412e+01 |       32
     28       6.716985e+05      -7.524466e+01 |       32
     29       6.716335e+05      -6.503289e+01 |       32
     30       6.715691e+05      -6.434633e+01 |       32
     31       6.715055e+05      -6.368249e+01 |       32
     32       6.714508e+05      -5.464755e+01 |       32
     33       6.714030e+05      -4.785888e+01 |       32
     34       6.713565e+05      -4.645788e+01 |       32
     35       6.713133e+05      -4.323749e+01 |       32
     36       6.712746e+05      -3.865887e+01 |       32
     37       6.712313e+05      -4.326766e+01 |       32
     38       6.711775e+05      -5.386266e+01 |       32
     39       6.711230e+05      -5.448507e+01 |       32
     40       6.710747e+05      -4.830439e+01 |       32
     41       6.710299e+05      -4.473106e+01 |       32
     42       6.709891e+05      -4.084475e+01 |       32
     43       6.709501e+05      -3.903779e+01 |       32
     44       6.709150e+05      -3.502518e+01 |       32
     45       6.708814e+05      -3.362876e+01 |       32
     46       6.708466e+05      -3.478192e+01 |       32
     47       6.708076e+05      -3.905131e+01 |       32
     48       6.707683e+05      -3.930698e+01 |       32
     49       6.707366e+05      -3.171798e+01 |       32
     50       6.707079e+05      -2.865131e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 670707.903762728)
┌ Info: K-means with 32000 data points using 50 iterations
└ 37.0 data points per parameter
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.421881
[ Info: iteration 2, average log likelihood -1.416895
[ Info: iteration 3, average log likelihood -1.415621
[ Info: iteration 4, average log likelihood -1.414727
[ Info: iteration 5, average log likelihood -1.413768
[ Info: iteration 6, average log likelihood -1.412803
[ Info: iteration 7, average log likelihood -1.412076
[ Info: iteration 8, average log likelihood -1.411653
[ Info: iteration 9, average log likelihood -1.411421
[ Info: iteration 10, average log likelihood -1.411276
[ Info: iteration 11, average log likelihood -1.411170
[ Info: iteration 12, average log likelihood -1.411085
[ Info: iteration 13, average log likelihood -1.411012
[ Info: iteration 14, average log likelihood -1.410947
[ Info: iteration 15, average log likelihood -1.410889
[ Info: iteration 16, average log likelihood -1.410835
[ Info: iteration 17, average log likelihood -1.410786
[ Info: iteration 18, average log likelihood -1.410740
[ Info: iteration 19, average log likelihood -1.410697
[ Info: iteration 20, average log likelihood -1.410657
[ Info: iteration 21, average log likelihood -1.410619
[ Info: iteration 22, average log likelihood -1.410583
[ Info: iteration 23, average log likelihood -1.410548
[ Info: iteration 24, average log likelihood -1.410515
[ Info: iteration 25, average log likelihood -1.410483
[ Info: iteration 26, average log likelihood -1.410452
[ Info: iteration 27, average log likelihood -1.410423
[ Info: iteration 28, average log likelihood -1.410394
[ Info: iteration 29, average log likelihood -1.410367
[ Info: iteration 30, average log likelihood -1.410340
[ Info: iteration 31, average log likelihood -1.410315
[ Info: iteration 32, average log likelihood -1.410291
[ Info: iteration 33, average log likelihood -1.410268
[ Info: iteration 34, average log likelihood -1.410245
[ Info: iteration 35, average log likelihood -1.410224
[ Info: iteration 36, average log likelihood -1.410204
[ Info: iteration 37, average log likelihood -1.410185
[ Info: iteration 38, average log likelihood -1.410166
[ Info: iteration 39, average log likelihood -1.410149
[ Info: iteration 40, average log likelihood -1.410132
[ Info: iteration 41, average log likelihood -1.410115
[ Info: iteration 42, average log likelihood -1.410099
[ Info: iteration 43, average log likelihood -1.410084
[ Info: iteration 44, average log likelihood -1.410069
[ Info: iteration 45, average log likelihood -1.410055
[ Info: iteration 46, average log likelihood -1.410041
[ Info: iteration 47, average log likelihood -1.410027
[ Info: iteration 48, average log likelihood -1.410013
[ Info: iteration 49, average log likelihood -1.410000
[ Info: iteration 50, average log likelihood -1.409988
┌ Info: EM with 100000 data points 50 iterations avll -1.409988
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.118998    -0.055222    -0.0654305   -0.0246592    0.0372815  -0.0206349    0.110706     0.139115     0.143805     0.0274817   0.171517   -0.066625     0.0361956     0.0925792  -0.0740315    0.00534211   0.012159    -0.0214742   -0.00321417   0.120627      0.00878814   0.139304     0.004697    0.0858483   -0.0752101  -0.00616574
  0.175808     0.161834     0.857334    -0.817582    -0.118723    1.00058      0.0602701   -0.082027    -0.0398192    0.0589045  -0.287769    0.0245515    0.361563      0.330124   -0.788208    -0.962623     0.127165    -0.488993     0.279888    -0.281061      0.0651815   -0.521305    -0.333523   -0.650283     0.35845     0.124763
 -0.883285    -0.489548     0.605276    -0.0315761   -0.700975   -0.00237745  -0.189883     0.290481     0.193406     0.0610914  -0.0889576   0.13203     -0.149656      0.554201    0.89127     -0.228968     0.178758     0.0907517   -0.532309     1.05689      -0.0125312   -0.00070087   0.120261   -0.164583    -0.503803   -0.0347162
 -0.234714     0.142994     0.233167    -0.261172     0.454441    0.279481    -0.531851    -0.269276     0.0284171   -0.187524   -0.171605    0.245503     0.370423      0.0834252   0.375331     0.62758      0.458836     0.181019     0.00114582   0.264858     -0.466212     0.0189653   -0.519699    0.790777     0.0237738  -0.207053
 -0.0637098   -0.186998     0.164951     0.139028     0.116567   -0.120911    -0.61345     -0.276663    -0.665402    -0.580512    0.114705   -0.179263    -0.289959      0.124715    0.338338    -0.215759     0.208751     0.642101    -0.274007    -0.131892      0.401859    -0.347537    -0.18859    -0.46812     -0.500234    0.073268
 -0.0749173   -0.232447     0.484294    -0.751379     0.308434   -0.131435    -0.344741    -0.0451394    0.411369     0.0142205   0.36309     0.218962     0.062352      0.313096    0.0245095   -0.244378    -0.314825     0.171034     0.315392    -0.342919     -0.52871     -0.354608    -0.28117     0.364565    -0.706239   -0.275958
  0.280621    -0.27786     -0.166347     0.279571     0.262353    0.0531271    0.898194    -0.634941    -0.401389    -0.0752073  -0.938832   -0.0513329   -0.528797      0.490791   -0.685447     0.267687    -0.732101    -1.33977     -0.076516    -0.475318      0.821266     0.372473    -0.275818   -0.253688    -0.213808    0.0653197
  0.058185    -0.262972     0.00913121  -0.142494     0.280354   -0.0453774    0.21379      0.0926196   -0.00255903  -0.0186165   0.371904    0.122869     0.0957496     0.136707   -0.0795882   -0.251988     0.187873     0.26288      0.114326    -0.0778699     0.224089     0.221714    -0.366079    0.200995    -0.0678824  -0.162602
 -0.374242    -0.0322248    0.207213     0.514024    -0.0318129  -0.731541    -0.248218    -0.499416     0.259698    -0.199342    0.189745   -0.00924756   0.224042      0.30691    -0.0577189    0.00695606  -0.0643962    0.159305    -0.318922     0.758645      0.445492    -0.368054    -0.0669873   0.557683     0.0655344   0.143116
 -0.0250458   -0.0755148   -0.00844109  -0.485204    -0.643214   -0.235538     0.285085     0.0200148    0.204031     0.246895   -0.463588   -0.288586    -0.293299      0.143939    0.20776     -0.436531    -0.369528    -0.545314     0.0945255   -0.821311      0.227948    -0.91805      0.137613   -0.219803    -0.657509    1.24617
  0.282279    -0.369633    -0.118196     0.440042    -0.22206    -0.558815     0.285049     0.0264215   -0.155335     0.0985433   0.380293    0.155508    -0.309219      0.277428   -0.215475    -0.416643    -0.201077    -0.361167    -0.0966765    0.356301      0.209688    -0.373199     0.105816   -0.73877      0.149319   -0.0203658
  0.069933     0.542149    -0.168591    -0.201979     0.0417324  -0.131903    -0.739894    -0.493713    -0.500606     0.448443   -0.190618    0.0561636    0.255075     -0.387977   -0.533645     0.320363     0.0741197   -0.17668     -0.551212    -0.0996714     0.330736    -0.282133     0.269462   -0.048297     0.890563   -0.424715
  0.894413    -0.329342     0.310878     0.216182    -0.0764679   0.582223    -0.206128     0.102906    -0.243949     0.703529   -0.362819   -0.342985    -0.151606     -0.366886    0.00175433  -0.168625     0.19137      0.549414    -0.051974    -0.0968543     0.0783505    0.321567    -0.0280067   0.294597     0.0755923  -0.0171028
 -0.105738    -0.369856    -0.127572    -0.45299      0.442433    0.310147    -0.229819     0.467102     0.106171     0.273179    0.837767    0.402444     0.851001      0.21798     0.110356     0.189312     0.089439     0.429704    -0.0553688   -0.0130577     0.458613    -0.0414426    0.424604   -0.243562    -0.531897    0.239746
 -0.139921    -0.103295    -0.242605    -0.211327    -0.353132    0.177592     0.311975     0.0746575    0.0378288    0.339827   -0.131573    0.0055304    0.0612249    -0.188409   -0.408029     0.361645    -0.144415    -0.468012     0.633276     0.302823     -0.309272     0.314015     0.175979    0.32149      0.244024   -0.444333
 -0.0938736   -0.0530464   -0.082342     0.0969056   -0.684923   -0.351693     0.0131772   -0.786939    -0.220525    -0.267435   -0.929288   -0.59398     -0.912129     -0.143222   -0.271842     0.0691808    0.412975    -0.716968     0.127328     0.214268     -0.294628    -0.200861    -0.938452    0.182371     0.453823   -0.0742471
  0.968917     0.200628    -0.0859118    0.173066    -0.450941   -0.467866    -0.0980741    0.31387      0.0948786    0.049478    0.440009    0.501033    -0.279382      0.749599    0.593083     0.434286     0.203061     0.151277    -0.0613069   -0.00829088    0.366735    -0.461772    -0.050782   -0.222195     0.464189   -0.0998519
 -0.254052    -0.259385     0.134061     0.00766573   0.566664   -0.0264956    0.598929    -0.271447    -0.0316373    0.572633   -0.387406   -0.0759738    0.614321     -0.742881   -0.538776    -0.843375     0.0242189   -0.258563    -0.15212     -0.0871141     0.0334306    0.337843    -0.235408    0.510966    -0.139161   -0.0275899
 -0.315729     0.237217     0.210039    -0.122101    -0.168519    0.0446937   -0.888304     0.60949     -0.0329419    0.0376269   1.12365     0.188169     0.643643     -0.432466    0.53838     -0.340828     0.101931     1.4051      -0.172588     0.289761     -0.239328    -0.368942     0.663037    0.175614     0.331102   -0.280879
 -0.550337     0.210773    -0.480029    -0.278139    -0.466933    0.205374    -0.127919     0.285113     0.166312    -0.528756    0.266236   -0.335278    -0.27576       0.660262   -0.0577339    0.334321    -0.0968909   -0.250398    -0.0288564   -0.0158595    -0.0743733   -0.0784291    0.264106   -0.625399    -0.0931748   0.235242
  0.00694972  -0.313168    -0.20253      0.249119    -0.232403    0.238127     0.38473      0.536011     0.386529    -0.212753    0.328861   -0.436205    -0.185487      0.153217    0.25392      0.080977    -0.15547      0.450971     0.522417     0.277021      0.0126612    0.538586    -0.128377    0.257629    -0.728456    0.0924204
  0.685626     0.178072    -0.105951     0.58588      0.591714    0.13061     -0.0105043    0.95389      0.021769    -0.351718   -0.17714     0.627948     0.790377     -0.135181   -0.336852     0.586467    -0.11127      0.468769    -0.129176    -0.0444635    -0.259408     0.353732     0.26049    -0.355679     0.0976878  -0.23074
  0.426911    -0.287776     0.264625     0.137139     0.0756414   0.528883    -0.560287    -0.189007    -0.742433     0.270247   -0.114576    0.434818     0.299132      0.142447   -0.57593      0.149955     0.136159    -0.167836     0.450718     0.0871754     0.364533    -0.114186    -0.124027    0.0707918   -0.382503    0.259864
 -0.961349    -0.297821    -0.221553    -0.717466    -0.269049   -0.401356    -0.00168307  -0.848562     0.175799     0.189397    0.3313     -0.608265    -0.535293     -0.0959416   0.478935    -0.675543     0.386296    -0.7492       0.124428     0.0433691     0.129156    -0.220627    -0.315187    0.340156    -0.238227    0.216032
  0.888207     0.320105    -0.337267    -0.11548      0.494915    0.11937      0.239314    -0.0692068    0.0335608   -0.0477153   0.0378989  -0.138318     0.000946638  -0.542823   -0.725383    -0.0973055   -0.256402     0.040476     0.44983     -1.12707       0.0596285    0.0537639    0.0913789   0.218647     0.361323   -0.12365
  0.155276     0.00272083   0.14896      0.354443    -0.218288    0.119866    -0.237057    -0.210967    -0.238256    -0.102748   -0.913219   -0.141279     0.193039     -0.130697    0.0468639    0.347995    -0.67196     -0.128675    -0.132346    -0.181113     -0.106586    -0.362361     0.989216    0.00500369  -0.236497    0.483669
 -0.249126     0.146311    -0.883458     0.709558     0.178633   -0.11155      0.0431167   -0.0398527   -0.442078    -0.0780704   0.170298    0.245849    -0.237026     -0.640744    0.135243     0.319014     0.22773     -0.00398369  -0.0011387    0.700793      0.142835     0.53157      0.0185104  -0.0426784    0.0981294   0.144367
 -0.178083     0.051396    -0.0313922    0.219635    -0.224285    0.0263975    0.0773561    0.00095725  -0.0807044   -0.0625764  -0.141827   -0.270411    -0.138661     -0.208579    0.0997731    0.0524672    0.0947898    0.270069    -0.454782     0.301023      0.00115564   0.520147     0.219535    0.0649158    0.114826   -0.0203753
  0.119208     0.577589    -0.152809     0.568861     0.145755    0.0924658    0.928126    -0.267908     0.534713     0.532441   -0.378472   -0.201854     0.140273      0.174008    0.117803    -0.161798     0.00431582  -0.753956     0.124937     0.524979     -0.0350684    0.105647    -0.197476    0.316335     0.460827    0.158475
  0.00710761   0.783713    -0.429789    -0.539467     0.212147    0.316994     0.361829    -0.0426437    0.0814701   -0.215122    0.127317   -0.0516441   -0.436364      0.120119    0.245742    -0.315699     0.466871    -0.256444    -0.2736      -0.589461     -0.329512    -0.0175711   -0.611498   -0.129303     0.232413   -0.206363
 -0.401545    -0.111148     0.0099604   -0.3077       0.176488   -1.10963      0.425381     0.107845     0.577121    -0.478197   -0.327506    0.373787    -0.250947      0.0357916   0.339197    -0.0985427   -0.202935    -0.398403    -0.280014     0.000243502  -0.470681    -0.00447541  -0.0044945   0.350754     0.417013   -0.32963
  0.0889039    0.0953728    0.0813707   -0.0567849    0.0083035  -0.0120776   -0.163245    -0.0255218    0.0134574    0.076535   -0.164419   -0.00137745   0.034727     -0.0193526  -0.0158258    0.0463165   -0.122286    -0.0847188   -0.0487206   -0.144235      0.00922457  -0.349564     0.0856382  -0.0106021    0.0208313   0.0678538[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.409975
[ Info: iteration 2, average log likelihood -1.409963
[ Info: iteration 3, average log likelihood -1.409951
[ Info: iteration 4, average log likelihood -1.409939
[ Info: iteration 5, average log likelihood -1.409927
[ Info: iteration 6, average log likelihood -1.409916
[ Info: iteration 7, average log likelihood -1.409905
[ Info: iteration 8, average log likelihood -1.409894
[ Info: iteration 9, average log likelihood -1.409884
[ Info: iteration 10, average log likelihood -1.409873
┌ Info: EM with 100000 data points 10 iterations avll -1.409873
└ 59.0 data points per parameter
[ Info: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.678561e+05
      1       2.230230e+04      -1.455538e+05 |        2
      2       7.823675e+03      -1.447862e+04 |        0
      3       7.823675e+03       0.000000e+00 |        0
K-means converged with 3 iterations (objv = 7823.67549422947)
┌ Info: K-means with 900 data points using 3 iterations
└ 150.0 data points per parameter
[ Info: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
[ Info: iteration 1, average log likelihood -2.043155
[ Info: iteration 2, average log likelihood -2.043154
[ Info: iteration 3, average log likelihood -2.043154
[ Info: iteration 4, average log likelihood -2.043154
[ Info: iteration 5, average log likelihood -2.043154
[ Info: iteration 6, average log likelihood -2.043154
[ Info: iteration 7, average log likelihood -2.043154
[ Info: iteration 8, average log likelihood -2.043154
[ Info: iteration 9, average log likelihood -2.043154
[ Info: iteration 10, average log likelihood -2.043154
┌ Info: EM with 900 data points 10 iterations avll -2.043154
└ 81.8 data points per parameter
   Testing GaussianMixtures tests passed 
