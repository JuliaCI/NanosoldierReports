Julia Version 1.5.0-DEV.57
Commit a2fe09e6a7 (2020-01-13 17:14 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-8.0.1 (ORCJIT, skylake)
Environment:
  JULIA_DEPOT_PATH = ::/usr/local/share/julia

 Resolving package versions...
 Installed GaussianMixtures ─── v0.3.0
 Installed Rmath ────────────── v0.6.0
 Installed Distances ────────── v0.8.2
 Installed OrderedCollections ─ v1.1.0
 Installed FileIO ───────────── v1.2.1
 Installed SpecialFunctions ─── v0.9.0
 Installed Parameters ───────── v0.12.0
 Installed Clustering ───────── v0.13.3
 Installed CMake ────────────── v1.1.2
 Installed HDF5 ─────────────── v0.12.5
 Installed FillArrays ───────── v0.8.4
 Installed StatsFuns ────────── v0.9.3
 Installed Blosc ────────────── v0.5.1
 Installed URIParser ────────── v0.4.0
 Installed NearestNeighbors ─── v0.4.4
 Installed OpenSpecFun_jll ──── v0.5.3+1
 Installed OpenBLAS_jll ─────── v0.3.7+4
 Installed Distributions ────── v0.22.1
 Installed BinaryProvider ───── v0.5.8
 Installed Arpack_jll ───────── v3.5.0+2
 Installed SortingAlgorithms ── v0.3.1
 Installed DataAPI ──────────── v1.1.0
 Installed JLD ──────────────── v0.9.1
 Installed DataStructures ───── v0.17.7
 Installed BinDeps ──────────── v1.0.0
 Installed Compat ───────────── v2.2.0
 Installed ScikitLearnBase ──── v0.5.0
 Installed StatsBase ────────── v0.32.0
 Installed StaticArrays ─────── v0.12.1
 Installed Missings ─────────── v0.4.3
 Installed PDMats ───────────── v0.9.10
 Installed Arpack ───────────── v0.4.0
 Installed QuadGK ───────────── v2.3.1
 Installed LegacyStrings ────── v0.4.1
 Installed CMakeWrapper ─────── v0.2.3
  Updating `~/.julia/environments/v1.5/Project.toml`
  [cc18c42c] + GaussianMixtures v0.3.0
  Updating `~/.julia/environments/v1.5/Manifest.toml`
  [7d9fca2a] + Arpack v0.4.0
  [68821587] + Arpack_jll v3.5.0+2
  [9e28174c] + BinDeps v1.0.0
  [b99e7846] + BinaryProvider v0.5.8
  [a74b3585] + Blosc v0.5.1
  [631607c0] + CMake v1.1.2
  [d5fb7624] + CMakeWrapper v0.2.3
  [aaaa29a8] + Clustering v0.13.3
  [34da2185] + Compat v2.2.0
  [9a962f9c] + DataAPI v1.1.0
  [864edb3b] + DataStructures v0.17.7
  [b4f34e82] + Distances v0.8.2
  [31c24e10] + Distributions v0.22.1
  [5789e2e9] + FileIO v1.2.1
  [1a297f60] + FillArrays v0.8.4
  [cc18c42c] + GaussianMixtures v0.3.0
  [f67ccb44] + HDF5 v0.12.5
  [4138dd39] + JLD v0.9.1
  [1b4a561d] + LegacyStrings v0.4.1
  [e1d29d7a] + Missings v0.4.3
  [b8a86587] + NearestNeighbors v0.4.4
  [4536629a] + OpenBLAS_jll v0.3.7+4
  [efe28fd5] + OpenSpecFun_jll v0.5.3+1
  [bac558e1] + OrderedCollections v1.1.0
  [90014a1f] + PDMats v0.9.10
  [d96e819e] + Parameters v0.12.0
  [1fd47b50] + QuadGK v2.3.1
  [79098fc4] + Rmath v0.6.0
  [6e75b9c4] + ScikitLearnBase v0.5.0
  [a2af1166] + SortingAlgorithms v0.3.1
  [276daf66] + SpecialFunctions v0.9.0
  [90137ffa] + StaticArrays v0.12.1
  [2913bbd2] + StatsBase v0.32.0
  [4c63d2b9] + StatsFuns v0.9.3
  [30578b45] + URIParser v0.4.0
  [2a0f44e3] + Base64 
  [ade2ca70] + Dates 
  [8bb1440f] + DelimitedFiles 
  [8ba89e20] + Distributed 
  [b77e0a4c] + InteractiveUtils 
  [76f85450] + LibGit2 
  [8f399da3] + Libdl 
  [37e2e46d] + LinearAlgebra 
  [56ddb016] + Logging 
  [d6f4376e] + Markdown 
  [a63ad114] + Mmap 
  [44cfe95a] + Pkg 
  [de0858da] + Printf 
  [9abbd945] + Profile 
  [3fa0cd96] + REPL 
  [9a3f8284] + Random 
  [ea8e919c] + SHA 
  [9e88b42a] + Serialization 
  [1a1011a3] + SharedArrays 
  [6462fe0b] + Sockets 
  [2f01184e] + SparseArrays 
  [10745b16] + Statistics 
  [4607b0f0] + SuiteSparse 
  [8dfed614] + Test 
  [cf7118a7] + UUIDs 
  [4ec0a83e] + Unicode 
  Building Rmath → `~/.julia/packages/Rmath/BoBag/deps/build.log`
  Updating `/tmp/jl_4i315B/Project.toml`
 [no changes]
  Updating `/tmp/jl_4i315B/Manifest.toml`
 [no changes]
  Building CMake → `~/.julia/packages/CMake/nSK2r/deps/build.log`
  Updating `/tmp/jl_pMSh8V/Project.toml`
 [no changes]
  Updating `/tmp/jl_pMSh8V/Manifest.toml`
 [no changes]
  Building Blosc → `~/.julia/packages/Blosc/lzFr0/deps/build.log`
  Updating `/tmp/jl_gLn4eN/Project.toml`
 [no changes]
  Updating `/tmp/jl_gLn4eN/Manifest.toml`
 [no changes]
  Building HDF5 ─→ `~/.julia/packages/HDF5/Zh9on/deps/build.log`
  Updating `/tmp/jl_bk2Tmk/Project.toml`
 [no changes]
  Updating `/tmp/jl_bk2Tmk/Manifest.toml`
 [no changes]
   Testing GaussianMixtures
  Updating `/tmp/jl_oO98Zh/Project.toml`
 [no changes]
  Updating `/tmp/jl_oO98Zh/Manifest.toml`
 [no changes]
Running sandbox
Status `/tmp/jl_oO98Zh/Project.toml`
  [aaaa29a8] Clustering v0.13.3
  [34da2185] Compat v2.2.0
  [31c24e10] Distributions v0.22.1
  [5789e2e9] FileIO v1.2.1
  [cc18c42c] GaussianMixtures v0.3.0
  [4138dd39] JLD v0.9.1
  [90014a1f] PDMats v0.9.10
  [6e75b9c4] ScikitLearnBase v0.5.0
  [276daf66] SpecialFunctions v0.9.0
  [2913bbd2] StatsBase v0.32.0
  [8bb1440f] DelimitedFiles 
  [8ba89e20] Distributed 
  [37e2e46d] LinearAlgebra 
  [56ddb016] Logging 
  [de0858da] Printf 
  [9a3f8284] Random 
  [10745b16] Statistics 
[ Info: Testing Data
(100000, -742930.0943848245, [87955.39273179488, 12044.607268205127], [-8863.962539675771 6880.227155956725 -13346.866726501908; 8836.078933798668 -6661.060257609868 13512.479062888646], [[89397.12563264558 5192.977188580818 -8972.827542252804; 5192.977188580817 86578.7725964606 5050.545113490564; -8972.827542252804 5050.545113490564 78665.27344588607], [9897.832249563317 -5341.769323106605 9063.1039449762; -5341.769323106605 13827.785690917619 -4992.9042944496505; 9063.1039449762 -4992.9042944496505 20837.177540943572]])
┌ Warning: rmprocs: process 1 not removed
└ @ Distributed /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Distributed/src/cluster.jl:1015
[ Info: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.043581e+03
      1       8.703325e+02      -1.732486e+02 |        4
      2       8.336967e+02      -3.663580e+01 |        4
      3       8.167235e+02      -1.697319e+01 |        0
      4       8.167235e+02       0.000000e+00 |        0
K-means converged with 4 iterations (objv = 816.7234999414827)
┌ Info: K-means with 272 data points using 4 iterations
└ 11.3 data points per parameter
[ Info: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
┌ Info: EM with 272 data points 0 iterations avll -2.057246
└ 5.8 data points per parameter
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:221
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:221
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:221
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:221
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:631 [inlined]
└ @ Core ./broadcast.jl:631
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:230
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:230
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:631 [inlined]
└ @ Core ./broadcast.jl:631
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:631 [inlined]
└ @ Core ./broadcast.jl:631
[ Info: iteration 1, lowerbound -3.806795
[ Info: iteration 2, lowerbound -3.675446
[ Info: iteration 3, lowerbound -3.535208
[ Info: iteration 4, lowerbound -3.382372
[ Info: iteration 5, lowerbound -3.243366
[ Info: iteration 6, lowerbound -3.141801
[ Info: dropping number of Gaussions to 7
[ Info: iteration 7, lowerbound -3.078726
[ Info: dropping number of Gaussions to 5
[ Info: iteration 8, lowerbound -3.027468
[ Info: iteration 9, lowerbound -2.980925
[ Info: dropping number of Gaussions to 4
[ Info: iteration 10, lowerbound -2.924498
[ Info: iteration 11, lowerbound -2.854409
[ Info: iteration 12, lowerbound -2.781746
[ Info: iteration 13, lowerbound -2.713531
[ Info: iteration 14, lowerbound -2.654612
[ Info: iteration 15, lowerbound -2.601901
[ Info: iteration 16, lowerbound -2.551019
[ Info: iteration 17, lowerbound -2.501738
[ Info: dropping number of Gaussions to 3
[ Info: iteration 18, lowerbound -2.451087
[ Info: iteration 19, lowerbound -2.399248
[ Info: iteration 20, lowerbound -2.357937
[ Info: iteration 21, lowerbound -2.327521
[ Info: iteration 22, lowerbound -2.310423
[ Info: iteration 23, lowerbound -2.308347
[ Info: dropping number of Gaussions to 2
[ Info: iteration 24, lowerbound -2.302916
[ Info: iteration 25, lowerbound -2.299259
[ Info: iteration 26, lowerbound -2.299256
[ Info: iteration 27, lowerbound -2.299254
[ Info: iteration 28, lowerbound -2.299254
[ Info: iteration 29, lowerbound -2.299253
[ Info: iteration 30, lowerbound -2.299253
[ Info: iteration 31, lowerbound -2.299253
[ Info: iteration 32, lowerbound -2.299253
[ Info: iteration 33, lowerbound -2.299253
[ Info: iteration 34, lowerbound -2.299253
[ Info: iteration 35, lowerbound -2.299253
[ Info: iteration 36, lowerbound -2.299253
[ Info: iteration 37, lowerbound -2.299253
[ Info: iteration 38, lowerbound -2.299253
[ Info: iteration 39, lowerbound -2.299253
[ Info: iteration 40, lowerbound -2.299253
[ Info: iteration 41, lowerbound -2.299253
[ Info: iteration 42, lowerbound -2.299253
[ Info: iteration 43, lowerbound -2.299253
[ Info: iteration 44, lowerbound -2.299253
[ Info: iteration 45, lowerbound -2.299253
[ Info: iteration 46, lowerbound -2.299253
[ Info: iteration 47, lowerbound -2.299253
[ Info: iteration 48, lowerbound -2.299253
[ Info: iteration 49, lowerbound -2.299253
[ Info: iteration 50, lowerbound -2.299253
[ Info: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
History[Mon Jan 13 21:04:31 2020: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
, Mon Jan 13 21:04:39 2020: K-means with 272 data points using 4 iterations
11.3 data points per parameter
, Mon Jan 13 21:04:41 2020: EM with 272 data points 0 iterations avll -2.057246
5.8 data points per parameter
, Mon Jan 13 21:04:43 2020: GMM converted to Variational GMM
, Mon Jan 13 21:04:51 2020: iteration 1, lowerbound -3.806795
, Mon Jan 13 21:04:51 2020: iteration 2, lowerbound -3.675446
, Mon Jan 13 21:04:51 2020: iteration 3, lowerbound -3.535208
, Mon Jan 13 21:04:51 2020: iteration 4, lowerbound -3.382372
, Mon Jan 13 21:04:51 2020: iteration 5, lowerbound -3.243366
, Mon Jan 13 21:04:51 2020: iteration 6, lowerbound -3.141801
, Mon Jan 13 21:04:52 2020: dropping number of Gaussions to 7
, Mon Jan 13 21:04:52 2020: iteration 7, lowerbound -3.078726
, Mon Jan 13 21:04:52 2020: dropping number of Gaussions to 5
, Mon Jan 13 21:04:52 2020: iteration 8, lowerbound -3.027468
, Mon Jan 13 21:04:52 2020: iteration 9, lowerbound -2.980925
, Mon Jan 13 21:04:52 2020: dropping number of Gaussions to 4
, Mon Jan 13 21:04:52 2020: iteration 10, lowerbound -2.924498
, Mon Jan 13 21:04:52 2020: iteration 11, lowerbound -2.854409
, Mon Jan 13 21:04:52 2020: iteration 12, lowerbound -2.781746
, Mon Jan 13 21:04:52 2020: iteration 13, lowerbound -2.713531
, Mon Jan 13 21:04:52 2020: iteration 14, lowerbound -2.654612
, Mon Jan 13 21:04:52 2020: iteration 15, lowerbound -2.601901
, Mon Jan 13 21:04:52 2020: iteration 16, lowerbound -2.551019
, Mon Jan 13 21:04:52 2020: iteration 17, lowerbound -2.501738
, Mon Jan 13 21:04:52 2020: dropping number of Gaussions to 3
, Mon Jan 13 21:04:52 2020: iteration 18, lowerbound -2.451087
, Mon Jan 13 21:04:52 2020: iteration 19, lowerbound -2.399248
, Mon Jan 13 21:04:52 2020: iteration 20, lowerbound -2.357937
, Mon Jan 13 21:04:52 2020: iteration 21, lowerbound -2.327521
, Mon Jan 13 21:04:52 2020: iteration 22, lowerbound -2.310423
, Mon Jan 13 21:04:52 2020: iteration 23, lowerbound -2.308347
, Mon Jan 13 21:04:52 2020: dropping number of Gaussions to 2
, Mon Jan 13 21:04:52 2020: iteration 24, lowerbound -2.302916
, Mon Jan 13 21:04:52 2020: iteration 25, lowerbound -2.299259
, Mon Jan 13 21:04:52 2020: iteration 26, lowerbound -2.299256
, Mon Jan 13 21:04:52 2020: iteration 27, lowerbound -2.299254
, Mon Jan 13 21:04:52 2020: iteration 28, lowerbound -2.299254
, Mon Jan 13 21:04:52 2020: iteration 29, lowerbound -2.299253
, Mon Jan 13 21:04:52 2020: iteration 30, lowerbound -2.299253
, Mon Jan 13 21:04:52 2020: iteration 31, lowerbound -2.299253
, Mon Jan 13 21:04:52 2020: iteration 32, lowerbound -2.299253
, Mon Jan 13 21:04:52 2020: iteration 33, lowerbound -2.299253
, Mon Jan 13 21:04:52 2020: iteration 34, lowerbound -2.299253
, Mon Jan 13 21:04:52 2020: iteration 35, lowerbound -2.299253
, Mon Jan 13 21:04:52 2020: iteration 36, lowerbound -2.299253
, Mon Jan 13 21:04:52 2020: iteration 37, lowerbound -2.299253
, Mon Jan 13 21:04:52 2020: iteration 38, lowerbound -2.299253
, Mon Jan 13 21:04:52 2020: iteration 39, lowerbound -2.299253
, Mon Jan 13 21:04:52 2020: iteration 40, lowerbound -2.299253
, Mon Jan 13 21:04:52 2020: iteration 41, lowerbound -2.299253
, Mon Jan 13 21:04:52 2020: iteration 42, lowerbound -2.299253
, Mon Jan 13 21:04:52 2020: iteration 43, lowerbound -2.299253
, Mon Jan 13 21:04:52 2020: iteration 44, lowerbound -2.299253
, Mon Jan 13 21:04:52 2020: iteration 45, lowerbound -2.299253
, Mon Jan 13 21:04:52 2020: iteration 46, lowerbound -2.299253
, Mon Jan 13 21:04:52 2020: iteration 47, lowerbound -2.299253
, Mon Jan 13 21:04:52 2020: iteration 48, lowerbound -2.299253
, Mon Jan 13 21:04:52 2020: iteration 49, lowerbound -2.299253
, Mon Jan 13 21:04:52 2020: iteration 50, lowerbound -2.299253
, Mon Jan 13 21:04:52 2020: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [95.95490777397379, 178.0450922260261]
β = [95.95490777397379, 178.0450922260261]
m = [2.0002292577752674 53.85198717246076; 4.250300733269812 79.28686694436043]
ν = [97.95490777397379, 180.0450922260261]
W = LinearAlgebra.UpperTriangular{Float64,Array{Float64,2}}[[0.37587636119501305 -0.008953123827348138; 0.0 0.012748664777409918], [0.18404155547483714 -0.00764404904232903; 0.0 0.008581705166331898]]
Kind: diag, size256
nx: 100000 sum(zeroth order stats): 100000.00000000006
avll from stats: -0.9883000034914807
avll from llpg:  -0.9883000034914808
avll direct:     -0.9883000034914808
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 100000.00000000001
avll from stats: -0.962093977187642
avll from llpg:  -0.9620939771876418
avll direct:     -0.9620939771876418
sum posterior: 100000.0
32×26 Array{Float64,2}:
  0.0716735   -0.0613784  -0.238123    -0.198177    -0.0665524   -0.0986515   -0.108669    -0.107131     0.0168875   -0.0140146    -0.0412692  -0.127057    -0.109513    -0.181885      0.174416    -0.0232693    0.049072     0.0577831    0.000657056   0.0869544   -0.205464    -0.182982   -0.0604008   -0.120922   -0.0672362     0.0120351
 -0.212293     0.138307    0.0650514   -0.119379    -0.0400618   -0.140254     0.0279248    0.0986323    0.0633862    0.0330362     0.0131783   0.0370216    0.185227     0.0896033     0.0516884    0.0412723   -0.0686728    0.175174    -0.0191526    -0.105044    -0.0341777   -0.117759    0.0172285   -0.153428   -0.139926     -0.117672
  0.124981     0.116454    0.0317932    0.12009     -0.0367423   -0.0262377    0.0674299   -0.0931681    0.0184037    0.0356558     0.0220686  -0.00407774   0.0281737    0.154781     -0.0637548    0.0794345   -0.0520337    0.115553     0.130847      0.0624548   -0.00427311   0.120648   -0.038212     0.0803621   0.195808      0.105925
 -0.0708058    0.0211261  -0.0395163    0.0858496   -0.0488451   -0.286085     0.178421    -0.020276    -0.0408341    0.108508     -0.0883668  -0.0269972   -0.0861827    0.125413     -0.106216    -0.125702     0.136967    -0.140199     0.104508      0.184263    -0.0340193    0.0151449  -0.109226    -0.178621   -0.132117     -0.17682
 -0.0152544    0.0323157   0.0186004   -0.0215109    0.230658    -0.0979909    0.0973585   -0.0727269   -0.0129942   -0.119355      0.154916   -0.113458     0.00633551  -0.0493831    -0.0108778    0.0489988    0.109756    -0.0612655   -0.0466912     0.096152     0.0215359    0.0212624  -0.043228    -0.11025     0.173202      0.02966
 -0.0698097   -0.0225855  -0.0953208    0.0827692   -0.0407127    0.0501895    0.116638     0.0267414    0.010564     0.0822905    -0.0657109   0.120028    -0.145101    -0.118768     -0.0773979   -0.00569875   0.0670731   -0.112043     0.104158     -0.0312661    0.0279704    0.113835   -0.105014     0.289351   -0.0516848     0.0511316
  0.112784    -0.104815   -0.0940419    0.00441298  -0.0280211    0.0764718   -0.0806498    0.0395232   -0.113083     0.0647071     0.0630518   0.100361     0.00175786   0.0339391    -0.123987     0.0477479   -0.122084     0.0772035   -0.026297     -0.128926    -0.0328057    0.0292521   0.100072    -0.065437    0.124046      0.115885
 -0.182012    -0.181575    0.110196     0.0750465   -0.104681    -0.119939    -0.0672245    0.0409714    0.191636    -0.117051     -0.0422458   0.197991    -0.0783266   -0.113194      0.0681384    0.0831283   -0.0220748   -0.0457015    0.229593     -0.0921157    0.115009     0.0435684   0.173097    -0.0163114   0.0682908     0.13957
 -0.00488579  -0.0839089   0.049883     0.0833629    0.0431395   -0.0211601    0.0955349    0.0290117   -0.172508     0.220453     -0.154975    0.179545    -0.219604     0.0377481     0.0794625   -0.0579748    0.182499    -0.0768526    0.0680072    -0.103068     0.0335655    0.149195   -0.0486506   -0.063406   -0.130549     -0.0187761
  0.0377078   -0.169711   -0.095988    -0.0790696    0.0216583    0.106548    -0.0877118   -0.0620716    0.0654933    0.0970955     0.0636787   0.133293     0.0481363    0.109576     -0.0306847    0.0998496    0.100417     0.0633496   -0.0448841     0.0821707    0.0501934   -0.014458    0.0546055   -0.0490907   0.0436039    -0.129674
  0.0307841    0.0809365  -0.0665869    0.0215972   -0.0704112   -0.0367109    0.07046      0.131741    -0.00858635   0.11486      -0.0159001  -0.0861981    0.0402301    0.151806      0.0507751   -0.0098085   -0.195863    -0.0577955    0.0222493    -0.251392    -0.0445599   -0.0326212  -0.0704077    0.106217    0.0111004    -0.210357
 -0.0732109   -0.138154    0.0179993    0.0371264    0.100774     0.0103094    0.107506     0.0825232    0.0139264    0.00617839    0.295029   -0.0715548    0.0390288   -0.0702817    -0.181104    -0.107906     0.059338    -0.13341     -0.12151      -0.0880606   -0.114942     0.0980847   0.0282489    0.0819551  -0.102708      0.0407865
  0.192812    -0.0738918  -0.0585158    0.0501485   -0.119908     0.00618158   0.069333    -0.148755    -0.0363743    0.0331381    -0.0398392   0.114248    -0.169103     0.240131     -0.114376    -0.0360848    0.0901601   -0.161559     0.344071     -0.0518711   -0.0744728   -0.107943   -0.00538741   0.0187358  -0.00395475    0.0348874
 -0.0531335   -0.134293   -0.158801     0.0422847   -0.10628     -0.085999     0.0975274   -0.110879     0.245632    -0.0484414     0.0958404   0.127773     0.0931173    0.0100614    -0.065799    -0.0445166   -0.00155269  -0.0213877   -0.0402411    -0.00416428   0.0903789    0.0352467  -0.0380254   -0.0530614  -0.0270156    -0.018982
  0.163742    -0.0746907  -0.0500564   -0.171549     0.255242     0.22905     -0.187178    -0.0438692   -0.0977898   -0.190887      0.153056    0.124019     0.0084663    0.0978599    -0.0709191   -0.126383    -0.0567899    0.0176333    0.107653     -0.129244     0.158865     0.0075688   0.178407    -0.040426   -0.01718       0.0938754
 -0.115415    -0.228125   -0.0914371   -0.182226     0.00676317  -0.125778    -0.0637132    0.221892     0.107369    -0.000975573  -0.0473901   0.187944    -0.0326875   -0.119245      0.0946362    0.0507955   -0.0106192   -0.0557504   -0.0371498    -0.0594331    0.0614992    0.122131    0.0575514    0.117153   -0.0706647    -0.0770075
 -0.132989     0.0174282  -0.029384    -0.074155     0.0625923    0.158248    -0.0780547    0.0194701    0.00545341  -0.0259065    -0.174508    0.0122476   -0.067769     0.00126346   -0.00294647  -0.0669691    0.176837     0.137351     0.0448812    -0.09137     -0.00987739   0.184949   -0.0284799    0.179134   -0.0928622    -0.0612665
  0.216803     0.0592636  -0.00609302  -0.0597829   -0.0961278    0.167407     0.0191401    0.0296151    0.02119      0.0247294    -0.0986997  -0.0411072    0.0382545   -0.0880076    -0.0197846    0.0284753    0.0846058   -0.276569    -0.0986311    -0.164028     0.0672399   -0.0177824   0.0335244   -0.101028   -0.199399      0.0213422
 -0.0214663    0.0632462  -0.0740279    0.100452     0.0487504    0.00136812   0.0123378   -0.26048      0.0610431   -0.106372      0.131262    0.132485    -0.0593307   -0.0556259    -0.117477     0.0522869   -0.0183091   -0.0330633   -0.0177392    -0.0636586   -0.0264053   -0.121612    0.0635625    0.0279861  -0.0546431    -0.0493809
  0.00104653  -0.0843855   0.122096    -0.195872     0.0642816   -0.0863035   -0.04848     -0.119897    -0.0964111    0.0284234     0.126898    0.109299    -0.159845     0.104782      0.0255199   -0.272111     0.103745    -0.00294202   0.00289262    0.024814    -0.0593733    0.0058981  -0.0892804   -0.075207    0.0586603    -0.0596992
  0.0423675   -0.085926   -0.112994     0.0909594    0.0165202    0.0017218   -0.113625    -0.0526252   -0.106965    -0.0365226     0.0603861   0.0724194   -0.10865      0.0523265    -0.118793     0.144697    -0.055022     0.0345944   -0.131296      0.0390215   -0.0718114    0.0713708  -0.193844     0.0475916  -0.0251164    -0.0670173
 -0.0682471    0.133856    0.087995     0.00580085  -0.166438    -0.0789461   -0.0031269   -0.174374     0.0139875    0.0609455    -0.0214148   0.117206    -0.0261173    0.0446308    -0.101619     0.123872    -0.0322027    0.0152112   -0.110049     -0.114131     0.150538     0.1955     -0.0277279   -0.18015    -0.0165199    -0.0795124
  0.0402158   -0.103034    0.0717856   -0.146416    -0.285044     0.0842918    0.0973876   -0.173732     0.0264264   -0.0195914     0.147206    0.0306077   -0.190933     0.0386156    -0.056721    -0.0361583   -0.0264661   -0.123311    -0.00896728    0.195922     0.158609    -0.0101269  -0.145925     0.0956151  -0.024094     -0.0276402
 -0.231341     0.0694577   0.0420309   -0.103319     0.0162551    0.00788352   0.0267862    0.094176     0.101238    -0.186946     -0.102556    0.131912    -0.107722    -0.0558623    -0.0252961   -0.00894099  -0.0203408    0.0374515    0.0301257     0.174598    -0.108082     0.0146807   0.18455     -0.0815906   0.0132225     0.0771483
  0.0106126   -0.109485    0.103332     0.0969774    0.00799845   0.0831307    0.00826543   0.047486    -0.0107764    0.0939958    -0.0154468  -0.153997    -0.0140337   -0.0110039    -0.0865467   -0.19338      0.0506794    0.0233573   -0.030795      0.0938086    0.120264    -0.10616     0.0952333    0.0311069   0.24382      -0.043949
 -0.0204166   -0.041405   -0.00590619   0.0546345   -0.009594    -0.0473966    0.0683759    0.00776068   0.03998     -0.0695198    -0.0624364  -0.0842499   -0.0828777   -0.000617214  -0.0941591   -0.0696778    0.0652057   -0.0093553   -0.0300663    -0.109363    -0.132367     0.0233393   0.033093    -0.051177    0.0698276    -0.054873
  0.0213914   -0.0253511  -0.0968827    0.0886848   -0.27778      0.0124122   -0.0843881   -0.14444     -0.10301      0.156336     -0.248331    0.00280208   0.0818911    0.131861     -0.0648217   -0.111871    -0.0812773   -0.0625523    0.0300702     0.0296804    0.0718274   -0.0260351   0.125077    -0.0153389  -0.141966     -0.0407012
 -0.0416753    0.0728359   0.0836507    0.142291    -0.00497473  -0.0618484   -0.0215897   -0.0446669    0.011318    -0.0277369     0.0603223  -0.0215388   -0.0204529   -0.0406555     0.0304921    0.195091    -0.169954    -0.0932541   -0.0385802    -0.0211114    0.163841     0.074049   -0.0821879    0.0581799   0.00125249    0.0094863
  0.142178     0.0205768  -0.0452763   -0.0342899    0.089664     0.00426239   0.0675564    0.203507    -0.0409032   -0.0701976    -0.0704792   0.0319909    0.139254     0.0473968     0.14108      0.0669705    0.0764305   -0.104907    -0.193274      0.0143227    0.0200347    0.0825537  -0.0188365    0.108105    0.000376436   0.048958
 -0.194786    -0.119944   -0.119158     0.1414       0.0731475    0.0246204   -0.0675391   -0.182287     0.175012     0.179319      0.0763587  -0.294871     0.106811     0.0530381     0.0736915    0.134107     0.114569    -0.0771925    0.126851     -0.0620465   -0.120541    -0.194604    0.0414599   -0.0204106   0.00979716    0.0885937
 -0.0816772   -0.0405869   0.166639     0.0259571    0.0239864   -0.27772     -0.101177     0.112708    -0.0987486    0.054598     -0.133735   -0.00438072   0.0126476    0.0269528    -0.0175163    0.182946    -0.161948     0.0350986   -0.197815     -0.0381948   -0.0822821   -0.0904206  -0.0178487   -0.074081    0.0449529     0.066084
 -0.0239935    0.0428035   0.0584797    0.0608846    0.0289728    0.0197356    0.111145     0.147624     0.0387796    0.0674774    -0.0104814   0.0117501    0.0480002    0.0675194     0.0477121   -0.112334     0.0116551   -0.11231      0.151244      0.0257734    0.0448599    0.154795   -0.274111     0.0152416  -0.141299      0.0330017kind diag, method split
┌ Info: 0: avll = 
└   tll[1] = -1.393840371703118
[ Info: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.393910
[ Info: iteration 2, average log likelihood -1.393839
[ Info: iteration 3, average log likelihood -1.393183
[ Info: iteration 4, average log likelihood -1.386601
[ Info: iteration 5, average log likelihood -1.370026
[ Info: iteration 6, average log likelihood -1.361777
[ Info: iteration 7, average log likelihood -1.360434
[ Info: iteration 8, average log likelihood -1.360120
[ Info: iteration 9, average log likelihood -1.359999
[ Info: iteration 10, average log likelihood -1.359937
[ Info: iteration 11, average log likelihood -1.359900
[ Info: iteration 12, average log likelihood -1.359874
[ Info: iteration 13, average log likelihood -1.359855
[ Info: iteration 14, average log likelihood -1.359838
[ Info: iteration 15, average log likelihood -1.359824
[ Info: iteration 16, average log likelihood -1.359811
[ Info: iteration 17, average log likelihood -1.359799
[ Info: iteration 18, average log likelihood -1.359788
[ Info: iteration 19, average log likelihood -1.359778
[ Info: iteration 20, average log likelihood -1.359768
[ Info: iteration 21, average log likelihood -1.359758
[ Info: iteration 22, average log likelihood -1.359749
[ Info: iteration 23, average log likelihood -1.359741
[ Info: iteration 24, average log likelihood -1.359732
[ Info: iteration 25, average log likelihood -1.359724
[ Info: iteration 26, average log likelihood -1.359717
[ Info: iteration 27, average log likelihood -1.359710
[ Info: iteration 28, average log likelihood -1.359703
[ Info: iteration 29, average log likelihood -1.359698
[ Info: iteration 30, average log likelihood -1.359693
[ Info: iteration 31, average log likelihood -1.359689
[ Info: iteration 32, average log likelihood -1.359685
[ Info: iteration 33, average log likelihood -1.359682
[ Info: iteration 34, average log likelihood -1.359680
[ Info: iteration 35, average log likelihood -1.359678
[ Info: iteration 36, average log likelihood -1.359676
[ Info: iteration 37, average log likelihood -1.359674
[ Info: iteration 38, average log likelihood -1.359673
[ Info: iteration 39, average log likelihood -1.359672
[ Info: iteration 40, average log likelihood -1.359672
[ Info: iteration 41, average log likelihood -1.359671
[ Info: iteration 42, average log likelihood -1.359671
[ Info: iteration 43, average log likelihood -1.359670
[ Info: iteration 44, average log likelihood -1.359670
[ Info: iteration 45, average log likelihood -1.359670
[ Info: iteration 46, average log likelihood -1.359669
[ Info: iteration 47, average log likelihood -1.359669
[ Info: iteration 48, average log likelihood -1.359669
[ Info: iteration 49, average log likelihood -1.359669
[ Info: iteration 50, average log likelihood -1.359669
┌ Info: EM with 100000 data points 50 iterations avll -1.359669
└ 952.4 data points per parameter
┌ Info: 1
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.3939101522760369
│     -1.3938390867550876
│      ⋮
└     -1.359668723015729
[ Info: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.359767
[ Info: iteration 2, average log likelihood -1.359681
[ Info: iteration 3, average log likelihood -1.359351
[ Info: iteration 4, average log likelihood -1.355875
[ Info: iteration 5, average log likelihood -1.344444
[ Info: iteration 6, average log likelihood -1.334135
[ Info: iteration 7, average log likelihood -1.328685
[ Info: iteration 8, average log likelihood -1.324371
[ Info: iteration 9, average log likelihood -1.320830
[ Info: iteration 10, average log likelihood -1.318318
[ Info: iteration 11, average log likelihood -1.316643
[ Info: iteration 12, average log likelihood -1.315614
[ Info: iteration 13, average log likelihood -1.314943
[ Info: iteration 14, average log likelihood -1.314456
[ Info: iteration 15, average log likelihood -1.314080
[ Info: iteration 16, average log likelihood -1.313782
[ Info: iteration 17, average log likelihood -1.313546
[ Info: iteration 18, average log likelihood -1.313365
[ Info: iteration 19, average log likelihood -1.313235
[ Info: iteration 20, average log likelihood -1.313144
[ Info: iteration 21, average log likelihood -1.313077
[ Info: iteration 22, average log likelihood -1.313025
[ Info: iteration 23, average log likelihood -1.312985
[ Info: iteration 24, average log likelihood -1.312955
[ Info: iteration 25, average log likelihood -1.312931
[ Info: iteration 26, average log likelihood -1.312913
[ Info: iteration 27, average log likelihood -1.312899
[ Info: iteration 28, average log likelihood -1.312889
[ Info: iteration 29, average log likelihood -1.312880
[ Info: iteration 30, average log likelihood -1.312873
[ Info: iteration 31, average log likelihood -1.312868
[ Info: iteration 32, average log likelihood -1.312863
[ Info: iteration 33, average log likelihood -1.312859
[ Info: iteration 34, average log likelihood -1.312855
[ Info: iteration 35, average log likelihood -1.312851
[ Info: iteration 36, average log likelihood -1.312847
[ Info: iteration 37, average log likelihood -1.312843
[ Info: iteration 38, average log likelihood -1.312839
[ Info: iteration 39, average log likelihood -1.312835
[ Info: iteration 40, average log likelihood -1.312831
[ Info: iteration 41, average log likelihood -1.312827
[ Info: iteration 42, average log likelihood -1.312823
[ Info: iteration 43, average log likelihood -1.312819
[ Info: iteration 44, average log likelihood -1.312815
[ Info: iteration 45, average log likelihood -1.312812
[ Info: iteration 46, average log likelihood -1.312808
[ Info: iteration 47, average log likelihood -1.312804
[ Info: iteration 48, average log likelihood -1.312800
[ Info: iteration 49, average log likelihood -1.312796
[ Info: iteration 50, average log likelihood -1.312792
┌ Info: EM with 100000 data points 50 iterations avll -1.312792
└ 473.9 data points per parameter
┌ Info: 2
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.35976671977139
│     -1.3596807255160106
│      ⋮
└     -1.312791513270776
[ Info: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.312956
[ Info: iteration 2, average log likelihood -1.312790
[ Info: iteration 3, average log likelihood -1.312211
[ Info: iteration 4, average log likelihood -1.306443
[ Info: iteration 5, average log likelihood -1.289569
[ Info: iteration 6, average log likelihood -1.277045
[ Info: iteration 7, average log likelihood -1.272049
[ Info: iteration 8, average log likelihood -1.269820
[ Info: iteration 9, average log likelihood -1.268471
[ Info: iteration 10, average log likelihood -1.267515
[ Info: iteration 11, average log likelihood -1.266597
[ Info: iteration 12, average log likelihood -1.265376
[ Info: iteration 13, average log likelihood -1.263514
[ Info: iteration 14, average log likelihood -1.261326
[ Info: iteration 15, average log likelihood -1.259579
[ Info: iteration 16, average log likelihood -1.258563
[ Info: iteration 17, average log likelihood -1.258064
[ Info: iteration 18, average log likelihood -1.257816
[ Info: iteration 19, average log likelihood -1.257672
[ Info: iteration 20, average log likelihood -1.257582
[ Info: iteration 21, average log likelihood -1.257525
[ Info: iteration 22, average log likelihood -1.257487
[ Info: iteration 23, average log likelihood -1.257460
[ Info: iteration 24, average log likelihood -1.257440
[ Info: iteration 25, average log likelihood -1.257425
[ Info: iteration 26, average log likelihood -1.257413
[ Info: iteration 27, average log likelihood -1.257404
[ Info: iteration 28, average log likelihood -1.257396
[ Info: iteration 29, average log likelihood -1.257390
[ Info: iteration 30, average log likelihood -1.257385
[ Info: iteration 31, average log likelihood -1.257381
[ Info: iteration 32, average log likelihood -1.257377
[ Info: iteration 33, average log likelihood -1.257374
[ Info: iteration 34, average log likelihood -1.257371
[ Info: iteration 35, average log likelihood -1.257369
[ Info: iteration 36, average log likelihood -1.257367
[ Info: iteration 37, average log likelihood -1.257365
[ Info: iteration 38, average log likelihood -1.257364
[ Info: iteration 39, average log likelihood -1.257362
[ Info: iteration 40, average log likelihood -1.257361
[ Info: iteration 41, average log likelihood -1.257360
[ Info: iteration 42, average log likelihood -1.257358
[ Info: iteration 43, average log likelihood -1.257357
[ Info: iteration 44, average log likelihood -1.257356
[ Info: iteration 45, average log likelihood -1.257356
[ Info: iteration 46, average log likelihood -1.257355
[ Info: iteration 47, average log likelihood -1.257354
[ Info: iteration 48, average log likelihood -1.257353
[ Info: iteration 49, average log likelihood -1.257353
[ Info: iteration 50, average log likelihood -1.257352
┌ Info: EM with 100000 data points 50 iterations avll -1.257352
└ 236.4 data points per parameter
┌ Info: 3
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.3129558635703955
│     -1.3127899911227048
│      ⋮
└     -1.2573519068815315
[ Info: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.257580
[ Info: iteration 2, average log likelihood -1.257349
[ Info: iteration 3, average log likelihood -1.256572
[ Info: iteration 4, average log likelihood -1.246447
[ Info: iteration 5, average log likelihood -1.218571
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.202915
[ Info: iteration 7, average log likelihood -1.203457
[ Info: iteration 8, average log likelihood -1.191448
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.183272
[ Info: iteration 10, average log likelihood -1.186929
[ Info: iteration 11, average log likelihood -1.178758
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 12, average log likelihood -1.174067
[ Info: iteration 13, average log likelihood -1.178493
[ Info: iteration 14, average log likelihood -1.169282
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 15, average log likelihood -1.165475
[ Info: iteration 16, average log likelihood -1.172947
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 17, average log likelihood -1.167063
[ Info: iteration 18, average log likelihood -1.172567
[ Info: iteration 19, average log likelihood -1.166369
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 20, average log likelihood -1.162040
[ Info: iteration 21, average log likelihood -1.165124
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 22, average log likelihood -1.155907
[ Info: iteration 23, average log likelihood -1.159489
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      9
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 24, average log likelihood -1.152466
[ Info: iteration 25, average log likelihood -1.174217
[ Info: iteration 26, average log likelihood -1.166839
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 27, average log likelihood -1.162149
[ Info: iteration 28, average log likelihood -1.165050
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 29, average log likelihood -1.156449
[ Info: iteration 30, average log likelihood -1.160131
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      9
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 31, average log likelihood -1.152678
[ Info: iteration 32, average log likelihood -1.174230
[ Info: iteration 33, average log likelihood -1.166850
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 34, average log likelihood -1.162139
[ Info: iteration 35, average log likelihood -1.164968
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 36, average log likelihood -1.156299
[ Info: iteration 37, average log likelihood -1.159926
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      9
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 38, average log likelihood -1.152576
[ Info: iteration 39, average log likelihood -1.174228
[ Info: iteration 40, average log likelihood -1.166854
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 41, average log likelihood -1.162136
[ Info: iteration 42, average log likelihood -1.164915
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 43, average log likelihood -1.156195
[ Info: iteration 44, average log likelihood -1.159793
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      9
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 45, average log likelihood -1.152515
[ Info: iteration 46, average log likelihood -1.174226
[ Info: iteration 47, average log likelihood -1.166857
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 48, average log likelihood -1.162136
[ Info: iteration 49, average log likelihood -1.164885
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 50, average log likelihood -1.156134
┌ Info: EM with 100000 data points 50 iterations avll -1.156134
└ 118.1 data points per parameter
┌ Info: 4
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.2575802540841208
│     -1.2573493865604328
│      ⋮
└     -1.1561335357612679
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.160072
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     17
│     18
│     27
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 2, average log likelihood -1.152389
[ Info: iteration 3, average log likelihood -1.158217
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      4
│     17
│     18
│     27
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.133714
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      4
│     26
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.102242
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      4
│      6
│      9
│     17
│     18
│     26
│     27
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.072079
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      4
│     26
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.081509
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      4
│     17
│     18
│     26
│     27
│     28
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.061462
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      4
│      6
│      9
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.065495
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      4
│     17
│     18
│     26
│     27
│     28
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.068265
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      4
│     26
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 11, average log likelihood -1.066732
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      4
│      6
│      9
│     17
│     18
│     26
│     27
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 12, average log likelihood -1.055918
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      1
│      4
│     26
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 13, average log likelihood -1.073868
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      4
│     17
│     18
│     26
│     27
│     28
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 14, average log likelihood -1.062959
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      4
│      6
│      9
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 15, average log likelihood -1.065052
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      4
│     17
│     18
│     26
│     27
│     28
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 16, average log likelihood -1.066774
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      1
│      4
│     26
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 17, average log likelihood -1.064642
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      4
│      6
│      9
│     17
│     18
│     26
│     27
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 18, average log likelihood -1.060867
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      4
│     26
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 19, average log likelihood -1.075991
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      1
│      4
│     17
│     18
│     26
│     27
│     28
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 20, average log likelihood -1.057441
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      4
│      6
│      9
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 21, average log likelihood -1.068971
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      4
│     17
│     18
│     26
│     27
│     28
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 22, average log likelihood -1.067900
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      1
│      4
│     26
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 23, average log likelihood -1.065563
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      4
│      6
│      9
│     17
│     18
│     26
│     27
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 24, average log likelihood -1.060780
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      4
│     26
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 25, average log likelihood -1.076019
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      1
│      4
│     17
│     18
│     26
│     27
│     28
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 26, average log likelihood -1.057458
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      4
│      6
│      9
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 27, average log likelihood -1.068961
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      4
│     17
│     18
│     26
│     27
│     28
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 28, average log likelihood -1.067874
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      1
│      4
│     26
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 29, average log likelihood -1.065562
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      4
│      6
│      9
│     17
│     18
│     26
│     27
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 30, average log likelihood -1.060758
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      4
│     26
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 31, average log likelihood -1.076012
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      1
│      4
│     17
│     18
│     26
│     27
│     28
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 32, average log likelihood -1.057446
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      4
│      6
│      9
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 33, average log likelihood -1.068952
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      4
│     17
│     18
│     26
│     27
│     28
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 34, average log likelihood -1.067861
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      1
│      4
│     26
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 35, average log likelihood -1.065553
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      4
│      6
│      9
│     17
│     18
│     26
│     27
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 36, average log likelihood -1.060745
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      4
│     26
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 37, average log likelihood -1.076001
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      1
│      4
│     12
│     17
│      ⋮
│     27
│     28
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 38, average log likelihood -1.057434
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      4
│      6
│      9
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 39, average log likelihood -1.068959
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      4
│     17
│     18
│     26
│     27
│     28
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 40, average log likelihood -1.067851
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      1
│      4
│     26
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 41, average log likelihood -1.065543
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      4
│      6
│      9
│     17
│     18
│     26
│     27
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 42, average log likelihood -1.060732
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      4
│     26
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 43, average log likelihood -1.075990
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      1
│      4
│     17
│     18
│     26
│     27
│     28
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 44, average log likelihood -1.057423
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      4
│      6
│      9
│     12
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 45, average log likelihood -1.068924
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      4
│     17
│     18
│     26
│     27
│     28
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 46, average log likelihood -1.067850
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      1
│      4
│     26
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 47, average log likelihood -1.065532
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      4
│      6
│      9
│     17
│     18
│     26
│     27
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 48, average log likelihood -1.060708
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      4
│     26
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 49, average log likelihood -1.075963
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      1
│      4
│     17
│     18
│     26
│     27
│     28
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 50, average log likelihood -1.057386
┌ Info: EM with 100000 data points 50 iterations avll -1.057386
└ 59.0 data points per parameter
┌ Info: 5
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.1600716496720187
│     -1.152389375370788
│      ⋮
└     -1.057386414850689
┌ Info: Total log likelihood: 
│   tll =
│    251-element Array{Float64,1}:
│     -1.393840371703118
│     -1.3939101522760369
│     -1.3938390867550876
│     -1.3931825999150234
│      ⋮
│     -1.0607081604897646
│     -1.0759632040687832
└     -1.057386414850689
32×26 Array{Float64,2}:
  0.0718432    -0.0615424   -0.226857   -0.186988     -0.0797058   -0.102437    -0.102847    -0.110147      0.0130504   -0.0145202    -0.0276525   -0.124197     -0.10643      -0.176244     0.179763    0.00259295   0.0189269     0.0647076    0.0385133    0.0846369   -0.201906    -0.179163   -0.0519297   -0.104252    -0.106737      0.0219777
 -0.0129988    -0.0414164    0.0100859   0.0672218    -0.0134086   -0.0519432    0.0852154    0.0103928     0.0446889   -0.0770794    -0.0630079   -0.0799401    -0.0804955    -0.0294166   -0.0964119  -0.070339     0.0706584    -0.0146054   -0.0396167   -0.115378    -0.129339     0.0172429   0.0683969   -0.0533943    0.153089     -0.078185
 -0.0289854     0.0391556    0.0609846   0.062794      0.0222102    0.0663864    0.0987007    0.147977      0.0219225    0.0666294    -0.0102492   -0.0135613     0.0306106     0.0668287    0.0556215  -0.144592     0.00930667   -0.163502     0.150809     0.026189     0.0532895    0.154964   -0.282859     0.0083528   -0.101638      0.039688
 -0.0404195    -0.0797393    0.0540276   0.0602935     0.0343996    0.0430395    0.0887699    0.0163275    -0.135086     0.220322     -0.159801     0.158929     -0.216253      0.0878571    0.0445831  -0.0380585    0.178015      0.00567685   0.0650366   -0.0975678    0.0206795    0.14896    -0.0542104   -0.0572567   -0.129757     -0.0161437
  0.0431412    -0.124652    -0.102427   -0.00354363    0.0296928    0.0662633   -0.0759533   -0.0475102    -0.023353     0.0275865     0.0662326    0.10288      -0.0242298     0.0677457   -0.0829737   0.119363     0.053202      0.0511988   -0.0780674    0.0639549   -0.0196901    0.0254343  -0.0578182   -0.00653446  -0.0147643    -0.0840328
 -0.162724     -0.117128    -0.133477    0.139947      0.0602754    0.0259102   -0.0738083   -0.171627      0.171937     0.173652      0.0783842   -0.257822      0.103595      0.027131     0.0949747   0.13156      0.09093      -0.0687684    0.117598    -0.0642864   -0.118227    -0.185055    0.039411    -0.0284256    0.0120018     0.100732
 -0.0230567    -0.0972316    0.124384   -0.21216       0.0607533   -0.0757891    0.249204    -0.179751     -0.186179     0.0200755     0.0980689    0.0437201    -0.89895       0.0893662    0.0851566  -0.266406     0.079039     -0.00331044   0.00829683  -0.00176037  -0.0582469    0.196009   -0.145798    -0.115831     0.0392645    -0.0597297
  0.000428522  -0.0766713    0.117083   -0.189055      0.0691357   -0.0766266   -0.373999    -0.120056     -0.295867     0.0358288     0.132731     0.125828      0.372075      0.0967372   -0.0212879  -0.269226     0.108725     -0.00678549  -0.0244229    0.0410441   -0.064412    -0.041494   -0.0563785   -0.0462456    0.0550811    -0.0577208
 -0.208125      0.14158      0.0619617  -0.104311     -0.0175845   -0.138954     0.015942     0.104441      0.054043     0.000124034   0.0553613    0.101949      0.183364      0.0836977    0.0448091   0.0182922   -0.068873      0.167999    -0.00566733  -0.116737    -0.047366    -0.147806    0.0167662   -0.181944    -0.137296     -0.115434
  0.182159     -0.0896321   -0.0601888   0.0499437    -0.12361      0.00787078   0.0668861   -0.149215     -0.0252799    0.00564466   -0.00181396   0.104476     -0.187772      0.233497    -0.119106   -0.0104281    0.122165     -0.154718     0.34279     -0.0586866   -0.091757    -0.0982185  -0.00778927   0.0308074   -0.000124867   0.0625028
 -0.00126375   -0.457469    -0.0619154   0.0839109    -0.0693417   -0.0351688    0.0778136    0.144545     -0.144714     0.055769     -0.0321873   -0.193389     -0.194935      0.128294     0.0476137  -0.00678782  -0.437952     -0.10568      0.0366272   -0.232222    -0.0881348    0.0192204  -0.152883     0.10579      0.0160444    -0.2586
  0.0645606     0.719382    -0.0710457  -0.000302799  -0.0704217   -0.0353056    0.0687669    0.112268      0.177044     0.200706     -0.012701     0.0255947     0.314799      0.180725     0.0536241  -0.0213543   -0.0726527    -0.0326165    0.00759579  -0.261173    -0.14106     -0.0905918  -0.0333675    0.107677    -0.00743883   -0.153041
  0.0575284    -0.0696038   -0.0137212   0.00412401    0.089224    -0.00172005   0.114614     0.147553     -0.0429087   -0.00974692    0.101729     0.00901117    0.100848     -0.00882287  -0.0107744   0.00202431   0.0645946    -0.12004     -0.171128    -0.0305965   -0.0558567    0.110579    0.012943     0.106605    -0.0503243     0.0490013
  0.0906295    -0.0367457    0.0517522   0.0429307    -0.0343227    0.117604     0.0163264    0.0394432     0.00533997   0.0601263    -0.0476888   -0.117327      0.00732082   -0.0568393   -0.0685714  -0.096861     0.0596324    -0.107129    -0.0886197   -0.0176587    0.0716493   -0.0206804   0.0662223   -0.0545061    0.0183261    -0.0111693
 -0.0752399     0.08177     -0.0924267   0.0946694    -0.0425498    0.0371477    0.133437     0.0838047     0.0170144    0.0721619    -0.12956      0.118624     -1.31746      -0.121758    -0.0531321  -0.0888192    0.0791677    -0.191544     0.0596159   -0.0498201    0.0416394    0.0512954  -0.113631     0.266837    -0.0373165     0.0726898
 -0.0909542    -0.0966084   -0.0725971   0.0946419    -0.0374596    0.0370826    0.122675    -0.000714064   0.0100752    0.100251     -0.00826062   0.114091      1.04617      -0.115271    -0.0946727   0.0602261    0.109077     -0.0526228    0.144765    -0.0182491    0.0309642    0.154609   -0.0954661    0.299095    -0.0647137     0.119779
 -0.183026      0.134166     0.102006    0.00367492   -0.100263    -0.0155946    0.0368942   -0.174867      0.034505     0.00511388   -0.0386669   -0.392418      0.000322749   0.0734042   -0.132726    0.141672    -0.0317892     0.00424317  -0.145797    -0.111066     0.148772     0.201307   -0.0220947   -0.13716     -0.0287831    -0.078692
  0.090062      0.132431     0.101543    0.00625352   -0.239737    -0.0960261   -0.00499694  -0.17494      -0.00794364   0.144893     -0.0278886    0.589224     -0.0332922     0.056721    -0.0771446   0.11927     -0.0385982     0.0249499   -0.0895453   -0.118348     0.162649     0.189455   -0.0323971   -0.287576    -0.0305143    -0.0796002
 -0.0756075     0.00548194  -0.0910026   0.108905     -0.0670582   -0.318256     0.19356     -0.0966824    -0.0310896    0.0510218    -0.137727    -0.109795     -0.0794254     0.160959    -0.119636   -0.110574     0.204286     -0.131995     0.186217     0.186771    -0.034557     0.0254842  -0.114973    -0.183436    -0.114942     -0.184177
 -0.0474688    -0.121811    -0.0120966   0.02921       0.0205651   -0.219243     0.0674239    0.112437     -0.0872542    0.101883      0.00237962   0.0528938    -0.0489682     0.0327463   -0.101033   -0.0991972    0.0254183    -0.125136    -0.0534903    0.189636    -0.0369707    0.0406697  -0.0536882   -0.188099    -0.0320339    -0.163566
 -0.0221154     0.0485662   -0.0878056   0.100966      0.0777654    0.0701701    0.0215289   -0.239715      0.0610114   -0.0775035     0.125087     0.106272     -0.0304056    -0.060424    -0.11844     0.0556317    0.00273322   -0.0332582   -0.00995305  -0.0623524   -0.0382598   -0.132465    0.0766414    0.0125974   -0.0515226    -0.0328496
  0.0430702    -0.015888    -0.0267513  -0.0162077     0.109197    -0.0366951    0.0207438   -0.0384971    -0.0474184   -0.034314      0.114061    -0.0209019     0.0108377    -0.0179166   -0.0481339   0.0444933    0.000947231   0.0080819   -0.0414797    0.00757193   0.010571     0.0169424   0.0389221   -0.0837439    0.133502      0.0728357
 -0.105915     -0.118392     0.041021    0.0158389    -0.00532005  -0.117596    -0.0492539    0.0678287     0.128935    -0.0366364    -0.00687318   0.113897     -0.049509     -0.085549     0.0461588   0.111981    -0.0865272    -0.0722957    0.0555081   -0.0524073    0.116764     0.0906742   0.0493966    0.0417692    0.0101653     0.0264544
  0.131258      0.112114     0.102565    0.116031     -0.0598475   -0.0210224    0.0668863   -0.093761      0.0192536    0.0558087     0.0471409    0.063393      0.0301664     0.154747    -0.0865226   0.0801367   -0.0452708     0.141148     0.148913     0.0606546    0.00732592   0.113139   -0.035711     0.0495845    0.189005      0.106428
  0.0288163    -0.0253543   -0.0840547   0.0734726    -0.277982     0.0118184   -0.0840566   -0.153206     -0.104589     0.163627     -0.287572     0.0125223     0.0774623     0.140305    -0.0751153  -0.100453    -0.0715395    -0.0635812    0.0305642    0.0303062    0.104748    -0.0128337   0.11969     -0.0143793   -0.141076     -0.0754158
 -0.0527678    -0.155302    -0.171804    0.0374277    -0.117517    -0.0957687    0.0939764   -0.163665      0.293131    -0.0460317     0.103314     0.128757      0.0912404     0.037282    -0.0566273  -0.0465447   -0.0114031    -0.0365539   -0.0405832    0.00272037   0.0824958    0.0721578  -0.0480696   -0.0517424   -0.0431539    -0.0119859
 -0.0966554    -0.0146623   -0.0412307  -0.0671275     0.170341    -1.62493     -0.0594392   -0.0648789     0.00357033  -0.0433092    -0.201792     0.000372274  -0.0571241    -0.0125925    0.102993   -0.0681378    0.181248      0.102502     0.0671917   -0.0923206   -0.0117987    0.199291   -0.26725      0.178593    -0.160592     -0.0320705
 -0.129063      0.0349048   -0.0481208  -0.0670318     0.0245069    1.31087     -0.0928346    0.0142768     0.00420275  -0.034917     -0.149066     0.00877876   -0.0643464    -0.0225793   -0.0686822  -0.0671359    0.180607      0.161405     0.0214343   -0.0935484   -0.00226669   0.152928    0.126686     0.178836     0.0188624    -0.121005
 -0.213487      0.0507077    0.0417299  -0.104192      0.0350314    0.0185528    0.0340307    0.10461       0.0991486   -0.16446      -0.0857223    0.109679     -0.108857     -0.0541589   -0.0385596   0.0092522   -0.0165927     0.0460031    0.0364709    0.171395    -0.106741     0.0263419   0.183465    -0.0773769    0.0142976     0.0778153
 -0.0714525    -0.0578256    0.134385    0.0265345     0.0149481   -0.276808    -0.123547     0.112836     -0.113005     0.0418667    -0.161463     0.0593472     0.014843      0.0265446   -0.012774    0.185824    -0.130564     -0.00982713  -0.185678    -0.0379591   -0.0673379   -0.128625   -0.0225771   -0.0612971    0.0272982     0.0661294
  0.143068     -0.0890266   -0.0489775  -0.172149      0.230367     0.219998    -0.150751     0.019344     -0.0752246   -0.192532      0.157553     0.129312      0.0045362     0.0904806   -0.0610401  -0.105334    -0.0711711    -0.0289655    0.0571702   -0.134071     0.101385    -0.0094898   0.154315    -0.0459857   -0.0479212     0.0584143
  0.0375246    -0.101409     0.0720761  -0.136215     -0.278723     0.0814704    0.106864    -0.169694      0.00474748  -0.0474718     0.147284     0.0276059    -0.186681      0.0418629   -0.0560844  -0.0301015   -0.00954045   -0.0990502   -0.00121924   0.19541      0.159205    -0.0182598  -0.138843     0.0923685   -0.0537842    -0.0301544[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      4
│      6
│      9
│     12
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 1, average log likelihood -1.068860
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      4
│      6
│      9
│     12
│      ⋮
│     27
│     28
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 2, average log likelihood -1.051878
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      1
│      4
│      6
│      9
│     12
│     26
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 3, average log likelihood -1.057837
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      4
│      6
│      9
│     12
│      ⋮
│     27
│     28
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.058995
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      4
│      6
│      9
│     12
│     20
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.055747
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      6
│      9
│     12
│     17
│      ⋮
│     28
│     29
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.049467
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      1
│      4
│      6
│      9
│     12
│     20
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.047743
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      6
│      9
│     12
│     17
│      ⋮
│     27
│     28
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.052284
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      4
│      6
│      9
│     12
│     20
│     26
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.049753
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      4
│      6
│      9
│     12
│      ⋮
│     27
│     28
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.055504
┌ Info: EM with 100000 data points 10 iterations avll -1.055504
└ 59.0 data points per parameter
kind diag, method kmeans
[ Info: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.249029e+05
      1       6.665017e+05      -2.584012e+05 |       32
      2       6.435932e+05      -2.290848e+04 |       32
      3       6.281468e+05      -1.544643e+04 |       32
      4       6.137526e+05      -1.439418e+04 |       32
      5       6.036792e+05      -1.007342e+04 |       32
      6       5.985952e+05      -5.083936e+03 |       32
      7       5.963477e+05      -2.247495e+03 |       32
      8       5.952026e+05      -1.145077e+03 |       32
      9       5.943451e+05      -8.575417e+02 |       32
     10       5.936427e+05      -7.024303e+02 |       32
     11       5.930798e+05      -5.628988e+02 |       32
     12       5.926006e+05      -4.791440e+02 |       32
     13       5.921177e+05      -4.829576e+02 |       32
     14       5.915156e+05      -6.020657e+02 |       32
     15       5.908015e+05      -7.140950e+02 |       32
     16       5.901892e+05      -6.123259e+02 |       32
     17       5.897461e+05      -4.431161e+02 |       32
     18       5.893673e+05      -3.788166e+02 |       32
     19       5.890064e+05      -3.608577e+02 |       32
     20       5.885729e+05      -4.335406e+02 |       32
     21       5.881022e+05      -4.706804e+02 |       32
     22       5.875028e+05      -5.993988e+02 |       32
     23       5.869510e+05      -5.517450e+02 |       32
     24       5.866827e+05      -2.683783e+02 |       32
     25       5.866021e+05      -8.059274e+01 |       32
     26       5.865682e+05      -3.386009e+01 |       31
     27       5.865483e+05      -1.986743e+01 |       32
     28       5.865319e+05      -1.645815e+01 |       30
     29       5.865149e+05      -1.700005e+01 |       31
     30       5.864985e+05      -1.633718e+01 |       31
     31       5.864788e+05      -1.974371e+01 |       29
     32       5.864608e+05      -1.802671e+01 |       29
     33       5.864329e+05      -2.789877e+01 |       31
     34       5.863981e+05      -3.476609e+01 |       29
     35       5.863693e+05      -2.883881e+01 |       31
     36       5.863350e+05      -3.425615e+01 |       30
     37       5.863063e+05      -2.872411e+01 |       29
     38       5.862765e+05      -2.974873e+01 |       30
     39       5.862448e+05      -3.174675e+01 |       31
     40       5.862167e+05      -2.810859e+01 |       30
     41       5.861887e+05      -2.794991e+01 |       31
     42       5.861635e+05      -2.525305e+01 |       31
     43       5.861411e+05      -2.237865e+01 |       30
     44       5.861232e+05      -1.794685e+01 |       29
     45       5.861040e+05      -1.911975e+01 |       29
     46       5.860850e+05      -1.905317e+01 |       29
     47       5.860693e+05      -1.563280e+01 |       29
     48       5.860534e+05      -1.596386e+01 |       29
     49       5.860404e+05      -1.295534e+01 |       27
     50       5.860292e+05      -1.117933e+01 |       25
K-means terminated without convergence after 50 iterations (objv = 586029.2497862987)
┌ Info: K-means with 32000 data points using 50 iterations
└ 37.0 data points per parameter
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.305995
[ Info: iteration 2, average log likelihood -1.277038
[ Info: iteration 3, average log likelihood -1.248572
[ Info: iteration 4, average log likelihood -1.221015
[ Info: iteration 5, average log likelihood -1.189132
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     12
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.137470
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.088360
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      5
│     17
│     20
│     21
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.064609
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      9
│     19
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.101438
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     11
│     12
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.104751
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     23
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 11, average log likelihood -1.085765
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     21
│     30
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 12, average log likelihood -1.061198
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      5
│      9
│     17
│     20
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 13, average log likelihood -1.060174
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     12
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 14, average log likelihood -1.098808
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     11
│     19
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 15, average log likelihood -1.061100
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     21
│     23
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 16, average log likelihood -1.052657
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      5
│      9
│     12
│     20
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 17, average log likelihood -1.058445
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     17
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 18, average log likelihood -1.098783
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 19, average log likelihood -1.066081
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     11
│     19
│     21
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 20, average log likelihood -1.038611
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      9
│     12
│     20
│     23
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 21, average log likelihood -1.064026
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 22, average log likelihood -1.092387
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     17
│     24
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 23, average log likelihood -1.052443
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      9
│     19
│     21
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 24, average log likelihood -1.048599
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     11
│     12
│     20
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 25, average log likelihood -1.076497
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      5
│     23
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 26, average log likelihood -1.088195
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     17
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 27, average log likelihood -1.066767
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      9
│     12
│     21
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 28, average log likelihood -1.047924
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     11
│     19
│     20
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 29, average log likelihood -1.077940
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 30, average log likelihood -1.088017
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     23
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 31, average log likelihood -1.066250
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      9
│     12
│     17
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 32, average log likelihood -1.049601
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     20
│     21
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 33, average log likelihood -1.061398
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      5
│     11
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 34, average log likelihood -1.061468
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      9
│     24
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 35, average log likelihood -1.070131
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     23
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 36, average log likelihood -1.077712
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     12
│     17
│     21
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 37, average log likelihood -1.040641
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      5
│     19
│     20
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 38, average log likelihood -1.066190
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      9
│     11
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 39, average log likelihood -1.074939
[ Info: iteration 40, average log likelihood -1.079868
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│     12
│     17
│     21
│     23
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 41, average log likelihood -1.029286
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      5
│      9
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 42, average log likelihood -1.077173
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     20
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 43, average log likelihood -1.092279
[ Info: iteration 44, average log likelihood -1.086960
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     11
│     12
│     21
│     23
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 45, average log likelihood -1.037887
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      5
│      9
│     17
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 46, average log likelihood -1.037659
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     19
│     20
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 47, average log likelihood -1.092047
[ Info: iteration 48, average log likelihood -1.105347
[ Info: iteration 49, average log likelihood -1.070231
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      9
│     12
│     21
│     23
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 50, average log likelihood -1.028764
┌ Info: EM with 100000 data points 50 iterations avll -1.028764
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
  0.0289706   -0.0253816  -0.08466      0.0736367   -0.277911    0.0117124   -0.0840014  -0.154028    -0.104398     0.16314     -0.287408     0.0125248    0.0776727    0.140724   -0.0757188   -0.10002     -0.0716119    -0.0634956    0.0304682     0.0302371    0.103436    -0.0149553    0.119266    -0.0144282   -0.141113     -0.0753665
  0.0380659   -0.100573    0.0710167   -0.136877    -0.279012    0.081907     0.105092   -0.170378     0.00651144  -0.0468769    0.147405     0.02826     -0.185785     0.0434802  -0.0565409   -0.0304671   -0.0105674    -0.0986062    0.000221439   0.194714     0.159173    -0.0170978   -0.138008     0.0941548   -0.0600699    -0.0297131
  0.0323232    0.155251   -0.0667744    0.0398698   -0.070048   -0.0353753    0.0733107   0.128553     0.0215985    0.130541    -0.0222626   -0.0792478    0.0694933    0.154461    0.0501299   -0.0151524   -0.248514     -0.0682425    0.0212786    -0.247321    -0.114745    -0.0380628   -0.0921802    0.106719     0.00365684   -0.203097
 -0.0532069    0.0465828  -0.00280157   0.0922183    0.0010361  -0.333803     0.10809     0.0923347   -0.067854     0.179774    -0.0402916    1.29122     -0.0800445    0.0435469  -0.109204    -0.156128     0.135069     -0.0867539    0.0230077     0.330152    -0.0403586   -0.0251971   -0.117892    -0.169063    -0.0538938    -0.184266
 -0.168006    -0.116481   -0.134904     0.143019     0.0599004   0.0248544   -0.0628085  -0.174834     0.173303     0.175638     0.0795731   -0.269606     0.10595      0.0275189   0.0914193    0.132003     0.0935007    -0.0766311    0.118236     -0.0623183   -0.120763    -0.186703     0.0379166   -0.0294787    0.0135227     0.100726
  0.131736     0.111958    0.102834     0.116359    -0.0610436  -0.0225557    0.0669261  -0.0940885    0.0174891    0.0557222    0.0480559    0.0683607    0.0299077    0.155198   -0.0876378    0.0807532   -0.0454465     0.142743     0.149352      0.0606415    0.00604978   0.11261     -0.0348167    0.0479036    0.189827      0.10607
  0.208842     0.0538791  -0.0173801   -0.0653722   -0.094443    0.164516     0.0163236   0.0279601    0.0208855    0.0317034   -0.0924798   -0.0517114    0.0379282   -0.0900284  -0.0290688    0.0355256    0.0758023    -0.27121     -0.124562     -0.161121     0.0763677   -0.0109634    0.0326074   -0.105691    -0.211115      0.0275563
 -0.0893483   -0.0908111  -0.0814318    0.123187    -0.0377623   0.0389969    0.119645   -0.0028311    0.00948621   0.104426    -0.01495      0.104916     1.42775     -0.115503   -0.0973978    0.0710372    0.107559     -0.0538589    0.139217     -0.0204916    0.0335366    0.137528    -0.095903     0.286708    -0.0624058     0.13385
 -0.0420325   -0.0793047   0.0546956    0.0611097    0.0382919   0.0418612    0.0893487   0.0159277   -0.134114     0.220464    -0.160616     0.156599    -0.219566     0.0854403   0.0502564   -0.0407503    0.182413      0.00974563   0.0663389    -0.0965265    0.0173479    0.148769    -0.0559194   -0.0577385   -0.131351     -0.0176429
 -0.0772036   -0.0717356  -0.074116     0.084486    -0.0586647  -0.271624     0.186067   -0.0904417   -0.0394758    0.0159565   -0.10999     -0.734128    -0.0656995    0.158331   -0.112311    -0.0916502    0.156849     -0.160513     0.14388       0.150504    -0.0327139    0.0576662   -0.0942682   -0.194915    -0.115416     -0.177776
  0.154215     0.0248802  -0.0289537   -0.0167816    0.0658923   0.00352022   0.105227    0.194752    -0.0408388   -0.036713    -0.054207     0.0957524    0.147883     0.0390932   0.115307     0.0619726    0.0730017    -0.103903    -0.20193       0.0133192   -0.0423262    0.111992     0.0127504    0.121968    -0.000504111   0.0625539
 -0.0535157   -0.156119   -0.177018     0.0388414   -0.121697   -0.0930898    0.0961624  -0.158381     0.293933    -0.0462057    0.100305     0.129227     0.0928627    0.0326996  -0.0524781   -0.0467942   -0.0137829    -0.0332697   -0.0427815     0.00515952   0.0906051    0.0738892   -0.0507249   -0.0525936   -0.0377745    -0.0110555
 -0.108572    -0.198865   -0.766264    -0.175128     0.0889702  -0.0764659   -0.0759312   0.0700678    0.665973    -1.5975      -0.204033     0.163652    -0.0323348   -0.117983    0.0912408   -0.072366    -0.24604      -0.129673    -0.188423     -0.0473579    0.195692     0.223374     0.0850281    0.148214    -0.0297556    -0.13581
 -0.0113393   -0.0415337   0.0108389    0.0659191   -0.0127293  -0.0515666    0.086002    0.0099889    0.0450994   -0.0765607   -0.0632383   -0.0819679   -0.0808404   -0.0292284  -0.0971519   -0.0745231    0.0677961    -0.0137844   -0.0428678    -0.110504    -0.132608     0.0181345    0.0660781   -0.0531441    0.163075     -0.0810856
 -0.145196    -0.0102532   0.0890293   -0.0415455    0.0272704  -0.128009    -0.0441173   0.105065    -0.00741137  -0.0632517   -0.122337     0.0792915   -0.0467351   -0.0141817  -0.0316104    0.0930731   -0.0735967     0.019248    -0.07327       0.0678146   -0.0856428   -0.054087     0.0830961   -0.0689057    0.0223018     0.0735202
  0.0445948   -0.125509   -0.105717    -0.0051256    0.0314366   0.0678944   -0.0847535  -0.0516529   -0.0235679    0.0313397    0.0718653    0.102701    -0.0237285    0.0777688  -0.0864585    0.119828     0.053012      0.0553544   -0.0837331     0.0645602   -0.0224333    0.0224796   -0.0589561   -0.010619    -0.00694371   -0.0862893
 -0.112068     0.014027   -0.046328    -0.0655529    0.0857676   0.104722    -0.0839215  -0.0105419    0.00405523  -0.0406292   -0.170236     0.00439033  -0.0616225   -0.0179162   0.00481564  -0.0671073    0.178806      0.140969     0.0390415    -0.0931386   -0.00678145   0.170216    -0.0314153    0.177483    -0.0590997    -0.0748726
 -0.0423819    0.0725841   0.0952747    0.144587     0.0740823  -0.0679758   -0.0261599  -0.0415011    0.031396    -0.0319129    0.0592584   -0.0199656   -0.0287919   -0.0396254   0.0180644    0.187527    -0.174475     -0.109286    -0.0136712    -0.00114601   0.164333     0.0626343   -0.0742524    0.040391    -0.00352795    0.01908
 -0.0165333    0.108407    0.0869813    0.00534092  -0.171671   -0.0559608    0.0148996  -0.157576    -0.00661454   0.0766117   -0.0221208    0.111863    -0.0134935    0.0608811  -0.10742      0.1134      -0.0482617     0.0139146   -0.126977     -0.124794     0.134086     0.190681    -0.0218799   -0.202396     0.00183829   -0.0726968
 -0.208389     0.137647    0.0643996   -0.0969963   -0.0126324  -0.138742     0.0150949   0.10473      0.0509126    0.0031544    0.0670415    0.089396     0.182693     0.0885529   0.0356087    0.0153311   -0.0707998     0.168703    -0.0070885    -0.117996    -0.0515924   -0.148172     0.0139285   -0.181706    -0.137263     -0.116545
  0.0786692   -0.0620494  -0.235619    -0.191344    -0.0813363  -0.0980385   -0.103644   -0.110679     0.00968995  -0.0140723   -0.0277576   -0.133731    -0.107098    -0.174621    0.180166     0.0028666    0.0226953     0.0666287    0.029703      0.0879309   -0.20775     -0.190853    -0.0592547   -0.104978    -0.108831      0.0254725
 -0.027558     0.0409203   0.0608746    0.0631053    0.0235252   0.0696452    0.101427    0.149664     0.0210052    0.066906    -0.010091    -0.018524     0.0300999    0.0668674   0.055196    -0.14506      0.00598538   -0.162383     0.149221      0.029075     0.0537053    0.155988    -0.286915     0.00501501  -0.105563      0.0366561
  0.151076    -0.0831459  -0.0424143   -0.174534     0.249823    0.227271    -0.162061    0.00733309  -0.0862583   -0.176948     0.170902     0.123229     0.00305448   0.113561   -0.0708589   -0.112629    -0.0871298    -0.0159609    0.0740468    -0.129616     0.112018    -0.00202798   0.163799    -0.0490272   -0.0386363     0.0644643
 -0.00756382   0.0392461   0.0167855   -0.0241934    0.231495   -0.0881263    0.0949643  -0.0934633   -0.0158944   -0.100887     0.163445    -0.116205     0.0104564   -0.0472085  -0.0137022    0.0458803    0.101974     -0.0587401   -0.0494277     0.135419     0.0334328    0.00271277  -0.00310114  -0.109286     0.157869      0.0457259
 -0.0807917    0.0660735  -0.0816561    0.0867586   -0.0407255   0.0442981    0.132304    0.0711528    0.0143047    0.0753669   -0.11478      0.123649    -1.4343      -0.12062    -0.0587729   -0.0826996    0.0851349    -0.169479     0.0687858    -0.0435515    0.0364834    0.076292    -0.11127      0.279953    -0.041739      0.0756928
 -0.0107147   -0.0859033   0.120722    -0.199818     0.0638557  -0.075947    -0.0750025  -0.149014    -0.243967     0.0279488    0.115469     0.0858361   -0.239154     0.0941049   0.0302238   -0.26793      0.0930767    -0.00525812  -0.00833926    0.0209773   -0.0612377    0.0735243   -0.0995147   -0.080254     0.0480034    -0.0587815
  0.181028    -0.0922984  -0.0599969    0.0486715   -0.124297    0.00680662   0.0664494  -0.149273    -0.0258936    0.00600248  -0.00178798   0.105974    -0.187136     0.234954   -0.120848    -0.00951121   0.124908     -0.152785     0.341912     -0.0589573   -0.093972    -0.0980168   -0.00778188   0.0324845   -0.000852106   0.0610511
 -0.0229782    0.04998    -0.085414     0.101353     0.0772915   0.0718268    0.0260254  -0.253632     0.0615057   -0.0770532    0.126044     0.101722    -0.0326059   -0.0619442  -0.122655     0.0573816    0.000111306  -0.0321393   -0.0117494    -0.0634574   -0.0347304   -0.129211     0.0761976    0.0107292   -0.0518474    -0.0339354
 -0.115868    -0.244989   -0.0258821   -0.192955     0.0119467  -0.126839    -0.0576814   0.229866     0.107418     0.117043    -0.0373723    0.197623    -0.039531    -0.117562    0.0949567    0.0628863   -0.0190074    -0.0177543   -0.0263129    -0.059393     0.054774     0.127296     0.0530468    0.0997203   -0.0679043    -0.0667975
 -0.0280142   -0.140702    0.0635903    0.0886838    0.0566348   0.0454091    0.0608424   0.0636824   -0.0201073    0.0529373    0.118369    -0.131398     0.00743154  -0.0482118  -0.130359    -0.146381     0.0504207    -0.0492236   -0.0794585     0.0158532   -0.00249924   0.0259503    0.0569734    0.0274161    0.0651685    -0.00679604
 -0.144805    -0.181649    0.0701972    0.0616889   -0.0954284  -0.143344    -0.0670099   0.0499718    0.146824    -0.0927576   -0.0297482    0.191944    -0.0749219   -0.0900436   0.0322124    0.0778093   -0.0405318    -0.0479897    0.21603      -0.0882741    0.0907402    0.054378     0.165856    -0.0144851    0.0807542     0.121916
  0.131624    -0.0955758  -0.139337     0.00306792   0.0114303   0.0717351   -0.0946775   0.0224901   -0.102091     0.097221     0.0759358    0.0799862    0.021626     0.033118   -0.130743     0.0450614   -0.137837      0.129671    -0.0789983    -0.221266    -0.0261029    0.0262196    0.112683    -0.0446189    0.112923      0.0901228[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      5
│     11
│     17
│     20
│     24
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 1, average log likelihood -1.055206
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      5
│     11
│     17
│     19
│     20
│     24
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 2, average log likelihood -1.021978
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      5
│      9
│     11
│     17
│     20
│     21
│     24
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 3, average log likelihood -1.008135
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      5
│     11
│     12
│     17
│      ⋮
│     24
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.024765
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      5
│     11
│     17
│     20
│     24
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.038903
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      5
│      9
│     11
│     17
│      ⋮
│     24
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.004383
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      5
│     11
│     12
│     17
│     20
│     24
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.035171
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      5
│     11
│     17
│     19
│      ⋮
│     24
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.022338
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      5
│      9
│     11
│     17
│     20
│     21
│     24
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.015757
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      5
│     11
│     12
│     17
│      ⋮
│     24
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.027709
┌ Info: EM with 100000 data points 10 iterations avll -1.027709
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.0387413    0.0976802     0.0260436    0.173495    -0.006989     -0.120316    -0.0561973   -0.00515277   0.0785632    0.0513871    0.186591    -0.0501062    0.0373996   -0.156222     0.0454005    -0.0165995     0.0478367     0.150791     -0.0923837    0.0255539    0.095216   -0.100454     0.0320652   -0.0814686    0.0965779    -0.08712
 -0.019199     0.101306      0.0215272   -0.0160956   -0.137225     -0.0323477   -0.0296382    0.0519552   -0.130066     0.115108    -0.127279     0.171491    -0.115858    -0.0500876    0.167926     -0.0559504     0.229507     -0.0189461     0.177962     0.071137    -0.10706     0.0461438   -0.0550171   -0.0536641   -0.0660259    -0.0667165
  0.0603492    0.0486805     0.154881     0.0708195   -0.0745677     0.0702736   -0.052926     0.00142646  -0.0488799    0.0842319    0.0802308   -0.0514082   -0.064135     0.147479     0.051916      0.0674043     0.0409938     0.00661765    0.168125     0.145707    -0.101351   -0.131921    -0.0129449    0.0346103   -0.121572     -0.18889
  0.250037     0.102439     -0.0114093   -0.126805    -0.026392     -0.311746    -0.0130492   -0.0322643   -0.106071     0.0362446    0.107065     0.0723606    0.190092    -0.211343    -0.0067005    -0.148341     -0.0833677     0.0104445     0.121854     0.0939175    0.0566714  -0.202906    -0.108422     0.150335     0.148866     -0.0450197
 -0.0597379   -0.0112411    -0.0538467    0.0546513   -0.0675353    -0.205998    -0.0230341    0.0888729    0.136144     0.0977466   -0.283422    -0.261219    -0.0171369    0.0260185    0.0336558     0.0870597    -0.160957      0.0346093     0.0618495    0.102322     0.0178266  -0.093497     0.0974835    0.0827142    0.0107696     0.0107533
 -0.0201192    0.0510515     0.22428      0.166598     0.121339      0.00928242  -0.0170757   -0.0287282    0.135501     0.0950299    0.18918     -0.109553     0.0594172   -0.0623279    0.116504      0.000879312   0.250961     -0.0375237    -0.128457    -0.0268964   -0.0979689   0.0782773   -0.141591     0.11523      0.0405849     0.128126
 -0.0578433    0.0398003    -0.0149571   -0.0531552    0.137791      0.221507    -0.0249416    0.0972956   -0.195522     0.0975829   -0.104511     0.0568938    0.0967111    0.15853      0.0254486    -0.214989     -0.166987      0.0177436    -0.021999     0.0671205   -0.0595452   0.124424    -0.123844    -0.12827      0.0838883    -0.034782
  0.0180002   -0.0302124    -0.0343997   -0.0762533   -0.106905     -0.0690815   -0.116921     0.0868505    0.0521054   -0.201431    -0.0759025   -0.0972956   -0.00559957   0.0348667   -0.0835095     0.0277827    -0.133065      0.193654      0.0825       0.00418039  -0.19008     0.17046     -0.121182     0.0229078   -0.0666833    -0.0337316
 -0.00630972   0.0419481    -0.0227221   -0.0515439   -0.0416439     0.205085     0.0346122    0.0875596   -0.1287       0.0713585   -0.0212268   -0.232966    -0.0314106   -0.0377435   -0.0569553    -0.13139      -0.1835       -0.0690603    -0.131089    -0.0983414   -0.0668913   0.118241    -0.0337483    0.0925165    0.0616691     0.073284
  0.0653032    0.239581      0.0106372   -0.0409282    0.121754      0.200834     0.0847486   -0.0514676    0.222101     0.00400549  -0.0418525    0.16111     -0.184262     0.00190975  -0.000311261  -0.0448035    -0.167438     -0.0560967     0.0598575   -0.109171    -0.0800256  -0.0585479    0.207964     0.0716513    0.0811777    -0.00556589
  0.0599856   -0.0225773    -0.07915     -0.166354     0.143111      0.0945599   -0.109667    -0.0301102   -0.0908847    0.0291392   -0.162712     0.094842    -0.0892841   -0.111016    -0.0312443    -0.000454172  -0.0409514     0.0780704     0.13608     -0.0947588   -0.0109612   0.138777     0.0931649    0.185649     0.220907     -0.112276
 -0.0310955    0.134137      0.133286     0.0228818    0.0232424    -0.0378944   -0.145738    -0.124176     0.0547191   -0.191919     0.00884115   0.26382      0.160128    -0.0459247   -0.164232     -0.242467      0.0277989     0.0127211     0.00288328   0.0264726    0.0322026   0.097935    -0.125724     0.0372144    0.0901068    -0.197075
  0.00650383  -0.00584562   -0.0930817    0.0693606   -0.0126948     0.0946035    0.0764552   -0.116422     0.00204721   0.0104559    0.0972278   -0.0645283   -0.0888307    0.114034     0.0300871    -0.109867      0.0224781     0.0993723     0.0366309    0.032707     0.0413647  -0.303411    -0.164088    -0.0250116   -0.0211086    -0.0403489
 -0.0120326   -0.113605      0.204036     0.130723     0.0811827     0.0604484    0.00698557  -0.0241932    0.00145678  -0.0642248    0.00192556   0.0760408    0.0789411   -0.144323     0.113765     -0.181508      0.0214212    -0.164864      0.041169    -0.0363058   -0.112256    0.0827244   -0.134584     0.0287395    0.041697     -0.0648334
  0.199012    -0.00184711    0.241481     0.0686491    0.105522      0.118969    -0.0660674   -0.219238     0.0963282   -0.166958    -0.061279    -0.0136452    0.0142182   -0.0582957   -0.0404821    -0.0307611    -0.0741828    -0.0315106    -0.0247717    0.00580553   0.0540761  -0.0558327    0.0996945   -0.0912899   -0.145678     -0.141689
  0.132406    -0.157132     -0.0856937   -0.199459     0.0567427    -0.109178     0.0728095    0.204765    -0.00675347  -0.0527759   -0.113857     0.00552187  -0.0340361    0.200602    -0.212557      0.040141      0.0054023     0.131171     -0.0755016   -0.00529678   0.0211961  -0.0373622   -0.159501     0.153926    -0.000803735   0.0592744
  0.104056    -0.146752     -0.111838     0.0493925    0.00188221   -0.068505     0.0218035    0.0802343   -0.0891882   -0.0126342    0.0178939    0.145729     0.060343     0.111566    -0.000103265  -0.0765555    -0.0394666    -0.0682332    -0.0162336   -0.218843     0.148579    0.0917502    0.0985321   -0.092108    -0.0240102     0.0248922
 -0.11733     -0.0214356     0.0369975   -0.00199997   0.0302629     0.0131762   -0.110202     0.110477    -0.0191071    0.0480529    0.0318076    0.0169538   -0.00935529  -0.101641     0.00886697   -0.00999289   -0.0311746    -0.000437742   0.0024321    0.0794786   -0.171624    0.089163    -0.168119    -0.223165    -0.00396646   -0.0740818
 -0.0620255    0.00808657    0.0636068   -0.012772    -0.0784607     0.0150441    0.0311434   -0.105403     0.112131     0.0708843   -0.340601    -0.0022446    0.0521182   -0.0328141    0.113026      0.106589      0.112725     -0.0514637    -0.0314601   -0.098566     0.0280343  -0.104817     0.030095     0.0940542   -0.0317037     0.0883752
  0.192393    -0.108435      0.00152813   0.13732     -0.0134172    -0.130337    -0.104392    -0.0205888    0.0150884    0.0129265    0.00190896   0.0109075    0.0187903   -0.120637    -0.154136     -0.0148707    -0.000747186  -0.0291172    -0.0493138    0.0923795    0.0320239   0.0577391    0.181962    -0.00441991   0.0216448     0.0427848
 -0.0917074    0.00563563    0.241339     0.121323     0.0508346    -0.130066    -0.172495    -0.00453539   0.0113975    0.0253154   -0.233177    -0.142967     0.158954     0.0867813   -0.0298702    -0.0931127     0.00849578   -0.0428691    -0.0130663   -0.111349     0.0417126   0.00263273   0.0458918   -0.0425889   -0.100767      0.0663578
  0.0656082    0.0124121     0.0139918   -0.0899684   -0.0920219     0.0295217   -0.054996     0.0667384   -0.175344     0.179446    -0.0593173   -0.0319042    0.152455     0.0640952   -0.0133758    -0.111782     -0.0538167     0.0972576    -0.0761166   -0.0544456    0.178917    0.021705    -0.0582959    0.133032     0.117997     -0.186048
 -0.0352797   -0.0380653    -0.0762254    0.153958    -0.0553546     0.0495038    0.1667      -0.0441019    0.0319261   -0.0110242   -0.0637278   -0.00977667  -0.057972    -0.129301     0.0838733     0.0295337     0.0488273    -0.0645453    -0.0358232    0.085706     0.108845   -0.155867     0.197061     0.0155815    0.15774       0.076041
 -0.0466241    0.0301467    -0.0601936   -0.137107    -0.00077815   -0.17274      0.0847301   -0.0343684   -0.0234806   -0.0171656   -0.068582     0.032496    -0.0625817    0.174369     0.0231978     0.00252513    0.0269366    -0.0327191     0.0869589    0.188338    -0.0807915   0.0190241   -0.121502     0.174952     0.147105      0.0542874
  0.100795     0.263062      0.0949171   -0.13873     -0.0769092     0.141027     0.0996779   -0.00518477   0.0259272   -0.0472356    0.0276827    0.105784     0.111931    -0.183692     0.0909627     0.141862      0.0564762    -0.206723      0.127279    -0.0629895   -0.055697   -0.0879075   -0.0333863   -0.0138138   -0.0877725    -0.0226142
  0.013613    -0.101226      0.0898757   -0.0518903   -0.0338434     0.0258814   -0.128534    -0.150848     0.126716    -0.00237696  -0.161358     0.194047    -0.125965     0.0548648    0.129806      0.0292898     0.00325088   -0.0436061    -0.0130699    0.0766507    0.0329827   0.0640303    0.0799019   -0.0988524   -0.0110586     0.291229
 -0.144165     0.0983927    -0.161588     0.080686     0.0300082    -0.155619    -0.0263977    0.0306208   -0.104139    -0.0953201   -0.0824972   -0.262388     0.0162627   -0.0326225    0.0184554     0.0174629    -0.189921     -0.000748726  -0.116669     0.129909     0.068481    0.0512263   -0.0347474    0.0543143   -0.0238549     0.00980822
  0.148366    -0.142661      0.0562637   -0.145578    -0.000597494   0.0548352    0.124566     0.107541     0.0567559    0.0733041    0.0307263   -0.00119884   0.121326    -0.0524765    0.0929605    -0.153225     -0.0829933    -0.069509     -0.00643482  -0.0655055    0.0117414  -0.0742806    0.00315621   0.231153    -0.127794      0.0950672
 -0.116951     0.152124      0.10638     -0.0329673   -0.0696031     0.106881     0.0370518   -0.0749967    0.0624774    0.203241     0.00754563  -0.0222343    0.130331    -0.082995     0.107505      0.21699      -0.136023      0.0421884     0.0214493   -0.045934     0.104812    0.060419    -0.0839982   -0.180095    -0.0722688     0.085118
 -0.134024     0.11897       0.0828868    0.0287391   -0.0566383     0.0359161   -0.265212    -0.0344617    0.101014     0.07472      0.0902277   -0.158463     0.0645632    0.0124138    0.104432     -0.17815      -0.0576417    -0.055173      0.0301545    0.136116     0.0192272  -0.00250729   0.046313     0.0334782    0.0353555     0.0919617
  0.0313564    0.000193234  -0.0555465    0.0226934   -0.184649     -0.0231976    0.0561138   -0.0358978    0.0839962   -0.171762     0.0297624    0.159555    -0.0867935   -0.00366929  -0.0642634    -0.116033     -0.0267747    -0.198413      0.0291014    0.084343    -0.234356   -0.0450544    0.0583626    0.161047     0.0478289    -0.0788792
  0.0134547   -0.0830094    -0.0385697   -0.096816    -0.0524862     0.0489854   -0.0152882   -0.0882031    0.132886    -0.0466033    0.134481    -0.0128899   -0.0913333    0.09579     -0.018854     -0.169466     -0.0656032     0.0340762     0.0364721   -0.0115692    0.0790788  -0.318441     0.047985    -0.0852398   -0.0174599    -0.142144kind full, method split
┌ Info: 0: avll = 
└   tll[1] = -1.4250924674522372
[ Info: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.425111
[ Info: iteration 2, average log likelihood -1.425043
[ Info: iteration 3, average log likelihood -1.424982
[ Info: iteration 4, average log likelihood -1.424907
[ Info: iteration 5, average log likelihood -1.424811
[ Info: iteration 6, average log likelihood -1.424698
[ Info: iteration 7, average log likelihood -1.424573
[ Info: iteration 8, average log likelihood -1.424436
[ Info: iteration 9, average log likelihood -1.424270
[ Info: iteration 10, average log likelihood -1.424020
[ Info: iteration 11, average log likelihood -1.423592
[ Info: iteration 12, average log likelihood -1.422884
[ Info: iteration 13, average log likelihood -1.421912
[ Info: iteration 14, average log likelihood -1.420921
[ Info: iteration 15, average log likelihood -1.420203
[ Info: iteration 16, average log likelihood -1.419813
[ Info: iteration 17, average log likelihood -1.419636
[ Info: iteration 18, average log likelihood -1.419561
[ Info: iteration 19, average log likelihood -1.419529
[ Info: iteration 20, average log likelihood -1.419516
[ Info: iteration 21, average log likelihood -1.419510
[ Info: iteration 22, average log likelihood -1.419507
[ Info: iteration 23, average log likelihood -1.419505
[ Info: iteration 24, average log likelihood -1.419504
[ Info: iteration 25, average log likelihood -1.419504
[ Info: iteration 26, average log likelihood -1.419503
[ Info: iteration 27, average log likelihood -1.419503
[ Info: iteration 28, average log likelihood -1.419503
[ Info: iteration 29, average log likelihood -1.419502
[ Info: iteration 30, average log likelihood -1.419502
[ Info: iteration 31, average log likelihood -1.419502
[ Info: iteration 32, average log likelihood -1.419502
[ Info: iteration 33, average log likelihood -1.419501
[ Info: iteration 34, average log likelihood -1.419501
[ Info: iteration 35, average log likelihood -1.419501
[ Info: iteration 36, average log likelihood -1.419501
[ Info: iteration 37, average log likelihood -1.419501
[ Info: iteration 38, average log likelihood -1.419501
[ Info: iteration 39, average log likelihood -1.419501
[ Info: iteration 40, average log likelihood -1.419501
[ Info: iteration 41, average log likelihood -1.419500
[ Info: iteration 42, average log likelihood -1.419500
[ Info: iteration 43, average log likelihood -1.419500
[ Info: iteration 44, average log likelihood -1.419500
[ Info: iteration 45, average log likelihood -1.419500
[ Info: iteration 46, average log likelihood -1.419500
[ Info: iteration 47, average log likelihood -1.419500
[ Info: iteration 48, average log likelihood -1.419500
[ Info: iteration 49, average log likelihood -1.419500
[ Info: iteration 50, average log likelihood -1.419500
┌ Info: EM with 100000 data points 50 iterations avll -1.419500
└ 952.4 data points per parameter
┌ Info: 1
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4251110579235096
│     -1.4250426276519852
│      ⋮
└     -1.4195000788270962
[ Info: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.419515
[ Info: iteration 2, average log likelihood -1.419447
[ Info: iteration 3, average log likelihood -1.419384
[ Info: iteration 4, average log likelihood -1.419306
[ Info: iteration 5, average log likelihood -1.419208
[ Info: iteration 6, average log likelihood -1.419095
[ Info: iteration 7, average log likelihood -1.418978
[ Info: iteration 8, average log likelihood -1.418870
[ Info: iteration 9, average log likelihood -1.418779
[ Info: iteration 10, average log likelihood -1.418708
[ Info: iteration 11, average log likelihood -1.418652
[ Info: iteration 12, average log likelihood -1.418608
[ Info: iteration 13, average log likelihood -1.418575
[ Info: iteration 14, average log likelihood -1.418549
[ Info: iteration 15, average log likelihood -1.418530
[ Info: iteration 16, average log likelihood -1.418515
[ Info: iteration 17, average log likelihood -1.418504
[ Info: iteration 18, average log likelihood -1.418496
[ Info: iteration 19, average log likelihood -1.418489
[ Info: iteration 20, average log likelihood -1.418484
[ Info: iteration 21, average log likelihood -1.418479
[ Info: iteration 22, average log likelihood -1.418475
[ Info: iteration 23, average log likelihood -1.418471
[ Info: iteration 24, average log likelihood -1.418468
[ Info: iteration 25, average log likelihood -1.418465
[ Info: iteration 26, average log likelihood -1.418462
[ Info: iteration 27, average log likelihood -1.418460
[ Info: iteration 28, average log likelihood -1.418457
[ Info: iteration 29, average log likelihood -1.418455
[ Info: iteration 30, average log likelihood -1.418453
[ Info: iteration 31, average log likelihood -1.418451
[ Info: iteration 32, average log likelihood -1.418449
[ Info: iteration 33, average log likelihood -1.418447
[ Info: iteration 34, average log likelihood -1.418446
[ Info: iteration 35, average log likelihood -1.418444
[ Info: iteration 36, average log likelihood -1.418443
[ Info: iteration 37, average log likelihood -1.418442
[ Info: iteration 38, average log likelihood -1.418440
[ Info: iteration 39, average log likelihood -1.418439
[ Info: iteration 40, average log likelihood -1.418438
[ Info: iteration 41, average log likelihood -1.418437
[ Info: iteration 42, average log likelihood -1.418436
[ Info: iteration 43, average log likelihood -1.418435
[ Info: iteration 44, average log likelihood -1.418434
[ Info: iteration 45, average log likelihood -1.418433
[ Info: iteration 46, average log likelihood -1.418432
[ Info: iteration 47, average log likelihood -1.418431
[ Info: iteration 48, average log likelihood -1.418430
[ Info: iteration 49, average log likelihood -1.418430
[ Info: iteration 50, average log likelihood -1.418429
┌ Info: EM with 100000 data points 50 iterations avll -1.418429
└ 473.9 data points per parameter
┌ Info: 2
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4195150751361445
│     -1.4194467244633144
│      ⋮
└     -1.418428932879225
[ Info: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.418438
[ Info: iteration 2, average log likelihood -1.418387
[ Info: iteration 3, average log likelihood -1.418344
[ Info: iteration 4, average log likelihood -1.418294
[ Info: iteration 5, average log likelihood -1.418233
[ Info: iteration 6, average log likelihood -1.418157
[ Info: iteration 7, average log likelihood -1.418066
[ Info: iteration 8, average log likelihood -1.417963
[ Info: iteration 9, average log likelihood -1.417855
[ Info: iteration 10, average log likelihood -1.417751
[ Info: iteration 11, average log likelihood -1.417656
[ Info: iteration 12, average log likelihood -1.417574
[ Info: iteration 13, average log likelihood -1.417506
[ Info: iteration 14, average log likelihood -1.417449
[ Info: iteration 15, average log likelihood -1.417402
[ Info: iteration 16, average log likelihood -1.417364
[ Info: iteration 17, average log likelihood -1.417332
[ Info: iteration 18, average log likelihood -1.417306
[ Info: iteration 19, average log likelihood -1.417284
[ Info: iteration 20, average log likelihood -1.417265
[ Info: iteration 21, average log likelihood -1.417248
[ Info: iteration 22, average log likelihood -1.417234
[ Info: iteration 23, average log likelihood -1.417221
[ Info: iteration 24, average log likelihood -1.417209
[ Info: iteration 25, average log likelihood -1.417198
[ Info: iteration 26, average log likelihood -1.417188
[ Info: iteration 27, average log likelihood -1.417178
[ Info: iteration 28, average log likelihood -1.417169
[ Info: iteration 29, average log likelihood -1.417160
[ Info: iteration 30, average log likelihood -1.417152
[ Info: iteration 31, average log likelihood -1.417144
[ Info: iteration 32, average log likelihood -1.417136
[ Info: iteration 33, average log likelihood -1.417128
[ Info: iteration 34, average log likelihood -1.417121
[ Info: iteration 35, average log likelihood -1.417113
[ Info: iteration 36, average log likelihood -1.417106
[ Info: iteration 37, average log likelihood -1.417099
[ Info: iteration 38, average log likelihood -1.417091
[ Info: iteration 39, average log likelihood -1.417084
[ Info: iteration 40, average log likelihood -1.417077
[ Info: iteration 41, average log likelihood -1.417070
[ Info: iteration 42, average log likelihood -1.417063
[ Info: iteration 43, average log likelihood -1.417055
[ Info: iteration 44, average log likelihood -1.417048
[ Info: iteration 45, average log likelihood -1.417041
[ Info: iteration 46, average log likelihood -1.417034
[ Info: iteration 47, average log likelihood -1.417027
[ Info: iteration 48, average log likelihood -1.417020
[ Info: iteration 49, average log likelihood -1.417012
[ Info: iteration 50, average log likelihood -1.417005
┌ Info: EM with 100000 data points 50 iterations avll -1.417005
└ 236.4 data points per parameter
┌ Info: 3
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.418438242537538
│     -1.4183873308164738
│      ⋮
└     -1.4170052324222848
[ Info: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.417007
[ Info: iteration 2, average log likelihood -1.416951
[ Info: iteration 3, average log likelihood -1.416903
[ Info: iteration 4, average log likelihood -1.416849
[ Info: iteration 5, average log likelihood -1.416786
[ Info: iteration 6, average log likelihood -1.416712
[ Info: iteration 7, average log likelihood -1.416626
[ Info: iteration 8, average log likelihood -1.416528
[ Info: iteration 9, average log likelihood -1.416422
[ Info: iteration 10, average log likelihood -1.416311
[ Info: iteration 11, average log likelihood -1.416202
[ Info: iteration 12, average log likelihood -1.416097
[ Info: iteration 13, average log likelihood -1.416001
[ Info: iteration 14, average log likelihood -1.415915
[ Info: iteration 15, average log likelihood -1.415840
[ Info: iteration 16, average log likelihood -1.415776
[ Info: iteration 17, average log likelihood -1.415721
[ Info: iteration 18, average log likelihood -1.415673
[ Info: iteration 19, average log likelihood -1.415632
[ Info: iteration 20, average log likelihood -1.415595
[ Info: iteration 21, average log likelihood -1.415564
[ Info: iteration 22, average log likelihood -1.415535
[ Info: iteration 23, average log likelihood -1.415510
[ Info: iteration 24, average log likelihood -1.415487
[ Info: iteration 25, average log likelihood -1.415466
[ Info: iteration 26, average log likelihood -1.415446
[ Info: iteration 27, average log likelihood -1.415428
[ Info: iteration 28, average log likelihood -1.415411
[ Info: iteration 29, average log likelihood -1.415394
[ Info: iteration 30, average log likelihood -1.415379
[ Info: iteration 31, average log likelihood -1.415364
[ Info: iteration 32, average log likelihood -1.415350
[ Info: iteration 33, average log likelihood -1.415336
[ Info: iteration 34, average log likelihood -1.415323
[ Info: iteration 35, average log likelihood -1.415310
[ Info: iteration 36, average log likelihood -1.415298
[ Info: iteration 37, average log likelihood -1.415286
[ Info: iteration 38, average log likelihood -1.415275
[ Info: iteration 39, average log likelihood -1.415263
[ Info: iteration 40, average log likelihood -1.415252
[ Info: iteration 41, average log likelihood -1.415242
[ Info: iteration 42, average log likelihood -1.415232
[ Info: iteration 43, average log likelihood -1.415222
[ Info: iteration 44, average log likelihood -1.415212
[ Info: iteration 45, average log likelihood -1.415203
[ Info: iteration 46, average log likelihood -1.415194
[ Info: iteration 47, average log likelihood -1.415185
[ Info: iteration 48, average log likelihood -1.415176
[ Info: iteration 49, average log likelihood -1.415168
[ Info: iteration 50, average log likelihood -1.415160
┌ Info: EM with 100000 data points 50 iterations avll -1.415160
└ 118.1 data points per parameter
┌ Info: 4
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4170074363006924
│     -1.4169514649139452
│      ⋮
└     -1.4151600582174884
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.415162
[ Info: iteration 2, average log likelihood -1.415096
[ Info: iteration 3, average log likelihood -1.415036
[ Info: iteration 4, average log likelihood -1.414964
[ Info: iteration 5, average log likelihood -1.414874
[ Info: iteration 6, average log likelihood -1.414759
[ Info: iteration 7, average log likelihood -1.414618
[ Info: iteration 8, average log likelihood -1.414452
[ Info: iteration 9, average log likelihood -1.414272
[ Info: iteration 10, average log likelihood -1.414086
[ Info: iteration 11, average log likelihood -1.413905
[ Info: iteration 12, average log likelihood -1.413737
[ Info: iteration 13, average log likelihood -1.413585
[ Info: iteration 14, average log likelihood -1.413451
[ Info: iteration 15, average log likelihood -1.413333
[ Info: iteration 16, average log likelihood -1.413229
[ Info: iteration 17, average log likelihood -1.413139
[ Info: iteration 18, average log likelihood -1.413059
[ Info: iteration 19, average log likelihood -1.412988
[ Info: iteration 20, average log likelihood -1.412925
[ Info: iteration 21, average log likelihood -1.412868
[ Info: iteration 22, average log likelihood -1.412817
[ Info: iteration 23, average log likelihood -1.412771
[ Info: iteration 24, average log likelihood -1.412728
[ Info: iteration 25, average log likelihood -1.412690
[ Info: iteration 26, average log likelihood -1.412654
[ Info: iteration 27, average log likelihood -1.412621
[ Info: iteration 28, average log likelihood -1.412591
[ Info: iteration 29, average log likelihood -1.412563
[ Info: iteration 30, average log likelihood -1.412537
[ Info: iteration 31, average log likelihood -1.412512
[ Info: iteration 32, average log likelihood -1.412489
[ Info: iteration 33, average log likelihood -1.412467
[ Info: iteration 34, average log likelihood -1.412447
[ Info: iteration 35, average log likelihood -1.412427
[ Info: iteration 36, average log likelihood -1.412409
[ Info: iteration 37, average log likelihood -1.412391
[ Info: iteration 38, average log likelihood -1.412375
[ Info: iteration 39, average log likelihood -1.412359
[ Info: iteration 40, average log likelihood -1.412343
[ Info: iteration 41, average log likelihood -1.412329
[ Info: iteration 42, average log likelihood -1.412315
[ Info: iteration 43, average log likelihood -1.412302
[ Info: iteration 44, average log likelihood -1.412289
[ Info: iteration 45, average log likelihood -1.412277
[ Info: iteration 46, average log likelihood -1.412265
[ Info: iteration 47, average log likelihood -1.412253
[ Info: iteration 48, average log likelihood -1.412243
[ Info: iteration 49, average log likelihood -1.412232
[ Info: iteration 50, average log likelihood -1.412222
┌ Info: EM with 100000 data points 50 iterations avll -1.412222
└ 59.0 data points per parameter
┌ Info: 5
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.415161902213277
│     -1.4150964116506475
│      ⋮
└     -1.4122218863343876
┌ Info: Total log likelihood: 
│   tll =
│    251-element Array{Float64,1}:
│     -1.4250924674522372
│     -1.4251110579235096
│     -1.4250426276519852
│     -1.424982497730839
│      ⋮
│     -1.4122425335155702
│     -1.4122320193447888
└     -1.4122218863343876
32×26 Array{Float64,2}:
  0.137954    -0.634406    -0.158937     0.0541268  -0.331357     0.129333   -0.453673     0.0397012   -0.489066    -0.275674    -0.228336    -0.483696    -0.0565853    0.281539    -0.72588     0.411556    0.0534379  -0.420655    0.568695    -0.373034     0.18602      0.301831   -0.33287      0.0682472    0.301494     0.385562
 -0.638488     0.94089      0.595356    -0.0383385  -0.05252      0.238393    0.218862    -0.0666154   -1.07138      0.0731564   -0.0287335   -0.0201129   -0.0962559    0.37235     -0.312129   -0.100128    0.241257    0.238759    0.259815    -0.129223     0.310789     0.461368   -0.205201    -0.0751351    0.957896     0.816802
 -0.834991     0.187707    -0.0153163   -0.872517   -0.00892427  -0.613092   -0.224984    -0.293482     0.606046    -0.269915    -0.0666172    0.0204529   -0.395378    -0.00938012  -0.0296497  -0.302438   -0.0578653  -0.0475636   0.0807341   -0.351846    -0.0654498    0.254022    0.10928      0.270508    -0.171439     0.413744
  0.054695    -0.128987     0.442167    -0.556273    0.45734      0.23856    -0.0350663   -0.273483     0.0645542   -0.178646    -0.319087     0.0665827    0.204023     0.224477    -0.266734   -0.523187    0.109522   -0.304238    0.379052    -0.667625     0.901233     0.515467   -0.475825    -0.126249     0.0499351   -0.304274
  0.568042    -0.284543    -0.411125     0.0576358   0.195575    -0.842529   -0.680018     0.374469     0.145282     0.47774      0.259646     0.613107     0.226685     0.244789     0.027606   -1.21318     0.188339    0.450493    0.0307674    0.28572      0.114576     0.081049   -0.557819     0.979926     0.0308468   -0.477955
  0.303038    -0.159456     0.216646    -0.324804   -0.0363698   -0.190336   -0.432537     0.407952    -0.330309     0.217472    -0.0465352   -0.0418487   -0.234696    -0.014893     0.447505    0.280863    0.184333    0.478386   -0.115085     0.730458    -0.161648    -0.0418127  -0.721096     0.747754     0.227704     0.785297
  0.0329528   -0.567231    -0.200788    -0.0604507   0.175085     0.162788   -0.292033    -0.703471    -0.154886     0.531078     0.574228     0.327559    -0.0338705    0.231976     0.456733    0.153655   -0.0097009  -0.934282    0.198716    -0.0582057    0.253147    -0.189471   -0.241727     0.0511676   -0.484242    -0.238379
  0.207345    -0.376531    -0.0584955   -0.6245     -0.413353     0.297046   -0.144265    -0.180593     0.763595     0.453961     0.128418    -0.141924     0.412646    -0.419967     0.598262    0.0622735  -0.434251    0.25904    -0.444256    -0.139858    -0.20101     -0.221755   -0.193776     0.214224    -0.943353    -0.327854
 -0.436567    -0.246419     0.294318     0.505774   -0.128864    -0.359494   -0.358693    -0.233244    -0.150527    -0.564531     0.11109     -0.194988    -0.465298     0.0389887   -0.548974    0.190123    0.403557    0.113865    0.213351    -0.246917    -0.306698     0.173344   -0.24055      0.228818     0.130066    -0.684089
 -0.730718    -0.413755    -0.0516328    0.775239   -0.592217    -0.242974   -0.215532     0.424141     0.00257618   0.1954      -0.528471    -0.0757222   -0.202578    -0.0291921    0.795904    0.584065    0.405349   -0.21075    -0.0171383    0.427916     0.073182    -0.167197    0.642546     0.452115     0.0647212   -0.465938
  0.677813    -0.66586     -0.0339633    0.643936   -0.095775     0.619923    0.109184     0.507965    -0.501971    -0.148717    -0.00326238  -0.10803      0.0924527   -0.325329    -0.168187    0.338895   -0.401933    0.0866714   0.1655       0.372448    -0.339039    -0.509523   -0.182225     0.00127908  -0.0969666   -0.370066
  0.564436    -0.433491     0.730999     0.684708   -0.669237     0.431501   -0.150355     0.545608    -0.0830908    1.03212     -0.0664368   -0.0242392    0.294387     0.321297     0.302169    0.0846777   0.0912015  -0.334164    0.0531845    0.101968     0.367539     0.431783   -0.218055     0.0123485    0.329205    -0.320205
  0.125027     0.239406    -0.102887    -0.1644      0.0909398   -0.463273    0.0324515    0.207185    -0.0270994   -0.0143315   -0.472042     0.144802    -0.116876    -0.0345666    0.130817   -0.271827   -0.39584     0.164486   -0.0520822    0.251814     0.0982307   -0.436886    0.512228    -0.0740148    0.282984     0.10258
 -0.272683    -0.0628715    0.132634     0.12346    -0.126686     0.256548   -0.107724    -0.183372     0.0290092   -0.0938779    0.27158     -0.175753    -0.0121707   -0.0299595   -0.111345    0.113357    0.226204   -0.0691695  -0.00259702  -0.147339    -0.124665     0.340419   -0.399949     0.0858512   -0.0738105   -0.163158
  0.650085     0.285037    -0.235109     0.738814   -0.484818    -0.309258    0.594881    -0.038758     0.210552    -0.192276     0.162168     0.337981     0.811823    -0.0886551   -0.0106189   0.422995    0.481119   -0.389785    0.32042      0.440792    -0.386788    -0.117029    0.454813    -0.496259     0.119979    -0.160807
  0.403528     0.654493     0.495663     0.512689    0.621951     0.102643    0.546038     0.00296701   0.0270081   -0.167389     0.339902     0.766773     0.630278    -0.495762     0.502913   -0.443151    0.0596481   0.747129   -0.175449     0.00487131   0.0912499   -0.484622    0.0108303   -0.252137     0.34914      0.314231
  0.0813554   -0.099455    -0.131717     0.0610619   0.00928817   0.0294609   0.137025    -0.101286    -0.101158     0.0757975   -0.0042485    0.017126    -0.0875085    0.109523    -0.189572   -0.0527191  -0.169625   -0.374891    0.176395    -0.0613577    0.0138974   -0.0447167   0.193897     0.0270289   -0.0328      -0.0634009
 -0.031386     0.255103     0.0967525    0.0217911   0.0603329   -0.0257302   0.110028     0.148265     0.15528     -0.271432    -0.0887318    0.0203304    0.0713086   -0.0627447   -0.19275    -0.168036   -0.0166604   0.417803   -0.0957805    0.00255517  -0.00148872  -0.0165159  -0.0463744   -0.107463     0.0624159    0.113796
 -0.172281    -0.274425     0.221612    -0.0594398  -0.701523     0.218454   -0.212961    -0.205139     0.191529     0.00659066  -0.100438     0.0223543    0.00867313  -0.0592771    0.131486    0.056804   -0.174317   -0.0797247   0.238535    -0.0853225   -0.260157     0.203515   -0.490768     0.0510903    0.0276974    0.065145
  0.00855376  -0.0596658    0.143064    -0.190243    0.272756     0.0628055  -0.413029    -0.0314433   -0.051867     0.198654    -0.00192027  -0.107136    -0.0531193    0.00446086   0.47817     0.0458422   0.197735    0.0754472  -0.154923    -0.106255     0.0722877    0.0454956  -0.211432     0.236489    -0.00902705  -0.066468
 -0.2074       0.615557    -0.347418     0.022622   -0.760717    -0.0986346   0.154915    -0.37253     -0.187       -0.644899     0.100418    -0.446069     0.831219     0.261517    -0.0680177   0.189011    0.138757    0.0409569   0.203065    -0.72878      0.23789      0.160564    0.0781335   -0.155426    -0.29292     -0.453249
 -0.12252      0.119326    -0.337846     0.259185    0.508173     0.0469538   0.60429     -0.40207      0.0474139   -0.267822     0.0437745   -0.543379     0.0419559   -0.282518    -0.284772    0.0730481  -0.131183   -0.4057     -0.215446    -0.175781     0.0296641   -0.257536    0.751974    -0.587923    -0.218668    -0.334914
  0.102341     0.456159    -0.674523    -0.70846     0.420667     0.323182    0.366228    -0.419671    -0.155042    -0.25598      0.322923     0.00236601   0.0944237    0.0316014   -0.318093   -0.41105    -0.276437    0.0594547   0.136565    -0.0275479   -0.14288     -0.0639867  -0.181583    -0.240128    -0.423443     0.535491
  0.557784     0.165857     0.145072    -0.150417   -0.0868775    0.0368439   0.502291    -0.533291     0.0297423   -0.0957554   -0.0857733    0.791974    -0.510843    -0.0893458   -0.182839    0.688755    0.443825   -0.110405    0.454366     0.100793    -0.305234    -0.224507   -0.311273     0.278403    -0.521706    -0.0781919
  0.476091     0.193033    -0.111182     0.0572459   0.23115     -0.301579    0.39737      0.0999976   -0.263754     0.351358    -0.0305785   -0.052604     0.280124     0.216224     0.0908137   0.377855    0.0593672  -0.193099    0.178613     0.576624     0.144258    -0.0466991   0.312783    -0.0493851    0.506822     0.573593
  0.398337    -0.0072943   -0.00248303  -0.189177    0.0712929   -0.737225   -0.065905     0.0679806   -0.161308    -0.205358    -0.556908     0.0326479    0.247154    -0.160341     0.171346   -0.126916   -0.32067     0.294835   -0.051761     0.257569     0.441729    -0.745554    0.325591    -0.143515     0.123854     0.0317149
 -0.170391     0.132821    -0.256439     0.618414   -0.226838    -0.252543   -0.309811     0.199035     0.574212     0.480503     0.191739    -0.237395     0.253006    -0.275013     0.0018718  -0.408501    0.1924     -0.196878   -0.596929     0.440303     0.0225174    0.0736809   0.225427    -0.399655    -0.13709     -0.204523
  0.0519936    0.453935     0.0485163    0.0387904   0.252297    -0.251192    0.392627    -0.197293     0.174248     0.119692     0.305366     0.332976     0.119449    -0.188051     0.358911   -0.0729323   0.136469    0.0903065  -0.512465     0.193326    -0.483849     0.0912304   0.453442     0.23595      0.132109    -0.533626
 -0.5413       0.285706     0.159735     0.114744    0.187764     0.334144   -0.78068      0.297827    -0.210971     0.0506914   -0.363995    -0.295162     0.0298435    0.250613     0.636565   -0.135911   -0.0279953   0.387974   -0.470378    -0.21318     -0.297429    -0.0572138  -0.00411789   0.111383     0.52808      0.251369
 -0.396924    -0.00353794   0.255927    -0.454593    0.433694     0.392213    0.00456427   0.382801     0.0471643    0.209333     0.213287    -0.775645    -0.4953      -0.0782889   -0.253714   -0.254019    0.225346    0.609649   -0.284577    -0.178321     0.121918    -0.0832389   0.0992698   -0.0955381   -0.0827777    0.024268
  0.268669     0.213419    -0.0570286    0.0773587  -0.363641     0.146555    0.0494107    0.309511     0.137569    -0.213196    -0.671219     0.353695    -0.0919101    0.203976    -0.368689   -0.390781   -1.05718     0.0520192   0.348568     0.127631     0.139215     0.0252245   0.174088     0.155467     0.0714371    0.0425953
  0.0926016   -0.100409     0.220888    -0.121694    0.530897     0.275879    0.253259     0.54748      0.489871     0.735751     0.0308728    0.512892    -0.430466    -0.328646     0.167003   -0.323924   -0.258268   -0.401871   -0.242146     0.743373    -0.120481    -0.307847    0.0470149   -0.0607599    0.0771112    0.563936[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.412212
[ Info: iteration 2, average log likelihood -1.412203
[ Info: iteration 3, average log likelihood -1.412194
[ Info: iteration 4, average log likelihood -1.412185
[ Info: iteration 5, average log likelihood -1.412176
[ Info: iteration 6, average log likelihood -1.412168
[ Info: iteration 7, average log likelihood -1.412160
[ Info: iteration 8, average log likelihood -1.412152
[ Info: iteration 9, average log likelihood -1.412145
[ Info: iteration 10, average log likelihood -1.412137
┌ Info: EM with 100000 data points 10 iterations avll -1.412137
└ 59.0 data points per parameter
kind full, method kmeans
[ Info: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.862490e+05
      1       7.123624e+05      -1.738866e+05 |       32
      2       6.940252e+05      -1.833715e+04 |       32
      3       6.873858e+05      -6.639390e+03 |       32
      4       6.843781e+05      -3.007733e+03 |       32
      5       6.826389e+05      -1.739232e+03 |       32
      6       6.815444e+05      -1.094448e+03 |       32
      7       6.807760e+05      -7.684011e+02 |       32
      8       6.801761e+05      -5.999483e+02 |       32
      9       6.796931e+05      -4.829517e+02 |       32
     10       6.792932e+05      -3.999270e+02 |       32
     11       6.789330e+05      -3.602005e+02 |       32
     12       6.786216e+05      -3.114191e+02 |       32
     13       6.783472e+05      -2.743614e+02 |       32
     14       6.780887e+05      -2.585143e+02 |       32
     15       6.778687e+05      -2.199828e+02 |       32
     16       6.776612e+05      -2.075560e+02 |       32
     17       6.774778e+05      -1.833983e+02 |       32
     18       6.773180e+05      -1.598191e+02 |       32
     19       6.771654e+05      -1.525267e+02 |       32
     20       6.770172e+05      -1.482207e+02 |       32
     21       6.768558e+05      -1.614415e+02 |       32
     22       6.766967e+05      -1.590886e+02 |       32
     23       6.765560e+05      -1.407206e+02 |       32
     24       6.764342e+05      -1.218049e+02 |       32
     25       6.763197e+05      -1.144693e+02 |       32
     26       6.762199e+05      -9.981960e+01 |       32
     27       6.761128e+05      -1.070800e+02 |       32
     28       6.760067e+05      -1.060450e+02 |       32
     29       6.759153e+05      -9.138957e+01 |       32
     30       6.758249e+05      -9.045054e+01 |       32
     31       6.757358e+05      -8.913606e+01 |       32
     32       6.756502e+05      -8.559021e+01 |       32
     33       6.755713e+05      -7.890874e+01 |       32
     34       6.754929e+05      -7.841007e+01 |       32
     35       6.754243e+05      -6.858979e+01 |       32
     36       6.753697e+05      -5.451914e+01 |       32
     37       6.753233e+05      -4.640932e+01 |       32
     38       6.752775e+05      -4.583410e+01 |       32
     39       6.752284e+05      -4.904926e+01 |       32
     40       6.751770e+05      -5.142394e+01 |       32
     41       6.751344e+05      -4.264197e+01 |       32
     42       6.750908e+05      -4.357646e+01 |       32
     43       6.750494e+05      -4.139176e+01 |       32
     44       6.750108e+05      -3.860797e+01 |       32
     45       6.749784e+05      -3.245178e+01 |       32
     46       6.749494e+05      -2.891126e+01 |       32
     47       6.749245e+05      -2.494746e+01 |       32
     48       6.749036e+05      -2.089881e+01 |       32
     49       6.748852e+05      -1.841851e+01 |       32
     50       6.748677e+05      -1.751522e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 674867.6644359072)
┌ Info: K-means with 32000 data points using 50 iterations
└ 37.0 data points per parameter
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.424308
[ Info: iteration 2, average log likelihood -1.419263
[ Info: iteration 3, average log likelihood -1.417859
[ Info: iteration 4, average log likelihood -1.416786
[ Info: iteration 5, average log likelihood -1.415674
[ Info: iteration 6, average log likelihood -1.414699
[ Info: iteration 7, average log likelihood -1.414056
[ Info: iteration 8, average log likelihood -1.413692
[ Info: iteration 9, average log likelihood -1.413478
[ Info: iteration 10, average log likelihood -1.413335
[ Info: iteration 11, average log likelihood -1.413228
[ Info: iteration 12, average log likelihood -1.413143
[ Info: iteration 13, average log likelihood -1.413072
[ Info: iteration 14, average log likelihood -1.413011
[ Info: iteration 15, average log likelihood -1.412956
[ Info: iteration 16, average log likelihood -1.412908
[ Info: iteration 17, average log likelihood -1.412863
[ Info: iteration 18, average log likelihood -1.412823
[ Info: iteration 19, average log likelihood -1.412785
[ Info: iteration 20, average log likelihood -1.412750
[ Info: iteration 21, average log likelihood -1.412717
[ Info: iteration 22, average log likelihood -1.412686
[ Info: iteration 23, average log likelihood -1.412658
[ Info: iteration 24, average log likelihood -1.412631
[ Info: iteration 25, average log likelihood -1.412607
[ Info: iteration 26, average log likelihood -1.412583
[ Info: iteration 27, average log likelihood -1.412562
[ Info: iteration 28, average log likelihood -1.412541
[ Info: iteration 29, average log likelihood -1.412522
[ Info: iteration 30, average log likelihood -1.412505
[ Info: iteration 31, average log likelihood -1.412488
[ Info: iteration 32, average log likelihood -1.412472
[ Info: iteration 33, average log likelihood -1.412457
[ Info: iteration 34, average log likelihood -1.412442
[ Info: iteration 35, average log likelihood -1.412428
[ Info: iteration 36, average log likelihood -1.412415
[ Info: iteration 37, average log likelihood -1.412402
[ Info: iteration 38, average log likelihood -1.412390
[ Info: iteration 39, average log likelihood -1.412378
[ Info: iteration 40, average log likelihood -1.412366
[ Info: iteration 41, average log likelihood -1.412354
[ Info: iteration 42, average log likelihood -1.412343
[ Info: iteration 43, average log likelihood -1.412332
[ Info: iteration 44, average log likelihood -1.412321
[ Info: iteration 45, average log likelihood -1.412311
[ Info: iteration 46, average log likelihood -1.412300
[ Info: iteration 47, average log likelihood -1.412290
[ Info: iteration 48, average log likelihood -1.412279
[ Info: iteration 49, average log likelihood -1.412269
[ Info: iteration 50, average log likelihood -1.412259
┌ Info: EM with 100000 data points 50 iterations avll -1.412259
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
  0.156896   -0.680203    0.232385   -0.137267    -0.186426    0.0943123   -0.252449    -0.473222    -0.0391059     0.657122    0.315391     0.199296   -0.257569     0.380527     0.375566      0.391781     0.0135807   -0.659297     0.244037    0.0880611   0.257951     0.00949773  -0.379481    0.243255    -0.215704    -0.186765
 -0.368869    0.77206     0.0443393   0.0434311    0.136099    0.156391     0.146169     0.269927    -1.12844       0.275692   -0.404542     0.217193   -0.240693     0.535805     0.0992834    -0.0938785    0.135563     0.287402     0.341397    0.0971171   0.00976007   0.183959    -0.28878     0.469153     0.800375     0.869382
  0.0719588   0.221558   -0.275309   -0.707079     0.0915319  -0.430126    -0.248183     0.218032    -0.073555     -0.358831   -0.537381     0.034903   -0.0491897   -0.069062    -0.0937925    -0.665117    -0.638965     0.513318    -0.135614   -0.0761096   0.377168    -0.248949     0.101641    0.183068     0.154877     0.160727
 -0.197372    0.535016   -0.271415    0.00325202  -0.896326    0.174351    -0.00993132  -0.17276     -0.063189     -0.251168    0.00311146  -0.237491    1.00736      0.515594     0.0527228    -0.162891    -0.027337     0.0463709    0.177183   -0.430081    0.475955     0.208544    -0.141824   -0.423491    -0.233841    -0.209184
  0.407595   -0.0785359   0.269112   -0.314507     0.226497   -0.50726      0.187231     0.0514508   -0.1439       -0.2175     -0.41996     -0.181688    0.0121362   -0.0270842    0.00653659    0.125073    -0.167699     0.227391     0.132474    0.491331    0.361161    -0.359567     0.14395    -0.0767603    0.378598     0.507686
  0.3246      0.0592769  -0.0151753   0.555523     0.239247    0.373737    -0.0377569    0.486212    -0.0833754     0.933128    0.154119    -0.0672767   0.258977     0.0649895    0.244506     -0.0850457    0.146169    -0.28352     -0.15906     0.201368    0.18645      0.127415     0.247258   -0.0823037    0.183717     0.205362
 -0.571525   -0.192219    0.389055    0.110358    -0.1214     -0.00149714  -0.301046    -0.016328    -0.00491664   -0.209776    0.1259      -0.16235    -0.502291    -0.00551598  -0.175404      0.132996     0.288865    -0.0417103    0.0525367   0.0173745  -0.282588     0.373647    -0.337466    0.33559      0.0557862   -0.125955
  0.371858   -0.572726   -0.0854724   0.276181    -0.352675    0.974575     0.256836     0.0956061   -0.155545     -0.149656   -0.00888233  -0.214411    0.0686942   -0.442311    -0.219881      0.363144    -0.375112    -0.00564839   0.268526    0.137056   -0.514926     0.00152996  -0.558341    0.00165314  -0.21838     -0.18469
  0.254724    0.154218   -0.384023    0.3776      -0.14635    -0.104506     0.309845    -0.0716801   -0.0298426    -0.182876   -0.459143     0.3127     -0.12861      0.581508    -0.328626     -0.187       -0.765209    -0.364757     0.537307   -0.0789603   0.144651    -0.108978     0.35034    -0.00213827   0.041599    -0.260216
 -0.342116    0.463939    0.490687    0.252651     0.290436   -0.267208    -0.508828     0.0327953   -0.114488      0.0190007  -0.0384603   -0.311727   -0.211784     0.126682     0.223142     -0.00301903   0.225503     0.229877    -0.522013   -0.099457   -0.312577     0.154517     0.410582   -0.0746047    1.00419     -0.000656646
 -0.0581196  -0.0593805  -0.298334    0.416581    -0.387263   -0.324794     0.0914431    0.419649    -0.0173401     0.083232    0.124812     0.315604   -0.0947874   -0.127808    -0.0550182     0.186451     0.0321216    0.150888    -0.152669    1.02873    -0.654317    -0.450503     0.236118    0.146306    -0.297823    -0.0137614
  0.469723    0.436583   -0.0745992   0.411449    -0.21842    -0.462816     0.440916    -0.352414     0.21172      -0.134499    0.291117     0.453211    0.415416    -0.225411     0.215619      0.324254     0.68057     -0.426724     0.26724    -0.0486332  -0.278209     0.139209     0.112323    0.0207622    0.182373    -0.257736
  0.573268    0.121748   -0.0992355   0.514844    -0.141843   -0.0844608    0.412029    -0.0831534    0.000464163   0.0699572   0.025977     0.22954     0.446057    -0.00218607   0.118197      0.23957      0.0818266   -0.212718     0.160147    0.255141   -0.0874454   -0.240021     0.342577   -0.351944     0.0737418   -0.215685
  0.0706186   0.481243    0.54448     0.765005     0.461822    0.149479     0.505661    -0.0291032   -0.0973218    -0.375026    0.358454     0.685574    0.363925    -0.560633     0.506944     -0.136486     0.0836507    0.966088    -0.0507702   0.0927468  -0.107848    -0.357099    -0.113177   -0.00495971   0.224512    -0.0345815
  0.0193176   0.0179003   0.0753984   0.0138662   -0.0950621   0.0655038   -0.0334193   -0.00322761  -0.0571353     0.0686785  -0.0632849    0.0030589   0.00889085   0.0883303    0.0228138     0.0245068   -0.0730544    0.0191589    0.0798606   0.02538    -0.00970198   0.0483258   -0.125604    0.0539566    0.139566     0.0989261
  0.662097    0.628325   -0.206722   -0.982583     0.37037     0.0256601    0.716308    -0.371828    -0.112107      0.0556655   0.406448     0.166556    0.273417    -0.0630095   -0.946373     -0.345436    -0.401909    -0.104539     0.387144    0.320917   -0.139436     0.286936     0.305429    0.0876595   -0.175875     0.574412
 -0.149194   -0.157275    0.43799    -0.626993     0.578076    0.0784049    0.0472015   -0.379405     0.114179     -0.323625   -0.273418     0.150943    0.137306     0.319833    -0.215991     -0.608816     0.249613    -0.446205     0.556885   -0.774567    0.858947     0.525097    -0.38689    -0.00938325  -0.00913557  -0.380221
 -0.371333   -0.571122    0.0998088   0.774362    -0.47131    -0.0205834   -0.484184     0.377144    -0.0996361     0.139543   -0.626824    -0.298567    0.240941     0.153098     1.07925       0.155338     0.183393    -0.0119264   -0.285596    0.0697445   0.232238    -0.343672     0.141769    0.190428     0.379495    -0.445669
 -0.558735   -0.0339451   0.126113   -0.141766    -0.330193   -0.0454457   -0.162173    -0.171979     0.384326     -0.366972   -0.0682941   -0.0668528  -0.263715    -0.103245    -0.183929     -0.238034    -0.0339003    0.0100649    0.187414   -0.380281   -0.229389     0.231042    -0.126353   -0.114027    -0.0656904   -0.0280829
  0.298368   -0.551796    0.232761    0.554731    -0.278218    0.269692    -0.231791     0.371535    -0.517037     -0.234173   -0.357362    -0.163775   -0.126126     0.160501    -1.02718       0.326401     0.00496945  -0.512311     0.839746   -0.219356    0.276343     0.0527428   -0.1935     -0.0160604    0.316541     0.536693
  0.682911   -0.334077   -0.6957      0.235573     0.281282   -0.395828    -0.834521     0.350238     0.503254      0.686661    0.377309     0.335349    0.279778     0.109678     0.11743      -0.894477     0.120728     0.00135868  -0.450953    0.350698    0.0211269    0.308715    -0.359364    0.641631    -0.202144    -0.744463
  0.0434737  -0.143491    0.41576    -0.245397    -0.522964   -0.15125     -0.793302     0.192137     0.286268      0.108992   -0.282268     0.279658   -0.058094    -0.0425982    0.0872647    -0.13043     -0.235981     0.188877     0.111696   -0.0386457  -0.0409513    0.24453     -0.562251    0.476537     0.26856      0.330187
 -0.638698   -0.145009    0.251618   -0.311267     0.50631     0.216685    -0.135652     0.418896     0.0956844     0.420928    0.217588    -0.625688   -1.16838      0.221507    -0.0726414    -0.232535     0.181554     0.416362     0.0309772  -0.0281461   0.144742    -0.346744     0.355291    0.0690618   -0.193004    -0.223199
 -0.230515    0.327697   -0.392234    0.176617     0.306772   -0.029008     0.618459    -0.264012     0.153065     -0.243561    0.0332476   -0.549358    0.20986     -0.473126    -0.258806     -0.0716087   -0.165662    -0.226964    -0.348637   -0.0174112  -0.052953    -0.198216     0.825828   -0.611749    -0.226685    -0.241754
  0.192323   -0.303137    0.668546    0.217414    -0.409796    0.0393965   -0.187546     0.150299     0.282961      0.432504    0.0435848   -0.38003     0.281889    -0.494837    -0.634297     -0.240173     0.253565     0.128434    -0.360084   -0.0651601   0.722275     0.537801    -0.318161   -0.414142    -0.0965171   -0.629852
 -0.0245306  -0.190949    0.144392   -0.730665    -0.296846    0.0849954   -0.0647309   -0.227044     0.666836      0.521232    0.101343    -0.0977537   0.326826    -0.521773     0.768         0.124877    -0.244091     0.323281    -0.490196   -0.0266117  -0.304308    -0.175004    -0.0689044   0.376212    -0.877828    -0.0866256
 -0.183459   -0.103802   -0.19291     0.0861354   -0.239617   -0.239038    -0.221351    -0.355125    -0.48635      -0.662417    0.179102    -0.62123     0.0606691    0.224758    -0.528474      0.588467     0.354053     0.142896     0.147835   -0.603318   -0.192753     0.330365    -0.181886    0.279408    -0.00358253  -0.475386
 -0.161199    0.522572   -0.108188   -0.314898     0.434427   -0.241279     0.066385     0.0491868    0.367595      0.135477    0.00845857   0.0538341   0.137342    -0.0393063    0.204901     -0.240688     0.0764228    0.121634    -0.608335    0.12168    -0.00472733   0.0341833    0.298118    0.191532    -0.175585    -0.139648
  0.408489    0.0914013   0.254003   -0.176535     0.384119    0.246993     0.355547     0.436433     0.479319      0.632658   -0.183042     0.741761   -0.094192    -0.319018     0.425856     -0.583932    -0.440937    -0.301745    -0.215823    0.6611      0.0598297   -0.57637      0.121437   -0.252262     0.18071      0.432428
 -0.148077    0.166432    0.0775238  -0.702532     0.612663    0.861714    -0.0606701    0.187554    -0.246586      0.0633182  -0.00842679  -0.554154    0.185038    -0.0482302    0.197131     -0.20258     -0.0671157    0.296331    -0.34424    -0.329742    0.0894488    0.346995    -0.546869   -0.154102     0.0886998    0.745419
  0.194358   -0.266718   -0.14855    -0.0255524    0.273025    0.0436025    0.0251812   -0.100371    -0.15859      -0.189537    0.0669936    0.0145638  -0.0515558   -0.297073    -0.000500042  -0.148069    -0.038065    -0.0925319   -0.0726785  -0.18288    -0.0455285   -0.333991     0.124091    0.00448608  -0.353668    -0.357736
 -0.37889     0.310728   -0.653932   -0.670089     0.252851    0.180357    -0.191923    -0.865454     0.128864     -0.294903    0.256832     0.120839   -0.379128     0.132744    -0.0256144     0.0108811    0.0171333   -0.0453315    0.150177   -0.327376   -0.210452    -0.402677    -0.364865   -0.117875    -0.545832     0.496034[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.412249
[ Info: iteration 2, average log likelihood -1.412240
[ Info: iteration 3, average log likelihood -1.412230
[ Info: iteration 4, average log likelihood -1.412220
[ Info: iteration 5, average log likelihood -1.412211
[ Info: iteration 6, average log likelihood -1.412202
[ Info: iteration 7, average log likelihood -1.412193
[ Info: iteration 8, average log likelihood -1.412184
[ Info: iteration 9, average log likelihood -1.412175
[ Info: iteration 10, average log likelihood -1.412166
┌ Info: EM with 100000 data points 10 iterations avll -1.412166
└ 59.0 data points per parameter
[ Info: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.678561e+05
      1       2.230230e+04      -1.455538e+05 |        2
      2       7.823675e+03      -1.447862e+04 |        0
      3       7.823675e+03       0.000000e+00 |        0
K-means converged with 3 iterations (objv = 7823.67549422947)
┌ Info: K-means with 900 data points using 3 iterations
└ 150.0 data points per parameter
[ Info: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
[ Info: iteration 1, average log likelihood -2.043155
[ Info: iteration 2, average log likelihood -2.043154
[ Info: iteration 3, average log likelihood -2.043154
[ Info: iteration 4, average log likelihood -2.043154
[ Info: iteration 5, average log likelihood -2.043154
[ Info: iteration 6, average log likelihood -2.043154
[ Info: iteration 7, average log likelihood -2.043154
[ Info: iteration 8, average log likelihood -2.043154
[ Info: iteration 9, average log likelihood -2.043154
[ Info: iteration 10, average log likelihood -2.043154
┌ Info: EM with 900 data points 10 iterations avll -2.043154
└ 81.8 data points per parameter
   Testing GaussianMixtures tests passed 
