Julia Version 1.5.0-DEV.147
Commit e0740fe5a6 (2020-01-24 14:13 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-8.0.1 (ORCJIT, skylake)
Environment:
  JULIA_DEPOT_PATH = ::/usr/local/share/julia
  JULIA_NUM_THREADS = 2

 Resolving package versions...
 Installed GaussianMixtures ─── v0.3.0
 Installed CMakeWrapper ─────── v0.2.3
 Installed LegacyStrings ────── v0.4.1
 Installed Missings ─────────── v0.4.3
 Installed URIParser ────────── v0.4.0
 Installed Compat ───────────── v2.2.0
 Installed NearestNeighbors ─── v0.4.4
 Installed StatsBase ────────── v0.32.0
 Installed OpenSpecFun_jll ──── v0.5.3+1
 Installed CMake ────────────── v1.1.2
 Installed OrderedCollections ─ v1.1.0
 Installed Arpack ───────────── v0.4.0
 Installed Distances ────────── v0.8.2
 Installed JLD ──────────────── v0.9.1
 Installed Distributions ────── v0.22.3
 Installed HDF5 ─────────────── v0.12.5
 Installed StatsFuns ────────── v0.9.3
 Installed FillArrays ───────── v0.8.4
 Installed PDMats ───────────── v0.9.11
 Installed DataStructures ───── v0.17.9
 Installed StaticArrays ─────── v0.12.1
 Installed DataAPI ──────────── v1.1.0
 Installed ScikitLearnBase ──── v0.5.0
 Installed QuadGK ───────────── v2.3.1
 Installed Arpack_jll ───────── v3.5.0+2
 Installed SpecialFunctions ─── v0.9.0
 Installed BinaryProvider ───── v0.5.8
 Installed OpenBLAS_jll ─────── v0.3.7+4
 Installed BinDeps ──────────── v1.0.0
 Installed Rmath ────────────── v0.6.0
 Installed SortingAlgorithms ── v0.3.1
 Installed FileIO ───────────── v1.2.1
 Installed Parameters ───────── v0.12.0
 Installed Blosc ────────────── v0.5.1
 Installed Clustering ───────── v0.13.3
  Updating `~/.julia/environments/v1.5/Project.toml`
  [cc18c42c] + GaussianMixtures v0.3.0
  Updating `~/.julia/environments/v1.5/Manifest.toml`
  [7d9fca2a] + Arpack v0.4.0
  [68821587] + Arpack_jll v3.5.0+2
  [9e28174c] + BinDeps v1.0.0
  [b99e7846] + BinaryProvider v0.5.8
  [a74b3585] + Blosc v0.5.1
  [631607c0] + CMake v1.1.2
  [d5fb7624] + CMakeWrapper v0.2.3
  [aaaa29a8] + Clustering v0.13.3
  [34da2185] + Compat v2.2.0
  [9a962f9c] + DataAPI v1.1.0
  [864edb3b] + DataStructures v0.17.9
  [b4f34e82] + Distances v0.8.2
  [31c24e10] + Distributions v0.22.3
  [5789e2e9] + FileIO v1.2.1
  [1a297f60] + FillArrays v0.8.4
  [cc18c42c] + GaussianMixtures v0.3.0
  [f67ccb44] + HDF5 v0.12.5
  [4138dd39] + JLD v0.9.1
  [1b4a561d] + LegacyStrings v0.4.1
  [e1d29d7a] + Missings v0.4.3
  [b8a86587] + NearestNeighbors v0.4.4
  [4536629a] + OpenBLAS_jll v0.3.7+4
  [efe28fd5] + OpenSpecFun_jll v0.5.3+1
  [bac558e1] + OrderedCollections v1.1.0
  [90014a1f] + PDMats v0.9.11
  [d96e819e] + Parameters v0.12.0
  [1fd47b50] + QuadGK v2.3.1
  [79098fc4] + Rmath v0.6.0
  [6e75b9c4] + ScikitLearnBase v0.5.0
  [a2af1166] + SortingAlgorithms v0.3.1
  [276daf66] + SpecialFunctions v0.9.0
  [90137ffa] + StaticArrays v0.12.1
  [2913bbd2] + StatsBase v0.32.0
  [4c63d2b9] + StatsFuns v0.9.3
  [30578b45] + URIParser v0.4.0
  [2a0f44e3] + Base64 
  [ade2ca70] + Dates 
  [8bb1440f] + DelimitedFiles 
  [8ba89e20] + Distributed 
  [b77e0a4c] + InteractiveUtils 
  [76f85450] + LibGit2 
  [8f399da3] + Libdl 
  [37e2e46d] + LinearAlgebra 
  [56ddb016] + Logging 
  [d6f4376e] + Markdown 
  [a63ad114] + Mmap 
  [44cfe95a] + Pkg 
  [de0858da] + Printf 
  [9abbd945] + Profile 
  [3fa0cd96] + REPL 
  [9a3f8284] + Random 
  [ea8e919c] + SHA 
  [9e88b42a] + Serialization 
  [1a1011a3] + SharedArrays 
  [6462fe0b] + Sockets 
  [2f01184e] + SparseArrays 
  [10745b16] + Statistics 
  [4607b0f0] + SuiteSparse 
  [8dfed614] + Test 
  [cf7118a7] + UUIDs 
  [4ec0a83e] + Unicode 
  Building CMake → `~/.julia/packages/CMake/nSK2r/deps/build.log`
  Updating `/tmp/jl_Wy9ARN/Project.toml`
 [no changes]
  Updating `/tmp/jl_Wy9ARN/Manifest.toml`
 [no changes]
  Building Blosc → `~/.julia/packages/Blosc/lzFr0/deps/build.log`
  Updating `/tmp/jl_OCI85R/Project.toml`
 [no changes]
  Updating `/tmp/jl_OCI85R/Manifest.toml`
 [no changes]
  Building HDF5 ─→ `~/.julia/packages/HDF5/Zh9on/deps/build.log`
  Updating `/tmp/jl_mHlKKX/Project.toml`
 [no changes]
  Updating `/tmp/jl_mHlKKX/Manifest.toml`
 [no changes]
  Building Rmath → `~/.julia/packages/Rmath/BoBag/deps/build.log`
  Updating `/tmp/jl_UfMwCp/Project.toml`
 [no changes]
  Updating `/tmp/jl_UfMwCp/Manifest.toml`
 [no changes]
   Testing GaussianMixtures
  Updating `/tmp/jl_m0x0Hc/Project.toml`
 [no changes]
  Updating `/tmp/jl_m0x0Hc/Manifest.toml`
 [no changes]
Running sandbox
Status `/tmp/jl_m0x0Hc/Project.toml`
  [aaaa29a8] Clustering v0.13.3
  [34da2185] Compat v2.2.0
  [31c24e10] Distributions v0.22.3
  [5789e2e9] FileIO v1.2.1
  [cc18c42c] GaussianMixtures v0.3.0
  [4138dd39] JLD v0.9.1
  [90014a1f] PDMats v0.9.11
  [6e75b9c4] ScikitLearnBase v0.5.0
  [276daf66] SpecialFunctions v0.9.0
  [2913bbd2] StatsBase v0.32.0
  [8bb1440f] DelimitedFiles 
  [8ba89e20] Distributed 
  [37e2e46d] LinearAlgebra 
  [56ddb016] Logging 
  [de0858da] Printf 
  [9a3f8284] Random 
  [10745b16] Statistics 
[ Info: Testing Data
(100000, -873774.6233211119, [99126.1046529068, 873.895347093204], [2197.507715923652 -568.1739557792122 320.0149932086533; -2159.242964513598 778.1711990789546 -344.87112560138615], [[94753.64927329148 1506.8248862798177 -682.5178854489012; 1506.8248862798177 98464.0717293631 443.6492623814754; -682.5178854489012 443.64926238147547 99242.13361144092], [5490.685991505744 -1701.8634445817906 741.2476001881008; -1701.8634445817906 1494.2643310223993 -267.08241288093564; 741.2476001881008 -267.08241288093564 861.3101364121112]])
┌ Warning: rmprocs: process 1 not removed
└ @ Distributed /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Distributed/src/cluster.jl:1015
[ Info: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.617773e+03
      1       1.254755e+03      -3.630185e+02 |        7
      2       1.027493e+03      -2.272624e+02 |        8
      3       8.544739e+02      -1.730186e+02 |        4
      4       8.395789e+02      -1.489500e+01 |        4
      5       7.889433e+02      -5.063558e+01 |        2
      6       7.871515e+02      -1.791788e+00 |        0
      7       7.871515e+02       0.000000e+00 |        0
K-means converged with 7 iterations (objv = 787.1515144671366)
┌ Info: K-means with 272 data points using 7 iterations
└ 11.3 data points per parameter
[ Info: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
┌ Info: EM with 272 data points 0 iterations avll -2.056016
└ 5.8 data points per parameter
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:221
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:221
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:221
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:221
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:631 [inlined]
└ @ Core ./broadcast.jl:631
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:230
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:230
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:631 [inlined]
└ @ Core ./broadcast.jl:631
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:631 [inlined]
└ @ Core ./broadcast.jl:631
[ Info: iteration 1, lowerbound -3.766935
[ Info: iteration 2, lowerbound -3.637099
[ Info: iteration 3, lowerbound -3.500267
[ Info: iteration 4, lowerbound -3.349073
[ Info: iteration 5, lowerbound -3.197695
[ Info: iteration 6, lowerbound -3.064858
[ Info: dropping number of Gaussions to 7
[ Info: iteration 7, lowerbound -2.961204
[ Info: dropping number of Gaussions to 6
[ Info: iteration 8, lowerbound -2.887098
[ Info: dropping number of Gaussions to 4
[ Info: iteration 9, lowerbound -2.819974
[ Info: iteration 10, lowerbound -2.770014
[ Info: iteration 11, lowerbound -2.743247
[ Info: dropping number of Gaussions to 3
[ Info: iteration 12, lowerbound -2.720054
[ Info: iteration 13, lowerbound -2.691528
[ Info: iteration 14, lowerbound -2.660647
[ Info: iteration 15, lowerbound -2.624076
[ Info: iteration 16, lowerbound -2.583078
[ Info: iteration 17, lowerbound -2.539763
[ Info: iteration 18, lowerbound -2.496686
[ Info: iteration 19, lowerbound -2.456045
[ Info: iteration 20, lowerbound -2.418875
[ Info: iteration 21, lowerbound -2.384970
[ Info: iteration 22, lowerbound -2.354033
[ Info: iteration 23, lowerbound -2.327811
[ Info: iteration 24, lowerbound -2.311069
[ Info: iteration 25, lowerbound -2.307906
[ Info: dropping number of Gaussions to 2
[ Info: iteration 26, lowerbound -2.302916
[ Info: iteration 27, lowerbound -2.299259
[ Info: iteration 28, lowerbound -2.299256
[ Info: iteration 29, lowerbound -2.299254
[ Info: iteration 30, lowerbound -2.299254
[ Info: iteration 31, lowerbound -2.299253
[ Info: iteration 32, lowerbound -2.299253
[ Info: iteration 33, lowerbound -2.299253
[ Info: iteration 34, lowerbound -2.299253
[ Info: iteration 35, lowerbound -2.299253
[ Info: iteration 36, lowerbound -2.299253
[ Info: iteration 37, lowerbound -2.299253
[ Info: iteration 38, lowerbound -2.299253
[ Info: iteration 39, lowerbound -2.299253
[ Info: iteration 40, lowerbound -2.299253
[ Info: iteration 41, lowerbound -2.299253
[ Info: iteration 42, lowerbound -2.299253
[ Info: iteration 43, lowerbound -2.299253
[ Info: iteration 44, lowerbound -2.299253
[ Info: iteration 45, lowerbound -2.299253
[ Info: iteration 46, lowerbound -2.299253
[ Info: iteration 47, lowerbound -2.299253
[ Info: iteration 48, lowerbound -2.299253
[ Info: iteration 49, lowerbound -2.299253
[ Info: iteration 50, lowerbound -2.299253
[ Info: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
History[Sat Jan 25 19:34:48 2020: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
, Sat Jan 25 19:34:56 2020: K-means with 272 data points using 7 iterations
11.3 data points per parameter
, Sat Jan 25 19:34:58 2020: EM with 272 data points 0 iterations avll -2.056016
5.8 data points per parameter
, Sat Jan 25 19:35:00 2020: GMM converted to Variational GMM
, Sat Jan 25 19:35:08 2020: iteration 1, lowerbound -3.766935
, Sat Jan 25 19:35:08 2020: iteration 2, lowerbound -3.637099
, Sat Jan 25 19:35:08 2020: iteration 3, lowerbound -3.500267
, Sat Jan 25 19:35:08 2020: iteration 4, lowerbound -3.349073
, Sat Jan 25 19:35:08 2020: iteration 5, lowerbound -3.197695
, Sat Jan 25 19:35:08 2020: iteration 6, lowerbound -3.064858
, Sat Jan 25 19:35:08 2020: dropping number of Gaussions to 7
, Sat Jan 25 19:35:08 2020: iteration 7, lowerbound -2.961204
, Sat Jan 25 19:35:08 2020: dropping number of Gaussions to 6
, Sat Jan 25 19:35:08 2020: iteration 8, lowerbound -2.887098
, Sat Jan 25 19:35:08 2020: dropping number of Gaussions to 4
, Sat Jan 25 19:35:08 2020: iteration 9, lowerbound -2.819974
, Sat Jan 25 19:35:08 2020: iteration 10, lowerbound -2.770014
, Sat Jan 25 19:35:08 2020: iteration 11, lowerbound -2.743247
, Sat Jan 25 19:35:08 2020: dropping number of Gaussions to 3
, Sat Jan 25 19:35:08 2020: iteration 12, lowerbound -2.720054
, Sat Jan 25 19:35:08 2020: iteration 13, lowerbound -2.691528
, Sat Jan 25 19:35:08 2020: iteration 14, lowerbound -2.660647
, Sat Jan 25 19:35:08 2020: iteration 15, lowerbound -2.624076
, Sat Jan 25 19:35:08 2020: iteration 16, lowerbound -2.583078
, Sat Jan 25 19:35:08 2020: iteration 17, lowerbound -2.539763
, Sat Jan 25 19:35:08 2020: iteration 18, lowerbound -2.496686
, Sat Jan 25 19:35:08 2020: iteration 19, lowerbound -2.456045
, Sat Jan 25 19:35:08 2020: iteration 20, lowerbound -2.418875
, Sat Jan 25 19:35:08 2020: iteration 21, lowerbound -2.384970
, Sat Jan 25 19:35:08 2020: iteration 22, lowerbound -2.354033
, Sat Jan 25 19:35:08 2020: iteration 23, lowerbound -2.327811
, Sat Jan 25 19:35:08 2020: iteration 24, lowerbound -2.311069
, Sat Jan 25 19:35:08 2020: iteration 25, lowerbound -2.307906
, Sat Jan 25 19:35:08 2020: dropping number of Gaussions to 2
, Sat Jan 25 19:35:08 2020: iteration 26, lowerbound -2.302916
, Sat Jan 25 19:35:08 2020: iteration 27, lowerbound -2.299259
, Sat Jan 25 19:35:08 2020: iteration 28, lowerbound -2.299256
, Sat Jan 25 19:35:08 2020: iteration 29, lowerbound -2.299254
, Sat Jan 25 19:35:08 2020: iteration 30, lowerbound -2.299254
, Sat Jan 25 19:35:08 2020: iteration 31, lowerbound -2.299253
, Sat Jan 25 19:35:08 2020: iteration 32, lowerbound -2.299253
, Sat Jan 25 19:35:08 2020: iteration 33, lowerbound -2.299253
, Sat Jan 25 19:35:08 2020: iteration 34, lowerbound -2.299253
, Sat Jan 25 19:35:08 2020: iteration 35, lowerbound -2.299253
, Sat Jan 25 19:35:08 2020: iteration 36, lowerbound -2.299253
, Sat Jan 25 19:35:08 2020: iteration 37, lowerbound -2.299253
, Sat Jan 25 19:35:08 2020: iteration 38, lowerbound -2.299253
, Sat Jan 25 19:35:08 2020: iteration 39, lowerbound -2.299253
, Sat Jan 25 19:35:08 2020: iteration 40, lowerbound -2.299253
, Sat Jan 25 19:35:08 2020: iteration 41, lowerbound -2.299253
, Sat Jan 25 19:35:08 2020: iteration 42, lowerbound -2.299253
, Sat Jan 25 19:35:08 2020: iteration 43, lowerbound -2.299253
, Sat Jan 25 19:35:08 2020: iteration 44, lowerbound -2.299253
, Sat Jan 25 19:35:08 2020: iteration 45, lowerbound -2.299253
, Sat Jan 25 19:35:08 2020: iteration 46, lowerbound -2.299253
, Sat Jan 25 19:35:08 2020: iteration 47, lowerbound -2.299253
, Sat Jan 25 19:35:08 2020: iteration 48, lowerbound -2.299253
, Sat Jan 25 19:35:08 2020: iteration 49, lowerbound -2.299253
, Sat Jan 25 19:35:08 2020: iteration 50, lowerbound -2.299253
, Sat Jan 25 19:35:08 2020: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [178.04509222607444, 95.95490777392574]
β = [178.04509222607444, 95.95490777392574]
m = [4.250300733269419 79.28686694435454; 2.0002292577748615 53.85198717245861]
ν = [180.04509222607444, 97.95490777392574]
W = LinearAlgebra.UpperTriangular{Float64,Array{Float64,2}}[[0.18404155547478235 -0.007644049042333597; 0.0 0.008581705166323988], [0.37587636119568063 -0.008953123827356009; 0.0 0.01274866477741161]]
Kind: diag, size256
nx: 100000 sum(zeroth order stats): 99999.99999999996
avll from stats: -1.0148558219674138
avll from llpg:  -1.0148558219674126
avll direct:     -1.0148558219674129
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 100000.0
avll from stats: -1.0207412133341305
avll from llpg:  -1.0207412133341305
avll direct:     -1.0207412133341305
sum posterior: 100000.0
32×26 Array{Float64,2}:
  0.0842011   -0.0329096     0.0158572   -0.0070117    0.0226155  -0.0544115    0.106185     0.0411863    0.136024     0.0439772    0.181563      0.200466    -0.0511013   -0.0207528    0.0242447    -0.0642287    0.19836     -0.0236886   0.0975745   0.158112      0.0747882    0.0963635   -0.0455194  -0.0780092   -0.144269     0.0581942
  0.178547     0.0954276     0.0744115   -0.0797488   -0.161684   -0.0888362    0.045155     0.125439    -0.06295     -0.142628    -0.0954861     0.0462133    0.156127    -0.00706768  -0.0513566     0.0140605   -0.0353875    0.0855198  -0.0423925  -0.000423456  -0.0795693   -0.0496933    0.0182242  -0.0338173   -0.00624462   0.169697
  0.0424906    0.110297      0.0613634   -0.265114    -0.018723   -0.011154     0.0302204    0.126333     0.0789617    0.0824069    0.0423836    -0.0857654    0.0551345   -0.0141011    0.151708     -0.0689408    0.124817     0.198908    0.0736285  -0.00616622   -0.122726    -0.108725     0.106602    0.0333095   -0.0819475   -0.0535651
  0.0914625   -0.0952586    -0.0878358    0.0392577    0.15896     0.0340875    0.101207    -0.0177157    0.0670846    0.0293213    0.0786581     0.00828433   0.0353219   -0.151904    -0.0183934     0.15792     -0.155869     0.0288365   0.0771665   0.131281     -0.0136729    0.0703626   -0.0922566  -0.117306     0.051573    -0.0102074
  0.13656     -0.10057       0.0904522   -0.0774376   -0.108745    0.0471908    0.0328705    0.13373     -0.0358955   -0.0604893   -0.0735674    -0.0235294    0.00904646   0.0107799   -0.168104     -0.0775259    0.132012    -0.201985    0.112326   -0.148276      0.106395    -0.0629621    0.121566   -0.0711635   -0.22321      0.118475
  0.131082    -0.0810621     0.0319812    0.0695961   -0.0285531   0.0891769    0.0203579    0.0240158   -0.116883    -0.0798848    0.129168      0.0509387   -0.170708    -0.0375102    0.0699863    -0.09944     -0.123217     0.0570852  -0.0537232  -0.111314     -0.24462      0.0246837    0.03879    -0.0418914   -0.0690493   -0.0116056
  0.160229    -0.130381     -0.116215     0.0926346   -0.101043    0.101339     0.29554      0.124977     0.0300843   -0.0344845   -0.00641667    0.0329212    0.168505     0.0252688   -0.0782036    -0.0932954   -0.102326     0.0876695  -0.0490933  -0.0459518     0.0829981   -0.0131265   -0.0276305   0.0969905   -0.00647238   0.159224
  0.206194     0.000759881  -0.0492021    0.0414325    0.0576682   0.0355844    0.0938954    0.109175     0.00293382   0.0494313   -0.034288      0.134348    -0.13559     -0.176378    -0.0753478    -0.0539551    0.00162125   0.1177     -0.0746558  -0.0587865     0.0206057    0.0186875    0.155191   -0.0750029    0.014916     0.00422187
 -0.232805     0.171401     -0.112714     0.0520343   -0.0806208   0.0328143    0.0737709   -0.0596563    0.108347    -0.0325843   -0.0850711    -0.0782076    0.0875738    0.0384875   -0.164889      0.0682561   -0.119107    -0.0765764   0.108652    0.0504422     0.0870114   -0.205577     0.124434    0.0109604   -0.107833     0.0335628
 -0.038179     0.00555832    0.0139717    0.123043    -0.0721095  -0.0933463   -0.00994973   0.0482559   -0.0718288    0.00477947  -0.14049       0.198668    -0.206574     0.0834205    0.160576     -0.0358884   -0.0675781    0.0119921  -0.166251    0.0125453     0.0800997   -0.130135    -0.154932    0.0265003   -0.061886    -0.179296
 -0.125668    -0.200268     -0.105952     0.00519809  -0.112674    0.0250935   -0.0512363    0.0193486    0.0807473    0.0224348    0.116967      0.133042     0.111311    -0.00429735   0.029272     -0.0731408    0.0114867   -0.0436429   0.0427209  -0.00833758   -0.125234    -0.154108     0.126753   -0.0125444   -0.115255     0.13722
  0.0238397   -0.298973      0.0191699   -0.125697     0.0369697  -0.105739    -0.0135707    0.227091     0.0634489    0.00431337  -0.000388523   0.00582117  -0.0016466   -0.109908     0.0709533     0.162961    -0.19754     -0.177137   -0.0304944  -0.190878      0.013031    -0.0235098   -0.0166705   0.109097    -0.142557     0.0185923
  0.0919783   -0.054288     -0.0570758    0.0990136   -0.12202     0.14037      0.163243     0.0613731    0.0335376    0.029337     0.0288099    -0.0266976    0.031017     0.0302734    0.0337977    -0.10607     -0.0918678   -0.013245    0.183647    0.0448409    -0.0270227   -0.133404     0.0786968   0.0522114    0.0286297    0.190671
 -0.073805    -0.240712     -0.108919    -0.0783555   -0.0457444  -0.133711     0.187767    -0.0746072    0.10726      0.0652978    0.136914     -0.0387228   -0.0392093    0.0900937   -0.0659305    -0.0445393    0.109118     0.0470781   0.0868951  -0.0711247     0.0430897   -0.0556757    0.0278175  -0.0779689    0.0888676   -0.028508
  0.115259     0.0885167     0.0265901    0.188115     0.0366367   0.0666771   -0.100953    -0.00972581   0.0663901    0.0249607   -0.0663558     0.0751431   -0.0101614    0.0758573   -0.118235     -0.0628127   -0.00493055  -0.0501509  -0.0314653  -0.0246542     0.163682     0.0493306   -0.0225565   0.0742075    0.099652    -0.043104
 -0.0650321    0.0800645    -0.0556575   -0.0828842    0.122962   -0.0343925   -0.0633396   -0.204358    -0.0416464   -0.208785     0.144624      0.0830782   -0.037176     0.171998     0.2014       -0.0714587    0.0164639    0.0217884  -0.0990586  -0.0696061    -0.00580484  -0.0777695    0.0789898  -0.0769803   -0.0164444   -0.0699859
  0.0470181    0.108403      0.0265414    0.13427      0.128587   -0.00215881  -0.176394     0.0405024   -0.0326863    0.0850187    0.167031      0.0737385    0.0723491   -0.0329497   -0.0546239    -0.0148687   -0.0679696    0.028159    0.0042413   0.0289813     0.0514758    0.107523     0.209903    0.206284     0.0228586   -0.041029
  0.0417404   -0.105277      0.00206155   0.0209961    0.109169    0.0991514    0.193051     0.00211815   0.0684419    0.0882798   -0.0461467     0.0127001    0.00695995   0.0300427    0.142824      0.173159    -0.111679     0.142947   -0.12872    -0.118266     -0.09134      0.0911161   -0.169494    0.0582      -0.0942214   -0.0985104
  0.00822548   0.0576755    -0.026368    -0.0489156    0.203827   -0.0960289    0.0541134   -0.129295     0.0101475    0.0140159   -0.0881821    -0.167861     0.0782482   -0.0338554   -0.105536     -0.105223     0.0585684   -0.0360501  -0.060102   -0.102716     -0.0400505    0.0221604   -0.0220657   0.0660002   -0.0416174   -0.102387
  0.100381    -0.116658      0.0894828    0.0781702    0.0346837   0.0583867    0.0437912   -0.0219288   -0.129768     0.0129644   -0.00175987   -0.068068     0.145562     0.119232    -0.1315       -0.08723      0.0813498   -0.0792334   0.141509   -0.284085     -0.0657061    0.0595879   -0.0666255   0.0924013    0.0564372    0.066071
 -0.0355204    0.112628      0.170419     0.116172     0.0974677  -0.146231     0.045649    -0.0980736    0.0545206    0.0531446    0.0802293     0.0629365   -0.0159349    0.0486957    0.000322717   0.143728    -0.0994052    0.0278394   0.046728    0.195704     -0.0233137   -0.0699229    0.0314734  -0.0544162   -0.00742063  -0.00509007
 -0.0647249   -0.0526301     0.117809    -0.0487553    0.172411   -0.0170992    0.130303     0.210917    -0.0768856    0.0517958   -0.0835285    -0.175094    -0.018487     0.0128697   -0.203311      0.0452563   -0.0316655   -0.0773241  -0.0927821   0.183617     -0.0810185    0.0211964   -0.0470173   0.027853     0.0506094    0.00367821
  0.0272427   -0.110255     -0.127168     0.0857698   -0.111955   -0.14145      0.0518568   -0.0694494    0.076801    -0.00251039  -0.117411     -0.188026    -0.17455      0.155317    -0.0029809    -0.013394     0.173368     0.204043    0.009523   -0.017762      0.0314554   -0.100295     0.114654   -0.0488974    0.144882    -0.0883543
  0.0232001    0.0112011     0.105466     0.0091892   -0.065443   -0.0965724    0.0235193    0.0864173    0.0171056    0.0894744   -0.00315684   -0.0103835    0.0579508   -0.0146295    0.113613     -0.0467026    0.0178672   -0.113505    0.150988    0.0177441     0.0972424   -0.178801    -0.116694    0.00219043   0.0527961   -0.0194218
 -0.109219     0.14926       0.0808347    0.125609    -0.0558178   0.0397042   -0.0701568    0.0397898   -0.0509213    0.0302469    0.0173732     0.00170718   0.0912652   -0.0685409   -0.126512      0.012478     0.0813121    0.117668   -0.187084    0.14671       0.0548205    0.0251449   -0.0722584  -0.0929002    0.034111     0.0373685
  0.0113199   -0.156174      0.108431     0.0523142   -0.0294043   0.0148339   -0.114986    -0.0596793   -0.10361     -0.0278617    0.222548      0.138096    -0.0276263   -0.0502055   -0.146437     -0.00454834  -0.160352     0.0944037   0.132831   -0.0980959     0.0638643    0.162451    -0.224514    0.0487307    0.0968064    0.10588
  0.0522111    0.0269268     0.0855662   -0.134747    -0.132619   -0.141631     0.0175511    0.00679063  -0.0323592    0.0316363   -0.225066      0.0170879    0.0672624    0.0321653    0.166663     -0.0442264    0.0604377   -0.108941    0.103883    0.0882762     0.21407      0.0106322    0.134622    0.128341     0.0444587    0.0541279
 -0.0802352    0.0482221    -0.151206     0.177292     0.0353778  -0.171437    -0.0864283   -0.0822387   -0.189361    -0.00722528  -0.0562531    -0.0066666    0.00828944   0.0747506   -0.124715      0.240902     0.0888432    0.105363   -0.0780972   0.189729      0.0964932    0.110027     0.0703684   0.0565467    0.0307095    0.151895
 -0.095235    -0.0981606    -0.105478    -0.0643493   -0.237939   -0.100219    -0.140059     0.0965161   -0.0669909   -0.0937048   -0.0320681     0.116982    -0.184804     0.145578    -0.0713025    -0.139394    -0.0485394    0.0922813   0.0494039  -0.0335695     0.145875     0.052406     0.0517956  -0.0313496    0.065866    -0.0335826
  0.25082     -0.153892      0.0193755   -0.0282336   -0.020901    0.0189639    0.0769818    0.067103     0.079186    -0.104038     0.211654      0.0163789   -0.0460951   -0.083984    -0.14414       0.124078     0.0356916    0.114384   -0.153991   -0.0579028    -0.129035     0.00020593  -0.0312974  -0.18364      0.0337319    0.0979328
  0.192924     0.134883     -0.0100447    0.01614      0.168209    0.0929923    0.0670658   -0.0708094   -0.0127494    0.0164455   -0.0313753    -0.0172081    0.116988     0.0203281   -0.00702437    0.11924     -0.033905    -0.132573   -0.0010767   0.170177     -0.0236104    0.11253      0.139904    0.0596887   -0.0527914   -0.0415855
 -0.152263     0.100326      0.0213618   -0.0123795    0.0134553  -0.00369658  -0.0873024   -0.0860406    0.0956428    0.0506201    0.0645906    -0.145023    -0.0110514   -0.0320923    0.0231978     0.14827      0.109799    -0.133318    0.0524441   0.0970643     0.161424     0.123502    -0.209614   -0.160113     0.0599       0.0105582kind diag, method split
┌ Info: 0: avll = 
└   tll[1] = -1.4163089422762383
[ Info: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.416444
[ Info: iteration 2, average log likelihood -1.416318
[ Info: iteration 3, average log likelihood -1.414829
[ Info: iteration 4, average log likelihood -1.401931
[ Info: iteration 5, average log likelihood -1.385456
[ Info: iteration 6, average log likelihood -1.380312
[ Info: iteration 7, average log likelihood -1.376836
[ Info: iteration 8, average log likelihood -1.374469
[ Info: iteration 9, average log likelihood -1.373335
[ Info: iteration 10, average log likelihood -1.372809
[ Info: iteration 11, average log likelihood -1.372531
[ Info: iteration 12, average log likelihood -1.372367
[ Info: iteration 13, average log likelihood -1.372261
[ Info: iteration 14, average log likelihood -1.372186
[ Info: iteration 15, average log likelihood -1.372129
[ Info: iteration 16, average log likelihood -1.372081
[ Info: iteration 17, average log likelihood -1.372039
[ Info: iteration 18, average log likelihood -1.372001
[ Info: iteration 19, average log likelihood -1.371967
[ Info: iteration 20, average log likelihood -1.371936
[ Info: iteration 21, average log likelihood -1.371908
[ Info: iteration 22, average log likelihood -1.371883
[ Info: iteration 23, average log likelihood -1.371858
[ Info: iteration 24, average log likelihood -1.371834
[ Info: iteration 25, average log likelihood -1.371808
[ Info: iteration 26, average log likelihood -1.371774
[ Info: iteration 27, average log likelihood -1.371713
[ Info: iteration 28, average log likelihood -1.371541
[ Info: iteration 29, average log likelihood -1.371004
[ Info: iteration 30, average log likelihood -1.370265
[ Info: iteration 31, average log likelihood -1.369723
[ Info: iteration 32, average log likelihood -1.369381
[ Info: iteration 33, average log likelihood -1.369140
[ Info: iteration 34, average log likelihood -1.368958
[ Info: iteration 35, average log likelihood -1.368823
[ Info: iteration 36, average log likelihood -1.368727
[ Info: iteration 37, average log likelihood -1.368660
[ Info: iteration 38, average log likelihood -1.368613
[ Info: iteration 39, average log likelihood -1.368578
[ Info: iteration 40, average log likelihood -1.368554
[ Info: iteration 41, average log likelihood -1.368536
[ Info: iteration 42, average log likelihood -1.368523
[ Info: iteration 43, average log likelihood -1.368513
[ Info: iteration 44, average log likelihood -1.368506
[ Info: iteration 45, average log likelihood -1.368500
[ Info: iteration 46, average log likelihood -1.368495
[ Info: iteration 47, average log likelihood -1.368492
[ Info: iteration 48, average log likelihood -1.368489
[ Info: iteration 49, average log likelihood -1.368487
[ Info: iteration 50, average log likelihood -1.368485
┌ Info: EM with 100000 data points 50 iterations avll -1.368485
└ 952.4 data points per parameter
┌ Info: 1
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4164444589451042
│     -1.4163182549440065
│      ⋮
└     -1.3684849120338092
[ Info: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.368691
[ Info: iteration 2, average log likelihood -1.368519
[ Info: iteration 3, average log likelihood -1.367847
[ Info: iteration 4, average log likelihood -1.361359
[ Info: iteration 5, average log likelihood -1.344219
[ Info: iteration 6, average log likelihood -1.332713
[ Info: iteration 7, average log likelihood -1.327463
[ Info: iteration 8, average log likelihood -1.325382
[ Info: iteration 9, average log likelihood -1.324398
[ Info: iteration 10, average log likelihood -1.323679
[ Info: iteration 11, average log likelihood -1.323017
[ Info: iteration 12, average log likelihood -1.322316
[ Info: iteration 13, average log likelihood -1.321565
[ Info: iteration 14, average log likelihood -1.320891
[ Info: iteration 15, average log likelihood -1.320393
[ Info: iteration 16, average log likelihood -1.320090
[ Info: iteration 17, average log likelihood -1.319902
[ Info: iteration 18, average log likelihood -1.319779
[ Info: iteration 19, average log likelihood -1.319689
[ Info: iteration 20, average log likelihood -1.319618
[ Info: iteration 21, average log likelihood -1.319560
[ Info: iteration 22, average log likelihood -1.319511
[ Info: iteration 23, average log likelihood -1.319469
[ Info: iteration 24, average log likelihood -1.319432
[ Info: iteration 25, average log likelihood -1.319398
[ Info: iteration 26, average log likelihood -1.319367
[ Info: iteration 27, average log likelihood -1.319337
[ Info: iteration 28, average log likelihood -1.319308
[ Info: iteration 29, average log likelihood -1.319278
[ Info: iteration 30, average log likelihood -1.319246
[ Info: iteration 31, average log likelihood -1.319211
[ Info: iteration 32, average log likelihood -1.319170
[ Info: iteration 33, average log likelihood -1.319124
[ Info: iteration 34, average log likelihood -1.319068
[ Info: iteration 35, average log likelihood -1.318989
[ Info: iteration 36, average log likelihood -1.318878
[ Info: iteration 37, average log likelihood -1.318739
[ Info: iteration 38, average log likelihood -1.318585
[ Info: iteration 39, average log likelihood -1.318442
[ Info: iteration 40, average log likelihood -1.318314
[ Info: iteration 41, average log likelihood -1.318192
[ Info: iteration 42, average log likelihood -1.318081
[ Info: iteration 43, average log likelihood -1.317975
[ Info: iteration 44, average log likelihood -1.317872
[ Info: iteration 45, average log likelihood -1.317772
[ Info: iteration 46, average log likelihood -1.317683
[ Info: iteration 47, average log likelihood -1.317609
[ Info: iteration 48, average log likelihood -1.317556
[ Info: iteration 49, average log likelihood -1.317519
[ Info: iteration 50, average log likelihood -1.317494
┌ Info: EM with 100000 data points 50 iterations avll -1.317494
└ 473.9 data points per parameter
┌ Info: 2
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.3686913757018773
│     -1.3685188725143809
│      ⋮
└     -1.3174942509250698
[ Info: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.317725
[ Info: iteration 2, average log likelihood -1.317489
[ Info: iteration 3, average log likelihood -1.316676
[ Info: iteration 4, average log likelihood -1.308952
[ Info: iteration 5, average log likelihood -1.290564
[ Info: iteration 6, average log likelihood -1.274744
[ Info: iteration 7, average log likelihood -1.266255
[ Info: iteration 8, average log likelihood -1.261719
[ Info: iteration 9, average log likelihood -1.258852
[ Info: iteration 10, average log likelihood -1.256437
[ Info: iteration 11, average log likelihood -1.254461
[ Info: iteration 12, average log likelihood -1.253014
[ Info: iteration 13, average log likelihood -1.252050
[ Info: iteration 14, average log likelihood -1.251461
[ Info: iteration 15, average log likelihood -1.251157
[ Info: iteration 16, average log likelihood -1.251028
[ Info: iteration 17, average log likelihood -1.250974
[ Info: iteration 18, average log likelihood -1.250950
[ Info: iteration 19, average log likelihood -1.250939
[ Info: iteration 20, average log likelihood -1.250933
[ Info: iteration 21, average log likelihood -1.250929
[ Info: iteration 22, average log likelihood -1.250927
[ Info: iteration 23, average log likelihood -1.250925
[ Info: iteration 24, average log likelihood -1.250924
[ Info: iteration 25, average log likelihood -1.250922
[ Info: iteration 26, average log likelihood -1.250921
[ Info: iteration 27, average log likelihood -1.250920
[ Info: iteration 28, average log likelihood -1.250919
[ Info: iteration 29, average log likelihood -1.250918
[ Info: iteration 30, average log likelihood -1.250917
[ Info: iteration 31, average log likelihood -1.250916
[ Info: iteration 32, average log likelihood -1.250916
[ Info: iteration 33, average log likelihood -1.250915
[ Info: iteration 34, average log likelihood -1.250914
[ Info: iteration 35, average log likelihood -1.250913
[ Info: iteration 36, average log likelihood -1.250912
[ Info: iteration 37, average log likelihood -1.250911
[ Info: iteration 38, average log likelihood -1.250909
[ Info: iteration 39, average log likelihood -1.250908
[ Info: iteration 40, average log likelihood -1.250907
[ Info: iteration 41, average log likelihood -1.250906
[ Info: iteration 42, average log likelihood -1.250905
[ Info: iteration 43, average log likelihood -1.250903
[ Info: iteration 44, average log likelihood -1.250902
[ Info: iteration 45, average log likelihood -1.250900
[ Info: iteration 46, average log likelihood -1.250898
[ Info: iteration 47, average log likelihood -1.250897
[ Info: iteration 48, average log likelihood -1.250895
[ Info: iteration 49, average log likelihood -1.250893
[ Info: iteration 50, average log likelihood -1.250892
┌ Info: EM with 100000 data points 50 iterations avll -1.250892
└ 236.4 data points per parameter
┌ Info: 3
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.317725300134391
│     -1.3174892592748226
│      ⋮
└     -1.2508915013286128
[ Info: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.251194
[ Info: iteration 2, average log likelihood -1.250860
[ Info: iteration 3, average log likelihood -1.249720
[ Info: iteration 4, average log likelihood -1.239219
[ Info: iteration 5, average log likelihood -1.207889
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     9
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.179561
[ Info: iteration 7, average log likelihood -1.175617
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.162867
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     9
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.166247
[ Info: iteration 10, average log likelihood -1.167012
[ Info: iteration 11, average log likelihood -1.152685
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     9
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 12, average log likelihood -1.146101
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 13, average log likelihood -1.154726
[ Info: iteration 14, average log likelihood -1.157432
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     9
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 15, average log likelihood -1.148199
[ Info: iteration 16, average log likelihood -1.155571
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 17, average log likelihood -1.146337
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     9
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 18, average log likelihood -1.151846
[ Info: iteration 19, average log likelihood -1.155874
[ Info: iteration 20, average log likelihood -1.146291
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      9
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 21, average log likelihood -1.141916
[ Info: iteration 22, average log likelihood -1.161541
[ Info: iteration 23, average log likelihood -1.148708
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     9
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 24, average log likelihood -1.143216
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 25, average log likelihood -1.152112
[ Info: iteration 26, average log likelihood -1.154434
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     9
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 27, average log likelihood -1.145629
[ Info: iteration 28, average log likelihood -1.153426
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 29, average log likelihood -1.145020
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     9
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 30, average log likelihood -1.151343
[ Info: iteration 31, average log likelihood -1.155813
[ Info: iteration 32, average log likelihood -1.146283
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      9
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 33, average log likelihood -1.141904
[ Info: iteration 34, average log likelihood -1.161525
[ Info: iteration 35, average log likelihood -1.148712
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     9
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 36, average log likelihood -1.143211
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 37, average log likelihood -1.152106
[ Info: iteration 38, average log likelihood -1.154431
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     9
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 39, average log likelihood -1.145631
[ Info: iteration 40, average log likelihood -1.153424
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 41, average log likelihood -1.145022
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     9
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 42, average log likelihood -1.151342
[ Info: iteration 43, average log likelihood -1.155814
[ Info: iteration 44, average log likelihood -1.146284
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      9
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 45, average log likelihood -1.141905
[ Info: iteration 46, average log likelihood -1.161524
[ Info: iteration 47, average log likelihood -1.148714
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     9
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 48, average log likelihood -1.143211
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 49, average log likelihood -1.152107
[ Info: iteration 50, average log likelihood -1.154432
┌ Info: EM with 100000 data points 50 iterations avll -1.154432
└ 118.1 data points per parameter
┌ Info: 4
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.251194061072998
│     -1.2508596303869035
│      ⋮
└     -1.1544316777959223
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     17
│     18
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 1, average log likelihood -1.145997
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     17
│     18
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 2, average log likelihood -1.143198
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     17
│     18
│     27
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 3, average log likelihood -1.140112
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     17
│     18
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.123411
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     17
│     18
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.078773
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      7
│      8
│     17
│     18
│     25
│     27
│     28
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.044931
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     17
│     18
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.077719
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     17
│     18
│     27
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.063587
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      7
│     17
│     18
│     25
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.041727
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      8
│     17
│     18
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.051306
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     17
│     18
│     27
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 11, average log likelihood -1.060291
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      7
│     17
│     18
│     30
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 12, average log likelihood -1.041231
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      8
│     17
│     18
│     22
│     25
│     27
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 13, average log likelihood -1.045940
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     17
│     18
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 14, average log likelihood -1.072631
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      7
│     17
│     18
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 15, average log likelihood -1.044076
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      8
│     17
│     18
│     22
│     27
│     28
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 16, average log likelihood -1.039708
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      7
│     17
│     18
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 17, average log likelihood -1.065015
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     17
│     18
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 18, average log likelihood -1.055277
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      8
│     17
│     18
│     22
│     27
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 19, average log likelihood -1.041442
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      7
│     17
│     18
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 20, average log likelihood -1.059075
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     17
│     18
│     25
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 21, average log likelihood -1.047458
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      8
│     17
│     18
│     22
│     27
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 22, average log likelihood -1.048386
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      7
│     17
│     18
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 23, average log likelihood -1.063965
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     17
│     18
│     30
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 24, average log likelihood -1.042140
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      7
│      8
│     17
│     18
│     22
│     25
│     27
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 25, average log likelihood -1.040400
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     17
│     18
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 26, average log likelihood -1.073825
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     17
│     18
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 27, average log likelihood -1.047958
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      7
│     11
│     17
│     18
│     22
│     27
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 28, average log likelihood -1.035487
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      8
│     17
│     18
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 29, average log likelihood -1.047545
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│     17
│     18
│     27
│     28
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 30, average log likelihood -1.064239
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      7
│     17
│     18
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 31, average log likelihood -1.051571
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      8
│     11
│     17
│     18
│     25
│     27
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 32, average log likelihood -1.040111
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      7
│     17
│     18
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 33, average log likelihood -1.065618
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│     17
│     18
│     22
│     27
│     28
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 34, average log likelihood -1.051797
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      8
│     17
│     18
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 35, average log likelihood -1.048006
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      7
│     11
│     17
│     18
│     27
│     28
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 36, average log likelihood -1.054355
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     17
│     18
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 37, average log likelihood -1.061923
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      7
│      8
│     17
│     18
│     25
│     27
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 38, average log likelihood -1.039680
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     17
│     18
│     30
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 39, average log likelihood -1.059543
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│     11
│     17
│     18
│     22
│     27
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 40, average log likelihood -1.057772
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      7
│     17
│     18
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 41, average log likelihood -1.054799
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      8
│     17
│     18
│     27
│     28
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 42, average log likelihood -1.043329
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     17
│     18
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 43, average log likelihood -1.058322
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      7
│     11
│     17
│     18
│     25
│     27
│     28
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 44, average log likelihood -1.041875
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      8
│     17
│     18
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 45, average log likelihood -1.060226
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│     17
│     18
│     22
│     27
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 46, average log likelihood -1.059969
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      7
│     17
│     18
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 47, average log likelihood -1.048679
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      8
│     11
│     17
│     18
│     27
│     28
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 48, average log likelihood -1.040072
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      7
│     17
│     18
│     22
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 49, average log likelihood -1.065272
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│     17
│     18
│     25
│     27
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 50, average log likelihood -1.058120
┌ Info: EM with 100000 data points 50 iterations avll -1.058120
└ 59.0 data points per parameter
┌ Info: 5
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.1459965535466323
│     -1.1431983273486415
│      ⋮
└     -1.0581204129955968
┌ Info: Total log likelihood: 
│   tll =
│    251-element Array{Float64,1}:
│     -1.4163089422762383
│     -1.4164444589451042
│     -1.4163182549440065
│     -1.4148286932901244
│      ⋮
│     -1.0400715581395115
│     -1.0652722297244117
└     -1.0581204129955968
32×26 Array{Float64,2}:
 -0.245699     0.0950823    0.0128157    0.0173308   -0.709388    -0.0351844   -0.131739   -0.0483684    0.0582623    0.0438808     0.0683995   -0.138671    -0.00596318  -3.002e-5     0.0402231    0.159792     0.115075    -0.13945      0.0456135    0.0914174    0.226772     0.143123    -0.197955     -0.161894     0.0585983   -0.000246127
 -0.115481     0.100039     0.0229859   -0.057109     0.917062     0.00675815   0.0585949  -0.10273      0.094335     0.067822     -0.00310607  -0.147487    -0.0168476   -0.0549888   -0.00589655   0.156297     0.112292    -0.125537     0.0569316    0.105672     0.0486145    0.0268386   -0.223016     -0.187219     0.0574667    0.039869
  0.15367      0.00865184   0.046683    -0.00526755  -0.109546    -0.0311825    0.0275595   0.0782265   -0.0883541   -0.134079      0.00714257   0.0227641   -0.00444841   0.0237304    0.00952022  -0.04701     -0.0753436    0.0805952   -0.0508519   -0.050981    -0.158668    -0.0209109    0.0270711    -0.0266995   -0.0478055    0.122364
 -0.129694     0.0368545   -0.0766322    0.0336652   -0.128038    -0.0447286    0.0126166   0.0239261   -0.00335832  -0.0485694    -0.0835486    0.0710173   -0.0774496    0.0850405   -0.0447465   -0.023783    -0.088199    -0.00381212   0.0101379    0.0051647    0.0817872   -0.123769     0.000748367  -0.0286998   -0.0430565   -0.0492583
  0.0999341    0.104135     0.0908244   -0.316964     0.0648475   -0.0198836    0.0283368   0.128573     0.12347      0.272141      0.0421625   -0.0307277    0.0322669    0.0238776    0.264692    -0.585415     0.119032     0.188995     0.0808433   -0.00338856  -0.120383    -0.1667       0.102449      0.371465    -0.12992     -0.0760964
  0.0183089    0.120343     0.0458155   -0.201419    -0.0489165   -0.0173199    0.0325996   0.122605    -0.0191337    0.0251758     0.0421299   -0.104916     0.0574604   -0.0373948    0.0243579    0.393701     0.13409      0.206773     0.0673093   -0.00863855  -0.126383    -0.0605077    0.104734     -0.207567    -0.0292407   -0.021994
  0.0368975   -0.102522    -0.0779799    0.0289034    0.139885     0.0250664    0.0869743  -0.0122393    0.0727542    0.0439111     0.0805135   -0.0110938    0.0258577   -0.118295    -0.00886641   0.126429    -0.141575     0.0434786    0.113359     0.111174    -0.0147963    0.102884    -0.0905794    -0.110412     0.0737066    0.0028531
 -0.0791444    0.0314079   -0.150343     0.223192     0.0253934   -0.159903    -0.117589   -0.0834252   -0.19171     -0.0449933    -0.0577554    0.00454021   0.0102434    0.0711113   -0.122744     0.238975     0.0994079    0.107061    -0.076874     0.186651     0.094673     0.107401     0.0627056     0.05517      0.0291435    0.152376
 -0.431709    -0.0539827    0.11811     -0.0669455    0.160215    -0.0131381    0.111176    0.221395    -0.0696319    0.0124739    -0.0273      -0.206748    -0.0922385    0.00411705  -0.244398     0.0909664   -0.041537    -0.0743251   -0.116069    -0.0371731   -0.0513835   -0.0422279   -0.0423438    -0.013674     0.0270184   -0.310435
  0.398215    -0.051889     0.0670181   -0.0238915    0.165831    -0.0300436    0.14351     0.206438    -0.105459     0.0918908    -0.142814    -0.0846892    0.00102721   0.0476628   -0.133866     0.0145036    0.0444938   -0.0735298   -0.050459     0.252671    -0.132619    -6.41776e-5  -0.0523872     0.0638985    0.064759     0.208625
 -0.04855     -0.212438    -0.102049    -0.0773175   -0.0398835   -0.125666     0.159975   -0.0717163    0.0629587    0.0513865     0.132542    -0.0214739   -0.0471699    0.0856008   -0.0259461   -0.0307956    0.0760564    0.0494588    0.0419142   -0.0854508    0.049584    -0.0195599    0.0328464    -0.0845137    0.0695046   -0.0328654
 -0.0145233    0.0669319    0.00091305   0.047444    -0.026223     0.0588522    0.0102003  -0.0168813   -0.0165258   -0.042769      0.0443687    0.0278576    0.0346372    0.0363243    0.0227891   -0.0630935    0.0174797    0.0411096   -0.0336316    0.0503032    0.00816574  -0.0591517    0.0209352    -0.0497918    0.0185195    0.0613528
 -0.135197    -0.204015    -0.0927383    0.00393568  -0.0996094   -0.011936    -0.0471993   0.00674458   0.0997448    0.0214586     0.149791     0.139537     0.0866142   -0.00754645   0.0464099   -0.0733894    0.0335328   -0.0466951    0.0435635    0.00224304  -0.127067    -0.17593      0.139317     -0.00942542  -0.0797614    0.137004
  0.237311    -0.141843     0.0195534    0.00323911  -0.0199165    0.0214974    0.0753491   0.0644946    0.0590813   -0.102555      0.174411     0.0103101   -0.043338    -0.111911    -0.136458     0.122486     0.0571388    0.113114    -0.158343    -0.0503903   -0.117515     0.00898652  -0.0296621    -0.1919       0.0284995    0.108647
 -0.00551271  -0.113849    -0.167249     0.0114224   -0.199234    -0.145186     0.0664277  -0.0085297    0.16912      0.0166153     0.0841691   -0.0745709   -0.148928    -0.214132    -0.147108     0.087979     0.18778      0.16077     -0.0147033    0.0703965    0.00975811  -0.107384     0.115926     -0.0513492    0.140125    -0.0976676
  0.0482293   -0.09744     -0.0673507    0.120135    -0.00797747  -0.139162     0.034299   -0.153689     0.00980239   0.000596638  -0.307214    -0.184943    -0.180368     0.506804     0.121185    -0.1061       0.188809     0.210134     0.040007    -0.069129     0.0215931   -0.0903438    0.118231     -0.0502247    0.153304    -0.0860715
  0.148635    -0.278006    -0.0674725    0.194726    -0.701475     0.0907943    0.268059    0.115607    -0.0363381   -0.0377111     0.130499     0.03455      0.15217      0.0222615   -0.102158    -0.116509    -0.131884     0.0408466    0.130747    -0.0460168    0.0895782    0.0439497   -0.0266541     0.109894    -0.0597541    0.155043
  0.168681    -0.0263439   -0.221752     0.0178025    0.53584      0.0891521    0.309853    0.130756     0.0799898   -0.0319229    -0.16788      0.0339889    0.180577     0.0294439   -0.0701307   -0.0567308   -0.0646103    0.0688675   -0.17508     -0.0463865    0.079124    -0.0776307   -0.0320192     0.103771     0.0308553    0.160379
  0.0125182   -0.152659     0.115899     0.0951163   -0.997449     0.0242083   -0.0758353   0.0444187   -0.173345    -0.034732      0.229092     0.064036    -0.0726709   -0.0235362   -0.106841    -0.0165351   -0.157682     0.100184     0.071368    -0.0911855    0.0332948    0.14825     -0.300092      0.040154    -0.00481062   0.0644672
  0.00937854  -0.162766     0.0952992    0.0479208    1.07624     -0.00727124  -0.161813   -0.125902    -0.0297785   -0.0243577     0.214464     0.18998      0.0367383   -0.0685002   -0.183139     0.00911871  -0.168376     0.0900271    0.178833    -0.0974701    0.108974     0.175322    -0.225106     -0.0321169    0.181181     0.110429
  0.182955     0.133465    -0.0614188    0.0331295    0.168022     0.101923     0.062811   -0.0136618   -0.0154374    0.0145299    -0.0172709   -0.0189869    0.114193     0.0199345   -0.00460428   0.102712    -0.037653    -0.144708    -0.0668179    0.162946    -0.0223192    0.111524     0.155227      0.0533747   -0.0477665   -0.0385139
  0.122142     0.0959979    0.00606963   0.184559     0.0354067    0.105057    -0.114658    0.0053585    0.0730629    0.0223691    -0.065842     0.0696575   -0.00674248   0.0752285   -0.114573    -0.0602067   -0.00962982  -0.0368126   -0.0336365   -0.0258043    0.170707     0.0214907   -0.0187283     0.0535345    0.0759792   -0.0491681
  0.025488    -0.29443      0.0131517   -0.103296     0.0236675   -0.095406    -0.0312131   0.228892     0.0640987    0.00183431   -0.00272643   0.013611     0.0145705   -0.100169     0.0736259    0.161414    -0.196233    -0.174556    -0.0331481   -0.191524    -0.01267     -0.0236867   -0.0183141     0.11003     -0.148784     0.0205763
  0.0799663   -0.0482447    0.018333     0.00138677   0.0133649   -0.0403563    0.102945    0.0371447    0.132877     0.0381598     0.202807     0.19979     -0.025863    -0.021484     0.0275705   -0.0741193    0.196552     0.00638567   0.108321     0.155172     0.0749383    0.0968431   -0.046381     -0.0944594   -0.145027     0.0571159
  0.0330649    0.00316685   0.100888     0.0086407   -0.0604792   -0.102783     0.0212405   0.085104     0.00375712   0.0894825     0.0232772   -0.0146352    0.0578976   -0.0570678    0.106507    -0.0461864    0.0321681   -0.121735     0.152964     0.0239063    0.093544    -0.221543    -0.111898     -0.00683512   0.0559059   -0.0101242
 -0.0151466    0.101893     0.177617     0.117323     0.092944    -0.138926     0.0403888  -0.0979315    0.0523095    0.054766      0.0794034    0.0866389   -0.051446     0.0820002    0.00563643   0.144247    -0.0976985    0.0235156    0.10139      0.186417    -0.0290534   -0.107144     0.030355     -0.0867216    0.0121522   -0.0148336
  0.0552992   -0.105857    -0.0063443    0.0209782    0.127475    -0.514665     0.377224    0.112062     0.0599077    0.0839835    -0.109        0.0632696    0.127426    -0.00436652   0.115832     0.152367    -0.110374     0.133144    -0.231249    -0.112467    -0.091342     0.135792    -0.144986      0.10178     -0.0947395   -0.0985739
  0.0550243   -0.106203    -0.0213784    0.0200223    0.092639     0.659008    -0.0365672  -0.140018     0.0676469    0.0945453     0.0494336   -0.039419     0.0158031    0.0305116    0.181699     0.171224    -0.118295     0.14118     -0.0720734   -0.121438    -0.0919933    0.112026    -0.132078     -0.119728    -0.105956    -0.0974406
  0.0500576   -0.0245014    0.0249142    0.00342476   0.127102    -0.0273949    0.0595085  -0.0794279   -0.0761917    0.00650646   -0.0591088   -0.114296     0.101994     0.0236087   -0.109694    -0.112074     0.0765494   -0.075357     0.0318793   -0.196161    -0.0436831    0.0565125   -0.0450025     0.0516276   -0.00149555  -0.0484559
  0.0496717    0.105635     0.0273981    0.17253      0.136395     0.0101147   -0.176123    0.0284545   -0.0300046    0.096343      0.173243     0.0602018    0.0726152   -0.0138458   -0.0531158   -0.00904376  -0.052643     0.0333705    0.00218211   0.0123964    0.0653891    0.100257     0.220334      0.201358     0.0121426   -0.0559274
  0.0675359    0.00934457   0.0830327   -0.096185    -0.134297    -0.157883     0.0482186   0.02175     -0.0671024    0.0037485    -0.206214     0.0268379    0.061198     0.0630874    0.155868    -0.0515348    0.0639428   -0.131932     0.0945516    0.0746491    0.220955     0.0229937    0.0936286     0.116211     0.0263567    0.0566742
  0.181148    -0.0526133    0.015231    -0.00139961  -0.0164932    0.0502944    0.0711533   0.111675     0.00079083  -0.00641999   -0.060763     0.0368988   -0.0729741   -0.0786911   -0.124566    -0.0583671    0.0678908   -0.0444997    0.0230426   -0.12562      0.0723393   -0.0222216    0.101352     -0.0920907   -0.111986     0.0709121[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      8
│     17
│     18
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 1, average log likelihood -1.044160
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      7
│      8
│     11
│     17
│      ⋮
│     27
│     28
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 2, average log likelihood -1.027656
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      8
│     17
│     18
│     25
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 3, average log likelihood -1.031353
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      7
│      8
│     11
│     17
│      ⋮
│     28
│     30
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.031743
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      8
│     17
│     18
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.037773
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      7
│      8
│     11
│     17
│      ⋮
│     27
│     28
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.022733
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      8
│     17
│     18
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.041513
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      7
│      8
│     11
│     17
│      ⋮
│     28
│     30
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.025751
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      8
│     17
│     18
│     25
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.033712
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      7
│      8
│     11
│     17
│      ⋮
│     27
│     28
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.033194
┌ Info: EM with 100000 data points 10 iterations avll -1.033194
└ 59.0 data points per parameter
kind diag, method kmeans
[ Info: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.950072e+05
      1       6.810613e+05      -2.139459e+05 |       32
      2       6.521402e+05      -2.892112e+04 |       32
      3       6.388419e+05      -1.329827e+04 |       32
      4       6.321179e+05      -6.724072e+03 |       32
      5       6.279010e+05      -4.216907e+03 |       32
      6       6.246628e+05      -3.238121e+03 |       32
      7       6.216105e+05      -3.052369e+03 |       32
      8       6.189951e+05      -2.615375e+03 |       32
      9       6.169336e+05      -2.061529e+03 |       32
     10       6.154082e+05      -1.525406e+03 |       32
     11       6.136267e+05      -1.781468e+03 |       32
     12       6.117271e+05      -1.899624e+03 |       32
     13       6.099686e+05      -1.758433e+03 |       32
     14       6.080947e+05      -1.873973e+03 |       32
     15       6.070109e+05      -1.083808e+03 |       32
     16       6.067105e+05      -3.003277e+02 |       32
     17       6.065785e+05      -1.320518e+02 |       32
     18       6.064897e+05      -8.874608e+01 |       32
     19       6.064220e+05      -6.771779e+01 |       32
     20       6.063756e+05      -4.638679e+01 |       30
     21       6.063277e+05      -4.794486e+01 |       31
     22       6.062812e+05      -4.649671e+01 |       30
     23       6.062330e+05      -4.821883e+01 |       31
     24       6.061834e+05      -4.957305e+01 |       29
     25       6.061264e+05      -5.703574e+01 |       30
     26       6.060765e+05      -4.984358e+01 |       32
     27       6.060349e+05      -4.159974e+01 |       32
     28       6.060077e+05      -2.719226e+01 |       27
     29       6.059914e+05      -1.635142e+01 |       31
     30       6.059792e+05      -1.216214e+01 |       29
     31       6.059688e+05      -1.041601e+01 |       28
     32       6.059606e+05      -8.191915e+00 |       27
     33       6.059547e+05      -5.884895e+00 |       26
     34       6.059479e+05      -6.787636e+00 |       25
     35       6.059395e+05      -8.392419e+00 |       22
     36       6.059268e+05      -1.274538e+01 |       27
     37       6.059092e+05      -1.759276e+01 |       25
     38       6.058872e+05      -2.199218e+01 |       28
     39       6.058668e+05      -2.039701e+01 |       28
     40       6.058381e+05      -2.875858e+01 |       28
     41       6.057938e+05      -4.427055e+01 |       30
     42       6.057516e+05      -4.217895e+01 |       32
     43       6.057091e+05      -4.252264e+01 |       31
     44       6.056483e+05      -6.078056e+01 |       31
     45       6.055715e+05      -7.675877e+01 |       31
     46       6.055083e+05      -6.319928e+01 |       29
     47       6.054484e+05      -5.997410e+01 |       31
     48       6.053973e+05      -5.108915e+01 |       29
     49       6.053573e+05      -3.996382e+01 |       31
     50       6.053189e+05      -3.836863e+01 |       30
K-means terminated without convergence after 50 iterations (objv = 605318.949621847)
┌ Info: K-means with 32000 data points using 50 iterations
└ 37.0 data points per parameter
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.302615
[ Info: iteration 2, average log likelihood -1.266004
[ Info: iteration 3, average log likelihood -1.237117
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     13
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.205545
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.174014
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     9
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.134969
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     16
│     20
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.078465
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      3
│     26
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.079594
[ Info: iteration 9, average log likelihood -1.078080
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      9
│     11
│     14
│     20
│     25
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.014515
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      2
│     16
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 11, average log likelihood -1.065627
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      4
│     26
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 12, average log likelihood -1.057555
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 13, average log likelihood -1.053891
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     20
│     25
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 14, average log likelihood -1.027475
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      9
│     14
│     16
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 15, average log likelihood -1.036835
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      3
│     26
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 16, average log likelihood -1.049969
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      5
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 17, average log likelihood -1.043044
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     20
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 18, average log likelihood -1.030302
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      2
│      3
│      4
│      9
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 19, average log likelihood -1.022932
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     14
│     26
│     27
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 20, average log likelihood -1.048526
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 21, average log likelihood -1.051264
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      3
│     20
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 22, average log likelihood -1.022120
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      9
│     26
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 23, average log likelihood -1.048312
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      2
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 24, average log likelihood -1.040910
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      5
│     14
│     27
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 25, average log likelihood -1.006423
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      3
│      9
│     20
│     25
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 26, average log likelihood -1.033192
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 27, average log likelihood -1.080692
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 28, average log likelihood -1.031733
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      2
│      5
│     30
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 29, average log likelihood -0.992418
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      3
│     14
│     20
│     25
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 30, average log likelihood -1.028542
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      4
│      9
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 31, average log likelihood -1.066234
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 32, average log likelihood -1.050481
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      5
│     16
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 33, average log likelihood -1.008770
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      3
│      9
│     20
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 34, average log likelihood -1.015624
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     14
│     26
│     27
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 35, average log likelihood -1.048699
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     2
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 36, average log likelihood -1.049977
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      3
│      5
│      9
│     16
│     20
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 37, average log likelihood -1.003167
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     25
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 38, average log likelihood -1.068724
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 39, average log likelihood -1.047938
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      5
│      9
│     14
│     16
│     20
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 40, average log likelihood -0.998281
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      3
│     30
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 41, average log likelihood -1.058247
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 42, average log likelihood -1.060935
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      2
│      5
│     20
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 43, average log likelihood -1.016325
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      4
│      9
│     16
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 44, average log likelihood -1.036268
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 45, average log likelihood -1.050915
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     14
│     25
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 46, average log likelihood -1.008818
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      5
│      9
│     20
│     26
│     30
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 47, average log likelihood -1.006233
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 48, average log likelihood -1.078121
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 49, average log likelihood -1.039523
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      2
│      5
│      9
│     20
│     25
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 50, average log likelihood -0.996029
┌ Info: EM with 100000 data points 50 iterations avll -0.996029
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.0144648   0.105075     0.177646     0.116268     0.0949392   -0.143026    0.0348934  -0.0970519    0.0529105    0.0527976     0.08027      0.0872373   -0.0549792    0.083717     0.00134046   0.143127    -0.0964882    0.0217888    0.101371     0.189999    -0.0292722   -0.103766      0.028109   -0.0827198    0.00757313  -0.0139609
  0.0411749  -0.0969145   -0.0944859    0.0315497    0.141089     0.0273124   0.0963473  -0.0127718    0.0821961    0.0327661     0.0808422   -0.0111821    0.0254888   -0.132806    -0.0102262    0.138195    -0.145029     0.044642     0.106975     0.137888    -0.0130361    0.097699     -0.0920654  -0.0989106    0.0763472    0.00701129
  0.228006    0.018653    -0.0321567    0.0758931    0.0418116    0.0409001   0.117392    0.107394    -0.00804233   0.0603359    -0.0540681    0.141873    -0.148365    -0.153245    -0.0399882   -0.0483738    0.00240298   0.126975    -0.0918837   -0.0801168    0.0477921    0.0266974     0.102077   -0.102038     0.0191755    0.00352354
 -0.0718319  -0.0719603   -0.0692161    0.0340595   -0.172011    -0.0942155  -0.0633817   0.0859835   -0.052269    -0.0510548    -0.0742285    0.158502    -0.177814     0.128088     0.0156733   -0.104667    -0.0601267    0.0498071   -0.0470279   -0.0171181    0.108778    -0.0526762    -0.0343657  -0.0658849    0.0141128   -0.0888077
 -0.0796471   0.0314622   -0.151902     0.227845     0.0253161   -0.162705   -0.115681   -0.0835972   -0.192765    -0.0448248    -0.0579343    0.00398894   0.010406     0.0706613   -0.123952     0.241907     0.0988636    0.107669    -0.0769728    0.188303     0.0954101    0.107819      0.0627534   0.0562016    0.0292937    0.152418
  0.0098169   0.0598575   -0.0287113   -0.052493     0.217803    -0.0947345   0.0801388  -0.129787     0.00482224   0.00986344   -0.089837    -0.147227     0.0844313   -0.0419439   -0.0991405   -0.0968509    0.0707425   -0.0596218   -0.068132    -0.102483    -0.0291094    0.0239008    -0.0245458   0.0335417   -0.0509556   -0.136449
 -1.45972    -0.0555377    0.0374256   -0.0636757    0.196333     0.0015635   0.0683135   0.190858    -0.0969308    0.0145628     0.0527137   -0.219111    -0.0375785   -0.12178     -0.196728     0.0879839    0.0165749   -0.0644562   -0.0852069   -0.119361    -0.0912908   -0.0744999    -0.045424   -0.00291847   0.0259962   -0.336481
 -0.0925594   0.153418     0.0935553    0.0985077   -0.051457     0.0415002  -0.0694314   0.0562213   -0.0428411    0.0282806     0.0244099    0.0138151    0.0894155   -0.0655658   -0.169703     0.0248228    0.113223     0.119726    -0.185329     0.138256     0.05603      0.0547268    -0.0881942  -0.125854     0.0378052    0.0366159
 -0.250102    0.195036    -0.119981     0.003549    -0.0888515    0.0273378   0.10367    -0.0610094    0.0788568   -0.0323865    -0.103635    -0.0679136    0.0882914    0.0463916   -0.164664     0.0685264   -0.119766    -0.0721749    0.122972     0.0481329    0.0630959   -0.20382       0.0606346   0.0133762   -0.115363     0.0252833
  0.56107    -0.0520737    0.122949    -0.0393704    0.151816    -0.0289903   0.155685    0.230619    -0.0853077    0.0722619    -0.150542    -0.123918    -0.0505174    0.0814515   -0.195073     0.0397208   -0.0190269   -0.0791606   -0.0829648    0.20733     -0.0980149    0.000609533  -0.0483288   0.0339752    0.0539159    0.0570394
  0.0251839  -0.294545     0.0138565   -0.103856     0.0241511   -0.0946064  -0.0311624   0.228112     0.0642517    0.00186753   -0.00280709   0.013518     0.0148369   -0.10021      0.0734023    0.161411    -0.196421    -0.174955    -0.0340386   -0.191855    -0.0127665   -0.0236082    -0.0178912   0.110128    -0.14873      0.0205695
  0.103005   -0.0556371   -0.0452887    0.0970985   -0.121397     0.139804    0.165116    0.0809932    0.0310624    0.0136928     0.00819037  -0.0248893    0.0649441    0.0236884    0.0332465   -0.122089    -0.0875982   -0.0139927    0.167925     0.0443205   -0.0269047   -0.127171      0.0831115   0.0585029    0.0443487    0.171261
  0.124541   -0.0847366    0.0360011    0.0920396   -0.0350902    0.0703379   0.0150934   0.0531834   -0.116409    -0.122978      0.128018     9.44215e-5  -0.181939    -0.0268575    0.0727735   -0.107291    -0.122267     0.0616411   -0.0510419   -0.112942    -0.243333     0.0273587     0.0403889   0.0125651   -0.0809698   -0.0100142
  0.0611645   0.012118     0.0790924   -0.101378    -0.137273    -0.160053    0.0347928   0.0142146   -0.0687831    0.000215593  -0.21322      0.0193173    0.0634969    0.0606073    0.164943    -0.0491088    0.0567161   -0.133289     0.099606     0.093649     0.225301     0.0259701     0.105437    0.114826     0.045636     0.0565302
  0.183013    0.133528    -0.0596548    0.0345231    0.167108     0.101546    0.0611219  -0.0147396   -0.0121219    0.0137728    -0.0125224   -0.0189153    0.111367     0.0213545   -0.00326101   0.0998854   -0.037031    -0.14516     -0.0666101    0.158408    -0.0222063    0.111407      0.150108    0.0534164   -0.0476205   -0.0422777
  0.0251378   0.00263809   0.110504     0.0155113   -0.059886    -0.107025    0.0239582   0.085119     0.00133018   0.0936568     0.0106315   -0.0069844    0.0426825   -0.0624539    0.120191    -0.0463422    0.0306008   -0.117747     0.13173      0.0261264    0.0899796   -0.237794     -0.118971   -0.00673439   0.0448534   -0.0180746
  0.0582561   0.112195     0.0678284   -0.258289     0.00723273  -0.0181574   0.030509    0.125565     0.0509875    0.14658       0.0422482   -0.0687042    0.0453622   -0.00738951   0.142835    -0.0886755    0.126741     0.198007     0.074026    -0.0060017   -0.123407    -0.112798      0.103655    0.0785643   -0.0788342   -0.0486354
  0.0803462  -0.0485041    0.018386     0.00111436   0.0138359   -0.0395764   0.101989    0.037478     0.133066     0.0379488     0.203288     0.200039    -0.0247636   -0.0217572    0.0276901   -0.0731735    0.196653     0.00305165   0.108848     0.156191     0.0749834    0.0979258    -0.0465319  -0.0952811   -0.145416     0.0572254
  0.100644   -0.123929     0.0876869    0.0750125    0.0432813    0.0558521   0.064427   -0.0274733   -0.156115     0.00973405   -0.0191435   -0.0716667    0.134281     0.0898238   -0.125171    -0.0985172    0.0577927   -0.0957154    0.150562    -0.295847    -0.062252     0.0809695    -0.0674933   0.0743959    0.0581015    0.0452296
  0.158346   -0.155807    -0.14449      0.107739    -0.0976916    0.0870754   0.286474    0.12295      0.0201255   -0.0353355    -0.0143448    0.0341307    0.165511     0.0250369   -0.0861002   -0.0868627   -0.0985488    0.0543942   -0.0177272   -0.0458325    0.0844114   -0.0132865    -0.0293956   0.107555    -0.0151504    0.158531
 -0.136043   -0.203707    -0.0913254    0.00380777  -0.0992491   -0.011382   -0.0487207   0.00769029   0.0990696    0.0218635     0.149899     0.140784     0.0839822   -0.00741181   0.0458171   -0.0733298    0.035414    -0.0469054    0.0435312    0.00261067  -0.127891    -0.178411      0.138516   -0.00806958  -0.0792337    0.137237
  0.130312   -0.0904463    0.0775764   -0.0632665   -0.0874468    0.0342893   0.0294628   0.101349    -0.003833    -0.0533376    -0.0898594   -0.0543032   -0.00530647   0.00680608  -0.159289    -0.0596509    0.131326    -0.199899     0.11659     -0.154081     0.107347    -0.0742661     0.0817164  -0.0685783   -0.244064     0.121317
  0.245873   -0.151723     0.0147625   -0.0048289   -0.019108     0.0219191   0.0808621   0.0643575    0.0591912   -0.104632      0.179149     0.00971005  -0.0451357   -0.106382    -0.136906     0.123167     0.0557071    0.113057    -0.157959    -0.0520752   -0.121948     0.0041944    -0.0303996  -0.194619     0.030031     0.111897
 -0.048389    0.090202    -0.0619727   -0.0815138    0.116134    -0.0363726  -0.0675321  -0.222451    -0.0457494   -0.197313      0.144897     0.0893662   -0.0332436    0.164918     0.186502    -0.0614413    0.0498223    0.0210852   -0.104135    -0.0650666   -0.00204796  -0.0726415     0.0733336  -0.0770252   -0.0235531   -0.0577729
  0.0540875  -0.105833    -0.00998543   0.0204409    0.110864     0.0928125   0.159935   -0.0179534    0.0639514    0.0894807    -0.0275483    0.0107227    0.0671473    0.0100994    0.152209     0.162259    -0.113975     0.137664    -0.149101    -0.117464    -0.0917849    0.122991     -0.140309   -0.0147773   -0.0997534   -0.098471
  0.115445    0.0951272    0.00422107   0.185583     0.0375096    0.106939   -0.106262   -0.00787911   0.0774581    0.0229399    -0.0706113    0.0695484   -0.00615213   0.0722648   -0.112679    -0.0607759   -0.00950984  -0.0419734   -0.0324797   -0.0276331    0.168649     0.0196609    -0.0136356   0.0487937    0.0819981   -0.0455504
 -0.0538313  -0.250891    -0.112167    -0.0797053   -0.0449905   -0.133318    0.18414    -0.073229     0.0928185    0.0797096     0.134869    -0.0330177   -0.0519204    0.0719995   -0.0286981   -0.0293843    0.0602061    0.0487916    0.0701822   -0.0906403    0.0525349   -0.0310788     0.0393783  -0.0777248    0.0762854   -0.0248673
  0.0108472  -0.158317     0.105027     0.0737259   -0.0272995    0.0125032  -0.114083   -0.0351683   -0.106719    -0.0278933     0.222751     0.121036    -0.0208735   -0.0464634   -0.143964    -0.00307575  -0.162706     0.096202     0.124719    -0.0944669    0.0692562    0.161764     -0.268189    0.0078319    0.0839073    0.0859711
 -0.184752    0.0985637    0.0183552   -0.0207251    0.0659397   -0.0151284  -0.0416271  -0.0745156    0.0773254    0.0553639     0.0368771   -0.144367    -0.0115252   -0.0265418    0.0196282    0.159985     0.114124    -0.133115     0.0519906    0.098413     0.139155     0.086815     -0.208312   -0.178728     0.0581346    0.0200785
  0.0505807   0.106942     0.0275384    0.171755     0.136254     0.0120546  -0.176101    0.0298738   -0.0303474    0.0997711     0.177828     0.0672965    0.0710995   -0.0166036   -0.0551216   -0.00758325  -0.0501404    0.0334635    0.00286756   0.0180398    0.0671341    0.103756      0.221682    0.205983     0.0112055   -0.0536657
  0.174504    0.0950794    0.0613499   -0.0776992   -0.161576    -0.125298    0.0468659   0.122917    -0.0591854   -0.13929      -0.0941136    0.0415897    0.148383     0.0644845   -0.046732     0.0134752   -0.0366776    0.103645    -0.0481956    0.00724853  -0.0823752   -0.0628688     0.0191066  -0.0696717   -0.0163127    0.232986
  0.0212466  -0.10604     -0.116484     0.0677488   -0.0957353   -0.14462     0.0491324  -0.0863556    0.0872697    0.00813839   -0.130865    -0.13999     -0.165956     0.178684    -0.00463307  -0.0181637    0.191126     0.185048     0.0158534   -0.00580766   0.0153938   -0.0983997     0.117669   -0.0506598    0.148934    -0.0941412[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     14
│     26
│     27
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 1, average log likelihood -1.055006
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      3
│     14
│     16
│     26
│     27
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 2, average log likelihood -1.012135
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│     14
│     16
│     20
│     25
│     26
│     27
│     30
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 3, average log likelihood -0.981787
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      5
│      9
│     14
│     26
│     27
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.005547
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      2
│      3
│     14
│     16
│     26
│     27
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.010350
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│     14
│     20
│     25
│     26
│     27
│     30
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -0.996492
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      3
│      5
│      9
│     14
│     16
│     26
│     27
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.010488
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     14
│     26
│     27
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.019827
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│     14
│     16
│     20
│     25
│     26
│     27
│     30
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -0.981836
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      2
│      3
│      5
│     14
│     26
│     27
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.012749
┌ Info: EM with 100000 data points 10 iterations avll -1.012749
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.0818827    0.00119596  -0.0815163    0.0116427    0.0376557   -0.227301    -0.112689     0.0169108    0.115567      0.0452516   -0.0430462     0.113581     0.176017     0.161529     -0.012077    -0.0131383    0.0921598   -0.118217     -0.0428997   -0.0230847    0.0192982    0.104329    -0.153575     0.0118966   -0.0437918  -0.100268
 -0.0871788    0.0677573   -0.037895    -0.0452135   -0.0536954    0.0860827   -0.0386756    0.103919    -0.0131946     0.0252787    0.191287      0.0792917   -0.0401106   -0.0380892    -0.053719    -0.0880343   -0.0306524    0.125806      0.0336472    0.0020657   -0.130419    -0.0106091   -0.0108783   -0.101189    -0.0490378  -0.0783263
  0.0114264    0.0440206   -0.130468     0.0782622   -0.0881787    0.117239     0.0687543    0.00196718   0.0367467    -0.00251119   0.00614761   -0.0887586   -0.0691056   -0.0319455     0.336981     0.119612    -0.0312321   -0.0135456     0.111789     0.0348051   -0.103146    -0.0215797   -0.0685593   -0.0930529    0.0559722   0.109642
 -0.0411566   -0.0220281   -0.120632    -0.0199043   -0.02306      0.111906     0.150724     0.0534566    0.0514669     0.0431047    0.0195664    -0.0230713    0.0696955    0.0421995    -0.0498881    0.137396    -0.0597824   -0.213394      0.057832    -0.0322272    0.00321761   0.0544316   -0.109643     0.0608704    0.0451529   0.154314
  0.0199521   -0.162356    -0.020911    -0.100616     0.094033     0.0412443    0.216955     0.144289     0.0324137     0.0381088   -0.0122329     0.0852251   -0.0399054   -0.000331896   0.0468457    0.00499408  -0.0976155   -0.030463     -0.0387091    0.113292     0.0295574   -0.0443887    0.228656    -0.0695674   -0.107691   -0.0269632
  0.0326402    0.0166234   -0.180863    -0.00797303   0.129577     0.0691073    0.0969471   -0.035072     0.121454      0.167794    -0.0359656     0.161524     0.0934553    0.0236534    -0.0249243    0.0772078    0.0199928   -0.0110381    -0.0079998    0.095851    -0.0567374   -0.00802994   0.0822051    0.0769677   -0.131439   -0.054426
  0.0827958   -0.00172177  -0.0333548   -0.0167348   -0.0525765   -0.111387     0.00581617  -0.0843813    0.104176     -0.0676311    0.128338     -0.120301    -0.0571196    0.0445309    -0.0721225    0.130869     0.0312937    0.103772     -0.107762     0.0134061   -0.0987       0.0749319    0.0123404    0.0105357   -0.0548478   0.122494
 -0.098549     0.0344615   -0.0951669    0.100245     0.0626506    0.0692411   -0.0296239    0.149605     0.056505     -0.123303     0.0682062     0.138361     0.0446482    0.053405     -0.00398458  -0.0075377    0.101284     0.0675417    -0.0303163    0.0975039    0.135848     0.0493468    0.227646    -0.00945672  -0.0065677   0.0422997
  0.103255     0.0742037   -0.145251    -0.136692    -0.0345684    0.0542056    0.21023      0.0268013    0.157859     -0.0103938   -0.0745058    -0.0940962   -0.0390218   -0.0338466    -0.0165217    0.149956    -0.138544     0.181873      0.0463159    0.0478921    0.00141741  -0.158554    -0.0770568    0.113623    -0.11498    -0.107979
  0.0751187   -0.0152712    0.00334312  -0.0865208   -0.205915     0.180739     0.198501     0.0815151    0.0496703     0.192525    -0.243819     -0.0966409   -0.0423191   -0.0236095    -0.0682907    0.0241536   -0.0675435    0.02497      -0.00133812  -0.0581178   -0.12656      0.0102924    0.127055     0.0956852    0.0299164   0.0390135
 -0.178558     0.0437734    0.041116    -0.00297752   0.0322061    0.0613861   -0.15128     -0.126417    -0.0179714    -0.0858452   -0.155243     -0.16561     -0.00141391  -0.0657476     0.152828     0.0159715   -0.0333188   -0.0610149    -0.0774105    0.152025     0.18425     -0.173379    -0.1425       0.0555199   -0.072944   -0.0224358
 -0.0294392   -0.0818181   -0.0246629   -0.121483    -0.116816    -0.132439     0.195922    -0.0025408    0.0631233     0.0576099    0.0880768     0.055766     0.0721079   -0.106826      0.0410842   -0.054558     0.00724276  -0.137415      0.0470817    0.00179291  -0.050809     0.133648    -0.126201     0.090115    -0.0837094   0.229203
  0.0517344   -0.0882216   -0.0308571    0.00366221   0.0718628    0.00596216   0.0678011   -0.0293599   -0.13359       0.0250606    0.0739094    -0.0886429    0.0591599   -0.0300761    -0.11776     -0.0794906   -0.0345856    0.071112      0.0477058   -0.0419235   -0.0987618   -0.209624     0.0788807   -0.0386638   -0.0822623  -0.112121
 -0.0601644   -0.0119773   -0.0107147    0.174028     0.151689    -0.0667577    0.0909425   -0.114659     0.00950626    0.0586143   -0.000957854  -0.0067925    0.0535741    0.140722      0.13303      0.146765    -0.144042    -0.0629684    -0.00419221  -0.0473856    0.171279    -0.230851    -0.0506394    0.00434573   0.0894344   0.0620167
  0.0792904    0.131922     0.0207572    0.0161599    0.0455388    0.0998762    0.0574079    0.02887      0.0263609    -0.0337851   -0.0557125     0.0603007   -0.0806677    0.0592639    -0.0310628   -0.0596429    0.0499911   -0.130406      0.0676776    0.140479     0.004885    -0.0861253    0.104804     0.022947    -0.0327597   0.0105826
  0.151459     0.0538334    0.0655772   -0.0721932   -0.0295442    0.0456893    0.0958846    0.034293    -0.107304     -0.193206    -0.115743      0.169817    -0.0986027    0.130503     -0.076569    -0.00412576  -0.0812593   -0.0210268     0.0997198   -0.0178848   -0.0364484   -0.16007      0.106755    -0.0662775   -0.055727   -0.123801
  0.191711    -0.207944     0.0744772   -0.0147991   -0.0997556   -0.0758336    0.0687781   -0.0250851    0.0217304    -0.14999      0.18989      -0.00856191  -0.0213568   -0.0790626    -0.121252    -0.0466047   -0.02429      0.047245     -0.0322955   -0.183437     0.183796    -0.0851543   -0.05013     -0.05798     -0.117304   -0.138523
  0.107576     0.102659    -0.0514754   -0.0866013   -0.036397    -0.135727    -0.0520179   -0.043358     0.0652512     0.0264057    0.113545     -0.030784     0.0675718   -0.0608039    -0.0203967    0.0441041   -0.117357     0.0891481    -0.0261432    0.00755628  -0.0273009   -0.017237     0.0463541    0.0835268    0.0156123   0.0103362
  0.137491     0.0142161    0.0450364    0.0247397    0.187134    -0.0161651   -0.119015     0.0708665   -0.0174604     0.0240162    0.0734173     0.0642936    0.0236319   -0.0806897    -0.0955391    0.129272     0.117628    -0.0821652    -0.107115    -0.0820875    0.161969     0.0235526    0.0512975    0.0362077    0.0386468  -0.0394989
  0.0778944   -0.0702991   -0.0180351   -0.0524136   -0.0150519   -0.184502     0.128765    -0.0909761   -0.0411799     0.0657215   -0.151881     -0.0394663    0.0518212   -0.0411504    -0.136881     0.162439     0.0683041   -0.0102428     0.046945    -0.055614     0.118035    -0.0670161    0.245749     0.13244      0.0328749   0.0922911
  0.0218668   -0.0281064   -0.00129898  -0.0114866    0.00756731   0.0317994    0.0366167    0.0417304    0.0118304    -0.0870752   -0.0386864    -0.10688      0.0514884   -0.321222      0.0509757    0.0327828    0.0114772    0.251018      0.0811514    0.021952     0.0858804    0.177743     0.161729    -0.209043     0.0060511   0.076769
 -0.0230561    0.0432026   -0.0157311   -0.060383    -0.101018    -0.168942    -0.0790288    0.122861    -0.0452676     0.0233059   -0.195711      0.0565896    0.100461     0.0837648     0.0471952   -0.159922     0.0917491   -0.0989911     0.0545073    0.141973    -0.0211511   -0.0695565    0.112442     0.0908578    0.0481112  -0.124308
  0.00186642  -0.0399952   -0.0945964    0.199063     0.0600941   -0.0910276   -0.05334     -0.00143974  -0.0956954    -0.032943    -0.1529       -0.0286302   -0.0655384   -0.119842      0.0582733    0.129142     0.0930039   -0.246801      0.150107     0.0171792    0.0562501    0.0408002    0.241987     0.0498363    0.086694   -0.142732
 -0.112608     0.0736619   -0.177168     0.192081     0.145454    -0.00520676   0.0517565    0.0300644   -0.00640721   -0.0369987   -0.0951122    -0.0983888   -0.0782722    0.00463347   -0.0315244    0.132121    -0.18846      0.0339412     0.0777883    0.207348     0.039584    -0.0772609   -0.180702     0.0537486   -0.141358   -0.0328937
 -0.103621    -0.00515212  -0.0812972    0.0725992    0.139087     0.180035    -0.10676      0.126906    -0.0133903    -0.0964663   -0.0165574     0.248533    -0.0837948   -0.119374      0.0214857   -0.174097     0.0710778   -0.0137229    -0.0653942    0.0281657    0.0446599   -0.120396    -0.0307381    0.0569587   -0.0432673  -0.142747
  0.155291    -0.0790251    0.0943389    0.148226    -0.118239    -0.0822564    0.17378      0.0683846   -0.000893099  -0.126275    -0.0189399     0.0933725    0.0135274    0.130268     -0.0359403   -0.153774    -0.109063     0.0235765     0.0284427   -0.0984199   -0.0516503   -0.0457115    0.0197147    0.128585     0.133778    0.0787611
 -0.0635553    0.11611      0.125813     0.257106     0.107883     0.137417    -0.0207412    0.0778088   -0.0944595     0.164719    -0.114153      0.141572     0.0646959    0.115366      0.092981    -0.140382    -0.0173686   -0.140618     -0.177154     0.00169206  -0.112419    -0.193182    -0.0425941   -0.0672596   -0.0329562  -0.0656905
  0.0531156    0.0605474   -0.00789171  -0.00489867  -0.0475018    0.165083    -0.0500127    0.0321798    0.176827     -0.0399285   -0.110303      0.0275732   -0.00465188   0.121183     -0.134345    -0.0285304   -0.0664702   -0.000354042  -0.136278     0.0280173    0.0381659   -0.109488    -0.198312     0.00752188   0.0119782   0.142096
 -0.103945    -0.122335    -0.0661829    0.167706     0.0304431   -0.085217     0.153573     0.135106     0.0929716    -0.122604     0.00477434    0.0723737    0.0586436    0.0198882    -0.0857115    0.158091    -0.0372374    0.0451258     0.0619555   -0.0424738    0.0214454    0.113725     0.00254871   0.063694     0.0534854  -0.0162233
 -0.0205347    0.064392     0.0170779   -0.0530953    0.171404    -0.0586478   -0.00443358   0.0770687    0.0508421     0.0664191   -0.174954      0.115853    -0.0597571   -0.0161539    -0.0237002   -0.0546457   -0.0189737    0.11618       0.0253129    0.018279     0.0722138   -0.0719716    0.158756    -0.132618    -0.0880938   0.0208107
 -0.130183     0.0711914   -0.0291289    0.0193652    0.0278889    0.109433    -0.0172516    0.0934598    0.0557784     0.00561573   0.224085     -0.119878     0.0951631    0.000465221   0.021613     0.00688423   0.026996     0.215127     -0.0541109   -0.145295     0.0664862    0.0716388   -0.0509292   -0.188754     0.024242    0.159491
  0.0153183   -0.0875525   -0.0579225    0.106751     0.151958    -0.022198    -0.0119719    0.0420667    0.0264574     0.0904182    0.152343      0.221092    -0.113791     0.0591699    -0.133772     0.0580461    0.0722432   -0.133015     -0.0565196   -0.020227     0.0530805   -0.077273     0.0931004    0.0977011   -0.010202    0.0702554kind full, method split
┌ Info: 0: avll = 
└   tll[1] = -1.4213831292875432
[ Info: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.421403
[ Info: iteration 2, average log likelihood -1.421356
[ Info: iteration 3, average log likelihood -1.421330
[ Info: iteration 4, average log likelihood -1.421301
[ Info: iteration 5, average log likelihood -1.421270
[ Info: iteration 6, average log likelihood -1.421236
[ Info: iteration 7, average log likelihood -1.421196
[ Info: iteration 8, average log likelihood -1.421152
[ Info: iteration 9, average log likelihood -1.421101
[ Info: iteration 10, average log likelihood -1.421033
[ Info: iteration 11, average log likelihood -1.420925
[ Info: iteration 12, average log likelihood -1.420720
[ Info: iteration 13, average log likelihood -1.420309
[ Info: iteration 14, average log likelihood -1.419550
[ Info: iteration 15, average log likelihood -1.418454
[ Info: iteration 16, average log likelihood -1.417364
[ Info: iteration 17, average log likelihood -1.416652
[ Info: iteration 18, average log likelihood -1.416314
[ Info: iteration 19, average log likelihood -1.416175
[ Info: iteration 20, average log likelihood -1.416119
[ Info: iteration 21, average log likelihood -1.416096
[ Info: iteration 22, average log likelihood -1.416086
[ Info: iteration 23, average log likelihood -1.416081
[ Info: iteration 24, average log likelihood -1.416079
[ Info: iteration 25, average log likelihood -1.416078
[ Info: iteration 26, average log likelihood -1.416077
[ Info: iteration 27, average log likelihood -1.416077
[ Info: iteration 28, average log likelihood -1.416076
[ Info: iteration 29, average log likelihood -1.416076
[ Info: iteration 30, average log likelihood -1.416075
[ Info: iteration 31, average log likelihood -1.416075
[ Info: iteration 32, average log likelihood -1.416075
[ Info: iteration 33, average log likelihood -1.416075
[ Info: iteration 34, average log likelihood -1.416075
[ Info: iteration 35, average log likelihood -1.416074
[ Info: iteration 36, average log likelihood -1.416074
[ Info: iteration 37, average log likelihood -1.416074
[ Info: iteration 38, average log likelihood -1.416074
[ Info: iteration 39, average log likelihood -1.416074
[ Info: iteration 40, average log likelihood -1.416074
[ Info: iteration 41, average log likelihood -1.416074
[ Info: iteration 42, average log likelihood -1.416074
[ Info: iteration 43, average log likelihood -1.416073
[ Info: iteration 44, average log likelihood -1.416073
[ Info: iteration 45, average log likelihood -1.416073
[ Info: iteration 46, average log likelihood -1.416073
[ Info: iteration 47, average log likelihood -1.416073
[ Info: iteration 48, average log likelihood -1.416073
[ Info: iteration 49, average log likelihood -1.416073
[ Info: iteration 50, average log likelihood -1.416073
┌ Info: EM with 100000 data points 50 iterations avll -1.416073
└ 952.4 data points per parameter
┌ Info: 1
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4214029749062569
│     -1.42135636785797
│      ⋮
└     -1.4160731272939941
[ Info: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.416093
[ Info: iteration 2, average log likelihood -1.416044
[ Info: iteration 3, average log likelihood -1.416016
[ Info: iteration 4, average log likelihood -1.415986
[ Info: iteration 5, average log likelihood -1.415953
[ Info: iteration 6, average log likelihood -1.415915
[ Info: iteration 7, average log likelihood -1.415872
[ Info: iteration 8, average log likelihood -1.415824
[ Info: iteration 9, average log likelihood -1.415772
[ Info: iteration 10, average log likelihood -1.415717
[ Info: iteration 11, average log likelihood -1.415659
[ Info: iteration 12, average log likelihood -1.415599
[ Info: iteration 13, average log likelihood -1.415537
[ Info: iteration 14, average log likelihood -1.415472
[ Info: iteration 15, average log likelihood -1.415403
[ Info: iteration 16, average log likelihood -1.415333
[ Info: iteration 17, average log likelihood -1.415262
[ Info: iteration 18, average log likelihood -1.415195
[ Info: iteration 19, average log likelihood -1.415134
[ Info: iteration 20, average log likelihood -1.415080
[ Info: iteration 21, average log likelihood -1.415036
[ Info: iteration 22, average log likelihood -1.414999
[ Info: iteration 23, average log likelihood -1.414970
[ Info: iteration 24, average log likelihood -1.414946
[ Info: iteration 25, average log likelihood -1.414925
[ Info: iteration 26, average log likelihood -1.414907
[ Info: iteration 27, average log likelihood -1.414892
[ Info: iteration 28, average log likelihood -1.414877
[ Info: iteration 29, average log likelihood -1.414864
[ Info: iteration 30, average log likelihood -1.414852
[ Info: iteration 31, average log likelihood -1.414840
[ Info: iteration 32, average log likelihood -1.414830
[ Info: iteration 33, average log likelihood -1.414819
[ Info: iteration 34, average log likelihood -1.414810
[ Info: iteration 35, average log likelihood -1.414801
[ Info: iteration 36, average log likelihood -1.414792
[ Info: iteration 37, average log likelihood -1.414784
[ Info: iteration 38, average log likelihood -1.414777
[ Info: iteration 39, average log likelihood -1.414770
[ Info: iteration 40, average log likelihood -1.414764
[ Info: iteration 41, average log likelihood -1.414758
[ Info: iteration 42, average log likelihood -1.414752
[ Info: iteration 43, average log likelihood -1.414747
[ Info: iteration 44, average log likelihood -1.414743
[ Info: iteration 45, average log likelihood -1.414738
[ Info: iteration 46, average log likelihood -1.414735
[ Info: iteration 47, average log likelihood -1.414731
[ Info: iteration 48, average log likelihood -1.414728
[ Info: iteration 49, average log likelihood -1.414725
[ Info: iteration 50, average log likelihood -1.414722
┌ Info: EM with 100000 data points 50 iterations avll -1.414722
└ 473.9 data points per parameter
┌ Info: 2
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4160927629108433
│     -1.4160442585838142
│      ⋮
└     -1.4147218534005002
[ Info: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.414738
[ Info: iteration 2, average log likelihood -1.414686
[ Info: iteration 3, average log likelihood -1.414653
[ Info: iteration 4, average log likelihood -1.414617
[ Info: iteration 5, average log likelihood -1.414578
[ Info: iteration 6, average log likelihood -1.414535
[ Info: iteration 7, average log likelihood -1.414488
[ Info: iteration 8, average log likelihood -1.414440
[ Info: iteration 9, average log likelihood -1.414392
[ Info: iteration 10, average log likelihood -1.414346
[ Info: iteration 11, average log likelihood -1.414302
[ Info: iteration 12, average log likelihood -1.414260
[ Info: iteration 13, average log likelihood -1.414220
[ Info: iteration 14, average log likelihood -1.414180
[ Info: iteration 15, average log likelihood -1.414139
[ Info: iteration 16, average log likelihood -1.414097
[ Info: iteration 17, average log likelihood -1.414052
[ Info: iteration 18, average log likelihood -1.414007
[ Info: iteration 19, average log likelihood -1.413960
[ Info: iteration 20, average log likelihood -1.413913
[ Info: iteration 21, average log likelihood -1.413867
[ Info: iteration 22, average log likelihood -1.413823
[ Info: iteration 23, average log likelihood -1.413782
[ Info: iteration 24, average log likelihood -1.413744
[ Info: iteration 25, average log likelihood -1.413710
[ Info: iteration 26, average log likelihood -1.413680
[ Info: iteration 27, average log likelihood -1.413653
[ Info: iteration 28, average log likelihood -1.413630
[ Info: iteration 29, average log likelihood -1.413609
[ Info: iteration 30, average log likelihood -1.413590
[ Info: iteration 31, average log likelihood -1.413573
[ Info: iteration 32, average log likelihood -1.413558
[ Info: iteration 33, average log likelihood -1.413545
[ Info: iteration 34, average log likelihood -1.413532
[ Info: iteration 35, average log likelihood -1.413521
[ Info: iteration 36, average log likelihood -1.413511
[ Info: iteration 37, average log likelihood -1.413501
[ Info: iteration 38, average log likelihood -1.413492
[ Info: iteration 39, average log likelihood -1.413483
[ Info: iteration 40, average log likelihood -1.413475
[ Info: iteration 41, average log likelihood -1.413467
[ Info: iteration 42, average log likelihood -1.413460
[ Info: iteration 43, average log likelihood -1.413453
[ Info: iteration 44, average log likelihood -1.413445
[ Info: iteration 45, average log likelihood -1.413438
[ Info: iteration 46, average log likelihood -1.413432
[ Info: iteration 47, average log likelihood -1.413425
[ Info: iteration 48, average log likelihood -1.413418
[ Info: iteration 49, average log likelihood -1.413411
[ Info: iteration 50, average log likelihood -1.413404
┌ Info: EM with 100000 data points 50 iterations avll -1.413404
└ 236.4 data points per parameter
┌ Info: 3
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4147383018011581
│     -1.4146859302423926
│      ⋮
└     -1.413404469562099
[ Info: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.413409
[ Info: iteration 2, average log likelihood -1.413358
[ Info: iteration 3, average log likelihood -1.413316
[ Info: iteration 4, average log likelihood -1.413271
[ Info: iteration 5, average log likelihood -1.413218
[ Info: iteration 6, average log likelihood -1.413155
[ Info: iteration 7, average log likelihood -1.413082
[ Info: iteration 8, average log likelihood -1.413000
[ Info: iteration 9, average log likelihood -1.412910
[ Info: iteration 10, average log likelihood -1.412815
[ Info: iteration 11, average log likelihood -1.412720
[ Info: iteration 12, average log likelihood -1.412627
[ Info: iteration 13, average log likelihood -1.412539
[ Info: iteration 14, average log likelihood -1.412457
[ Info: iteration 15, average log likelihood -1.412381
[ Info: iteration 16, average log likelihood -1.412312
[ Info: iteration 17, average log likelihood -1.412249
[ Info: iteration 18, average log likelihood -1.412191
[ Info: iteration 19, average log likelihood -1.412137
[ Info: iteration 20, average log likelihood -1.412088
[ Info: iteration 21, average log likelihood -1.412043
[ Info: iteration 22, average log likelihood -1.412001
[ Info: iteration 23, average log likelihood -1.411962
[ Info: iteration 24, average log likelihood -1.411925
[ Info: iteration 25, average log likelihood -1.411891
[ Info: iteration 26, average log likelihood -1.411860
[ Info: iteration 27, average log likelihood -1.411830
[ Info: iteration 28, average log likelihood -1.411802
[ Info: iteration 29, average log likelihood -1.411775
[ Info: iteration 30, average log likelihood -1.411750
[ Info: iteration 31, average log likelihood -1.411726
[ Info: iteration 32, average log likelihood -1.411703
[ Info: iteration 33, average log likelihood -1.411682
[ Info: iteration 34, average log likelihood -1.411661
[ Info: iteration 35, average log likelihood -1.411641
[ Info: iteration 36, average log likelihood -1.411623
[ Info: iteration 37, average log likelihood -1.411605
[ Info: iteration 38, average log likelihood -1.411587
[ Info: iteration 39, average log likelihood -1.411570
[ Info: iteration 40, average log likelihood -1.411554
[ Info: iteration 41, average log likelihood -1.411538
[ Info: iteration 42, average log likelihood -1.411523
[ Info: iteration 43, average log likelihood -1.411508
[ Info: iteration 44, average log likelihood -1.411493
[ Info: iteration 45, average log likelihood -1.411479
[ Info: iteration 46, average log likelihood -1.411465
[ Info: iteration 47, average log likelihood -1.411451
[ Info: iteration 48, average log likelihood -1.411437
[ Info: iteration 49, average log likelihood -1.411424
[ Info: iteration 50, average log likelihood -1.411410
┌ Info: EM with 100000 data points 50 iterations avll -1.411410
└ 118.1 data points per parameter
┌ Info: 4
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4134086180021277
│     -1.413358294212097
│      ⋮
└     -1.4114104718209115
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.411408
[ Info: iteration 2, average log likelihood -1.411340
[ Info: iteration 3, average log likelihood -1.411278
[ Info: iteration 4, average log likelihood -1.411207
[ Info: iteration 5, average log likelihood -1.411121
[ Info: iteration 6, average log likelihood -1.411016
[ Info: iteration 7, average log likelihood -1.410890
[ Info: iteration 8, average log likelihood -1.410747
[ Info: iteration 9, average log likelihood -1.410591
[ Info: iteration 10, average log likelihood -1.410432
[ Info: iteration 11, average log likelihood -1.410276
[ Info: iteration 12, average log likelihood -1.410130
[ Info: iteration 13, average log likelihood -1.409995
[ Info: iteration 14, average log likelihood -1.409873
[ Info: iteration 15, average log likelihood -1.409764
[ Info: iteration 16, average log likelihood -1.409666
[ Info: iteration 17, average log likelihood -1.409578
[ Info: iteration 18, average log likelihood -1.409500
[ Info: iteration 19, average log likelihood -1.409430
[ Info: iteration 20, average log likelihood -1.409367
[ Info: iteration 21, average log likelihood -1.409310
[ Info: iteration 22, average log likelihood -1.409259
[ Info: iteration 23, average log likelihood -1.409212
[ Info: iteration 24, average log likelihood -1.409168
[ Info: iteration 25, average log likelihood -1.409128
[ Info: iteration 26, average log likelihood -1.409091
[ Info: iteration 27, average log likelihood -1.409057
[ Info: iteration 28, average log likelihood -1.409024
[ Info: iteration 29, average log likelihood -1.408993
[ Info: iteration 30, average log likelihood -1.408964
[ Info: iteration 31, average log likelihood -1.408936
[ Info: iteration 32, average log likelihood -1.408909
[ Info: iteration 33, average log likelihood -1.408884
[ Info: iteration 34, average log likelihood -1.408859
[ Info: iteration 35, average log likelihood -1.408835
[ Info: iteration 36, average log likelihood -1.408812
[ Info: iteration 37, average log likelihood -1.408790
[ Info: iteration 38, average log likelihood -1.408768
[ Info: iteration 39, average log likelihood -1.408747
[ Info: iteration 40, average log likelihood -1.408727
[ Info: iteration 41, average log likelihood -1.408707
[ Info: iteration 42, average log likelihood -1.408688
[ Info: iteration 43, average log likelihood -1.408669
[ Info: iteration 44, average log likelihood -1.408651
[ Info: iteration 45, average log likelihood -1.408634
[ Info: iteration 46, average log likelihood -1.408617
[ Info: iteration 47, average log likelihood -1.408600
[ Info: iteration 48, average log likelihood -1.408584
[ Info: iteration 49, average log likelihood -1.408568
[ Info: iteration 50, average log likelihood -1.408553
┌ Info: EM with 100000 data points 50 iterations avll -1.408553
└ 59.0 data points per parameter
┌ Info: 5
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.411407566570411
│     -1.4113403185229516
│      ⋮
└     -1.4085532144574968
┌ Info: Total log likelihood: 
│   tll =
│    251-element Array{Float64,1}:
│     -1.4213831292875432
│     -1.4214029749062569
│     -1.42135636785797
│     -1.4213295242511994
│      ⋮
│     -1.4085840562885972
│     -1.4085684202149422
└     -1.4085532144574968
32×26 Array{Float64,2}:
  0.21282    -0.885904    -0.0814354  -0.149917     0.40667     0.131365     0.067476     0.752354   -0.231087    -0.818658   -0.480381    -0.296667   -0.0549885   -0.635286    -0.157003     -0.291282   -0.150685   -0.0168271  -0.395562   -0.327905     0.152343     0.410461    0.218124    0.102605     0.130138     -0.191108
  0.278945   -0.626095    -0.316284   -0.488286     0.578302    0.0401361    0.174552     0.0915545   0.183539    -0.198621    1.02625      0.561021    0.449718    -0.481114    -0.0145731    -0.326965    0.123965    0.548937   -0.0921995  -0.125783    -0.349476     0.204821    0.0501902   0.348462     0.533329      0.292513
  0.0272692   0.172406     0.23364     0.199956    -0.320662   -0.215119    -0.159372     0.110138    0.4741       0.314625   -0.0494137    0.242299   -0.146404    -0.509047     0.0503782     0.655373   -0.54312     0.405657   -0.446628   -0.214765    -0.559154     0.214892   -0.30463    -0.0479351    0.513338     -0.495807
  0.410774   -0.0457094    0.161523    0.176397    -0.538172   -0.20458     -0.390276    -0.372526   -0.378236    -0.212401    0.0305955    0.212936   -0.343951     0.285102    -0.242671      0.0914352  -0.489284    0.0908936  -0.84494     0.0174557   -0.094376     0.167544    0.589205   -0.202652     0.12314       0.132354
 -0.193035    0.414353    -0.641951   -0.123755    -0.0340336   0.574624    -0.0761677    0.17966    -0.299463    -0.0339016   0.286591     0.128387   -0.627175     0.184247    -0.486381     -0.39405    -0.300182    0.579127    0.224185    0.605547    -0.426025     0.297752    0.45355    -0.188136     0.106411      0.553245
 -0.0885862   0.300627    -0.541528    0.208421     0.0511284  -0.00149091  -0.0414839    0.243331    0.0348857   -0.170854    0.126363    -0.268127   -0.29651     -0.265127    -0.329402      0.711715   -0.291338   -0.0159594   0.182845    0.313909    -0.265411     0.482076    0.08156     0.0536575    0.0911587     0.0743366
  0.990119   -0.0628194   -0.544886   -0.152872     0.0421765   0.394928     0.114086     0.169671   -0.374472    -0.158434    0.835417    -0.840455    0.131176     0.239666     0.254981      0.714305   -0.369075    0.250164    0.335028    0.0640264   -0.0348415   -0.105727    0.439119    0.014987    -0.863828      0.351048
  0.597875   -0.198649     0.0652951   0.460508    -0.131498   -0.462516     0.281493     0.022477   -0.479882    -0.673271    0.447506    -0.189715    0.0192064   -0.435348     0.358961      0.881903    0.112024    0.0326063  -0.24305    -0.226015    -0.292407     0.168496    0.061579   -0.449183    -0.163589      0.372014
  0.109705   -0.0799256    0.113001   -0.474185    -0.0182643  -0.172699    -0.212716     0.582733    0.231368     0.212859   -0.141237     0.127441    0.0826168   -0.0222521    0.173277      0.0498725  -0.0535065   0.106773   -0.0800179  -0.321391     0.455339    -0.0957092  -0.453274   -0.303574    -0.41502      -0.0864852
  0.147331    0.0912174   -0.112622    0.235743    -0.199123    0.451749     0.0341455   -0.250409   -0.0138773    0.0289831  -0.210134    -0.0297637  -0.476387     0.214292     0.318267     -0.0808968   0.211202    0.637704   -0.134484   -0.460113    -0.189567     0.16311    -0.640152   -0.0931441   -0.27231       0.141449
  0.177795   -0.129776    -0.0356471  -0.136173    -0.386672   -0.180125     0.171729     0.0707027   0.0868385   -0.046891   -0.0173092    0.0539247  -0.0660895    0.0627539   -0.00757952    0.136048   -0.150822   -0.0398085   0.112044   -0.0251904   -0.0941872    0.0207245   0.135822   -0.0941859    0.199286      0.033437
 -0.0449325   0.0808976   -0.0795996   0.0398671    0.371213    0.1548       0.00751581  -0.0518494   0.0847177    0.0471772   0.0173474    0.0621388   0.145142     0.0171882    0.000559594  -0.1184      0.0956009  -0.0743639  -0.0102256   0.0332819    0.163571    -0.0825198  -0.0123765   0.180117    -0.0687874    -0.0341585
 -0.131133    0.186515    -0.238648    0.644433     0.310684    0.583098    -0.388342    -0.686261   -0.37576     -0.217231   -0.119884    -0.640519    0.216264     0.149932     0.24833      -0.0259093  -0.236331   -0.165346   -0.300506   -0.130794    -0.126182    -0.158738    0.649685   -0.134591    -0.0488506    -0.131198
 -0.363932   -0.0510943   -0.119       0.555977     0.221189    0.0577557   -0.0115149    0.0560568  -0.205585    -0.374374   -0.0435938   -0.631978    0.0519273    0.00775544  -0.1478        0.0200895   0.146939    0.0168219   0.262341    0.00396778   0.15693     -0.587909    0.214881   -0.185597    -0.587489     -0.188443
 -0.179195    0.335949     0.504577    0.219458     0.263721   -0.02432      0.10381     -0.133708   -0.597677    -0.190117    0.121869     0.3936     -0.0745484   -0.263616     0.100035     -0.206227    0.463034   -0.0509678  -0.142646    0.291528    -0.383338    -0.204399    0.069496    0.349333    -0.000652052   0.14653
  0.422498    0.982834     0.836414    0.8099       0.149695    0.00516141   0.679225    -0.0951596   0.112678     0.314006   -0.142181     0.787247   -0.0109403    1.02741      0.253945      0.343983    0.273754   -0.157951    0.194081    0.269717     0.266361    -0.187346   -0.295534   -0.42384      0.221134      0.224844
 -0.134324   -0.413074     0.544363   -0.198742    -0.871952    0.103588    -0.209944     0.251131   -0.00553449   0.101798   -0.256659    -0.397535    0.352039    -0.0668043    0.0243834    -0.117434   -0.777521   -0.0512121   0.208514   -0.0608514   -0.0343454   -0.716742    0.161908   -0.375038     0.114782     -0.0784637
 -0.240169    0.293065     0.209871    0.317724    -0.722103   -0.354625    -0.011884     0.0472797  -0.0221825   -0.457647   -0.286915    -0.675839   -0.212074     0.371653    -0.0499201     0.515663   -0.178101    0.0399913  -0.251294   -0.137001     0.465108    -0.581236    0.301783   -0.739514    -0.637888     -0.706522
  0.099737   -0.00912665  -0.196527   -1.05602      0.29084    -0.136889     0.0683929    0.475285    0.0198153    0.0718821  -0.00674321  -0.125757    0.374609     0.447203     0.0840831    -0.608721    0.244885   -0.369046    0.450395    0.110645     0.836872    -0.224974    0.113497   -0.403886    -0.376274      0.121792
 -0.329592    0.0481876    0.135558    0.273467     0.59589    -0.116162     0.501431     0.416955    0.0625934    0.162312   -0.0819786   -0.353877    0.770174    -0.480144     0.125002      0.0794753   0.361606   -0.473129    0.487355   -0.349486     0.13102     -0.721431   -0.249294   -0.00404265  -0.121301     -0.215916
 -0.932502   -0.103726    -0.104587    0.532253     0.402977    0.29783     -0.318602    -0.242771    0.267369     0.204923   -0.555498    -0.540676   -0.32026     -0.22715     -0.0585076    -0.536871   -0.297585    0.223854   -0.940631   -0.174763    -0.0801623   -0.334871   -0.712836   -0.219915    -0.567934      0.365052
 -0.653925   -0.306231    -0.0163469   0.247653     0.192271    0.62558     -0.176902    -0.0834843   0.143157    -0.447132   -0.0540491   -0.671466   -0.226587    -0.141809    -0.00774692   -0.0515992   0.414658    1.03856     0.69558     0.0909301    0.229255     0.0551147  -0.406703   -0.111025    -0.391601     -0.310507
  0.0125164   0.0137993    0.0144602   0.148325     0.0591423   0.00147942   0.185203    -0.0406927  -0.102427     0.0443954  -0.00105452   0.283452   -0.0843547   -0.0382608   -0.238944      0.0954597  -0.113022   -0.112194   -0.0318746   0.25273     -0.429918     0.066553    0.0243952   0.16115      0.194917      0.149108
  0.406205    0.200351    -0.165823   -0.233538    -0.20162    -0.216315    -0.379452     0.333567    0.237828    -0.390061    0.00329885   0.254871   -0.0399006    0.0706374    0.401774      0.352845    0.208011    0.135239   -0.230598   -0.265932     0.393024     0.727571   -0.0714999  -0.201846     0.00469401   -0.184291
  0.340483   -0.793234    -0.488522    0.269477     0.118071    0.193505     0.145648    -0.0509094  -0.247737     0.0285423  -0.450151     0.054072   -0.0754491    0.322094    -0.665044     -0.179533    0.288126   -0.658377    0.0547581   0.0685646    0.234151    -0.364665    0.19368     0.71699     -0.2359        0.163615
  0.085254    0.252787    -0.167638    0.23505     -0.437627    0.192125     0.120967    -0.580477    0.120812     0.29583     0.0649827   -0.183833   -0.365094     0.41644     -0.501898     -0.0666065   0.127843   -0.177956    0.821833    0.428696    -0.00509655  -0.764817    0.5405      0.616386     0.142023      0.047685
  0.20788     0.181732    -0.436708   -0.108758     0.338454    0.0425253    0.0236269   -0.247912    0.450541     0.735419    0.22577      0.237698   -0.144581    -0.111458    -0.097341     -0.417751   -0.389445   -0.539173   -0.186643   -0.13508      0.0802906    0.159339   -0.0313107   0.944626     0.327409      0.167466
  0.3084     -0.232061    -0.126476    0.0190624    0.291227   -0.365625     0.21107      0.114546    0.364252    -0.0286876   0.271204     0.110442    0.788099     0.353212    -0.455954      0.522176   -0.0460889  -0.91846    -0.108657    0.390761     0.0613129   -0.0243649   0.718454    0.260038     0.202946      0.140344
 -0.318215   -0.182238     0.252408   -0.498787    -0.472162    0.0318503    0.457242     0.464997    0.383519     0.259855    0.128712     0.420163   -0.00901971   0.347837     0.154381     -0.421523   -0.0341004   0.245461    0.767813    0.271225    -0.460185    -0.0766874  -0.507196    0.0231504    0.363049      0.331544
  0.117037    0.585942    -0.146887   -0.748332    -0.333198   -0.133573     0.304675    -0.737147    0.132502     0.433196    0.0120832    0.950366   -0.322825    -0.0395165    0.0831317    -0.103979    0.213317    0.433807    0.149939   -0.216703     0.348324     0.356528    0.0689631   0.0845321    0.374604     -0.120624
 -0.710386    0.0527566    0.206021   -0.00824107   0.318259   -0.0606696   -0.002332     0.0673285   0.198798     0.467732   -0.693346     0.583541   -0.0490587    0.0477091   -0.417703     -0.687471    0.0780642  -0.265381   -0.0738311   0.248599     0.210154    -0.148368   -0.148699    0.15623      0.309672     -0.239895
 -0.247854    0.225773     0.207886    0.0848482    0.456554   -0.346527    -0.0115471   -0.285733    0.202597     0.272123   -0.132599     0.520468    0.546416    -0.0866449    0.998085     -0.401787    0.711553   -0.359808   -0.184688    0.297679     0.289971     0.136616   -0.155023    0.0731129   -0.28223      -0.43194[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.408538
[ Info: iteration 2, average log likelihood -1.408524
[ Info: iteration 3, average log likelihood -1.408510
[ Info: iteration 4, average log likelihood -1.408496
[ Info: iteration 5, average log likelihood -1.408483
[ Info: iteration 6, average log likelihood -1.408470
[ Info: iteration 7, average log likelihood -1.408457
[ Info: iteration 8, average log likelihood -1.408445
[ Info: iteration 9, average log likelihood -1.408432
[ Info: iteration 10, average log likelihood -1.408420
┌ Info: EM with 100000 data points 10 iterations avll -1.408420
└ 59.0 data points per parameter
kind full, method kmeans
[ Info: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.726958e+05
      1       6.988606e+05      -2.738351e+05 |       32
      2       6.870769e+05      -1.178374e+04 |       32
      3       6.824627e+05      -4.614191e+03 |       32
      4       6.800375e+05      -2.425131e+03 |       32
      5       6.785103e+05      -1.527195e+03 |       32
      6       6.774295e+05      -1.080889e+03 |       32
      7       6.766181e+05      -8.113212e+02 |       32
      8       6.759459e+05      -6.722418e+02 |       32
      9       6.754220e+05      -5.239248e+02 |       32
     10       6.749754e+05      -4.465475e+02 |       32
     11       6.745926e+05      -3.827904e+02 |       32
     12       6.742892e+05      -3.034022e+02 |       32
     13       6.740489e+05      -2.403526e+02 |       32
     14       6.738505e+05      -1.983481e+02 |       32
     15       6.736596e+05      -1.908801e+02 |       32
     16       6.734731e+05      -1.865627e+02 |       32
     17       6.733055e+05      -1.675982e+02 |       32
     18       6.731540e+05      -1.514451e+02 |       32
     19       6.730104e+05      -1.436013e+02 |       32
     20       6.728850e+05      -1.254825e+02 |       32
     21       6.727741e+05      -1.108364e+02 |       32
     22       6.726697e+05      -1.043871e+02 |       32
     23       6.725764e+05      -9.330639e+01 |       32
     24       6.724946e+05      -8.184481e+01 |       32
     25       6.724176e+05      -7.701606e+01 |       32
     26       6.723347e+05      -8.287239e+01 |       32
     27       6.722575e+05      -7.719564e+01 |       32
     28       6.721818e+05      -7.567350e+01 |       32
     29       6.721047e+05      -7.707911e+01 |       32
     30       6.720279e+05      -7.685997e+01 |       32
     31       6.719563e+05      -7.161947e+01 |       32
     32       6.718890e+05      -6.723443e+01 |       32
     33       6.718293e+05      -5.971345e+01 |       32
     34       6.717818e+05      -4.749408e+01 |       32
     35       6.717333e+05      -4.850441e+01 |       32
     36       6.716878e+05      -4.556015e+01 |       32
     37       6.716374e+05      -5.041010e+01 |       32
     38       6.715877e+05      -4.969545e+01 |       32
     39       6.715416e+05      -4.601946e+01 |       32
     40       6.715056e+05      -3.604194e+01 |       32
     41       6.714684e+05      -3.716548e+01 |       32
     42       6.714302e+05      -3.826107e+01 |       32
     43       6.713921e+05      -3.803439e+01 |       32
     44       6.713565e+05      -3.567711e+01 |       32
     45       6.713257e+05      -3.076297e+01 |       32
     46       6.712920e+05      -3.370501e+01 |       32
     47       6.712581e+05      -3.385963e+01 |       32
     48       6.712197e+05      -3.840614e+01 |       32
     49       6.711846e+05      -3.511780e+01 |       32
     50       6.711555e+05      -2.909233e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 671155.5139186991)
┌ Info: K-means with 32000 data points using 50 iterations
└ 37.0 data points per parameter
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.420647
[ Info: iteration 2, average log likelihood -1.415547
[ Info: iteration 3, average log likelihood -1.414176
[ Info: iteration 4, average log likelihood -1.413177
[ Info: iteration 5, average log likelihood -1.412126
[ Info: iteration 6, average log likelihood -1.411130
[ Info: iteration 7, average log likelihood -1.410417
[ Info: iteration 8, average log likelihood -1.410007
[ Info: iteration 9, average log likelihood -1.409777
[ Info: iteration 10, average log likelihood -1.409629
[ Info: iteration 11, average log likelihood -1.409520
[ Info: iteration 12, average log likelihood -1.409432
[ Info: iteration 13, average log likelihood -1.409357
[ Info: iteration 14, average log likelihood -1.409290
[ Info: iteration 15, average log likelihood -1.409230
[ Info: iteration 16, average log likelihood -1.409176
[ Info: iteration 17, average log likelihood -1.409125
[ Info: iteration 18, average log likelihood -1.409078
[ Info: iteration 19, average log likelihood -1.409034
[ Info: iteration 20, average log likelihood -1.408994
[ Info: iteration 21, average log likelihood -1.408955
[ Info: iteration 22, average log likelihood -1.408919
[ Info: iteration 23, average log likelihood -1.408885
[ Info: iteration 24, average log likelihood -1.408853
[ Info: iteration 25, average log likelihood -1.408822
[ Info: iteration 26, average log likelihood -1.408793
[ Info: iteration 27, average log likelihood -1.408766
[ Info: iteration 28, average log likelihood -1.408739
[ Info: iteration 29, average log likelihood -1.408714
[ Info: iteration 30, average log likelihood -1.408690
[ Info: iteration 31, average log likelihood -1.408666
[ Info: iteration 32, average log likelihood -1.408644
[ Info: iteration 33, average log likelihood -1.408622
[ Info: iteration 34, average log likelihood -1.408601
[ Info: iteration 35, average log likelihood -1.408581
[ Info: iteration 36, average log likelihood -1.408561
[ Info: iteration 37, average log likelihood -1.408542
[ Info: iteration 38, average log likelihood -1.408524
[ Info: iteration 39, average log likelihood -1.408507
[ Info: iteration 40, average log likelihood -1.408490
[ Info: iteration 41, average log likelihood -1.408473
[ Info: iteration 42, average log likelihood -1.408458
[ Info: iteration 43, average log likelihood -1.408443
[ Info: iteration 44, average log likelihood -1.408428
[ Info: iteration 45, average log likelihood -1.408414
[ Info: iteration 46, average log likelihood -1.408401
[ Info: iteration 47, average log likelihood -1.408388
[ Info: iteration 48, average log likelihood -1.408376
[ Info: iteration 49, average log likelihood -1.408364
[ Info: iteration 50, average log likelihood -1.408353
┌ Info: EM with 100000 data points 50 iterations avll -1.408353
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.0794565  -0.0107469   0.0416977   0.0227537   -0.221622   -0.0164024  -0.00922909   0.231996    -0.0538242   -0.120671   -0.238186    -0.246385    0.00853993  -0.0849953  -0.0140226   0.128318    -0.174652    -0.0169013    0.0347482  -0.128065    0.104711   -0.124267   -0.0481328   -0.269855    -0.198071    -0.0627561
  0.0740638  -0.286542   -0.253142   -1.05853      0.228935   -0.077591   -0.0210257    0.427151    -0.289399    -0.0998263   0.415859    -0.167525    0.0680308    0.189202   -0.254139   -0.742059     0.29707     -0.295941     0.693715    0.439073    0.533359   -0.546922    0.228823    -0.0600015   -0.435475     0.361453
 -0.0627289   0.116669   -0.0334643   0.132262     0.250074    0.218206    0.00709355  -0.0388188   -0.0591275   -0.0806473   0.0192997   -0.0484346   0.0314828    0.0307348   0.0361442  -0.0287458    0.110992     0.0781706    0.0633121   0.0369526   0.0332667  -0.11418     0.0056568    0.0273748   -0.183559     0.00834499
  0.0359774   0.160463    0.240383    0.316533    -0.381713   -0.174825   -0.0884683    0.0638167    0.426111     0.241648    0.0349658    0.150235   -0.215072    -0.466186   -0.0793898   0.814991    -0.702085     0.294781    -0.205526   -0.0246894  -0.878059    0.108922   -0.271243     0.0374138    0.560169    -0.402002
  0.187401    0.531978   -0.0899907  -1.02063     -0.527038   -0.135705    0.154934    -0.617265    -0.0282035    0.251043    0.0280874    0.736737   -0.0855576   -0.0120266  -0.0426548   0.267355     0.341648     0.425053     0.248239   -0.129923    0.583476    0.272607    0.41831      0.139212     0.510072    -0.372183
 -0.361041    0.0665819   0.134513    0.37214      0.615038   -0.299741    0.180886     0.0674987    0.00964656  -0.0252598  -0.449055     0.363745    0.131231    -0.204759   -0.470724   -0.175441     0.410139    -0.588615    -0.192645    0.626645    0.0168534   0.106631   -0.0243311    0.428255     0.476739    -0.140472
  0.809164    0.0805494  -0.457928    0.0226503    0.0849958   0.374573    0.0891921    0.0639402   -0.443171    -0.159724    0.761923    -0.700216    0.122622    -0.0571924   0.393559    0.646536    -0.356405     0.388099     0.24687     0.0403904  -0.0112476   0.0727948   0.199938    -0.180734    -0.887282     0.350002
  0.325902   -0.142976    0.103987    0.158459    -0.419339    0.295637   -0.308054    -0.603142    -0.241012    -0.244347    0.0162923    0.298415   -0.296545     0.791161   -0.195256   -0.236285    -0.440119     0.143817    -0.635539    0.24088    -0.269261    0.21611     0.579177    -0.133824     0.153868     0.141517
  0.497225   -0.374514   -0.280194   -0.252756     0.755321   -0.105054    0.620847     0.259779     0.176203    -0.248736    1.67412      0.53795     1.30604     -0.50118     0.118273   -0.295333     0.358455     0.372168    -0.0610647  -0.206892   -0.575632    0.0183428   0.352803     0.102378     0.546506     0.443906
 -0.303971   -0.374761   -0.0970783   0.058777     0.210829   -0.0491312   0.432564     0.58046      0.237829    -0.374685   -0.0208902   -0.971105    0.0949997   -0.652387    0.0996109   0.0296661    0.269534     0.218085     0.664635   -0.655112    0.232557   -0.418045   -0.0817091    0.105055    -0.211037    -0.639788
  0.170658   -0.321935    0.642867   -0.533175     0.31329     0.197104    0.14994      0.845367    -0.315167    -0.0781146  -0.0172516    0.629025    0.166556    -0.279261   -0.325991   -0.22048      0.424552     0.105067    -0.3584     -0.0641409  -0.176407   -0.0697963  -0.912735     0.151973    -0.00309807   0.0415485
 -0.0452842  -0.157207   -0.479138    0.388789     0.333676    0.463364   -0.362444    -0.397567    -0.419438    -0.173412   -0.0405969   -0.634525    0.129367    -0.0831551  -0.105634   -0.0305276   -0.0955153   -0.355083    -0.231762   -0.182952   -0.0748631  -0.371079    0.563734     0.53184     -0.109949     0.191391
  0.414705    1.08337     1.00837     0.807587     0.103025   -0.144205    0.801938    -0.209024     0.100858     0.4495     -0.0131237    0.806797   -0.187644     0.896201    0.335104    0.349445     0.237418    -0.0539179    0.268756    0.320823    0.27935    -0.20175    -0.206064    -0.38689      0.321633     0.26108
 -0.450159    0.191735    0.0172773   0.785304    -0.180483    0.207978   -0.0582207   -0.283438    -0.338337    -0.484308   -0.4103      -0.623964   -0.0508337    0.236911    0.0661014   0.11597     -0.0945977    0.069973     0.0739033   0.208945    0.191561   -0.309931    0.366226    -0.505101    -0.456021    -0.533107
  0.25237     0.219718   -0.198373    0.347378    -0.215992    0.4118      0.0394103   -0.181768    -0.039291     0.0753897  -0.217223     0.075805   -0.497534     0.112047    0.293317    0.063565     0.397138     0.772706    -0.0607724  -0.635759   -0.168568    0.13917    -0.701646    -0.230961    -0.359817     0.221425
 -0.75554     0.180805    0.276322    0.145455     0.0690788  -0.195579    0.253135    -0.218551    -0.285637     0.687374   -0.113602     0.439451    0.299094    -0.128386    0.344874   -0.410127     0.488217    -0.400319     0.299609    0.303114   -0.148658   -0.489772    0.130918     0.146946    -0.478041    -0.173993
  0.0790833   0.225271   -0.265099   -0.156466     0.470045   -0.373387   -0.435081     0.509138     0.223127    -0.389465   -0.148186     0.488654    0.231992    -0.309981    0.714932    0.038599     0.511632    -0.00924655  -0.285841    0.0475773   0.394733    0.840575   -0.118587    -0.240387    -0.131244    -0.185016
 -0.219284   -0.125851    0.0904088  -0.40932     -0.107514   -0.103353   -0.225577     0.295138     0.286524     0.351497   -0.386836     0.236818   -0.129158    -0.204714    0.328254   -0.219625    -0.420566     0.416775    -0.124963   -0.196007    0.247596    0.0560432  -0.430333    -0.229195    -0.0774037   -0.1935
  0.183138   -0.0683387  -0.413415   -0.0271293    0.163608    0.0480349   0.0858117   -0.304654     0.400539     0.462748   -0.150448     0.264064   -0.0997444    0.184403   -0.380476   -0.460743    -0.0283336   -0.459697    -0.100618   -0.332324    0.410793   -0.210282   -0.0905516    0.80742      0.0783443   -0.0469334
  0.532385   -0.23862    -0.180087    0.423241     0.225292   -0.326991    0.326338    -0.141554    -0.0629231   -0.213326    0.286181     0.0026097   0.2674       0.275004   -0.478622    0.564421     0.115206    -0.840443    -0.0292321   0.305989   -0.0931359  -0.141423    0.789821     0.535903     0.038925     0.344584
  0.391855   -0.326454    0.296455    0.289494    -0.164523   -0.535365    0.402337     0.00576669  -0.669968    -0.613782    0.215347    -0.151996    0.126246    -0.331987    0.216412    0.485314     0.131156    -0.15421     -0.247999   -0.356112   -0.215635    0.144845    0.00446946  -0.40791     -0.078111     0.394641
  0.0208576   0.0335552   0.592069   -0.00137692   0.537268   -0.156939    0.0376949   -0.442515     0.446445     0.226862   -0.0619174   -0.098561    0.95635     -0.112773    1.10636    -0.00990164   0.55606     -0.4435      -0.312883   -0.245614    0.518835   -0.202704   -0.497835     0.00668215  -0.121591    -0.50332
  0.0373762  -0.149602    0.080588   -0.0820902    0.156723    0.206246    0.136293     0.684876     0.368059     0.160471   -0.228655    -0.0726367   0.917307     0.260325   -0.336303    0.152683    -0.28202     -0.815301     0.127207    0.0119917   0.479044   -0.388623    0.221898    -0.193424     0.0804365    0.00371088
  0.314016    0.435124   -0.726025   -0.131566    -0.0751901  -0.310813   -0.0267097   -0.0726259    0.645557     0.0778687   0.366131    -0.136742   -0.0902018    0.626879    0.104116    0.643449    -0.059368    -0.142507     0.571342    0.708375   -0.445221    0.089961    0.226746     0.172888    -0.341648    -0.137917
  0.181723    0.171083    0.285728    0.118593    -0.292866   -0.647813   -0.54072      0.066025    -0.40022     -0.0985025  -0.0217929   -0.172215   -0.0244671   -0.395894   -0.0664122   0.36072     -0.516094     0.0314038   -0.779026   -0.313528   -0.0345921  -0.430184    0.571711    -0.591813     0.0402319   -0.0902541
 -0.246414    0.0290587  -0.0893377  -0.344724     0.326289    0.32386     0.2159      -0.0924362    0.350863     0.783814   -0.249423     0.602862   -0.085413     0.374209   -0.107624   -0.999489    -0.00679897  -0.0175773    0.135149    0.134984    0.0625138  -0.0457597  -0.100179     0.315496     0.301927     0.0955737
  0.0788827  -0.238094   -0.422173    0.0330322   -0.122896    0.1991     -0.016997     0.640255    -0.0985491   -0.36714     0.00195606  -0.414491   -0.541135    -0.162343   -0.504672    0.402605    -0.397027     0.224257     0.183645    0.115541   -0.152343    0.279447    0.474223    -0.0306255    0.165349     0.199864
 -1.08238    -0.0212337  -0.0979931   0.55602      0.451732    0.497205   -0.324806    -0.159948     0.257669    -0.0887526  -0.137072    -0.585146   -0.198346    -0.07715    -0.440721   -0.275508     0.0492543    0.449589    -0.0925674   0.300297   -0.0570803  -0.298021   -0.625678    -0.0999855   -0.42115      0.0958381
  0.249161   -0.113466   -0.0721207  -0.154522    -0.27306    -0.27648     0.137118     0.0717357    0.162933     0.012863    0.0481278    0.173768    0.00381282   0.0210695  -0.069559    0.173407    -0.137094    -0.155563    -0.0303311  -0.0301471  -0.0797478   0.116614    0.10623     -0.0116109    0.275747     0.0310015
 -0.351189   -0.0680758   0.276517   -0.425787    -0.62778     0.0664126   0.473013     0.344104     0.374177     0.147494    0.140046     0.325961   -0.00774081   0.327746    0.197829   -0.33463     -0.0354869    0.226273     0.884231    0.250032   -0.475069   -0.157915   -0.380252    -0.0306262    0.51788      0.443795
  0.372238   -0.0618571   0.448244   -0.377855    -0.581136   -0.386312   -0.0748278    0.354375     0.382828    -0.305354   -0.0311781   -0.238544   -0.0268939    0.878753    0.184137    0.0228972    0.170516    -0.108901    -0.165666   -0.474461    0.933173   -0.235474   -0.0401241   -0.738676    -0.640111    -0.488845
  0.132648    0.13114    -0.398058   -0.0321084    0.118755    0.010413   -0.0277477   -0.473295    -0.0951308   -0.113765    0.394059     0.575744   -0.534085    -0.563056   -0.108719   -0.0948089   -0.108028     0.301192    -0.180252    0.22282    -0.316502    0.674251   -0.0178018    0.599705     0.619221     0.263885[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.408342
[ Info: iteration 2, average log likelihood -1.408332
[ Info: iteration 3, average log likelihood -1.408322
[ Info: iteration 4, average log likelihood -1.408312
[ Info: iteration 5, average log likelihood -1.408303
[ Info: iteration 6, average log likelihood -1.408294
[ Info: iteration 7, average log likelihood -1.408286
[ Info: iteration 8, average log likelihood -1.408278
[ Info: iteration 9, average log likelihood -1.408270
[ Info: iteration 10, average log likelihood -1.408263
┌ Info: EM with 100000 data points 10 iterations avll -1.408263
└ 59.0 data points per parameter
[ Info: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.678561e+05
      1       2.230230e+04      -1.455538e+05 |        2
      2       7.823675e+03      -1.447862e+04 |        0
      3       7.823675e+03       0.000000e+00 |        0
K-means converged with 3 iterations (objv = 7823.67549422947)
┌ Info: K-means with 900 data points using 3 iterations
└ 150.0 data points per parameter
[ Info: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
[ Info: iteration 1, average log likelihood -2.043155
[ Info: iteration 2, average log likelihood -2.043154
[ Info: iteration 3, average log likelihood -2.043154
[ Info: iteration 4, average log likelihood -2.043154
[ Info: iteration 5, average log likelihood -2.043154
[ Info: iteration 6, average log likelihood -2.043154
[ Info: iteration 7, average log likelihood -2.043154
[ Info: iteration 8, average log likelihood -2.043154
[ Info: iteration 9, average log likelihood -2.043154
[ Info: iteration 10, average log likelihood -2.043154
┌ Info: EM with 900 data points 10 iterations avll -2.043154
└ 81.8 data points per parameter
   Testing GaussianMixtures tests passed 
