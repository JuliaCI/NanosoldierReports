Julia Version 1.5.0-DEV.0
Commit 13d1bd4689 (2019-12-31 18:18 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-8.0.1 (ORCJIT, skylake)
Environment:
  JULIA_DEPOT_PATH = ::/usr/local/share/julia

 Resolving package versions...
 Installed DataAPI ──────────── v1.1.0
 Installed GaussianMixtures ─── v0.3.0
 Installed Compat ───────────── v2.2.0
 Installed Rmath ────────────── v0.6.0
 Installed OrderedCollections ─ v1.1.0
 Installed DataStructures ───── v0.17.6
 Installed Arpack ───────────── v0.4.0
 Installed NearestNeighbors ─── v0.4.4
 Installed Arpack_jll ───────── v3.5.0+2
 Installed StatsFuns ────────── v0.9.3
 Installed BinaryProvider ───── v0.5.8
 Installed URIParser ────────── v0.4.0
 Installed OpenBLAS_jll ─────── v0.3.7+2
 Installed CMake ────────────── v1.1.2
 Installed SpecialFunctions ─── v0.9.0
 Installed FillArrays ───────── v0.8.2
 Installed HDF5 ─────────────── v0.12.5
 Installed FileIO ───────────── v1.2.1
 Installed CMakeWrapper ─────── v0.2.3
 Installed SortingAlgorithms ── v0.3.1
 Installed QuadGK ───────────── v2.3.1
 Installed PDMats ───────────── v0.9.10
 Installed Distances ────────── v0.8.2
 Installed LegacyStrings ────── v0.4.1
 Installed StaticArrays ─────── v0.12.1
 Installed BinDeps ──────────── v1.0.0
 Installed ScikitLearnBase ──── v0.5.0
 Installed StatsBase ────────── v0.32.0
 Installed Missings ─────────── v0.4.3
 Installed Parameters ───────── v0.12.0
 Installed Blosc ────────────── v0.5.1
 Installed JLD ──────────────── v0.9.1
 Installed Clustering ───────── v0.13.3
 Installed OpenSpecFun_jll ──── v0.5.3+1
 Installed Distributions ────── v0.21.11
  Updating `~/.julia/environments/v1.5/Project.toml`
  [cc18c42c] + GaussianMixtures v0.3.0
  Updating `~/.julia/environments/v1.5/Manifest.toml`
  [7d9fca2a] + Arpack v0.4.0
  [68821587] + Arpack_jll v3.5.0+2
  [9e28174c] + BinDeps v1.0.0
  [b99e7846] + BinaryProvider v0.5.8
  [a74b3585] + Blosc v0.5.1
  [631607c0] + CMake v1.1.2
  [d5fb7624] + CMakeWrapper v0.2.3
  [aaaa29a8] + Clustering v0.13.3
  [34da2185] + Compat v2.2.0
  [9a962f9c] + DataAPI v1.1.0
  [864edb3b] + DataStructures v0.17.6
  [b4f34e82] + Distances v0.8.2
  [31c24e10] + Distributions v0.21.11
  [5789e2e9] + FileIO v1.2.1
  [1a297f60] + FillArrays v0.8.2
  [cc18c42c] + GaussianMixtures v0.3.0
  [f67ccb44] + HDF5 v0.12.5
  [4138dd39] + JLD v0.9.1
  [1b4a561d] + LegacyStrings v0.4.1
  [e1d29d7a] + Missings v0.4.3
  [b8a86587] + NearestNeighbors v0.4.4
  [4536629a] + OpenBLAS_jll v0.3.7+2
  [efe28fd5] + OpenSpecFun_jll v0.5.3+1
  [bac558e1] + OrderedCollections v1.1.0
  [90014a1f] + PDMats v0.9.10
  [d96e819e] + Parameters v0.12.0
  [1fd47b50] + QuadGK v2.3.1
  [79098fc4] + Rmath v0.6.0
  [6e75b9c4] + ScikitLearnBase v0.5.0
  [a2af1166] + SortingAlgorithms v0.3.1
  [276daf66] + SpecialFunctions v0.9.0
  [90137ffa] + StaticArrays v0.12.1
  [2913bbd2] + StatsBase v0.32.0
  [4c63d2b9] + StatsFuns v0.9.3
  [30578b45] + URIParser v0.4.0
  [2a0f44e3] + Base64 
  [ade2ca70] + Dates 
  [8bb1440f] + DelimitedFiles 
  [8ba89e20] + Distributed 
  [b77e0a4c] + InteractiveUtils 
  [76f85450] + LibGit2 
  [8f399da3] + Libdl 
  [37e2e46d] + LinearAlgebra 
  [56ddb016] + Logging 
  [d6f4376e] + Markdown 
  [a63ad114] + Mmap 
  [44cfe95a] + Pkg 
  [de0858da] + Printf 
  [9abbd945] + Profile 
  [3fa0cd96] + REPL 
  [9a3f8284] + Random 
  [ea8e919c] + SHA 
  [9e88b42a] + Serialization 
  [1a1011a3] + SharedArrays 
  [6462fe0b] + Sockets 
  [2f01184e] + SparseArrays 
  [10745b16] + Statistics 
  [4607b0f0] + SuiteSparse 
  [8dfed614] + Test 
  [cf7118a7] + UUIDs 
  [4ec0a83e] + Unicode 
  Building CMake → `~/.julia/packages/CMake/nSK2r/deps/build.log`
  Updating `/tmp/jl_EoDu9R/Project.toml`
 [no changes]
  Updating `/tmp/jl_EoDu9R/Manifest.toml`
 [no changes]
  Building Blosc → `~/.julia/packages/Blosc/lzFr0/deps/build.log`
  Updating `/tmp/jl_rr7FRy/Project.toml`
 [no changes]
  Updating `/tmp/jl_rr7FRy/Manifest.toml`
 [no changes]
  Building Rmath → `~/.julia/packages/Rmath/BoBag/deps/build.log`
  Updating `/tmp/jl_aX5G0p/Project.toml`
 [no changes]
  Updating `/tmp/jl_aX5G0p/Manifest.toml`
 [no changes]
  Building HDF5 ─→ `~/.julia/packages/HDF5/Zh9on/deps/build.log`
  Updating `/tmp/jl_1wuQhy/Project.toml`
 [no changes]
  Updating `/tmp/jl_1wuQhy/Manifest.toml`
 [no changes]
   Testing GaussianMixtures
  Updating `/tmp/jl_aEGwm9/Project.toml`
 [no changes]
  Updating `/tmp/jl_aEGwm9/Manifest.toml`
 [no changes]
Running sandbox
Status `/tmp/jl_aEGwm9/Project.toml`
  [aaaa29a8] Clustering v0.13.3
  [34da2185] Compat v2.2.0
  [31c24e10] Distributions v0.21.11
  [5789e2e9] FileIO v1.2.1
  [cc18c42c] GaussianMixtures v0.3.0
  [4138dd39] JLD v0.9.1
  [90014a1f] PDMats v0.9.10
  [6e75b9c4] ScikitLearnBase v0.5.0
  [276daf66] SpecialFunctions v0.9.0
  [2913bbd2] StatsBase v0.32.0
  [8bb1440f] DelimitedFiles 
  [8ba89e20] Distributed 
  [37e2e46d] LinearAlgebra 
  [56ddb016] Logging 
  [de0858da] Printf 
  [9a3f8284] Random 
  [10745b16] Statistics 
[ Info: Testing Data
(100000, -944645.9509065711, [99980.96018075538, 19.039819244620393], [160.53322233959375 11.241794923533732 393.9739481701687; -30.752773893345225 44.947980043293626 20.696178480150397], [[99677.51373320907 -551.0766055299013 686.4760554265587; -551.0766055299013 99853.64892646426 -349.1082358043866; 686.4760554265587 -349.1082358043866 100500.88915301125], [76.91118835692187 -59.991736543758066 -28.36581451442526; -59.991736543758066 113.74927458151816 45.0256085527975; -28.365814514425267 45.0256085527975 44.62400291586716]])
┌ Warning: rmprocs: process 1 not removed
└ @ Distributed /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Distributed/src/cluster.jl:1015
[ Info: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.322584e+03
      1       9.968364e+02      -3.257480e+02 |        3
      2       9.534672e+02      -4.336926e+01 |        2
      3       9.516250e+02      -1.842230e+00 |        2
      4       9.314599e+02      -2.016506e+01 |        2
      5       9.034052e+02      -2.805465e+01 |        2
      6       8.966936e+02      -6.711648e+00 |        0
      7       8.966936e+02       0.000000e+00 |        0
K-means converged with 7 iterations (objv = 896.6935971611274)
┌ Info: K-means with 272 data points using 7 iterations
└ 11.3 data points per parameter
[ Info: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
┌ Info: EM with 272 data points 0 iterations avll -2.071966
└ 5.8 data points per parameter
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:221
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:221
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:221
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:221
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:630 [inlined]
└ @ Core ./broadcast.jl:630
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:230
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:230
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:630 [inlined]
└ @ Core ./broadcast.jl:630
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:630 [inlined]
└ @ Core ./broadcast.jl:630
[ Info: iteration 1, lowerbound -3.804440
[ Info: iteration 2, lowerbound -3.660695
[ Info: iteration 3, lowerbound -3.493095
[ Info: iteration 4, lowerbound -3.291436
[ Info: iteration 5, lowerbound -3.079658
[ Info: iteration 6, lowerbound -2.885926
[ Info: dropping number of Gaussions to 7
[ Info: iteration 7, lowerbound -2.722789
[ Info: dropping number of Gaussions to 6
[ Info: iteration 8, lowerbound -2.594852
[ Info: dropping number of Gaussions to 5
[ Info: iteration 9, lowerbound -2.504760
[ Info: dropping number of Gaussions to 3
[ Info: iteration 10, lowerbound -2.433415
[ Info: iteration 11, lowerbound -2.379247
[ Info: iteration 12, lowerbound -2.344322
[ Info: iteration 13, lowerbound -2.319951
[ Info: iteration 14, lowerbound -2.308173
[ Info: dropping number of Gaussions to 2
[ Info: iteration 15, lowerbound -2.303083
[ Info: iteration 16, lowerbound -2.299264
[ Info: iteration 17, lowerbound -2.299258
[ Info: iteration 18, lowerbound -2.299255
[ Info: iteration 19, lowerbound -2.299254
[ Info: iteration 20, lowerbound -2.299253
[ Info: iteration 21, lowerbound -2.299253
[ Info: iteration 22, lowerbound -2.299253
[ Info: iteration 23, lowerbound -2.299253
[ Info: iteration 24, lowerbound -2.299253
[ Info: iteration 25, lowerbound -2.299253
[ Info: iteration 26, lowerbound -2.299253
[ Info: iteration 27, lowerbound -2.299253
[ Info: iteration 28, lowerbound -2.299253
[ Info: iteration 29, lowerbound -2.299253
[ Info: iteration 30, lowerbound -2.299253
[ Info: iteration 31, lowerbound -2.299253
[ Info: iteration 32, lowerbound -2.299253
[ Info: iteration 33, lowerbound -2.299253
[ Info: iteration 34, lowerbound -2.299253
[ Info: iteration 35, lowerbound -2.299253
[ Info: iteration 36, lowerbound -2.299253
[ Info: iteration 37, lowerbound -2.299253
[ Info: iteration 38, lowerbound -2.299253
[ Info: iteration 39, lowerbound -2.299253
[ Info: iteration 40, lowerbound -2.299253
[ Info: iteration 41, lowerbound -2.299253
[ Info: iteration 42, lowerbound -2.299253
[ Info: iteration 43, lowerbound -2.299253
[ Info: iteration 44, lowerbound -2.299253
[ Info: iteration 45, lowerbound -2.299253
[ Info: iteration 46, lowerbound -2.299253
[ Info: iteration 47, lowerbound -2.299253
[ Info: iteration 48, lowerbound -2.299253
[ Info: iteration 49, lowerbound -2.299253
[ Info: iteration 50, lowerbound -2.299253
[ Info: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
History[Thu Jan  2 14:36:53 2020: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
, Thu Jan  2 14:37:01 2020: K-means with 272 data points using 7 iterations
11.3 data points per parameter
, Thu Jan  2 14:37:04 2020: EM with 272 data points 0 iterations avll -2.071966
5.8 data points per parameter
, Thu Jan  2 14:37:06 2020: GMM converted to Variational GMM
, Thu Jan  2 14:37:15 2020: iteration 1, lowerbound -3.804440
, Thu Jan  2 14:37:15 2020: iteration 2, lowerbound -3.660695
, Thu Jan  2 14:37:15 2020: iteration 3, lowerbound -3.493095
, Thu Jan  2 14:37:15 2020: iteration 4, lowerbound -3.291436
, Thu Jan  2 14:37:15 2020: iteration 5, lowerbound -3.079658
, Thu Jan  2 14:37:15 2020: iteration 6, lowerbound -2.885926
, Thu Jan  2 14:37:15 2020: dropping number of Gaussions to 7
, Thu Jan  2 14:37:15 2020: iteration 7, lowerbound -2.722789
, Thu Jan  2 14:37:15 2020: dropping number of Gaussions to 6
, Thu Jan  2 14:37:15 2020: iteration 8, lowerbound -2.594852
, Thu Jan  2 14:37:15 2020: dropping number of Gaussions to 5
, Thu Jan  2 14:37:15 2020: iteration 9, lowerbound -2.504760
, Thu Jan  2 14:37:15 2020: dropping number of Gaussions to 3
, Thu Jan  2 14:37:15 2020: iteration 10, lowerbound -2.433415
, Thu Jan  2 14:37:15 2020: iteration 11, lowerbound -2.379247
, Thu Jan  2 14:37:15 2020: iteration 12, lowerbound -2.344322
, Thu Jan  2 14:37:15 2020: iteration 13, lowerbound -2.319951
, Thu Jan  2 14:37:15 2020: iteration 14, lowerbound -2.308173
, Thu Jan  2 14:37:15 2020: dropping number of Gaussions to 2
, Thu Jan  2 14:37:15 2020: iteration 15, lowerbound -2.303083
, Thu Jan  2 14:37:15 2020: iteration 16, lowerbound -2.299264
, Thu Jan  2 14:37:15 2020: iteration 17, lowerbound -2.299258
, Thu Jan  2 14:37:15 2020: iteration 18, lowerbound -2.299255
, Thu Jan  2 14:37:15 2020: iteration 19, lowerbound -2.299254
, Thu Jan  2 14:37:15 2020: iteration 20, lowerbound -2.299253
, Thu Jan  2 14:37:15 2020: iteration 21, lowerbound -2.299253
, Thu Jan  2 14:37:15 2020: iteration 22, lowerbound -2.299253
, Thu Jan  2 14:37:15 2020: iteration 23, lowerbound -2.299253
, Thu Jan  2 14:37:15 2020: iteration 24, lowerbound -2.299253
, Thu Jan  2 14:37:15 2020: iteration 25, lowerbound -2.299253
, Thu Jan  2 14:37:15 2020: iteration 26, lowerbound -2.299253
, Thu Jan  2 14:37:15 2020: iteration 27, lowerbound -2.299253
, Thu Jan  2 14:37:15 2020: iteration 28, lowerbound -2.299253
, Thu Jan  2 14:37:15 2020: iteration 29, lowerbound -2.299253
, Thu Jan  2 14:37:15 2020: iteration 30, lowerbound -2.299253
, Thu Jan  2 14:37:16 2020: iteration 31, lowerbound -2.299253
, Thu Jan  2 14:37:16 2020: iteration 32, lowerbound -2.299253
, Thu Jan  2 14:37:16 2020: iteration 33, lowerbound -2.299253
, Thu Jan  2 14:37:16 2020: iteration 34, lowerbound -2.299253
, Thu Jan  2 14:37:16 2020: iteration 35, lowerbound -2.299253
, Thu Jan  2 14:37:16 2020: iteration 36, lowerbound -2.299253
, Thu Jan  2 14:37:16 2020: iteration 37, lowerbound -2.299253
, Thu Jan  2 14:37:16 2020: iteration 38, lowerbound -2.299253
, Thu Jan  2 14:37:16 2020: iteration 39, lowerbound -2.299253
, Thu Jan  2 14:37:16 2020: iteration 40, lowerbound -2.299253
, Thu Jan  2 14:37:16 2020: iteration 41, lowerbound -2.299253
, Thu Jan  2 14:37:16 2020: iteration 42, lowerbound -2.299253
, Thu Jan  2 14:37:16 2020: iteration 43, lowerbound -2.299253
, Thu Jan  2 14:37:16 2020: iteration 44, lowerbound -2.299253
, Thu Jan  2 14:37:16 2020: iteration 45, lowerbound -2.299253
, Thu Jan  2 14:37:16 2020: iteration 46, lowerbound -2.299253
, Thu Jan  2 14:37:16 2020: iteration 47, lowerbound -2.299253
, Thu Jan  2 14:37:16 2020: iteration 48, lowerbound -2.299253
, Thu Jan  2 14:37:16 2020: iteration 49, lowerbound -2.299253
, Thu Jan  2 14:37:16 2020: iteration 50, lowerbound -2.299253
, Thu Jan  2 14:37:16 2020: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [95.9549077739862, 178.0450922260138]
β = [95.9549077739862, 178.0450922260138]
m = [2.000229257775371 53.85198717246131; 4.250300733269911 79.28686694436185]
ν = [97.9549077739862, 180.0450922260138]
W = LinearAlgebra.UpperTriangular{Float64,Array{Float64,2}}[[0.3758763611948398 -0.008953123827346166; 0.0 0.012748664777409421], [0.18404155547484935 -0.007644049042327522; 0.0 0.008581705166333463]]
Kind: diag, size256
nx: 100000 sum(zeroth order stats): 100000.00000000003
avll from stats: -1.016701188579482
avll from llpg:  -1.0167011885794826
avll direct:     -1.0167011885794826
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 100000.0
avll from stats: -0.9994856873012703
avll from llpg:  -0.9994856873012703
avll direct:     -0.9994856873012703
sum posterior: 100000.0
32×26 Array{Float64,2}:
 -0.108007     0.136609      0.306907     0.0070388   -0.0643506     0.0198508   -0.193709    -0.0826785    0.14257    -0.0601708    0.0844138   -0.0545846   -0.0800315   -0.0522874    0.253218     0.179338    -0.0908183   -0.16921      0.10594       0.036671     0.142051    -0.162853    -0.116548     0.089834     -0.0126436    0.0538706
  0.136202    -0.0125122    -0.0176665   -0.0806925   -0.0323367     0.0527173   -0.0834175   -0.0571388    0.140011   -0.0106403   -0.00289864  -0.0298913   -0.0760448    0.0994852    0.0473183    0.197341     0.177007    -0.0452909   -0.0132526    -0.0115487   -0.0251983    0.0541221   -0.151676    -0.0599672    -0.0817467    0.109704
  0.117753     0.163649      0.26747      0.0629526    0.0432087     0.132508    -0.286081    -0.00406728   0.0910176   0.0790507    0.075779     0.136771    -0.0496608    0.0139511    0.100886     0.107346     0.0128279    0.0688405   -0.0653484     0.100824    -0.010524     0.129162     0.173778    -0.0830184     0.0203881    0.0804418
  0.06846      0.128224     -0.0833833   -0.088889     0.087158     -0.0382045    0.143361     0.0648932   -0.120156   -0.0180633    0.1998       0.0650935    0.148583    -0.234353     0.170231     0.0358492    0.0607352    0.0100096   -0.00305574   -0.0241316   -0.0243785   -0.198068    -0.0010342    0.054349      0.00981922  -0.142651
 -0.0360766   -0.118101     -0.250023    -0.0184914    0.153027      0.140839     0.0340681    0.0370868    0.100294    0.11249      0.0132224    0.0876418    0.143265    -0.155875    -0.109561     0.127382    -0.0646305   -0.0374049   -0.0289591     0.0132241   -0.00645025  -0.1591       0.207056    -0.00101177    0.175447     0.0928146
 -0.0438087   -0.0557992     0.0871421   -0.0148559    0.0181237    -0.00697565  -0.178826     0.10059     -0.116971   -0.0601394   -0.00375065  -0.105496     0.0957042    0.00236033  -0.213937     0.134078    -0.0548114    0.145367     0.10295       0.0798988   -0.0228388   -0.0943898   -0.0807551    0.158192     -0.010978     0.171743
 -0.0601657   -0.0948183     0.19763     -0.138769     0.00705027    0.0296383    0.0697313   -0.0333646   -0.045596    0.134764     0.0792323   -0.0353038   -0.0247884   -0.0408333   -0.0826167    0.013234     0.188073    -0.0161964   -0.0282715     0.135008    -0.0588942   -0.207176    -0.0653499    0.091291     -0.0957156    0.00609123
 -0.114806     0.0942262     0.0947027    0.0135642    0.105113      0.142127    -0.178516     0.063874    -0.0875954   0.0334332    0.168034    -0.0357636    0.0328168   -0.202154    -0.125875    -0.0436578    0.0813919   -0.040758    -0.0103276     0.130538    -0.0207606    0.0261978   -0.11048     -0.082169      0.12636      0.0418579
  0.0557403   -0.222491     -0.188229     0.0978481   -0.0219651    -0.0629713    0.0922554   -0.0243544    0.164165    0.0936846    0.0522856    0.12655     -0.120298    -0.0651084    0.108801     0.112502     0.0875287   -0.0250298   -0.138917     -0.145992     0.0448221    0.105769    -0.0130116    0.00807376    0.097437    -0.240767
 -0.142783    -0.0169899     0.176875    -0.0936047    0.0768054    -0.155212     0.134862     0.0894324   -0.0733175  -0.0221211    0.258707     0.0333812   -0.039884     0.139472    -0.0285645    0.120684    -0.0880803    0.0237815    0.000785276   0.219181     0.0627635    0.171062    -0.0118607    0.162483     -0.0218826   -0.000593846
 -0.0405222    0.105297     -0.0838971   -0.0641216    0.0116877     0.0234627    0.0855216    0.0815593    0.137112   -0.0326783    0.0153277   -0.0343581   -0.0257106    0.0214036    0.0404207   -0.0672466   -0.0138905   -0.194988     0.0183558    -0.0495288   -0.0225813   -0.0118612   -0.10318      0.0394283    -0.15493     -0.136558
 -0.0842329   -0.0979783     0.168954    -0.0659994   -0.00980597    0.0345523   -0.0387866    0.0606382    0.114194   -0.0721722    0.0358402   -0.172778    -0.0371465   -0.0505995   -0.0193705   -0.0901783   -0.143382     0.0404885    0.0770145    -0.110361     0.0722854   -0.0318233   -0.0641031   -0.129018     -0.0377933    0.250034
  0.0264763   -0.000363028   0.0345481    0.200269     0.0107863     0.0931525    0.112022     0.145507    -0.0800965  -0.00902335   0.0721472   -0.121685    -0.127246    -0.239172     0.00556495  -0.11967      0.0533612   -0.143638     0.0983889    -0.0418217   -0.0335816    0.109208     0.153315    -0.129231     -0.0795135   -0.0387714
  0.0230444   -0.110724     -0.0888899   -0.0947235   -0.017504     -0.0621928   -0.0723253    0.0198987   -0.108782    0.0964563   -0.0283724    0.0583776    0.0951915    0.0285877   -0.0429056    0.0283018    0.0713915    0.0592392    0.110977     -0.044562    -0.00421382   0.0567498    0.0401355    0.0520996     0.0768261   -0.155989
  0.109396     0.0222238     0.0377996   -0.0875365    0.023777      0.266688     0.0232818    0.0386232    0.00667     0.0438715    0.117703    -0.0127894    0.0563451   -0.0449087    0.128423    -0.036994    -0.0386644    0.075532    -0.0619266     0.101394     0.0687807   -0.102979     0.0390522   -0.0879845    -0.0289757   -0.165837
 -0.0786767    0.0384599    -0.145795     0.0588188   -0.11062       0.0198173    0.0119616    0.0576224   -0.0623998  -0.139693    -0.0322317    0.119241    -0.0952931    0.0638043   -0.0508075    0.19364      0.0365996    0.0689998   -0.0623285     0.06204      0.0718527   -0.118922     0.0165636   -0.0393601     0.124659     0.0909789
  0.13762      0.0308159    -0.044763    -0.0359355   -0.0497226     0.0611958   -0.124546    -0.109172    -0.0779652   0.045889     0.161316     0.0458556    0.0444544    0.0563657    0.0298142    0.0925336   -0.0351945    0.110485    -0.0806248    -0.0140406   -0.126589    -0.0465934   -0.0472379    0.0904973    -0.0507316   -0.023078
  0.0502619    0.0661758    -0.185564     0.0224137   -0.116059      0.183077    -0.087169    -0.0144978   -0.0799105   0.132712     0.0339162    0.0901888   -0.00444488  -0.04035      0.2394       0.206972     0.00585733   0.0298796    0.093747      0.0348125   -0.0484644   -0.143691    -0.049817    -0.0388743    -0.0532858   -0.152838
  0.0809443    0.0164244     0.00868828   0.110142    -0.0792563    -0.241293     0.00413149  -0.148722    -0.0996779   0.13662      0.120675     0.00168709   0.174331     0.0716484    0.0475429   -0.0335838   -0.0553346    0.0403489    0.0331791     0.0419092    0.102897     0.106412     0.135682     0.0558265    -0.187387     0.0935488
  0.0855757    0.0710676    -0.09958      0.0851328   -0.0195542     0.0222784   -0.0846595    0.0171154   -0.0782048   0.142671     0.0895052    0.0540067    0.183809    -0.0141791   -0.171543     0.0977542   -0.0739314   -0.0850826   -0.112496      0.0255943   -0.0203077   -0.0412699    0.0746708   -0.000556273  -0.0699536   -0.0199486
  0.0872456   -0.11278      -0.11035     -0.046515     0.0107786    -0.00973637  -0.0576662    0.00762318  -0.04041    -0.0689371   -0.167457     0.0501091    0.00109232  -0.0250274    0.00645041  -0.171575     0.162273    -0.0234486   -0.0563428    -0.00364682   0.0586717    0.0901127   -0.0502767    0.0366578    -0.0756094   -0.180179
 -0.220525     0.0458168     0.0916993    0.0218142   -0.0151774    -0.00121106  -0.0123988    0.0152482    0.0569398   0.0483658    0.205757    -0.222986    -0.0438433   -0.0569055    0.00135436  -0.0585758   -0.1581      -0.110683    -0.16526       0.0291558    0.169058     0.0514506   -0.11541      0.15358       0.0135376   -0.0106511
  0.0543854   -0.0669882     0.0872047    0.00265509   0.0670893    -0.0163445   -0.0523055   -0.0749018   -0.0751352   0.0116663    0.212601    -0.119409     0.0875576   -0.0108091   -0.0142252   -0.10857     -0.016682    -0.0413607   -0.117233      0.0449276    0.233531    -0.100016    -0.0703366   -0.0152988     0.0208684    0.125312
  0.0602526   -0.171069     -0.0353683    0.159487     0.000701116  -0.00494251  -0.0357528   -0.109413     0.152786    0.0559262    0.161167    -0.135584    -0.115735    -0.132288    -0.101165     0.0343098    0.114581    -0.033563    -0.119547      0.0439815    0.0853264   -0.0083099    0.189781    -0.0116161    -0.0014117   -0.128021
 -0.00380389  -0.0937906    -0.0485865   -0.0678641    0.116456     -0.0563672    0.162827    -0.0225548    0.0535398  -0.0955254    0.189986     0.0391957    0.0351321    0.101446    -0.0612029   -0.0789399   -0.0156537    0.0364164   -0.0894465    -0.00449533  -0.170465     0.00211467   0.00370509   0.0701631     0.0847253    0.0296538
  0.160329     0.0076609    -0.172399     0.0392447   -0.145429      0.00193943  -0.0534808   -0.116512     0.0935855  -0.0760486   -0.192577     0.0829904    0.096554     0.12781     -0.0328586   -0.0957483    0.0893053    0.0226413    0.00158971    0.0119241    0.0834224    0.0177798   -0.0349467   -0.00210257   -0.0387249   -0.0766026
 -0.0579851   -0.0818994    -0.00108441  -0.0244967   -0.0241893     0.189867    -0.0111926   -0.155089     0.151964    0.0294884   -0.0868857    0.0845677   -0.00976109  -0.141578     0.178813    -0.103191     0.00863523  -0.0672073   -0.171476     -0.177359     0.031292    -0.113991     0.103062     0.0264149    -0.0783242   -0.0052639
 -0.107389     0.187056     -0.0119912    0.024431    -0.0720867    -0.0518839    0.0695455    0.00947124  -0.0547989   0.0637018   -0.0332934   -0.0118353   -0.0279252    0.0254992    0.111556    -0.0469097    0.0336119   -0.0761355   -0.0537319    -0.016219     0.0178218   -0.106756    -0.0924875    0.00406206   -0.0313487    0.0740067
 -0.150805    -0.0897642     0.105672     0.0882435    0.162344      0.0408406    0.142122     0.0424222   -0.144885    0.159162    -0.0610387   -0.0788833   -0.00993784   0.0292816   -0.0613896   -0.040214    -0.0524066   -0.00677205  -0.0929026     0.0228595   -0.0508435    0.0957475    0.0720577    0.0307815    -0.0091949    0.139657
  0.10262      0.0701048    -0.109991     0.00349878  -0.21937      -0.12077      0.173641     0.0509942    0.145667    0.054907    -0.0489825   -0.113835     0.0391354    0.0781015   -0.0306061    0.00334555   0.0452564   -0.0272128   -0.0986718    -0.0976091    0.0421945   -0.00709406   0.0940984    0.0380863    -0.0100671    0.00978402
 -0.0525528    0.181222      0.145215     0.10203     -0.08077       0.0777205   -0.095834    -0.200263    -0.0777524  -0.0835964    0.0182595   -0.0637597    0.131923    -0.075422     0.124739    -0.0687949    0.0905835    0.036183    -0.0305402    -0.0281017   -0.0212614   -0.129013    -0.150148    -0.0916614    -0.0236672   -0.0303629
  0.137504    -0.14144      -0.139717     0.0622593   -0.0160835    -0.048602     0.0374924   -0.102513    -0.156086   -0.0246168   -0.0567518   -0.127172     0.059967     0.0140688    0.0588726   -0.0124539   -0.00797401  -0.032956    -0.0133655    -0.0383359   -0.0344208   -0.126431    -0.189932    -0.0574799    -0.0261955   -0.0894502kind diag, method split
┌ Info: 0: avll = 
└   tll[1] = -1.4513635182460578
[ Info: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.451436
[ Info: iteration 2, average log likelihood -1.451376
[ Info: iteration 3, average log likelihood -1.451118
[ Info: iteration 4, average log likelihood -1.448533
[ Info: iteration 5, average log likelihood -1.438099
[ Info: iteration 6, average log likelihood -1.427946
[ Info: iteration 7, average log likelihood -1.425036
[ Info: iteration 8, average log likelihood -1.424022
[ Info: iteration 9, average log likelihood -1.423522
[ Info: iteration 10, average log likelihood -1.423255
[ Info: iteration 11, average log likelihood -1.423100
[ Info: iteration 12, average log likelihood -1.422999
[ Info: iteration 13, average log likelihood -1.422925
[ Info: iteration 14, average log likelihood -1.422861
[ Info: iteration 15, average log likelihood -1.422796
[ Info: iteration 16, average log likelihood -1.422728
[ Info: iteration 17, average log likelihood -1.422658
[ Info: iteration 18, average log likelihood -1.422586
[ Info: iteration 19, average log likelihood -1.422513
[ Info: iteration 20, average log likelihood -1.422442
[ Info: iteration 21, average log likelihood -1.422374
[ Info: iteration 22, average log likelihood -1.422310
[ Info: iteration 23, average log likelihood -1.422250
[ Info: iteration 24, average log likelihood -1.422192
[ Info: iteration 25, average log likelihood -1.422132
[ Info: iteration 26, average log likelihood -1.422068
[ Info: iteration 27, average log likelihood -1.421990
[ Info: iteration 28, average log likelihood -1.421880
[ Info: iteration 29, average log likelihood -1.421697
[ Info: iteration 30, average log likelihood -1.421389
[ Info: iteration 31, average log likelihood -1.421034
[ Info: iteration 32, average log likelihood -1.420720
[ Info: iteration 33, average log likelihood -1.420452
[ Info: iteration 34, average log likelihood -1.420208
[ Info: iteration 35, average log likelihood -1.419972
[ Info: iteration 36, average log likelihood -1.419743
[ Info: iteration 37, average log likelihood -1.419527
[ Info: iteration 38, average log likelihood -1.419328
[ Info: iteration 39, average log likelihood -1.419138
[ Info: iteration 40, average log likelihood -1.418955
[ Info: iteration 41, average log likelihood -1.418778
[ Info: iteration 42, average log likelihood -1.418601
[ Info: iteration 43, average log likelihood -1.418420
[ Info: iteration 44, average log likelihood -1.418243
[ Info: iteration 45, average log likelihood -1.418089
[ Info: iteration 46, average log likelihood -1.417961
[ Info: iteration 47, average log likelihood -1.417860
[ Info: iteration 48, average log likelihood -1.417780
[ Info: iteration 49, average log likelihood -1.417714
[ Info: iteration 50, average log likelihood -1.417658
┌ Info: EM with 100000 data points 50 iterations avll -1.417658
└ 952.4 data points per parameter
┌ Info: 1
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4514359485465775
│     -1.451376005890563
│      ⋮
└     -1.4176584210194911
[ Info: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.417749
[ Info: iteration 2, average log likelihood -1.417586
[ Info: iteration 3, average log likelihood -1.417189
[ Info: iteration 4, average log likelihood -1.413894
[ Info: iteration 5, average log likelihood -1.401813
[ Info: iteration 6, average log likelihood -1.390838
[ Info: iteration 7, average log likelihood -1.385587
[ Info: iteration 8, average log likelihood -1.382521
[ Info: iteration 9, average log likelihood -1.380378
[ Info: iteration 10, average log likelihood -1.378691
[ Info: iteration 11, average log likelihood -1.377279
[ Info: iteration 12, average log likelihood -1.376021
[ Info: iteration 13, average log likelihood -1.374832
[ Info: iteration 14, average log likelihood -1.373844
[ Info: iteration 15, average log likelihood -1.373124
[ Info: iteration 16, average log likelihood -1.372550
[ Info: iteration 17, average log likelihood -1.372013
[ Info: iteration 18, average log likelihood -1.371476
[ Info: iteration 19, average log likelihood -1.370915
[ Info: iteration 20, average log likelihood -1.370269
[ Info: iteration 21, average log likelihood -1.369444
[ Info: iteration 22, average log likelihood -1.368460
[ Info: iteration 23, average log likelihood -1.367534
[ Info: iteration 24, average log likelihood -1.366795
[ Info: iteration 25, average log likelihood -1.366215
[ Info: iteration 26, average log likelihood -1.365757
[ Info: iteration 27, average log likelihood -1.365411
[ Info: iteration 28, average log likelihood -1.365152
[ Info: iteration 29, average log likelihood -1.364963
[ Info: iteration 30, average log likelihood -1.364836
[ Info: iteration 31, average log likelihood -1.364754
[ Info: iteration 32, average log likelihood -1.364699
[ Info: iteration 33, average log likelihood -1.364661
[ Info: iteration 34, average log likelihood -1.364633
[ Info: iteration 35, average log likelihood -1.364612
[ Info: iteration 36, average log likelihood -1.364595
[ Info: iteration 37, average log likelihood -1.364580
[ Info: iteration 38, average log likelihood -1.364566
[ Info: iteration 39, average log likelihood -1.364553
[ Info: iteration 40, average log likelihood -1.364539
[ Info: iteration 41, average log likelihood -1.364525
[ Info: iteration 42, average log likelihood -1.364511
[ Info: iteration 43, average log likelihood -1.364495
[ Info: iteration 44, average log likelihood -1.364479
[ Info: iteration 45, average log likelihood -1.364462
[ Info: iteration 46, average log likelihood -1.364444
[ Info: iteration 47, average log likelihood -1.364425
[ Info: iteration 48, average log likelihood -1.364405
[ Info: iteration 49, average log likelihood -1.364385
[ Info: iteration 50, average log likelihood -1.364365
┌ Info: EM with 100000 data points 50 iterations avll -1.364365
└ 473.9 data points per parameter
┌ Info: 2
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4177493426088128
│     -1.4175857412774557
│      ⋮
└     -1.3643646526568136
[ Info: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.364538
[ Info: iteration 2, average log likelihood -1.364320
[ Info: iteration 3, average log likelihood -1.363457
[ Info: iteration 4, average log likelihood -1.354453
[ Info: iteration 5, average log likelihood -1.330289
[ Info: iteration 6, average log likelihood -1.313847
[ Info: iteration 7, average log likelihood -1.308952
[ Info: iteration 8, average log likelihood -1.306778
[ Info: iteration 9, average log likelihood -1.304816
[ Info: iteration 10, average log likelihood -1.303195
[ Info: iteration 11, average log likelihood -1.302201
[ Info: iteration 12, average log likelihood -1.301634
[ Info: iteration 13, average log likelihood -1.301284
[ Info: iteration 14, average log likelihood -1.301009
[ Info: iteration 15, average log likelihood -1.300765
[ Info: iteration 16, average log likelihood -1.300573
[ Info: iteration 17, average log likelihood -1.300439
[ Info: iteration 18, average log likelihood -1.300356
[ Info: iteration 19, average log likelihood -1.300306
[ Info: iteration 20, average log likelihood -1.300273
[ Info: iteration 21, average log likelihood -1.300250
[ Info: iteration 22, average log likelihood -1.300232
[ Info: iteration 23, average log likelihood -1.300217
[ Info: iteration 24, average log likelihood -1.300204
[ Info: iteration 25, average log likelihood -1.300192
[ Info: iteration 26, average log likelihood -1.300182
[ Info: iteration 27, average log likelihood -1.300172
[ Info: iteration 28, average log likelihood -1.300163
[ Info: iteration 29, average log likelihood -1.300155
[ Info: iteration 30, average log likelihood -1.300148
[ Info: iteration 31, average log likelihood -1.300140
[ Info: iteration 32, average log likelihood -1.300134
[ Info: iteration 33, average log likelihood -1.300127
[ Info: iteration 34, average log likelihood -1.300121
[ Info: iteration 35, average log likelihood -1.300116
[ Info: iteration 36, average log likelihood -1.300110
[ Info: iteration 37, average log likelihood -1.300105
[ Info: iteration 38, average log likelihood -1.300100
[ Info: iteration 39, average log likelihood -1.300096
[ Info: iteration 40, average log likelihood -1.300092
[ Info: iteration 41, average log likelihood -1.300087
[ Info: iteration 42, average log likelihood -1.300084
[ Info: iteration 43, average log likelihood -1.300080
[ Info: iteration 44, average log likelihood -1.300077
[ Info: iteration 45, average log likelihood -1.300073
[ Info: iteration 46, average log likelihood -1.300070
[ Info: iteration 47, average log likelihood -1.300067
[ Info: iteration 48, average log likelihood -1.300065
[ Info: iteration 49, average log likelihood -1.300062
[ Info: iteration 50, average log likelihood -1.300060
┌ Info: EM with 100000 data points 50 iterations avll -1.300060
└ 236.4 data points per parameter
┌ Info: 3
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.3645383966137892
│     -1.3643200934456554
│      ⋮
└     -1.300059951157497
[ Info: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.300293
[ Info: iteration 2, average log likelihood -1.300038
[ Info: iteration 3, average log likelihood -1.299478
[ Info: iteration 4, average log likelihood -1.293656
[ Info: iteration 5, average log likelihood -1.268525
[ Info: iteration 6, average log likelihood -1.240628
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      5
│     11
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.220361
[ Info: iteration 8, average log likelihood -1.232858
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     11
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.218159
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.220675
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     11
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 11, average log likelihood -1.219917
[ Info: iteration 12, average log likelihood -1.220665
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      5
│     11
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 13, average log likelihood -1.210411
[ Info: iteration 14, average log likelihood -1.224120
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     11
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 15, average log likelihood -1.212055
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 16, average log likelihood -1.215822
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     11
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 17, average log likelihood -1.215635
[ Info: iteration 18, average log likelihood -1.216640
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      5
│     11
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 19, average log likelihood -1.206330
[ Info: iteration 20, average log likelihood -1.219705
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     11
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 21, average log likelihood -1.207137
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 22, average log likelihood -1.211617
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     11
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 23, average log likelihood -1.212018
[ Info: iteration 24, average log likelihood -1.213741
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      5
│     11
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 25, average log likelihood -1.203884
[ Info: iteration 26, average log likelihood -1.219006
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     11
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 27, average log likelihood -1.206230
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 28, average log likelihood -1.210536
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     11
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 29, average log likelihood -1.211684
[ Info: iteration 30, average log likelihood -1.213117
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      5
│     11
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 31, average log likelihood -1.203033
[ Info: iteration 32, average log likelihood -1.218683
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     11
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 33, average log likelihood -1.205729
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 34, average log likelihood -1.209971
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     11
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 35, average log likelihood -1.211444
[ Info: iteration 36, average log likelihood -1.212843
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      5
│     11
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 37, average log likelihood -1.202791
[ Info: iteration 38, average log likelihood -1.218596
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     11
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 39, average log likelihood -1.205642
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 40, average log likelihood -1.209916
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     11
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 41, average log likelihood -1.211424
[ Info: iteration 42, average log likelihood -1.212832
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      5
│     11
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 43, average log likelihood -1.202779
[ Info: iteration 44, average log likelihood -1.218595
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     11
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 45, average log likelihood -1.205639
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 46, average log likelihood -1.209915
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     11
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 47, average log likelihood -1.211423
[ Info: iteration 48, average log likelihood -1.212831
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      5
│     11
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 49, average log likelihood -1.202779
[ Info: iteration 50, average log likelihood -1.218595
┌ Info: EM with 100000 data points 50 iterations avll -1.218595
└ 118.1 data points per parameter
┌ Info: 4
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.300293141565422
│     -1.30003765124009
│      ⋮
└     -1.2185949599417631
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     21
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 1, average log likelihood -1.205923
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      9
│     10
│     21
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 2, average log likelihood -1.202574
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     21
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 3, average log likelihood -1.202935
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      9
│     10
│     21
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.180596
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     17
│     21
│     22
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.154651
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      9
│     10
│     18
│     21
│     22
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.143267
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      5
│     21
│     22
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.141759
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      9
│     10
│     17
│     21
│     22
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.136536
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│     18
│     21
│     22
│     29
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.133332
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      9
│     10
│     21
│     22
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.137358
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      5
│     17
│     21
│     22
│     25
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 11, average log likelihood -1.130443
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      9
│     10
│     18
│     21
│     22
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 12, average log likelihood -1.141301
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     21
│     22
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 13, average log likelihood -1.144072
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      9
│     10
│     17
│     21
│     22
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 14, average log likelihood -1.126440
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      5
│     18
│     21
│     22
│     29
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 15, average log likelihood -1.127416
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      9
│     10
│     21
│     22
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 16, average log likelihood -1.144617
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│     17
│     21
│     22
│     25
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 17, average log likelihood -1.131806
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      9
│     10
│     18
│     21
│     22
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 18, average log likelihood -1.131245
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      5
│     17
│     21
│     22
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 19, average log likelihood -1.129227
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      9
│     10
│     15
│     18
│     21
│     22
│     25
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 20, average log likelihood -1.132757
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     21
│     22
│     29
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 21, average log likelihood -1.130242
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      9
│     10
│     17
│     21
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 22, average log likelihood -1.127636
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      5
│     18
│     21
│     22
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 23, average log likelihood -1.119729
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      9
│     10
│     21
│     22
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 24, average log likelihood -1.130481
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│     17
│     21
│     22
│     26
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 25, average log likelihood -1.115952
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      9
│     10
│     15
│     18
│     21
│     22
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 26, average log likelihood -1.126317
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      5
│     21
│     22
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 27, average log likelihood -1.118819
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      9
│     10
│     17
│     21
│     22
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 28, average log likelihood -1.124338
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│     18
│     21
│     22
│     25
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 29, average log likelihood -1.133790
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      5
│      9
│     10
│     21
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 30, average log likelihood -1.126387
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│     15
│     17
│     21
│     22
│     29
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 31, average log likelihood -1.120689
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      9
│     10
│     18
│     21
│     22
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 32, average log likelihood -1.127175
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      5
│     21
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 33, average log likelihood -1.129456
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      9
│     10
│     17
│     21
│     22
│     26
│     29
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 34, average log likelihood -1.111442
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     18
│     21
│     22
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 35, average log likelihood -1.136607
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      9
│     10
│     21
│     22
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 36, average log likelihood -1.119973
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      5
│     15
│     17
│     21
│     22
│     29
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 37, average log likelihood -1.113150
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      9
│     10
│     18
│     21
│     22
│     25
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 38, average log likelihood -1.130885
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     21
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 39, average log likelihood -1.129682
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      5
│      9
│     10
│     17
│      ⋮
│     29
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 40, average log likelihood -1.108693
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│     15
│     18
│     21
│     22
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 41, average log likelihood -1.143101
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      9
│     10
│     21
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 42, average log likelihood -1.129487
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      5
│     17
│     21
│     22
│     26
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 43, average log likelihood -1.110176
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      9
│     10
│     18
│     21
│     22
│     25
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 44, average log likelihood -1.130787
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     21
│     22
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 45, average log likelihood -1.129651
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      5
│      9
│     10
│     15
│     17
│     21
│     22
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 46, average log likelihood -1.113724
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│     18
│     21
│     22
│     25
│     26
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 47, average log likelihood -1.128262
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      9
│     10
│     21
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 48, average log likelihood -1.130363
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│     17
│     21
│     22
│     29
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 49, average log likelihood -1.114813
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      5
│      9
│     10
│     18
│     21
│     22
│     25
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 50, average log likelihood -1.124948
┌ Info: EM with 100000 data points 50 iterations avll -1.124948
└ 59.0 data points per parameter
┌ Info: 5
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.205923435702409
│     -1.202573699059685
│      ⋮
└     -1.124947896971758
┌ Info: Total log likelihood: 
│   tll =
│    251-element Array{Float64,1}:
│     -1.4513635182460578
│     -1.4514359485465775
│     -1.451376005890563
│     -1.451117761726277
│      ⋮
│     -1.13036266021367
│     -1.114812747634343
└     -1.124947896971758
32×26 Array{Float64,2}:
  0.125534     -0.0157205    0.0332378   -0.847745      0.0278753    0.310564     0.0236502    0.0499687   -0.0113653   0.0702641    0.116734    -0.0393959    0.0950096   -0.0171793    0.150484     -0.0404949   -0.0582682    0.0319286   -0.0437663    0.080114     0.125638    -0.245972      0.03719     -0.0867496     0.0529853    -0.0191318
  0.0522845     0.0615397    0.0464364    0.426636      0.0522494    0.23389      0.0129787    0.0653641    0.0879495  -0.00443115   0.112813    -0.00745697  -0.00820931  -0.0651183    0.119562     -0.0308521   -0.0320723    0.0929154   -0.0784929    0.10307      0.0992131    0.0206186     0.0407249   -0.0667786    -0.0810656    -0.247535
 -0.0409725    -0.0892938   -0.0533202   -0.0704324     0.112165    -0.0580539    0.158745    -0.0398126    0.0496699  -0.0918298    0.198531     0.0441979    0.0290417    0.0872714   -0.0641739    -0.0725821   -0.0207578    0.0121947   -0.0752754   -0.0165198   -0.187889    -0.0113374    -0.00580191   0.0699169     0.086934      0.026322
 -0.0870183     0.0463776   -0.00610908  -0.00201162   -0.0652776    0.0908617    0.0142462   -0.0812516    0.0552312   0.0356375   -0.0505251    0.0644195   -0.0169211   -0.0592906    0.157212     -0.0642438    0.0308805   -0.0631305   -0.124446    -0.0880791    0.0160011   -0.101818      0.0128839    0.000384655  -0.0645616     0.0131492
  0.0725223     0.129553    -0.0955902   -0.089212      0.118123    -0.0457026    0.136116     0.0713356   -0.119228    0.0280135    0.201646     0.0681174    0.149393    -0.240701     0.178484      0.041372     0.0452243    0.00561983  -0.00306777  -0.00896249  -0.0253685   -0.201507      0.0136118    0.0605637     0.0173644    -0.147535
 -0.0460058    -0.0544333    0.0901007   -0.0210125     0.00184219  -0.0263476   -0.15677      0.100481    -0.12054    -0.0473094   -0.00859746  -0.106525     0.0985011    0.0130348   -0.207753      0.143148     0.00261231   0.126717     0.110944     0.0746312   -0.0229148   -0.0692234    -0.0854782    0.123937     -0.0336463     0.164514
  0.168235     -0.0843352    0.193071    -0.00514943    0.0739028    0.0812148    0.0176758   -0.0564552   -0.951996    0.00243493   0.140783    -0.134951     0.0516083   -0.0143811    0.0453549    -0.122588    -0.0413729   -0.10018      0.0125063   -0.136508     0.232204    -0.117093     -0.15117     -0.0352383    -0.0192278     0.0359336
 -0.0106546    -0.111191     0.0926653    0.0153425     0.113776    -0.110617     0.00386743  -0.105366     0.803175    0.015734     0.229163    -0.108237     0.114952    -0.0119512   -0.0578448    -0.0951393   -0.0642402   -0.0249893   -0.113765     0.183601     0.232004    -0.127557     -0.0104397    0.00144018    0.0174772     0.189206
  0.0519746    -0.0744361   -0.0318444    0.377033     -0.118163     0.201323    -0.087606    -0.0529191   -0.0496207   0.132957     0.0326026    0.0214482   -0.162497    -0.0441865   -0.0344209     0.206856     0.0116783    0.0399793    0.0694727    0.299567    -0.0486378   -0.343446     -0.0395459   -0.153614     -0.0782779    -0.0513289
  0.0538394     0.22245     -0.270869    -0.272713     -0.109398     0.194205    -0.0478474    0.00601477  -0.076071    0.133464     0.0340297    0.0658979    0.0713457   -0.0393334    0.432877      0.206809     0.025156     0.0281343    0.12064     -0.0449808   -0.0484583    0.0709483    -0.0245136    0.0120418    -0.0380748    -0.17435
 -0.141156      0.0416035   -0.150901     0.0673657    -0.0858492    0.0202711    0.18442      0.0296428   -0.0216401  -0.100386    -0.0589576    0.123375    -0.141216     0.0665034   -0.045557      0.0489637    0.246276     0.0763727   -0.0441968    0.0585513    0.0478105   -0.851387      0.0159865   -0.0206841     0.0437697     0.128508
  0.000469081   0.0391559   -0.139475     0.0808864    -0.136144     0.0164048   -0.0987729    0.0628001   -0.0832004  -0.165277    -0.0356532    0.114714    -0.0341075    0.0606551   -0.0661902     0.269532    -0.120336     0.122019    -0.0706539    0.0645259    0.0813345    0.631786      0.0215841   -0.0213157     0.177472      0.0377736
  0.0713799    -0.176753    -0.21406      0.110723     -0.0476818   -0.0525812    0.0826687    0.0500865    0.191186    0.1631      -0.0145355   -0.135939    -0.112858    -0.0484482    0.00337007    0.119847    -0.0158071   -0.030831    -0.140361    -0.173969     0.0882195    0.207816     -0.0861334    0.141592      0.0158759    -0.214545
  0.0355553    -0.266861    -0.144978     0.0674546     0.0082279   -0.0696268    0.110531    -0.105827     0.131409    0.0442114    0.150421     0.24955     -0.128048    -0.0738356    0.199743      0.0990511    0.183428    -0.020325    -0.126011    -0.120607     0.0211575    0.00957395    0.0971075    0.0408959     0.290659     -0.25239
  0.151221      0.0066217   -0.181081     0.0359506    -0.115535     0.0115317   -0.0591257    0.055665     0.0550574   0.143747    -0.133586     0.0956076    0.0425007    0.115134    -0.0180952     0.0914855    0.126608     0.0609413   -0.0012972   -0.0728882    0.0427529    0.0652576    -0.403087    -0.0446475    -0.0506582    -0.00628205
  0.160444      0.0350435   -0.206912     0.0430656    -0.174452    -0.00868025  -0.049656    -0.231707     0.120623   -0.223132    -0.162274     0.0804244    0.104628     0.137066    -0.0463707    -0.207082     0.107512     0.0192182    0.00373967   0.0260668    0.104145     0.00858666    0.391269     0.0286414    -0.0314586    -0.0703906
 -0.0840859    -0.0976166    0.17413     -0.0598689    -0.0103245    0.0381626   -0.0339464    0.0650317    0.105668   -0.0706104    0.0223902   -0.16593     -0.0371766   -0.050579     0.0100935    -0.109042    -0.216136     0.0341395    0.0546875   -0.109358     0.0680092   -0.0391611    -0.0391845   -0.124813     -0.0147534     0.243306
  0.0643977    -0.00929985   0.0232325    0.199404      0.0158627    0.0947583    0.111541     0.145372    -0.0997084  -0.00794987   0.064343    -0.147401    -0.141012    -0.249637     0.000430243  -0.129223     0.0248141   -0.144103     0.0683011   -0.0444457   -0.00420675   0.0956312     0.143502    -0.114048     -0.0565558    -0.0354942
 -0.135484     -0.0190421    0.0989437    0.0463818     0.0669752    0.0289891    0.0549193    0.0169023   -0.025753    0.0897637    0.0488229   -0.157278    -0.0590025    0.00651246  -0.0254219     0.00383804  -0.0529341   -0.0410527   -0.119615     0.0368982    0.0360308    0.0675218    -0.0463127    0.0698978    -0.010627      0.0639405
  0.0816448    -0.12587     -0.0988392   -0.0446043     0.0110705   -0.00297778  -0.0441603    0.00847351  -0.0393552  -0.0795607   -0.146945     0.0393798    0.00950692  -0.0350966    0.0249175    -0.156239     0.192093    -0.0153659   -0.0293501   -0.00346186   0.060482     0.0891438    -0.0547007    0.0357343    -0.056041     -0.155674
 -0.0797139     0.136396     0.304859    -0.000889158  -0.0567729    0.0615255   -0.192781     0.020188     0.142513   -0.25379      0.147953    -0.0557273   -0.0771397   -0.777484     0.327465      0.275758    -0.0205487   -0.17962      0.109602     0.313681     0.196093    -0.165158     -0.11038      0.089813     -0.0179451     0.0593524
 -0.125523      0.136123     0.305451     0.00328016   -0.0919684   -0.0527003   -0.190967    -0.219875     0.142699    0.20505      0.0441334   -0.0480278   -0.0637667    0.676756     0.205625      0.03774     -0.136063    -0.16931      0.10801     -0.232142     0.16955     -0.168777     -0.106717     0.0774591    -0.0351109     0.0578123
  0.0740257     0.0240069   -0.0265003    0.123126     -0.0657211   -0.272885     0.00748969  -0.145692    -0.0961718   0.121749     0.123812    -0.00573109   0.233299     0.0584218    0.052298     -0.0326743   -0.0556347    0.0283389    0.0414111    0.0401911    0.0858082    0.0761471     0.148654     0.0592597    -0.187354      0.0868339
 -0.0428785    -0.0977485    0.188409    -0.132777      0.00111991   0.0414034    0.0545901   -0.050995    -0.0437584   0.108043     0.0755963   -0.0343526   -0.0146221   -0.0420591   -0.0727568     0.00341307   0.174307    -0.017915    -0.0317579    0.128075    -0.0207537   -0.204219     -0.0596704    0.0514072    -0.0694156    -0.0294694
  0.135807     -0.124384    -0.134859     0.0515034    -0.0162094   -0.0124452    0.0557324   -0.13462     -0.1339     -0.0300153   -0.134879    -0.160563     0.0624434   -0.011508     0.0909535    -0.00912145  -0.00874255  -0.0388717   -0.0551262   -0.0386971   -0.0401536   -0.1027       -0.192452    -0.0633806    -0.0277729    -0.0771067
  0.0248476    -0.128864    -0.0903519   -0.103731     -0.0204679   -0.0682507   -0.0646321    0.0432437   -0.0945099   0.0930008    0.0245102    0.0722907    0.0466038    0.0164602   -0.0283244     0.0261583    0.0685513    0.0599103    0.101229    -0.0281896   -0.00156947   0.0684057     0.0407944    0.0504485     0.0584544    -0.152946
  0.0553736     0.00242816  -0.0869349    0.0270738    -0.0231047    0.0212113   -0.0200428    0.00216585   0.0688853   0.0793514    0.0758124   -0.0280069   -0.00572681  -0.019935    -0.0613961     0.0411839    0.036966    -0.119421    -0.070161     0.00224536   0.00371832   0.000768008   0.0140647    0.00315255   -0.0662233    -0.0750643
  0.118045      0.0878035    0.11275      0.00845198    0.0172494    0.0903588   -0.205171    -0.0653698    0.0380324   0.0607965    0.122418     0.0743748   -0.00595427   0.0277916    0.0678015     0.11052     -0.00594768   0.0728071   -0.0728524    0.0341933   -0.0739938    0.0606222     0.0439454    0.0107495    -0.0162006     0.0309532
 -0.189046      0.0256342    0.159517    -0.0750891     0.0616137   -0.138318     0.111014     0.0516748   -0.0801116  -0.0627519    0.232104     0.0132687   -0.014323     0.107339    -0.0280323     0.104837    -0.076428     0.00992249   0.0510412    0.182202     0.056247     0.158918     -0.0278734    0.159297     -0.0254385    -0.00986749
 -0.107561      0.133956     0.107763     0.0830216     0.0286654    0.132722    -0.157209    -0.0542929   -0.0866581  -0.0082723    0.127495    -0.0462063    0.0710005   -0.164532    -0.0290652    -0.063883     0.0911764   -0.0123636   -0.040649     0.0630928   -0.0235589   -0.0391936    -0.11244     -0.0981886     0.070846      0.00175294
 -0.0188687    -0.0960726   -0.212497    -0.0146054     0.14806      0.175552     0.0310702    0.0394624    0.107468    0.0949994    0.0584601    0.0788188    0.124896    -0.140411    -0.0905653     0.137069    -0.0701283   -0.0462412   -0.0421015    0.0101914    0.00304066  -0.140444      0.156967     0.00764345    0.129497      0.0979329
  0.119429      0.102401    -0.0872638   -0.000337549  -0.18084     -0.142827     0.181539     0.0255284    0.137738    0.0647652   -0.0497471   -0.125584     0.0843047    0.0709983   -0.0356934     0.00920991   0.0449105   -0.00820078  -0.107865    -0.117083     0.0418975   -0.00738482    0.0541216    0.0239286    -0.000724803   0.010075[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     15
│     21
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 1, average log likelihood -1.133643
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      9
│     10
│     15
│     17
│     21
│     22
│     26
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 2, average log likelihood -1.106137
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      5
│     18
│     21
│     22
│     25
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 3, average log likelihood -1.107345
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      9
│     10
│     15
│     17
│     21
│     22
│     26
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.123715
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     15
│     21
│     22
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.115434
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      5
│      9
│     10
│     17
│      ⋮
│     29
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.099405
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     15
│     21
│     22
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.131387
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      9
│     10
│     15
│     17
│     21
│     22
│     26
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.107542
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      5
│     18
│     21
│     22
│     29
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.107222
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      9
│     10
│     15
│     17
│     21
│     22
│     25
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.124438
┌ Info: EM with 100000 data points 10 iterations avll -1.124438
└ 59.0 data points per parameter
[ Info: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
kind diag, method kmeans
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.212625e+05
      1       7.366094e+05      -1.846530e+05 |       32
      2       7.096746e+05      -2.693485e+04 |       32
      3       6.912318e+05      -1.844282e+04 |       32
      4       6.789828e+05      -1.224902e+04 |       32
      5       6.723563e+05      -6.626493e+03 |       32
      6       6.688689e+05      -3.487319e+03 |       32
      7       6.666801e+05      -2.188803e+03 |       32
      8       6.647181e+05      -1.962060e+03 |       32
      9       6.631735e+05      -1.544579e+03 |       32
     10       6.619791e+05      -1.194348e+03 |       32
     11       6.608951e+05      -1.084075e+03 |       32
     12       6.599815e+05      -9.135493e+02 |       32
     13       6.592886e+05      -6.929702e+02 |       32
     14       6.587558e+05      -5.327446e+02 |       32
     15       6.584032e+05      -3.526071e+02 |       32
     16       6.581303e+05      -2.729329e+02 |       32
     17       6.578932e+05      -2.370582e+02 |       32
     18       6.576572e+05      -2.360086e+02 |       31
     19       6.574612e+05      -1.960014e+02 |       32
     20       6.573130e+05      -1.482370e+02 |       32
     21       6.572199e+05      -9.307688e+01 |       32
     22       6.571492e+05      -7.064494e+01 |       32
     23       6.570900e+05      -5.921969e+01 |       32
     24       6.570426e+05      -4.744891e+01 |       31
     25       6.570115e+05      -3.111004e+01 |       32
     26       6.569895e+05      -2.196489e+01 |       30
     27       6.569746e+05      -1.490633e+01 |       32
     28       6.569652e+05      -9.399819e+00 |       29
     29       6.569580e+05      -7.237789e+00 |       28
     30       6.569550e+05      -2.979502e+00 |       25
     31       6.569514e+05      -3.558248e+00 |       26
     32       6.569479e+05      -3.532282e+00 |       25
     33       6.569451e+05      -2.759940e+00 |       20
     34       6.569439e+05      -1.203414e+00 |       17
     35       6.569429e+05      -1.010978e+00 |       18
     36       6.569420e+05      -8.658369e-01 |       17
     37       6.569409e+05      -1.112334e+00 |       16
     38       6.569399e+05      -1.062326e+00 |       10
     39       6.569393e+05      -5.772130e-01 |       13
     40       6.569384e+05      -8.836831e-01 |       16
     41       6.569375e+05      -9.180685e-01 |       13
     42       6.569369e+05      -6.100485e-01 |        8
     43       6.569367e+05      -2.271330e-01 |        8
     44       6.569363e+05      -3.386727e-01 |        6
     45       6.569361e+05      -1.800560e-01 |        2
     46       6.569361e+05      -4.390446e-02 |        2
     47       6.569361e+05      -1.622442e-02 |        5
     48       6.569360e+05      -1.129650e-01 |        5
     49       6.569359e+05      -9.815913e-02 |        3
     50       6.569358e+05      -4.301171e-02 |        2
K-means terminated without convergence after 50 iterations (objv = 656935.8215711887)
┌ Info: K-means with 32000 data points using 50 iterations
└ 37.0 data points per parameter
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.360366
[ Info: iteration 2, average log likelihood -1.329804
[ Info: iteration 3, average log likelihood -1.300646
[ Info: iteration 4, average log likelihood -1.264462
[ Info: iteration 5, average log likelihood -1.218843
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     14
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.160804
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      7
│     13
│     18
│     23
│     29
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.138081
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      2
│      3
│     17
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.161755
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     25
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.163928
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      9
│     12
│     14
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.138825
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      7
│     23
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 11, average log likelihood -1.130424
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      2
│      3
│     13
│     18
│     28
│     30
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 12, average log likelihood -1.122158
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     17
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 13, average log likelihood -1.177160
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      9
│     14
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 14, average log likelihood -1.120598
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      7
│     23
│     25
│     27
│     28
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 15, average log likelihood -1.107333
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      3
│     12
│     18
│     30
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 16, average log likelihood -1.150941
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      2
│     17
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 17, average log likelihood -1.168887
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      1
│      9
│     14
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 18, average log likelihood -1.136578
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     22
│     29
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 19, average log likelihood -1.129442
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      3
│      7
│     18
│     23
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 20, average log likelihood -1.106310
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      2
│     12
│     13
│     14
│     17
│     25
│     27
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 21, average log likelihood -1.131121
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     9
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 22, average log likelihood -1.190899
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     29
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 23, average log likelihood -1.133333
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      1
│      3
│      7
│     14
│     23
│     28
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 24, average log likelihood -1.100440
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      2
│     17
│     18
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 25, average log likelihood -1.158468
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      9
│     22
│     25
│     27
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 26, average log likelihood -1.138334
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     12
│     28
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 27, average log likelihood -1.147457
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      3
│      7
│     14
│     23
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 28, average log likelihood -1.135205
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      2
│     17
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 29, average log likelihood -1.142874
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      9
│     18
│     28
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 30, average log likelihood -1.124153
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     14
│     22
│     27
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 31, average log likelihood -1.131097
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      1
│      3
│      7
│     12
│     23
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 32, average log likelihood -1.129409
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      2
│     17
│     28
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 33, average log likelihood -1.149422
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      9
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 34, average log likelihood -1.153035
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     13
│     14
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 35, average log likelihood -1.119668
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      3
│      7
│     22
│     23
│     28
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 36, average log likelihood -1.107757
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      2
│     12
│     17
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 37, average log likelihood -1.144193
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      9
│     14
│     18
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 38, average log likelihood -1.139494
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     28
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 39, average log likelihood -1.134333
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      3
│      7
│     23
│     25
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 40, average log likelihood -1.104489
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      2
│      9
│     13
│     14
│     17
│     22
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 41, average log likelihood -1.121224
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     27
│     28
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 42, average log likelihood -1.159431
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     12
│     18
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 43, average log likelihood -1.145862
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      3
│      7
│     14
│     23
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 44, average log likelihood -1.101144
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      2
│      9
│     17
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 45, average log likelihood -1.123084
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│     13
│     22
│     25
│     29
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 46, average log likelihood -1.127153
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     12
│     14
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 47, average log likelihood -1.138251
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      3
│      7
│     18
│     23
│     28
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 48, average log likelihood -1.108544
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      2
│      9
│     17
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 49, average log likelihood -1.149926
32×26 Array{Float64,2}:
  0.141393    0.00559381  -0.0401923   -0.0331756   -0.026756     0.0596535    -0.126872    -0.120413    -0.0744644   0.0452469    0.156216     0.031055     0.0179794    0.033898     0.0368186     0.0883254   -0.0393344    0.098699    -0.0823888   -0.00524374  -0.147251    -0.0219891   -0.0443973   0.0952141    -0.0340708    -0.0228452
  0.0933169   0.0650939   -0.186344     0.0951606   -0.0967383    0.0469755    -0.120857    -0.0184536   -0.123646    0.203614     0.0844966    0.0748901    0.24464     -0.00376545  ┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     14
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 50, average log likelihood -1.130984
┌ Info: EM with 100000 data points 50 iterations avll -1.130984
└ 59.0 data points per parameter
-0.261074      0.12292     -0.0659519   -0.189372    -0.134432     0.0629031   -0.00514085  -0.0667766    0.0598094   0.000268601  -0.0440908    -0.0258033
  0.130709    0.103018    -0.140048    -0.0111328   -0.208836    -0.192032      0.170336     0.0424663    0.11267     0.0768186   -0.0229473   -0.118952     0.112961     0.0751371   -0.0640329     0.00222864   0.0296666   -0.00837508  -0.105049    -0.124346     0.0420175   -0.00625811   0.0253484   0.0469846    -0.0238923     0.0107027
 -0.0630527   0.177182     0.144094     0.116237    -0.079049     0.0921867    -0.0955217   -0.195668    -0.0830806  -0.077115    -0.00772042  -0.0539614    0.13489     -0.100562     0.120708     -0.0774268    0.0877799    0.0318984   -0.0702855   -0.0271497   -0.0190145   -0.106321    -0.142932   -0.111494     -0.0263727    -0.0267101
 -0.0935433  -0.0846909    0.00076575  -0.0267736   -0.0526386    0.194758     -0.0151922   -0.144514     0.147174    0.012601    -0.0822863    0.113726    -0.0104592   -0.133204     0.178203     -0.0840255    0.0360994   -0.0808244   -0.18321     -0.177868    -0.00344112  -0.109803     0.0982603   0.0510509    -0.0858541    -0.034992
 -0.0699506   0.0403657   -0.143861     0.0737916   -0.111464     0.0184604     0.041256     0.0470803   -0.0519937  -0.131698    -0.0469464    0.11867     -0.0858558    0.063131    -0.0531043     0.156636     0.065212     0.0982516   -0.0572658    0.0587014    0.0647716   -0.107077     0.01674    -0.0172288     0.106482      0.081678
 -0.165044    0.117541     0.100901     0.0186827    0.126334     0.150812     -0.179091     0.0617426   -0.0918656   0.0415163    0.253746    -0.0155897    0.0318487   -0.188991    -0.117915     -0.0387951    0.074584    -0.0987955   -0.00037935   0.145417    -0.0209276    0.0382714   -0.0991997  -0.0783109     0.129149      0.0388783
  0.0909981   0.0296016    0.0389092   -0.194768     0.0382136    0.26877       0.0192373    0.0570382    0.0382179   0.0331359    0.114372    -0.0183418    0.0442868   -0.04273      0.137575     -0.0362102   -0.0452537    0.0615973   -0.06094      0.0910434    0.109038    -0.11132      0.0383669  -0.0788811    -0.0154964    -0.134058
 -0.0670702   0.210586    -0.0203662    0.0160671   -0.0595975   -0.0417044     0.0530992    0.00146994  -0.0620335   0.0623622   -0.0133159    0.0329653   -0.0253598    0.0276034    0.170301     -0.0565077    0.00987635  -0.0531506   -0.0568259    0.0227166    0.0374458   -0.155283    -0.0721349  -0.0959067    -0.0296176     0.0891217
  0.0802829  -0.096168     0.141407     0.0048716    0.0910375   -0.0158079     0.00933368  -0.0803184   -0.0958314   0.00888254   0.181507    -0.122092     0.0815203   -0.0124871   -0.00350366   -0.109395    -0.0550892   -0.0655163   -0.0512577    0.020878     0.231619    -0.121302    -0.0837974  -0.0161514    -0.000219174   0.105985
 -0.152819   -0.101576     0.114435     0.0554775    0.180601     0.0431924     0.188123     0.0423233   -0.142263    0.148513    -0.0616392   -0.0807131   -0.0105849    0.0118128   -0.0641965    -0.0287566   -0.0451992   -0.00983963  -0.116051     0.0174724   -0.0472241    0.0926754    0.0504133   0.0425518    -0.0159198     0.130097
  0.0191035  -0.138877    -0.0599493   -0.0910866   -0.0220195   -0.0703452    -0.0740075    0.0578346   -0.0954485   0.0951869    0.0171467    0.0643094    0.0413251    0.0184973   -0.0588676     0.0312346    0.0683765    0.057767     0.0808669   -0.0290397    0.00245666   0.0839187    0.0449626   0.0492296     0.0563846    -0.147061
 -0.0143337  -0.0982144   -0.244705    -0.0290181    0.159288     0.176038      0.0329586    0.0410807    0.10447     0.104213     0.0351908    0.0796424    0.124845    -0.151467    -0.086012      0.146029    -0.0615952   -0.0434996   -0.0316023    0.0145078   -0.00838168  -0.156141     0.167964   -0.000416979   0.149966      0.0976208
  0.0717207   0.128181    -0.0900394   -0.0794743    0.102737    -0.0415848     0.122568     0.0704506   -0.114428    0.0310375    0.201873     0.0670465    0.152145    -0.229466     0.169988      0.0297472    0.0485279    0.00780732  -0.00319299  -0.0041101   -0.0231328   -0.192594     0.0175436   0.0590598     0.0190178    -0.142909
  0.116468   -0.0796962   -0.113305    -0.0561597    0.0219331   -0.0160034    -0.0768608    0.0159328   -0.0343622  -0.130899    -0.0934471    0.0953733    0.106681    -0.0567826   -0.103683     -0.143851     0.147625    -0.0618677    0.0988909    0.0111692    0.0398438    0.10442     -0.0386472   0.0228185    -0.10633      -0.123701
  0.101188   -0.118226    -0.187727     0.0658959   -0.0783398   -0.0355926     0.0337239   -0.0742932    0.134282    0.0220001   -0.0547328    0.07984     -0.038276     0.026393     0.0387065     0.0184791    0.0947239   -0.00243468  -0.0703521   -0.0920294    0.065452     0.0785216    0.024011    0.0422121     0.0667359    -0.157393
 -0.318669    0.02111      0.165226    -0.0768281    0.0520312   -0.121357      0.11702      0.0699044   -0.114715   -0.0921631    0.243108     0.0278042   -0.0455644    0.148686    -0.0590846     0.104638    -0.0698138    0.0347482    0.0595451    0.186119     0.0522367    0.156759    -0.0182687   0.218246     -0.0180932     0.000259849
 -0.180706   -0.136736     0.22074     -0.0830523   -0.00670572   0.0403351    -0.0227502    0.046574     0.100404   -0.0585587    0.0481249   -0.157118    -0.0346609   -0.0425919   -0.0691142    -0.101008    -0.203692     0.110765    -0.0133724   -0.0765244    0.0039034   -0.0305241   -0.149351   -0.118344     -0.00716731    0.200898
  0.0581206  -0.0585248   -0.0327488   -0.0748604    0.0274681    0.00212943    0.0248525   -0.052145     0.0913295  -0.0471186    0.0932164   -0.0122835   -0.0266858    0.112681    -0.00514062    0.070848     0.0856969   -0.0250544   -0.0403545   -0.0155421   -0.105831     0.0333005   -0.0882084   0.00808466    0.00137789    0.0687388
  0.0751312  -0.15046     -0.0902019   -0.0393612    0.0057189    0.00542933   -0.0246536    0.00889458  -0.0432559  -0.0640999   -0.18592      0.0195957   -0.0434195   -0.0152465    0.0994383    -0.159654     0.231758     0.0133145   -0.100011    -0.00912114   0.0665555    0.0840212   -0.0667827   0.0420093    -0.0350183    -0.180443
 -0.0365994  -0.0964877    0.194507    -0.130516     0.00100639   0.0473378     0.0581355   -0.0477461   -0.0507579   0.114817     0.0592082   -0.0322827   -0.0209124   -0.0365087   -0.0738199     0.00308021   0.180224    -0.00567826  -0.0284807    0.131467    -0.033593    -0.200793    -0.0726616   0.0548225    -0.0687166    -0.0189917
  0.0607421  -0.0136042    0.0191909    0.195148     0.0133912    0.090489      0.110197     0.144265    -0.094837   -0.00959418   0.0636944   -0.152716    -0.134981    -0.238121    -0.000500326  -0.135758     0.0171126   -0.143141     0.0635171   -0.044296    -0.00481262   0.0888723    0.147593   -0.108486     -0.0537975    -0.0301951
  0.0663035  -0.143255    -0.0767309    0.158441     0.0246235    0.00995715   -0.0560001   -0.07469      0.117294    0.0553545    0.166624    -0.0893984   -0.103956    -0.132146    -0.105757      0.0305918    0.109209     0.0248833   -0.113597     0.0589155    0.0868197    0.0242221    0.19464    -0.0307037     0.0217496    -0.110245
 -0.0454264  -0.0538414    0.0878776   -0.0189739    0.00179391  -0.0212754    -0.153483     0.100615    -0.120428   -0.0557727   -0.0046343   -0.103889     0.095452     0.0155651   -0.211747      0.144842    -0.00232097   0.13102      0.112744     0.0755141   -0.023047    -0.0639962   -0.0857747   0.123424     -0.0356504     0.164927
  0.131105   -0.127107    -0.13798      0.0238225   -0.0150055   -0.018466      0.0449551   -0.113618    -0.133051   -0.0211815   -0.111258    -0.115373     0.0657899    0.00267596   0.103018      0.0009778   -0.005035    -0.0229949   -0.00931828  -0.0302597   -0.0401541   -0.0819733   -0.159794   -0.0496325    -0.013389     -0.0887537
  0.0744263   0.0279528   -0.0141702    0.105937    -0.0641622   -0.276048      0.00849238  -0.138427    -0.0979642   0.11558      0.131279    -0.00802067   0.221812     0.0529641    0.0447648    -0.0306189   -0.0525276    0.0175431    0.0437726    0.0458847    0.0840227    0.0807601    0.135962    0.0638916    -0.176301      0.0864875
 -0.219936    0.0806111    0.0905857    0.0617234   -0.0170819   -0.000895885  -0.0126807    0.0200103    0.0678742   0.0538886    0.215702    -0.224967    -0.0918254   -0.0513705    0.000990849  -0.054712    -0.153873    -0.101146    -0.163226     0.0532125    0.160407     0.0411532   -0.116965    0.145844      0.00689873   -0.0221274
 -0.0976936   0.133295     0.305183     0.00180514  -0.076337     0.0036503    -0.19038     -0.10036      0.14207    -0.0239969    0.0911079   -0.0503221   -0.0694236   -0.0562005    0.263018      0.15507     -0.0733337   -0.170287     0.107396     0.0434075    0.181772    -0.163978    -0.113855    0.0813499    -0.0245341     0.056965
  0.0534336   0.0957733   -0.170991    -0.00137108  -0.114199     0.196395     -0.0642546   -0.0175128   -0.0608031   0.133261     0.0333975    0.0492631   -0.0244442   -0.041155     0.237459      0.206909     0.0210952    0.0330789    0.0987512    0.100311    -0.048441    -0.100937    -0.031189   -0.0562989    -0.0550003    -0.122482
 -0.0798981  -0.0935818    0.186862    -0.0625004   -0.0127029    0.0325401    -0.0346792    0.0625164    0.109851   -0.0710303    0.0220968   -0.170489    -0.0372808   -0.0540854    0.0143093    -0.119861    -0.209348     0.0213403    0.0876878   -0.112409     0.088802    -0.0352082   -0.0233803  -0.12312      -0.015029      0.248758
 -0.0266975   0.0809672   -0.0796159   -0.0615696    0.0190628   -0.000161131   0.0793558    0.110511     0.105047    0.0596122    0.0294065   -0.0214636   -0.0444079   -0.00285373   0.035325     -0.0532714   -0.00769832  -0.182092    -0.0103722   -0.0408477   -0.0315738    0.0130609   -0.0846452   0.0486136    -0.118765     -0.130217
  0.100571    0.184576     0.260982     0.049224     0.0729916    0.122453     -0.205633     0.00352992   0.124878    0.0790635    0.085425     0.0795835   -0.00441513   0.0261299    0.056846      0.116929     0.0122468    0.0551821   -0.0679723    0.0893932   -0.00351662   0.111852     0.148412   -0.0640854     0.00581791    0.0651672[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│     13
│     22
│     25
│     28
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 1, average log likelihood -1.103258
┌ Warning: Variances had to be floored 
│   ind =
│    14-element Array{Int64,1}:
│      3
│      7
│      9
│     12
│      ⋮
│     28
│     30
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 2, average log likelihood -1.051829
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      2
│     13
│     22
│     25
│     27
│     28
│     29
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 3, average log likelihood -1.082906
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      3
│      7
│     12
│     13
│      ⋮
│     28
│     30
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.061193
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      9
│     13
│     14
│     17
│      ⋮
│     28
│     30
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.058710
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      2
│      3
│      7
│     12
│      ⋮
│     29
│     30
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.064514
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│     13
│     22
│     25
│     27
│     28
│     30
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.075826
┌ Warning: Variances had to be floored 
│   ind =
│    15-element Array{Int64,1}:
│      3
│      7
│      9
│     12
│      ⋮
│     28
│     30
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.030994
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      2
│     13
│     22
│     25
│     27
│     28
│     29
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.074378
┌ Warning: Variances had to be floored 
│   ind =
│    14-element Array{Int64,1}:
│      3
│      7
│      9
│     12
│      ⋮
│     28
│     30
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.047349
┌ Info: EM with 100000 data points 10 iterations avll -1.047349
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.142301      0.0148765   -0.0241925   -0.1333       0.0809327   -0.0408367   0.0314962   -0.162901      0.0284022    0.017535    -0.0740507  -0.0156324    0.0321192   -0.155143    -0.0183364   -0.110944    -0.125664    -0.320609     0.061048   -0.215759      0.0140878    -0.148759     0.0277578    0.0792588   -0.0116982   -0.0437354
 -0.00435209    0.0761163    0.0980121    0.0556321    0.0816397   -0.0390413  -0.0370792   -0.0613578    -0.0751898    0.11519      0.107756   -0.0152042   -0.0335831   -0.0642476   -0.0233042   -0.0297395   -0.0913357   -0.0483889   -0.0708322  -0.00819956    0.0800544    -0.0690166   -0.00820184  -0.141215    -0.180053     0.0494258
  0.0514831     0.00681575   0.0414832    0.0484031    0.0475721   -0.0312508  -0.0612856    0.0247203    -0.196673    -0.0747828   -0.0576917  -0.0845914    0.126956    -0.105002     0.0303811    0.173662    -0.00998602   0.180654    -0.112629    0.0208212    -0.0149008    -0.0733595    0.0801523    0.0101201   -0.139023    -0.0614388
 -0.0642045     0.144091    -0.0702081   -0.116881    -0.0502131   -0.0710414  -0.0568408   -0.000671657  -0.0315074   -0.109187    -0.154557    0.152171     0.155468     0.0109392   -0.0943461   -0.0184835    0.0865408   -0.0163867   -0.0585593  -0.0434739    -0.0230616     0.0326699   -0.097278     0.107567    -0.152532     0.121905
  0.216805     -0.143299     0.083161    -0.100971     0.240232     0.213822    0.0841991   -0.141382      0.0168427   -0.0440345   -0.0330555  -0.184756     0.036176     0.018082    -0.0123844    0.0878736    0.0791514   -0.0035743    0.11849    -0.0289573    -0.145795      0.0476623   -0.0203498   -0.22669      0.0951263    0.0403486
 -0.00505854    0.039545     0.140763     0.056735     0.0671514   -0.0262619  -0.0761249    0.0703598     0.0435473   -0.191605     0.122165    0.161764     0.0371286   -0.0621102   -0.175318     0.123225    -0.0403058   -0.0975385   -0.0483835   0.000215005   0.114905     -0.00749323  -0.0649299    0.0298529   -0.133065     0.0451863
 -0.0994432     0.105275     0.245856     0.00131702   0.120312     0.09134    -0.0331935   -0.035961      0.0455861   -0.111547    -0.100292   -0.0050098   -0.155038     0.0100883   -0.0850592    0.0687131    0.0306925   -0.0937409    0.102463   -0.121028     -0.0385918    -0.0138441    0.0360121   -0.0424507    0.0707062    0.00913257
 -0.0938907     0.0907557   -0.0675973   -0.0137985    0.029137     0.149282   -0.124868    -0.0276219    -0.12282     -0.135116    -0.0449057  -0.22317     -0.211043    -0.0324625    0.00632716   0.0629626   -0.11317      0.115159    -0.213234   -0.0593327     0.0560429    -0.0177728   -0.0350916    0.0654285    0.0649247    0.139427
 -0.0796978    -0.00457689   0.0639156    0.0345797   -0.0524012   -0.0403061   0.114581     0.0519008    -0.0345664   -0.0160476   -0.167944    0.0233139    0.08226     -0.0510211   -0.0499354   -0.0550666    0.145271    -0.00685107   0.146825    0.140264     -0.0917507     0.214463    -0.179981     0.026439     0.0152305    0.0387499
  0.0180271    -0.149462     0.206373     0.0701413    0.0379894    0.105435    0.111938    -0.0808664     0.0624638    0.0453961   -0.0654433   0.196172     0.0893081    0.0749511   -0.0188794    0.0277644   -0.15806     -0.16265      0.182686    0.0807357    -0.190202      0.0556679    0.045083    -0.142344    -0.0543888    0.188667
  0.000492574  -0.0609098   -0.0599608   -0.0129385   -0.0755367    0.0545116  -0.116367    -0.0123383    -0.0983509   -0.0283198    0.128347   -0.127533     0.0552745   -0.180547     0.00278004   0.00745534   0.111574    -0.145825     0.0209932   0.0539122    -0.163107     -0.090368     0.0746667    0.0797562   -0.0941586   -0.0498419
  0.0533676    -0.013814    -0.0713251   -0.101619     0.197569     0.0143762   0.0765078   -0.0570706    -0.0579109    0.10833     -0.19031     0.161353    -0.0667162   -0.157426    -0.0587972   -0.0629318   -0.0530757   -0.033974    -0.130789    0.00106472   -0.0485639    -0.0246476   -0.0477192    0.0942972   -0.0169764   -0.0199161
  0.231168      0.0846234    0.0693856    0.0840169    0.0327681   -0.0675221   0.0773195    0.113715     -0.0726655   -0.122125     0.229533   -0.00842019   0.114305     0.0417557   -0.0583499   -0.114866    -0.0456244   -0.118709    -0.0534926   0.0382358    -0.00561992   -0.030619    -0.0218403   -0.0102991    0.0544324    0.0196574
 -0.0642073    -0.146109    -0.100059    -0.0584051   -0.124869     0.10932    -0.0290803    0.0722677     0.13349     -0.233026    -0.196931    0.0581895    0.166132     0.0434402   -0.0276762    0.147739    -0.204793     0.0425394   -0.176269    0.0770732     0.0748897     0.124852    -0.0616263    0.00321963  -0.105039     0.0183045
 -0.0375649    -0.0341549   -0.0703259    0.159875     0.0808686    0.130825    0.0558095   -0.0301754     0.00694421  -0.115284    -0.203598   -0.116913     0.040214     0.00357671   0.134649    -0.150999    -0.00144953  -0.027352     0.0452839   0.068902     -5.70968e-5    0.11422      0.125135    -0.146018     0.0943594   -0.13559
  0.0855305    -0.016946    -0.198758     0.016141    -0.0765332   -0.163695   -0.0452798    0.0405422     0.108424     0.0303128   -0.0605747  -0.0139919    0.202586    -0.00332893   0.140489    -0.0323741    0.0140912    0.141598     0.0218944  -0.0514164     0.188271      0.0888168    0.0703124    0.0456634    0.168426     0.128816
  0.0332604     0.0476878   -0.0542865   -0.131853     0.16959      0.0686952   0.051521    -0.0469531    -0.0679474   -0.00621388   0.0696881   0.250424    -0.178834    -0.0188671   -0.196022    -0.0227036    0.0937268    0.12765     -0.030614   -0.0433688     0.113626     -0.0836169   -0.0622275    0.137066    -0.108635    -0.0541217
 -0.0366549     0.0403953    0.0342146   -0.0849818   -0.0557512   -0.0181307  -0.0165191   -0.0873958     0.117022    -0.00414557   0.0519476  -0.165881    -0.208014    -0.0238403    0.0363615    0.0235945   -0.017823    -0.00765676  -0.12976    -0.090293     -0.0186735    -0.140018    -0.0238389   -0.0390679    0.0810162    0.0240664
  0.0144729     0.075094     0.161987     0.197908    -0.00501633  -0.0718732  -0.00475039  -0.0286065     0.236792    -0.0578694    0.0192737  -0.074011     0.0586965    0.0778264   -0.132077     0.00805254  -0.0312057   -0.0152319    0.0184703  -0.0559656    -0.126522     -0.0249053    0.191173     0.157103     0.0157203    0.115551
 -0.0726709     0.0678898    0.0653987    0.0324309    0.165583    -0.170392   -0.124884     0.103013     -0.142958    -0.0729504    0.152951   -0.0851373   -0.0365449   -0.138862     0.0805211    0.0100108    0.0426316    0.0129487   -0.0226793  -0.00297959    0.129542      0.0596881    0.00155592   0.0132762    0.133782     0.238879
  0.0732469     0.0949996   -0.0718141   -0.0136564   -0.00826786   0.0855302   0.0140184    0.0723996    -0.188041    -0.10183     -0.0247664   0.155856    -0.0313484   -0.142954    -0.0191939   -0.168215     0.065556     0.0567745   -0.125095   -0.00648005    0.127672     -0.0172971    0.198184    -0.0935973    0.0656603    0.114416
 -0.0644993    -0.0470892   -0.0586693    0.00901831  -0.129271     0.0888269  -0.114524    -0.0689639     0.0340992   -0.043497    -0.08755    -0.239537     0.186239    -0.160619    -0.0626672   -0.0969545   -0.165394     0.0369547   -0.0509342  -0.0817617     0.135201     -0.0172519    0.206939    -0.0144496   -0.0831429    0.110268
 -0.0254678    -0.0242195   -0.00209392  -0.00916868  -0.0675217    0.0539721   0.184864     0.0988722     0.0364867    0.137659    -0.0324518   0.00219655   0.00379607   0.174682    -0.0446947   -0.0634783   -0.104503    -0.0517967   -0.122491    0.112412      0.153844      0.0539127    0.0230243    0.114181     0.00958734   0.105932
 -0.0217581     0.0692868   -0.0675086   -0.269956    -0.116889     0.136604    0.0178976   -0.0600827     0.0743358    0.0111762    0.0338888   0.19873     -0.0387867   -0.160709     0.0643376   -0.00777336   0.103981     0.103555    -0.0431433  -0.0534054    -0.127899      0.0692235    0.0100939    0.0552241    0.158416     0.00636265
 -0.147495     -0.0407918   -0.0492689   -0.0225768    0.00229205  -0.100133    0.0292774   -0.00470624   -0.068511    -0.0309212   -0.0891334   0.0747996   -0.102828    -0.114283     0.0631082   -0.106906     0.042046    -0.11165      0.0760727  -0.116634      0.092741     -0.116359    -0.0495617   -0.0610107   -0.0844747   -0.0324988
 -0.131279      0.0701323   -0.145701     0.145648    -0.0759096    0.0741674   0.150083     0.0184812     0.0591148    0.106465     0.0658458  -0.00588453   0.0260223   -0.0297664   -0.133949    -0.0141593   -0.0185034   -0.170972    -0.0917947   0.0892188     0.165955     -0.0184191    0.103101     0.0374664   -0.0282414    0.0846944
  0.0218921     0.22331      0.128595     0.183433    -0.16173      0.0269997   0.195755     0.115347      0.0153661    0.0158032   -0.055136   -0.0736948    0.0543459   -0.102131     0.0571682   -0.0636037   -0.120141     0.159544     0.105854    0.137245     -0.125564     -0.12991      0.114806     0.0221207    0.00130587   0.229173
 -0.106666     -0.0596244    0.161255     0.0951605   -0.0930013    0.14841     0.0184257    0.127497      0.202269     0.00266031   0.0561573   0.128641     0.281616    -0.0789008    0.0871121   -0.0667577    0.134732    -0.269736     0.0218685  -0.155525     -0.000579139  -0.100696     0.0728808   -0.0604603    0.0144051    0.0939627
  0.0559842    -0.165467     0.0222591    0.0503338   -0.00982395   0.0769523  -0.110382    -0.0604417     0.09477      0.00336305  -0.163252   -0.0997388    0.010976    -0.021144     0.0165228    0.130393    -0.0397152    0.137368     0.166757   -0.154039      0.109665     -0.0574424   -0.0636815   -0.138609     0.0318525    0.0291195
 -0.0126439    -0.0659876    0.0295853    0.165906    -0.0555367    0.140812   -0.00845234   0.051141      0.0244388   -0.044264     0.104468    0.101309     0.114755    -0.0620199   -0.0907737   -0.0276476   -0.086628     0.113076    -0.153336    0.174251      0.00483617   -0.0363404   -0.0334518   -0.00550179   0.0552385    0.0239735
  0.0788535    -0.00901012  -0.0345238    0.141276    -0.0181508   -0.142936   -0.110743    -0.0658581     0.0596698    0.0191426    0.113975    0.063859    -0.128014    -0.127006     0.0375109   -0.0437676   -0.000561    -0.152874    -0.151568    0.149004     -0.0605582     0.0718569   -0.0339758   -0.0568657    0.216222     0.0071576
 -0.166511      0.0895568   -0.0343884   -0.0279073   -0.0924344    0.241542   -0.114756     0.033011     -0.0293129   -0.0710972    0.242772    0.104755     0.097059    -0.0892374   -0.0769125   -0.231643     0.00953958   0.145349    -0.0308516   0.0171392    -0.0301527    -0.233286     0.0718532   -0.246066     0.0142251   -0.0664219kind full, method split
┌ Info: 0: avll = 
└   tll[1] = -1.436432248648739
[ Info: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.436452
[ Info: iteration 2, average log likelihood -1.436370
[ Info: iteration 3, average log likelihood -1.436294
[ Info: iteration 4, average log likelihood -1.436195
[ Info: iteration 5, average log likelihood -1.436070
[ Info: iteration 6, average log likelihood -1.435921
[ Info: iteration 7, average log likelihood -1.435759
[ Info: iteration 8, average log likelihood -1.435592
[ Info: iteration 9, average log likelihood -1.435404
[ Info: iteration 10, average log likelihood -1.435144
[ Info: iteration 11, average log likelihood -1.434738
[ Info: iteration 12, average log likelihood -1.434117
[ Info: iteration 13, average log likelihood -1.433296
[ Info: iteration 14, average log likelihood -1.432455
[ Info: iteration 15, average log likelihood -1.431810
[ Info: iteration 16, average log likelihood -1.431426
[ Info: iteration 17, average log likelihood -1.431233
[ Info: iteration 18, average log likelihood -1.431143
[ Info: iteration 19, average log likelihood -1.431102
[ Info: iteration 20, average log likelihood -1.431084
[ Info: iteration 21, average log likelihood -1.431076
[ Info: iteration 22, average log likelihood -1.431072
[ Info: iteration 23, average log likelihood -1.431070
[ Info: iteration 24, average log likelihood -1.431068
[ Info: iteration 25, average log likelihood -1.431068
[ Info: iteration 26, average log likelihood -1.431067
[ Info: iteration 27, average log likelihood -1.431067
[ Info: iteration 28, average log likelihood -1.431067
[ Info: iteration 29, average log likelihood -1.431067
[ Info: iteration 30, average log likelihood -1.431066
[ Info: iteration 31, average log likelihood -1.431066
[ Info: iteration 32, average log likelihood -1.431066
[ Info: iteration 33, average log likelihood -1.431066
[ Info: iteration 34, average log likelihood -1.431066
[ Info: iteration 35, average log likelihood -1.431066
[ Info: iteration 36, average log likelihood -1.431065
[ Info: iteration 37, average log likelihood -1.431065
[ Info: iteration 38, average log likelihood -1.431065
[ Info: iteration 39, average log likelihood -1.431065
[ Info: iteration 40, average log likelihood -1.431065
[ Info: iteration 41, average log likelihood -1.431065
[ Info: iteration 42, average log likelihood -1.431065
[ Info: iteration 43, average log likelihood -1.431065
[ Info: iteration 44, average log likelihood -1.431065
[ Info: iteration 45, average log likelihood -1.431065
[ Info: iteration 46, average log likelihood -1.431065
[ Info: iteration 47, average log likelihood -1.431065
[ Info: iteration 48, average log likelihood -1.431065
[ Info: iteration 49, average log likelihood -1.431065
[ Info: iteration 50, average log likelihood -1.431065
┌ Info: EM with 100000 data points 50 iterations avll -1.431065
└ 952.4 data points per parameter
┌ Info: 1
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4364521919622153
│     -1.4363696619113122
│      ⋮
└     -1.4310647450502343
[ Info: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.431081
[ Info: iteration 2, average log likelihood -1.430999
[ Info: iteration 3, average log likelihood -1.430919
[ Info: iteration 4, average log likelihood -1.430812
[ Info: iteration 5, average log likelihood -1.430673
[ Info: iteration 6, average log likelihood -1.430507
[ Info: iteration 7, average log likelihood -1.430333
[ Info: iteration 8, average log likelihood -1.430175
[ Info: iteration 9, average log likelihood -1.430045
[ Info: iteration 10, average log likelihood -1.429947
[ Info: iteration 11, average log likelihood -1.429875
[ Info: iteration 12, average log likelihood -1.429825
[ Info: iteration 13, average log likelihood -1.429791
[ Info: iteration 14, average log likelihood -1.429767
[ Info: iteration 15, average log likelihood -1.429749
[ Info: iteration 16, average log likelihood -1.429735
[ Info: iteration 17, average log likelihood -1.429724
[ Info: iteration 18, average log likelihood -1.429714
[ Info: iteration 19, average log likelihood -1.429704
[ Info: iteration 20, average log likelihood -1.429696
[ Info: iteration 21, average log likelihood -1.429688
[ Info: iteration 22, average log likelihood -1.429681
[ Info: iteration 23, average log likelihood -1.429673
[ Info: iteration 24, average log likelihood -1.429667
[ Info: iteration 25, average log likelihood -1.429660
[ Info: iteration 26, average log likelihood -1.429655
[ Info: iteration 27, average log likelihood -1.429649
[ Info: iteration 28, average log likelihood -1.429644
[ Info: iteration 29, average log likelihood -1.429639
[ Info: iteration 30, average log likelihood -1.429635
[ Info: iteration 31, average log likelihood -1.429630
[ Info: iteration 32, average log likelihood -1.429627
[ Info: iteration 33, average log likelihood -1.429623
[ Info: iteration 34, average log likelihood -1.429620
[ Info: iteration 35, average log likelihood -1.429617
[ Info: iteration 36, average log likelihood -1.429614
[ Info: iteration 37, average log likelihood -1.429611
[ Info: iteration 38, average log likelihood -1.429609
[ Info: iteration 39, average log likelihood -1.429607
[ Info: iteration 40, average log likelihood -1.429605
[ Info: iteration 41, average log likelihood -1.429603
[ Info: iteration 42, average log likelihood -1.429601
[ Info: iteration 43, average log likelihood -1.429599
[ Info: iteration 44, average log likelihood -1.429598
[ Info: iteration 45, average log likelihood -1.429597
[ Info: iteration 46, average log likelihood -1.429595
[ Info: iteration 47, average log likelihood -1.429594
[ Info: iteration 48, average log likelihood -1.429593
[ Info: iteration 49, average log likelihood -1.429592
[ Info: iteration 50, average log likelihood -1.429591
┌ Info: EM with 100000 data points 50 iterations avll -1.429591
└ 473.9 data points per parameter
┌ Info: 2
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4310807382375108
│     -1.4309994622715978
│      ⋮
└     -1.4295908498719814
[ Info: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.429608
[ Info: iteration 2, average log likelihood -1.429560
[ Info: iteration 3, average log likelihood -1.429529
[ Info: iteration 4, average log likelihood -1.429495
[ Info: iteration 5, average log likelihood -1.429455
[ Info: iteration 6, average log likelihood -1.429409
[ Info: iteration 7, average log likelihood -1.429358
[ Info: iteration 8, average log likelihood -1.429305
[ Info: iteration 9, average log likelihood -1.429252
[ Info: iteration 10, average log likelihood -1.429201
[ Info: iteration 11, average log likelihood -1.429155
[ Info: iteration 12, average log likelihood -1.429111
[ Info: iteration 13, average log likelihood -1.429069
[ Info: iteration 14, average log likelihood -1.429027
[ Info: iteration 15, average log likelihood -1.428984
[ Info: iteration 16, average log likelihood -1.428938
[ Info: iteration 17, average log likelihood -1.428890
[ Info: iteration 18, average log likelihood -1.428838
[ Info: iteration 19, average log likelihood -1.428784
[ Info: iteration 20, average log likelihood -1.428728
[ Info: iteration 21, average log likelihood -1.428672
[ Info: iteration 22, average log likelihood -1.428617
[ Info: iteration 23, average log likelihood -1.428565
[ Info: iteration 24, average log likelihood -1.428516
[ Info: iteration 25, average log likelihood -1.428473
[ Info: iteration 26, average log likelihood -1.428434
[ Info: iteration 27, average log likelihood -1.428401
[ Info: iteration 28, average log likelihood -1.428372
[ Info: iteration 29, average log likelihood -1.428346
[ Info: iteration 30, average log likelihood -1.428324
[ Info: iteration 31, average log likelihood -1.428304
[ Info: iteration 32, average log likelihood -1.428285
[ Info: iteration 33, average log likelihood -1.428268
[ Info: iteration 34, average log likelihood -1.428253
[ Info: iteration 35, average log likelihood -1.428238
[ Info: iteration 36, average log likelihood -1.428223
[ Info: iteration 37, average log likelihood -1.428209
[ Info: iteration 38, average log likelihood -1.428196
[ Info: iteration 39, average log likelihood -1.428183
[ Info: iteration 40, average log likelihood -1.428171
[ Info: iteration 41, average log likelihood -1.428159
[ Info: iteration 42, average log likelihood -1.428148
[ Info: iteration 43, average log likelihood -1.428137
[ Info: iteration 44, average log likelihood -1.428127
[ Info: iteration 45, average log likelihood -1.428117
[ Info: iteration 46, average log likelihood -1.428108
[ Info: iteration 47, average log likelihood -1.428100
[ Info: iteration 48, average log likelihood -1.428092
[ Info: iteration 49, average log likelihood -1.428084
[ Info: iteration 50, average log likelihood -1.428077
┌ Info: EM with 100000 data points 50 iterations avll -1.428077
└ 236.4 data points per parameter
┌ Info: 3
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4296082332665638
│     -1.4295599510649533
│      ⋮
└     -1.4280771254594766
[ Info: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.428081
[ Info: iteration 2, average log likelihood -1.428021
[ Info: iteration 3, average log likelihood -1.427969
[ Info: iteration 4, average log likelihood -1.427911
[ Info: iteration 5, average log likelihood -1.427841
[ Info: iteration 6, average log likelihood -1.427758
[ Info: iteration 7, average log likelihood -1.427662
[ Info: iteration 8, average log likelihood -1.427556
[ Info: iteration 9, average log likelihood -1.427447
[ Info: iteration 10, average log likelihood -1.427339
[ Info: iteration 11, average log likelihood -1.427236
[ Info: iteration 12, average log likelihood -1.427141
[ Info: iteration 13, average log likelihood -1.427054
[ Info: iteration 14, average log likelihood -1.426976
[ Info: iteration 15, average log likelihood -1.426906
[ Info: iteration 16, average log likelihood -1.426845
[ Info: iteration 17, average log likelihood -1.426790
[ Info: iteration 18, average log likelihood -1.426741
[ Info: iteration 19, average log likelihood -1.426697
[ Info: iteration 20, average log likelihood -1.426657
[ Info: iteration 21, average log likelihood -1.426621
[ Info: iteration 22, average log likelihood -1.426588
[ Info: iteration 23, average log likelihood -1.426557
[ Info: iteration 24, average log likelihood -1.426528
[ Info: iteration 25, average log likelihood -1.426501
[ Info: iteration 26, average log likelihood -1.426476
[ Info: iteration 27, average log likelihood -1.426453
[ Info: iteration 28, average log likelihood -1.426431
[ Info: iteration 29, average log likelihood -1.426412
[ Info: iteration 30, average log likelihood -1.426393
[ Info: iteration 31, average log likelihood -1.426376
[ Info: iteration 32, average log likelihood -1.426361
[ Info: iteration 33, average log likelihood -1.426346
[ Info: iteration 34, average log likelihood -1.426333
[ Info: iteration 35, average log likelihood -1.426321
[ Info: iteration 36, average log likelihood -1.426309
[ Info: iteration 37, average log likelihood -1.426299
[ Info: iteration 38, average log likelihood -1.426289
[ Info: iteration 39, average log likelihood -1.426280
[ Info: iteration 40, average log likelihood -1.426272
[ Info: iteration 41, average log likelihood -1.426264
[ Info: iteration 42, average log likelihood -1.426257
[ Info: iteration 43, average log likelihood -1.426250
[ Info: iteration 44, average log likelihood -1.426243
[ Info: iteration 45, average log likelihood -1.426237
[ Info: iteration 46, average log likelihood -1.426231
[ Info: iteration 47, average log likelihood -1.426225
[ Info: iteration 48, average log likelihood -1.426220
[ Info: iteration 49, average log likelihood -1.426215
[ Info: iteration 50, average log likelihood -1.426209
┌ Info: EM with 100000 data points 50 iterations avll -1.426209
└ 118.1 data points per parameter
┌ Info: 4
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4280812253368218
│     -1.4280214230238542
│      ⋮
└     -1.4262093215384086
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.426213
[ Info: iteration 2, average log likelihood -1.426156
[ Info: iteration 3, average log likelihood -1.426104
[ Info: iteration 4, average log likelihood -1.426046
[ Info: iteration 5, average log likelihood -1.425974
[ Info: iteration 6, average log likelihood -1.425885
[ Info: iteration 7, average log likelihood -1.425779
[ Info: iteration 8, average log likelihood -1.425657
[ Info: iteration 9, average log likelihood -1.425525
[ Info: iteration 10, average log likelihood -1.425388
[ Info: iteration 11, average log likelihood -1.425254
[ Info: iteration 12, average log likelihood -1.425127
[ Info: iteration 13, average log likelihood -1.425010
[ Info: iteration 14, average log likelihood -1.424903
[ Info: iteration 15, average log likelihood -1.424808
[ Info: iteration 16, average log likelihood -1.424723
[ Info: iteration 17, average log likelihood -1.424649
[ Info: iteration 18, average log likelihood -1.424582
[ Info: iteration 19, average log likelihood -1.424523
[ Info: iteration 20, average log likelihood -1.424470
[ Info: iteration 21, average log likelihood -1.424423
[ Info: iteration 22, average log likelihood -1.424379
[ Info: iteration 23, average log likelihood -1.424338
[ Info: iteration 24, average log likelihood -1.424300
[ Info: iteration 25, average log likelihood -1.424265
[ Info: iteration 26, average log likelihood -1.424231
[ Info: iteration 27, average log likelihood -1.424199
[ Info: iteration 28, average log likelihood -1.424168
[ Info: iteration 29, average log likelihood -1.424139
[ Info: iteration 30, average log likelihood -1.424111
[ Info: iteration 31, average log likelihood -1.424083
[ Info: iteration 32, average log likelihood -1.424057
[ Info: iteration 33, average log likelihood -1.424032
[ Info: iteration 34, average log likelihood -1.424007
[ Info: iteration 35, average log likelihood -1.423983
[ Info: iteration 36, average log likelihood -1.423960
[ Info: iteration 37, average log likelihood -1.423938
[ Info: iteration 38, average log likelihood -1.423916
[ Info: iteration 39, average log likelihood -1.423896
[ Info: iteration 40, average log likelihood -1.423876
[ Info: iteration 41, average log likelihood -1.423857
[ Info: iteration 42, average log likelihood -1.423838
[ Info: iteration 43, average log likelihood -1.423821
[ Info: iteration 44, average log likelihood -1.423804
[ Info: iteration 45, average log likelihood -1.423788
[ Info: iteration 46, average log likelihood -1.423772
[ Info: iteration 47, average log likelihood -1.423757
[ Info: iteration 48, average log likelihood -1.423743
[ Info: iteration 49, average log likelihood -1.423729
[ Info: iteration 50, average log likelihood -1.423716
┌ Info: EM with 100000 data points 50 iterations avll -1.423716
└ 59.0 data points per parameter
┌ Info: 5
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4262125635214524
│     -1.4261561708200394
│      ⋮
└     -1.4237157947201582
┌ Info: Total log likelihood: 
│   tll =
│    251-element Array{Float64,1}:
│     -1.436432248648739
│     -1.4364521919622153
│     -1.4363696619113122
│     -1.4362937443740447
│      ⋮
│     -1.4237429562060575
│     -1.423729124697426
└     -1.4237157947201582
32×26 Array{Float64,2}:
 -0.749296     0.640798    0.269217    0.268739     0.100975   -0.200551     0.787478     0.159692      0.167996   -0.0583545  -0.0801655   0.198987    -0.00370929   0.680737   -0.492238    -0.34475     0.367589     0.839624    -0.142334   -0.469929    0.392121    -0.280295    0.502226   -0.421649    -0.374921   -0.403073
 -0.128993    -0.0304324   0.0357092  -0.0167262   -0.197444    0.331915     1.06505      0.209813      0.260228   -0.0387439   0.402905   -0.0264093    0.0472083   -0.0539163   0.258838    -0.0929343   0.0828368    0.764164     0.66409    -0.317295   -0.514532    -0.201322   -0.089999   -0.00593495  -0.212317   -0.22556
 -0.232639     0.331326    0.385043    0.00881123   0.151662    0.00178534   0.0198728   -0.15095      -0.572989   -0.169815    0.242283    0.073078     0.201247    -0.543504   -0.115565    -0.257992    0.00680446  -0.0370565    0.0989312  -0.201994   -0.00679082  -0.163636    0.299474    0.109692     0.196629    0.0565037
  6.88761e-5   0.211271   -0.408279    0.00790174   0.319172    0.109486     0.800841     0.20856      -0.0795401   0.337963    0.583586    0.258733     0.195622     1.09235    -0.501803     0.0956836  -0.120945     0.209624     0.600655    0.271954   -0.0190742   -0.0259654  -0.0130586  -0.0652619   -0.216919    0.298148
  0.178222    -0.241688    0.183755    0.154842     0.325541   -0.143427     0.311932     0.000318294   0.331381   -0.481715   -0.125351    0.361901    -0.461022    -0.405113   -0.820792     0.641102    0.366636     0.158473     0.205347   -0.335462    0.43587     -0.339357    0.0669026  -0.180487    -0.590697    0.472922
  0.422013     0.352995   -0.110915    0.0704138   -0.0571368   0.31943      0.550162    -0.0143944     0.367125   -0.0840333  -0.37545     0.131452     0.17819      0.697254   -0.220473     0.855754    0.39768     -0.0896703    0.298221   -0.204609    0.245683     0.342445   -0.190486    0.183898     0.401409   -0.0186361
  0.604039     0.0381981  -0.167766   -0.245821     0.650269    0.188236     0.231479     0.250913      0.650168    0.284871   -0.360711   -0.0529094   -0.0151958    0.0794923  -0.0524414    0.473942   -0.42101      0.118389    -0.017089    0.644232   -0.35934     -0.180907    0.501245    0.133868    -0.0634136   0.611091
  0.0457107    0.198899   -0.362513   -0.506314    -0.0743136   0.0951202   -0.244283     0.0372066     0.215126    0.411236   -0.0545091   0.0770249   -0.477909    -0.164472   -0.211887     0.242853   -0.205477    -0.0346007    0.243184    0.492505    0.472836    -0.0297701  -0.0927178  -0.276477    -0.922176    1.04355
 -0.0894888   -0.40081     0.120168    1.23985      0.0488788  -0.0645002   -0.374404    -0.214156     -0.0326154  -0.333641    0.0832117  -0.212883     0.0826734   -0.439086    0.208519    -0.44584     0.288602    -0.033105    -0.308262   -0.959486    0.0279182   -0.152378   -0.677679    0.186494     0.0849552  -0.657825
 -0.0261287    0.0216611   0.478103    0.237188    -0.187574    0.273049    -0.880016    -0.123739      0.298893   -0.177259   -0.0297155   0.0659656   -0.0778507    0.234312    0.303979    -0.155071   -0.279538    -0.318966    -0.0699048   0.0965774  -0.128615     0.732822   -0.378909    0.0727765    0.527369   -0.0180291
 -0.238789     0.0351149  -0.318912    0.148805    -0.129881   -0.632408    -0.192659     0.257722     -0.242563   -0.440313    0.0477397  -0.652833     0.155846    -0.167079   -0.437873    -0.215437   -0.402319    -0.355203    -0.476427    0.777904    0.409663     0.137236   -0.149414    0.357628    -0.551905   -0.088543
  0.105658     0.0275964  -0.0333697   0.00957925   0.295167   -0.593163    -0.451542    -0.0661324    -0.476313    0.011979   -0.290178    0.0963962    0.428402    -0.0292078  -0.2957      -0.0626981  -0.147039    -0.602329    -0.974274    0.387983    0.69878      0.283186   -0.28646    -0.274325     0.396239    0.239817
 -0.14818     -0.778547   -0.244543   -0.270331     0.10728    -0.39596     -0.0303175    0.0843219     0.50903    -0.0258209  -0.54668     0.226805     0.312529    -0.327522    0.247355    -0.0360992   0.0970254   -0.230898    -0.815257    0.164693   -0.595813    -0.167108    0.256903   -0.732935    -0.367511    0.0768529
  0.0225832   -0.986797   -0.869297    0.0544462   -0.278247   -0.283684    -0.275916    -0.0626484    -0.530433    0.0147621  -0.235004    0.262855    -0.05914     -0.0916531   0.42649      0.115212   -0.116309    -0.427362    -0.199576   -0.258154   -0.199619    -0.207596    0.249649   -0.0300104    0.353424   -0.0809173
 -0.506663    -0.808664   -0.235374    0.378536    -0.136585    0.622761    -0.31774      0.299247      0.661907    0.279272   -0.431541   -1.32031     -0.277811    -0.749253    0.817197     0.443177   -0.149942     0.474756    -0.605334    0.316291    0.549622    -0.685091   -0.579365    1.17724      0.408298    0.410612
  0.216887     0.02856    -0.253933   -0.0363649   -0.483212    0.579619    -0.215592    -0.0300254    -0.0398836   0.165423    0.064255   -0.553252    -1.13991     -0.548304    0.327856    -0.0070656   0.118981     0.561114     0.396461    0.220876    0.779213    -0.507442   -0.121279    0.235044     0.173107   -0.455335
 -0.137752    -0.059754   -0.143049   -0.367717    -0.524876    0.122523    -0.0333974   -0.146005      0.0401543   0.530568   -0.0600783  -0.221846    -0.0103637    0.518819   -0.0243943    0.234199   -0.493759    -0.0349449    0.385208   -0.0316923   0.398032    -0.0882098   0.152017    0.0383979   -0.534011   -0.117781
 -0.123803    -0.18621    -0.668014    0.36247     -0.283504   -0.0766909   -0.0293888    0.126255      0.130128    0.319972   -0.123652    0.130416    -0.474109     0.43754    -0.0172265    0.0214881   0.162766     0.047096     0.319074    0.0854534   0.313514    -0.0786915  -0.368489    0.302854    -0.190259    0.278186
 -0.0511132    0.147556    0.0653804   0.0447601    0.140213   -0.0212822    0.254956     0.00986528    0.121539   -0.196738    0.0706901  -0.119033    -0.0529725    0.0601607  -0.235553     0.0820355  -0.176423     0.122695     0.0248617   0.0900552   0.0531948    0.0113746   0.0542593   0.0672462   -0.160008   -0.0909302
  0.0290043   -0.132109   -0.063258   -0.0495441   -0.101843    0.0790584   -0.157612    -0.0431394    -0.105582    0.0812632  -0.114639    0.10892      0.0634172   -0.126996    0.117647    -0.0257747   0.137845    -0.146326    -0.0240509  -0.0994174  -0.0562539   -0.0787393  -0.0254589  -0.0674259    0.107246    0.16213
  0.328333    -0.469077    0.241359    0.224838    -0.506164    0.0485577    0.0406      -0.0276622     0.728308   -0.189615   -1.16585    -0.493537    -0.100798    -0.18101     0.512641     0.0847813  -0.230107     0.0114219   -0.369949   -0.126182   -0.155174     0.0627511  -0.100161   -0.061438     0.198286   -0.476797
  0.0234897   -0.196858    0.383886    0.392963     0.209573   -0.249454    -0.42938      0.210154      0.329965    0.054118    0.165409   -0.359597    -0.24104     -0.548949    0.390527    -0.0393028  -0.493999     0.00900716  -0.548739    0.0605176   0.0653872   -0.0590253  -0.205621    0.0706535    0.181971   -0.197455
 -0.138382     0.718424    1.00033     0.0950774    0.394783    0.480501     0.035691    -0.457876      0.379082    0.0811298   0.482811   -0.923143    -0.0673109   -0.298095   -0.118806     0.152167   -0.364226     0.107609    -0.123508   -0.0613804   0.0552573    0.0105348  -0.0448173  -0.0692854   -0.400633   -0.0767114
  0.278715     0.90791     0.496848    0.326883     0.127527    0.385118     0.127429     0.167962      0.0357403   0.0384517   0.529513    0.217548    -0.0772974    0.0026564  -0.176056    -0.434364   -0.118962     0.0944802    0.390438   -0.179997    0.180843    -0.0872949  -0.127323    0.439241    -0.0945038   0.444878
 -0.236556     0.0316752   0.297099   -0.934571    -0.204698   -0.0418096   -0.38542      0.120489     -0.0914631  -0.376399    0.0821205   0.329934     0.386527    -0.864622    0.262528    -0.0552659   0.141702    -0.357019    -0.29577     0.0835451  -0.254487     0.0688335   0.262996   -0.17706     -0.0682846   0.23355
 -0.511255     0.238036    0.101525    0.348252    -0.0905331   0.302411    -0.357108    -0.101763     -0.434543   -0.764745   -0.184061   -0.00894087  -0.177672    -0.615287    0.00479472  -0.369484    0.517817    -0.170346    -0.125681    0.0417564  -0.351219     0.139346    0.375626    0.187207    -0.149316    0.140498
  0.103525    -0.194247   -0.0270064   0.136003     0.44793     0.166522    -0.00324375  -0.173565     -0.628781    0.29377    -0.377628   -0.566113     0.641021    -0.355292   -0.352831    -0.0608103   0.0374747   -0.0580482   -0.0129981  -0.417107   -0.331867    -0.522525    0.572629   -0.0576113   -0.248317    0.314664
 -0.0567551   -0.103881   -0.521822   -0.258453     0.0536377   0.11948      0.103415    -0.136936     -0.0843629   0.35249     0.157137    0.129207     0.029715    -0.739267   -0.278361    -0.402191   -0.0363993    0.647887    -0.339621    0.433731    0.185564    -0.401671    0.460881   -0.562104    -0.326293   -0.0547756
 -0.0617218    0.0589006   0.0599894   0.0581712   -0.235793   -0.0678537    0.13548     -0.0413673    -0.0979932  -0.508667    0.33522    -0.0371935   -0.125867     0.28497     0.145401    -0.150108    0.0769819   -0.273811     0.100789   -0.0124099   0.065287     0.345528   -0.492642    0.0898788    0.817592   -0.488988
  0.313821     0.300524    0.220203   -0.15433     -0.360714    0.440215    -0.106302    -0.00233519   -0.284178    0.39787     0.208043    0.246836     0.198993     0.084297    0.729777    -0.463218    0.0789198    0.191603    -0.112993   -0.104415   -0.291946    -0.0144471   0.08834    -0.0308127    0.843831   -0.0711594
 -0.200069    -0.156927    0.296014    0.0669159    0.486569   -0.345877     0.0250627    0.0434218     0.0222164  -0.192539    0.0726421   0.723951     0.757847     0.240775   -0.221389     0.100405    0.155145    -0.198322     0.0158344  -0.500844   -0.394654     0.288406    0.020995   -0.13172      0.268925    0.343134
  0.261972     0.596298   -0.047287   -0.708152     0.287857   -0.0427187    0.290612    -0.266075     -0.622407   -0.17144     0.379956    0.83481      0.206235     0.449348   -0.620332    -0.379957    0.585176    -0.278274     0.555748    0.0572045  -0.561059     0.238013    0.546333   -0.496852    -0.172965   -0.0400398[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.423703
[ Info: iteration 2, average log likelihood -1.423691
[ Info: iteration 3, average log likelihood -1.423679
[ Info: iteration 4, average log likelihood -1.423667
[ Info: iteration 5, average log likelihood -1.423656
[ Info: iteration 6, average log likelihood -1.423645
[ Info: iteration 7, average log likelihood -1.423634
[ Info: iteration 8, average log likelihood -1.423624
[ Info: iteration 9, average log likelihood -1.423614
[ Info: iteration 10, average log likelihood -1.423605
┌ Info: EM with 100000 data points 10 iterations avll -1.423605
└ 59.0 data points per parameter
[ Info: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
kind full, method kmeans
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.473734e+05
      1       7.278383e+05      -2.195351e+05 |       32
      2       7.120291e+05      -1.580918e+04 |       32
      3       7.062102e+05      -5.818935e+03 |       32
      4       7.032266e+05      -2.983554e+03 |       32
      5       7.014281e+05      -1.798496e+03 |       32
      6       7.001790e+05      -1.249062e+03 |       32
      7       6.992768e+05      -9.022108e+02 |       32
      8       6.985758e+05      -7.010350e+02 |       32
      9       6.979742e+05      -6.015553e+02 |       32
     10       6.974197e+05      -5.545153e+02 |       32
     11       6.969571e+05      -4.626140e+02 |       32
     12       6.965603e+05      -3.968163e+02 |       32
     13       6.962296e+05      -3.307269e+02 |       32
     14       6.959454e+05      -2.841694e+02 |       32
     15       6.957270e+05      -2.183766e+02 |       32
     16       6.955319e+05      -1.951268e+02 |       32
     17       6.953540e+05      -1.779145e+02 |       32
     18       6.951864e+05      -1.675860e+02 |       32
     19       6.950191e+05      -1.672760e+02 |       32
     20       6.948582e+05      -1.609158e+02 |       32
     21       6.947159e+05      -1.423108e+02 |       32
     22       6.945823e+05      -1.335717e+02 |       32
     23       6.944477e+05      -1.345900e+02 |       32
     24       6.943080e+05      -1.397591e+02 |       32
     25       6.941737e+05      -1.342811e+02 |       32
     26       6.940487e+05      -1.249772e+02 |       32
     27       6.939227e+05      -1.260345e+02 |       32
     28       6.938180e+05      -1.046846e+02 |       32
     29       6.937286e+05      -8.941950e+01 |       32
     30       6.936345e+05      -9.410644e+01 |       32
     31       6.935430e+05      -9.150927e+01 |       32
     32       6.934687e+05      -7.427274e+01 |       32
     33       6.934012e+05      -6.744504e+01 |       32
     34       6.933384e+05      -6.281493e+01 |       32
     35       6.932749e+05      -6.355084e+01 |       32
     36       6.932150e+05      -5.986627e+01 |       32
     37       6.931546e+05      -6.044832e+01 |       32
     38       6.931021e+05      -5.248569e+01 |       32
     39       6.930562e+05      -4.587949e+01 |       32
     40       6.930111e+05      -4.510435e+01 |       32
     41       6.929658e+05      -4.533013e+01 |       32
     42       6.929229e+05      -4.289977e+01 |       32
     43       6.928817e+05      -4.121467e+01 |       32
     44       6.928403e+05      -4.135371e+01 |       32
     45       6.927941e+05      -4.618938e+01 |       32
     46       6.927573e+05      -3.684624e+01 |       32
     47       6.927158e+05      -4.145871e+01 |       32
     48       6.926739e+05      -4.186081e+01 |       32
     49       6.926270e+05      -4.693691e+01 |       32
     50       6.925853e+05      -4.171357e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 692585.294791898)
┌ Info: K-means with 32000 data points using 50 iterations
└ 37.0 data points per parameter
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.435497
[ Info: iteration 2, average log likelihood -1.430425
[ Info: iteration 3, average log likelihood -1.428970
[ Info: iteration 4, average log likelihood -1.427834
[ Info: iteration 5, average log likelihood -1.426691
[ Info: iteration 6, average log likelihood -1.425753
[ Info: iteration 7, average log likelihood -1.425181
[ Info: iteration 8, average log likelihood -1.424879
[ Info: iteration 9, average log likelihood -1.424705
[ Info: iteration 10, average log likelihood -1.424587
[ Info: iteration 11, average log likelihood -1.424494
[ Info: iteration 12, average log likelihood -1.424414
[ Info: iteration 13, average log likelihood -1.424344
[ Info: iteration 14, average log likelihood -1.424281
[ Info: iteration 15, average log likelihood -1.424222
[ Info: iteration 16, average log likelihood -1.424167
[ Info: iteration 17, average log likelihood -1.424116
[ Info: iteration 18, average log likelihood -1.424069
[ Info: iteration 19, average log likelihood -1.424024
[ Info: iteration 20, average log likelihood -1.423981
[ Info: iteration 21, average log likelihood -1.423942
[ Info: iteration 22, average log likelihood -1.423904
[ Info: iteration 23, average log likelihood -1.423869
[ Info: iteration 24, average log likelihood -1.423836
[ Info: iteration 25, average log likelihood -1.423804
[ Info: iteration 26, average log likelihood -1.423774
[ Info: iteration 27, average log likelihood -1.423746
[ Info: iteration 28, average log likelihood -1.423720
[ Info: iteration 29, average log likelihood -1.423694
[ Info: iteration 30, average log likelihood -1.423670
[ Info: iteration 31, average log likelihood -1.423647
[ Info: iteration 32, average log likelihood -1.423625
[ Info: iteration 33, average log likelihood -1.423605
[ Info: iteration 34, average log likelihood -1.423585
[ Info: iteration 35, average log likelihood -1.423567
[ Info: iteration 36, average log likelihood -1.423549
[ Info: iteration 37, average log likelihood -1.423533
[ Info: iteration 38, average log likelihood -1.423517
[ Info: iteration 39, average log likelihood -1.423502
[ Info: iteration 40, average log likelihood -1.423488
[ Info: iteration 41, average log likelihood -1.423475
[ Info: iteration 42, average log likelihood -1.423462
[ Info: iteration 43, average log likelihood -1.423450
[ Info: iteration 44, average log likelihood -1.423439
[ Info: iteration 45, average log likelihood -1.423428
[ Info: iteration 46, average log likelihood -1.423418
[ Info: iteration 47, average log likelihood -1.423408
[ Info: iteration 48, average log likelihood -1.423399
[ Info: iteration 49, average log likelihood -1.423390
[ Info: iteration 50, average log likelihood -1.423381
┌ Info: EM with 100000 data points 50 iterations avll -1.423381
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.108997   -0.13864     -0.237998    -0.0324458   -0.00204022  -0.304001    -0.0022363    0.0422965  -0.278215   -0.263512     0.199429    -0.6319       -0.179772    -0.449673     0.0286976   -0.654887     -0.38324      0.0466713   -0.481141    0.88389       0.163263    -0.400819    -0.0996956   -0.203122    -0.228716    -0.527414
 -0.071453    0.172493    -0.443408    -0.270768     0.141347     0.0913417   -0.168063    -0.0310255   0.0886939   0.477981     0.128413     0.249601     -0.531313    -0.350404    -0.39799      0.0350637    -0.0735908    0.269672     0.350711    0.304912      0.442748    -0.537285     0.189374    -0.345153    -0.913527     0.913979
  0.404798    0.251214     0.159768     0.195855    -0.077663     0.500701     0.315233     0.0783781   0.0479973   0.232977     0.239768     0.409212     -0.0407555   -0.145221     0.388818    -0.486117      0.151484     0.728829    -0.0164647  -0.403826     -0.298374    -0.261555     0.259674    -0.0157308    0.230098    -0.0915731
 -0.500287   -0.797785    -0.244701     0.486483    -0.0800806    0.400027    -0.384941     0.260549    0.55728     0.276356    -0.359441    -1.12985      -0.31886     -0.803814     0.743899     0.348832     -0.201605     0.509426    -0.564231    0.197827      0.506259    -0.631053    -0.521633     0.9033       0.2863       0.316818
  0.102618   -0.00705546  -0.188292     0.137287     0.0684021   -0.798475    -0.804239     0.0145949  -0.529294    0.0851375   -0.299139    -0.0378042     0.412956     0.086695    -0.357866    -0.151147     -0.294591    -0.574479    -0.876123    0.565379      0.83721      0.712118    -0.276115    -0.108746     0.0783361    0.285065
 -0.21577    -0.0785767    0.285641     0.155236    -0.0636231   -0.0846059   -0.313671    -0.0441405  -0.273871   -0.194237     0.0505364    0.0273046     0.199233    -0.434545     0.224645    -0.439258      0.0453385   -0.276094    -0.29227    -0.193108     -0.245114    -0.051223     0.172445     0.0667538    0.378923    -0.345379
 -0.41397    -0.224613    -0.255058    -0.418111    -0.373508    -0.170497    -0.329891     0.203223   -0.0796853  -0.119944    -0.320088     0.440842      0.108834    -0.668554     0.238346    -0.136516      0.362572    -0.0257286   -0.447681    0.0617348    -0.134158    -0.0258586    0.492218    -0.275064    -0.271657     0.16762
  0.0527239   0.080889    -0.294211     0.118899     0.400124     0.177644     0.147718    -0.181196   -0.999015   -0.710878    -0.0931313   -0.00774417   -0.314053    -0.314699    -0.05902     -0.0475982     0.752401    -0.488297    -0.299771   -0.0104866     0.180828    -0.276217     0.275423     0.195592     0.78776      0.216261
  0.452727    0.717614    -0.00791756  -0.882662     0.195723    -0.231777     0.240784    -0.359256   -1.00255    -0.185497     0.548759     1.09012       0.344183     0.336052    -0.57911     -0.518094      0.581217    -0.494595     0.591061    0.0763847    -0.617528     0.31736      0.579068    -0.742594    -0.271255    -0.178364
  0.386039   -0.095831    -0.451548    -0.177376     0.104368     0.181124     0.266861     0.169396    0.355509    0.174838    -0.307031    -0.356297     -0.193235     0.109444    -0.089936     0.674231     -0.231355    -0.0277454    0.168745    0.370379      0.00859638  -0.177924     0.20157      0.225875    -0.430141     0.400652
  0.322692   -0.532331     0.186915     0.0986384   -0.644224    -0.00435381  -0.114186    -0.136022    0.688252   -0.213167    -1.22755     -0.431691     -0.140692    -0.256084     0.780312     0.07269      -0.145147    -0.08215     -0.406746   -0.234952     -0.166868     0.128976    -0.229559    -0.104501     0.404653    -0.488481
  0.0643918   0.0657639    0.175654     0.923887     0.155462    -0.0513863   -0.418324    -0.392335    0.527954   -0.272387     0.0779187    0.2406       -0.45302      0.149742    -0.0715517   -0.000298333   0.113153     0.144435     0.0564522  -0.412245      0.404632     0.471383    -0.948152    -0.200057     0.00640444  -0.0371526
 -0.477094    0.346245     0.210082    -0.107878     0.0975511    0.0476611    1.18616      0.128918    0.366799   -0.198468     0.138499     0.406853     -0.0810046    0.331314    -0.358919    -0.0199315     0.511327     0.727992     0.33139    -0.445275     -0.0677729   -0.476873     0.500383    -0.321193    -0.499917    -0.335533
  0.0638975   0.191828    -0.201364    -0.00091625  -0.742279     0.571988    -0.411322     0.06907    -0.138529    0.2543       0.347866    -0.279105     -1.01936     -0.558826     0.254901     0.00804794    0.215308     0.744462     0.391633    0.0535436     0.87408     -0.404201    -0.0604201    0.407084     0.281322    -0.575177
  0.0492493  -0.075727     0.0935838    0.0853778    0.377398     0.178539     0.0170892   -0.041429   -0.554832    0.164798    -0.17417     -0.429142      0.674604    -0.323401    -0.377601    -0.170734      0.00366492   6.86829e-5   0.0160221  -0.414899     -0.504024    -0.312514     0.621861    -0.00198033  -0.259184     0.324283
 -0.0351595   0.615302     0.840936     0.194212     0.236031     0.305516    -0.0795391   -0.0765798   0.617363   -0.0138158    0.240371    -0.713171     -0.199935    -0.320613     0.00319041  -0.0896639    -0.339267     0.158711    -0.213049    0.0167433     0.0417655    0.0100116    0.0400923    0.0331102   -0.265899    -0.0839539
 -0.250267   -0.277741     0.128572     0.372163    -0.283368     0.0786192   -0.60681     -0.445267   -0.385851   -0.104448     0.0219408   -0.112723      0.032884    -0.082951     0.414029    -0.352977      0.164865    -0.511158    -0.239653   -0.420683     -0.0433895   -0.0835356   -0.221109    -0.0618425    0.37852     -0.276943
  0.198008    0.230393    -0.0242268   -0.0242205   -0.0446625    0.358102     0.822535     0.121987    0.297403    0.199632     0.449215    -0.344797     -0.0183914    0.283303     0.0790666   -0.111024      0.0835275    0.344427     0.792814    0.152773     -0.129833     0.0595995   -0.957742     0.172678     0.124617     0.085912
 -0.373227   -0.0317992   -0.00774982   0.366864     0.134641    -0.412282     0.2314       0.256788    0.268275   -0.503297     0.00253418  -0.134685     -0.0364714   -0.230954    -0.612555     0.202401     -0.00763178  -0.191443    -0.12415     0.0257222     0.125981    -0.0263808   -0.161638     0.437118    -0.635782     0.188786
  0.209178    0.399676     0.115274    -0.234146     0.101252     0.133615     0.212749     0.215296    0.444447   -0.067564    -0.132318     0.605772      0.128779     0.777394    -0.43231      0.439814      0.298053    -0.166508     0.417581   -0.0496602    -0.00477898   0.521147     0.207142     0.120936     0.099019     0.413714
 -0.239083   -0.243545     0.137427    -0.371988     0.650509    -0.235616    -0.0619021   -0.057878    0.261723   -0.0478514   -0.30606      0.559877      0.96228     -0.061888    -0.0576668   -0.164385     -0.113949    -0.204448    -0.57952     0.0686031    -0.621439     0.0810677    0.0131036   -0.814291    -0.162782     0.57567
  0.0905191   0.36461      0.73586     -0.0155614    0.416074     0.510809     0.224384    -1.00682    -0.621867    0.0283055    0.146238    -0.12491       0.447866    -0.198923    -0.298488     1.05252      -0.103954     0.138229     0.0795139  -0.325979      0.516916     0.0407412   -0.00501558   0.136184    -0.304466     0.104884
 -0.238456    0.0599389    0.475867    -0.0378263    0.311773    -0.18125     -0.119909    -0.0752345   0.0103148  -0.477703     0.262357     0.440877      0.347836    -0.183508     0.0522518   -0.0228888     0.184637    -0.172734    -0.0595332  -0.35442      -0.405944     0.390678    -0.106748    -0.131301     0.465034    -0.087763
  0.101742   -0.100516    -0.323373    -0.0214951   -0.531875     0.191668     0.0679057   -0.0634575   0.259048    0.486992    -0.332988    -0.404599     -0.197785     0.531       -0.0580013    0.282426     -0.333706     0.0601988    0.31923     0.118162      0.400331    -0.264495     0.0773234    0.270211    -0.537707    -0.161403
  0.482705   -0.173394     0.0832497   -0.158288    -0.212406     0.207488    -0.81521      0.166267    0.0724144  -0.108053     0.143282     0.167656      0.00301192  -0.409198     0.409521     0.0631536    -0.327213    -0.695157     0.100953    0.255965     -0.245804     0.163794    -0.384843     0.334064     0.318794     0.670337
 -0.705043    0.782248     0.239537     0.157001    -0.0634822    0.115755     0.0369255    0.195546   -0.753403   -0.132915     0.661119     0.0430973     0.150305     0.13439     -0.294819    -0.753208     -0.171565     0.299385     0.592592   -0.00541134    0.237445     0.253774     0.201771     0.274441    -0.146336     0.433578
  0.33131     0.296239     0.241169    -0.845821    -0.244049    -0.191119    -5.26389e-5   0.130872    0.0442941   0.751214     0.368585     0.202948      0.0121043    0.41644      0.36179      0.018684     -0.480638    -0.182687    -0.215319    1.26573      -0.340005    -0.165786     1.14461      0.0224731    1.02586      0.745421
 -0.0335097   0.0350037   -0.0663361   -0.0148503   -0.0450872    0.024818     0.0364957   -0.0241191  -0.0449132  -0.00153923  -0.0093625    0.000242949  -0.0378662    0.00738494  -0.0491743   -0.0136955    -0.0251541   -0.00822769   0.046774    0.0394153     0.0722561   -0.00742065  -0.0485739    0.00637415   0.00797215   0.0629167
 -0.670022   -0.712708    -0.856336    -0.337177    -0.333829     0.0517477    0.279293    -0.341534   -0.559355    0.275682     0.225331    -0.171205     -0.273539     0.549505    -0.0616848    0.172566     -0.282053     0.135235     0.0743698  -0.167931     -0.203851     0.440336     0.55437     -1.06592     -0.246187    -0.29055
 -0.002634    0.0680707    0.145599     0.139646    -0.395484     0.0113162    0.236847     0.342736   -0.0305596  -0.335649     0.231371    -0.282101      0.187545     0.450082     0.108993    -0.201679     -0.185615    -0.134925    -0.11191     0.000852735  -0.0160897    0.58754     -0.294931     0.309623     0.89954     -0.992237
  0.475442   -0.330727    -0.135364    -0.100979     0.60676     -0.223052     0.235852     0.139749    0.498286    0.0171122   -0.346119     0.0268937     0.0388383   -0.207664    -0.0994931    0.488562     -0.391629    -0.0446667   -0.278902    0.239773     -0.0104875   -0.336271     0.287797    -0.205579     0.0126755    0.0541752
  0.163961   -0.617983    -0.719202     0.384871     0.0585783   -0.18515      0.377559     0.125653   -0.253048    0.34053     -0.15151      0.980804      0.194244     0.84427     -0.066874     0.300047      0.085507     0.0220833    0.231429   -0.4757        0.114754    -0.377486    -0.330682    -0.0299366    0.589654     0.254045[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.423373
[ Info: iteration 2, average log likelihood -1.423366
[ Info: iteration 3, average log likelihood -1.423358
[ Info: iteration 4, average log likelihood -1.423351
[ Info: iteration 5, average log likelihood -1.423344
[ Info: iteration 6, average log likelihood -1.423338
[ Info: iteration 7, average log likelihood -1.423332
[ Info: iteration 8, average log likelihood -1.423326
[ Info: iteration 9, average log likelihood -1.423320
[ Info: iteration 10, average log likelihood -1.423314
┌ Info: EM with 100000 data points 10 iterations avll -1.423314
└ 59.0 data points per parameter
[ Info: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.678561e+05
┌ Info: K-means with 900 data points using 3 iterations
└ 150.0 data points per parameter
[ Info: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
      1       2.230230e+04      -1.455538e+05 |        2
      2       7.823675e+03      -1.447862e+04 |        0
      3       7.823675e+03       0.000000e+00 |        0
K-means converged with 3 iterations (objv = 7823.67549422947)
[ Info: iteration 1, average log likelihood -2.043155
[ Info: iteration 2, average log likelihood -2.043154
[ Info: iteration 3, average log likelihood -2.043154
[ Info: iteration 4, average log likelihood -2.043154
[ Info: iteration 5, average log likelihood -2.043154
[ Info: iteration 6, average log likelihood -2.043154
[ Info: iteration 7, average log likelihood -2.043154
[ Info: iteration 8, average log likelihood -2.043154
[ Info: iteration 9, average log likelihood -2.043154
[ Info: iteration 10, average log likelihood -2.043154
┌ Info: EM with 900 data points 10 iterations avll -2.043154
└ 81.8 data points per parameter
   Testing GaussianMixtures tests passed 
