Julia Version 1.5.0-DEV.462
Commit b0619303e4 (2020-03-15 15:46 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-9.0.1 (ORCJIT, skylake)
Environment:
  JULIA_DEPOT_PATH = ::/usr/local/share/julia
  JULIA_NUM_THREADS = 2

  Resolving package versions...
  Installed NaNMath ────────────────────── v0.3.3
  Installed SortingAlgorithms ──────────── v0.3.1
  Installed HTML_Entities ──────────────── v1.0.0
  Installed MacroTools ─────────────────── v0.5.4
  Installed DataStructures ─────────────── v0.17.10
  Installed DataDeps ───────────────────── v0.7.2
  Installed ForwardDiff ────────────────── v0.10.9
  Installed Transformers ───────────────── v0.1.3
  Installed OpenSpecFun_jll ────────────── v0.5.3+3
  Installed Reexport ───────────────────── v0.2.0
  Installed FixedPointNumbers ──────────── v0.7.1
  Installed LightXML ───────────────────── v0.8.1
  Installed StaticArrays ───────────────── v0.12.1
  Installed BinaryProvider ─────────────── v0.5.8
  Installed NNlib ──────────────────────── v0.6.6
  Installed CUDAdrv ────────────────────── v6.0.0
  Installed IniFile ────────────────────── v0.5.0
  Installed Juno ───────────────────────── v0.8.1
  Installed ZygoteRules ────────────────── v0.2.0
  Installed FFTW_jll ───────────────────── v3.3.9+4
  Installed GPUArrays ──────────────────── v2.0.1
  Installed FFTW ───────────────────────── v1.2.0
  Installed Parsers ────────────────────── v0.3.12
  Installed HTTP ───────────────────────── v0.8.12
  Installed MbedTLS_jll ────────────────── v2.16.0+1
  Installed MKL_jll ────────────────────── v2019.0.117+2
  Installed DiffRules ──────────────────── v1.0.1
  Installed ArrayLayouts ───────────────── v0.1.5
  Installed LLVM ───────────────────────── v1.3.4
  Installed StrTables ──────────────────── v1.0.1
  Installed AbstractFFTs ───────────────── v0.5.0
  Installed IRTools ────────────────────── v0.3.1
  Installed Requires ───────────────────── v0.5.2
  Installed OrderedCollections ─────────── v1.1.0
  Installed CommonSubexpressions ───────── v0.2.0
  Installed TimerOutputs ───────────────── v0.5.3
  Installed Missings ───────────────────── v0.4.3
  Installed JSON ───────────────────────── v0.21.0
  Installed SpecialFunctions ───────────── v0.10.0
  Installed TranscodingStreams ─────────── v0.9.5
  Installed IntelOpenMP_jll ────────────── v2018.0.3+0
  Installed Flux ───────────────────────── v0.10.3
  Installed CEnum ──────────────────────── v0.2.0
  Installed Adapt ──────────────────────── v1.0.1
  Installed Media ──────────────────────── v0.5.0
  Installed ZipFile ────────────────────── v0.8.4
  Installed WordTokenizers ─────────────── v0.5.4
  Installed StatsBase ──────────────────── v0.32.2
  Installed CuArrays ───────────────────── v1.7.3
  Installed CUDAnative ─────────────────── v2.10.2
  Installed AbstractTrees ──────────────── v0.3.2
  Installed FillArrays ─────────────────── v0.8.5
  Installed InternedStrings ────────────── v0.7.0
  Installed MbedTLS ────────────────────── v1.0.1
  Installed CodecZlib ──────────────────── v0.6.0
  Installed Zygote ─────────────────────── v0.4.10
  Installed CompilerSupportLibraries_jll ─ v0.2.0+1
  Installed ColorTypes ─────────────────── v0.9.1
  Installed DiffResults ────────────────── v1.0.2
  Installed DataAPI ────────────────────── v1.1.0
  Installed CUDAapi ────────────────────── v3.1.0
  Installed Colors ─────────────────────── v0.11.2
  Installed BSON ───────────────────────── v0.2.5
  Installed BytePairEncoding ───────────── v0.1.1
#=#=#                                                                         ######################################################################## 100.0%
#=#=#                                                                         ##########                                                                14.5%#################################                                         46.6%############################################################              84.4%######################################################################## 100.0%
#=#=#                                                                         #############################                                             40.8%######################################################################## 100.0%
#=#=#                                                                         #########################################                                 57.5%######################################################################## 100.0%
#=#=#                                                                         ##                                                                         3.2%######                                                                     9.3%############                                                              17.5%####################                                                      28.4%#############################                                             40.4%#########################################                                 57.4%########################################################                  78.3%######################################################################## 100.0%
   Updating `~/.julia/environments/v1.5/Project.toml`
   21ca0261 + Transformers v0.1.3
   Updating `~/.julia/environments/v1.5/Manifest.toml`
   621f4979 + AbstractFFTs v0.5.0
   1520ce14 + AbstractTrees v0.3.2
   79e6a3ab + Adapt v1.0.1
   4c555306 + ArrayLayouts v0.1.5
   fbb218c0 + BSON v0.2.5
   b99e7846 + BinaryProvider v0.5.8
   a4280ba5 + BytePairEncoding v0.1.1
   fa961155 + CEnum v0.2.0
   3895d2a7 + CUDAapi v3.1.0
   c5f51814 + CUDAdrv v6.0.0
   be33ccc6 + CUDAnative v2.10.2
   944b1d66 + CodecZlib v0.6.0
   3da002f7 + ColorTypes v0.9.1
   5ae59095 + Colors v0.11.2
   bbf7d656 + CommonSubexpressions v0.2.0
   e66e0078 + CompilerSupportLibraries_jll v0.2.0+1
   3a865a2d + CuArrays v1.7.3
   9a962f9c + DataAPI v1.1.0
   124859b0 + DataDeps v0.7.2
   864edb3b + DataStructures v0.17.10
   163ba53b + DiffResults v1.0.2
   b552c78f + DiffRules v1.0.1
   7a1cc6ca + FFTW v1.2.0
   f5851436 + FFTW_jll v3.3.9+4
   1a297f60 + FillArrays v0.8.5
   53c48c17 + FixedPointNumbers v0.7.1
   587475ba + Flux v0.10.3
   f6369f11 + ForwardDiff v0.10.9
   0c68f7d7 + GPUArrays v2.0.1
   7693890a + HTML_Entities v1.0.0
   cd3eb016 + HTTP v0.8.12
   7869d1d1 + IRTools v0.3.1
   83e8ac13 + IniFile v0.5.0
   1d5cc7b8 + IntelOpenMP_jll v2018.0.3+0
   7d512f48 + InternedStrings v0.7.0
   682c06a0 + JSON v0.21.0
   e5e0dc1b + Juno v0.8.1
   929cbde3 + LLVM v1.3.4
   9c8b4983 + LightXML v0.8.1
   856f044c + MKL_jll v2019.0.117+2
   1914dd2f + MacroTools v0.5.4
   739be429 + MbedTLS v1.0.1
   c8ffd9c3 + MbedTLS_jll v2.16.0+1
   e89f7d12 + Media v0.5.0
   e1d29d7a + Missings v0.4.3
   872c559c + NNlib v0.6.6
   77ba4419 + NaNMath v0.3.3
   efe28fd5 + OpenSpecFun_jll v0.5.3+3
   bac558e1 + OrderedCollections v1.1.0
   69de0a69 + Parsers v0.3.12
   189a3867 + Reexport v0.2.0
   ae029012 + Requires v0.5.2
   a2af1166 + SortingAlgorithms v0.3.1
   276daf66 + SpecialFunctions v0.10.0
   90137ffa + StaticArrays v0.12.1
   2913bbd2 + StatsBase v0.32.2
   9700d1a9 + StrTables v1.0.1
   a759f4b9 + TimerOutputs v0.5.3
   3bb67fe8 + TranscodingStreams v0.9.5
   21ca0261 + Transformers v0.1.3
   796a5d58 + WordTokenizers v0.5.4
   a5390f91 + ZipFile v0.8.4
   e88e6eb3 + Zygote v0.4.10
   700de1a5 + ZygoteRules v0.2.0
   2a0f44e3 + Base64
   ade2ca70 + Dates
   8bb1440f + DelimitedFiles
   8ba89e20 + Distributed
   b77e0a4c + InteractiveUtils
   76f85450 + LibGit2
   8f399da3 + Libdl
   37e2e46d + LinearAlgebra
   56ddb016 + Logging
   d6f4376e + Markdown
   a63ad114 + Mmap
   44cfe95a + Pkg
   de0858da + Printf
   9abbd945 + Profile
   3fa0cd96 + REPL
   9a3f8284 + Random
   ea8e919c + SHA
   9e88b42a + Serialization
   6462fe0b + Sockets
   2f01184e + SparseArrays
   10745b16 + Statistics
   8dfed614 + Test
   cf7118a7 + UUIDs
   4ec0a83e + Unicode
   Building HTML_Entities → `~/.julia/packages/HTML_Entities/g4t7p/deps/build.log`
   Building NNlib ────────→ `~/.julia/packages/NNlib/FAI3o/deps/build.log`
   Building LightXML ─────→ `~/.julia/packages/LightXML/W8FVT/deps/build.log`
   Building FFTW ─────────→ `~/.julia/packages/FFTW/qqcBj/deps/build.log`
   Building ZipFile ──────→ `~/.julia/packages/ZipFile/DW0Qr/deps/build.log`
   Building CodecZlib ────→ `~/.julia/packages/CodecZlib/5t9zO/deps/build.log`
    Testing Transformers
     Status `/tmp/jl_lXIIa4/Project.toml`
   79e6a3ab Adapt v1.0.1
   fbb218c0 BSON v0.2.5
   a4280ba5 BytePairEncoding v0.1.1
   be33ccc6 CUDAnative v2.10.2
   3a865a2d CuArrays v1.7.3
   124859b0 DataDeps v0.7.2
   587475ba Flux v0.10.3
   cd3eb016 HTTP v0.8.12
   7d512f48 InternedStrings v0.7.0
   682c06a0 JSON v0.21.0
   9c8b4983 LightXML v0.8.1
   1914dd2f MacroTools v0.5.4
   ae029012 Requires v0.5.2
   21ca0261 Transformers v0.1.3
   796a5d58 WordTokenizers v0.5.4
   a5390f91 ZipFile v0.8.4
   700de1a5 ZygoteRules v0.2.0
   ade2ca70 Dates
   8bb1440f DelimitedFiles
   37e2e46d LinearAlgebra
   d6f4376e Markdown
   44cfe95a Pkg
   9a3f8284 Random
   10745b16 Statistics
   8dfed614 Test
   4ec0a83e Unicode
     Status `/tmp/jl_lXIIa4/Manifest.toml`
   621f4979 AbstractFFTs v0.5.0
   1520ce14 AbstractTrees v0.3.2
   79e6a3ab Adapt v1.0.1
   4c555306 ArrayLayouts v0.1.5
   fbb218c0 BSON v0.2.5
   b99e7846 BinaryProvider v0.5.8
   a4280ba5 BytePairEncoding v0.1.1
   fa961155 CEnum v0.2.0
   3895d2a7 CUDAapi v3.1.0
   c5f51814 CUDAdrv v6.0.0
   be33ccc6 CUDAnative v2.10.2
   944b1d66 CodecZlib v0.6.0
   3da002f7 ColorTypes v0.9.1
   5ae59095 Colors v0.11.2
   bbf7d656 CommonSubexpressions v0.2.0
   e66e0078 CompilerSupportLibraries_jll v0.2.0+1
   3a865a2d CuArrays v1.7.3
   9a962f9c DataAPI v1.1.0
   124859b0 DataDeps v0.7.2
   864edb3b DataStructures v0.17.10
   163ba53b DiffResults v1.0.2
   b552c78f DiffRules v1.0.1
   7a1cc6ca FFTW v1.2.0
   f5851436 FFTW_jll v3.3.9+4
   1a297f60 FillArrays v0.8.5
   53c48c17 FixedPointNumbers v0.7.1
   587475ba Flux v0.10.3
   f6369f11 ForwardDiff v0.10.9
   0c68f7d7 GPUArrays v2.0.1
   7693890a HTML_Entities v1.0.0
   cd3eb016 HTTP v0.8.12
   7869d1d1 IRTools v0.3.1
   83e8ac13 IniFile v0.5.0
   1d5cc7b8 IntelOpenMP_jll v2018.0.3+0
   7d512f48 InternedStrings v0.7.0
   682c06a0 JSON v0.21.0
   e5e0dc1b Juno v0.8.1
   929cbde3 LLVM v1.3.4
   9c8b4983 LightXML v0.8.1
   856f044c MKL_jll v2019.0.117+2
   1914dd2f MacroTools v0.5.4
   739be429 MbedTLS v1.0.1
   c8ffd9c3 MbedTLS_jll v2.16.0+1
   e89f7d12 Media v0.5.0
   e1d29d7a Missings v0.4.3
   872c559c NNlib v0.6.6
   77ba4419 NaNMath v0.3.3
   efe28fd5 OpenSpecFun_jll v0.5.3+3
   bac558e1 OrderedCollections v1.1.0
   69de0a69 Parsers v0.3.12
   189a3867 Reexport v0.2.0
   ae029012 Requires v0.5.2
   a2af1166 SortingAlgorithms v0.3.1
   276daf66 SpecialFunctions v0.10.0
   90137ffa StaticArrays v0.12.1
   2913bbd2 StatsBase v0.32.2
   9700d1a9 StrTables v1.0.1
   a759f4b9 TimerOutputs v0.5.3
   3bb67fe8 TranscodingStreams v0.9.5
   21ca0261 Transformers v0.1.3
   796a5d58 WordTokenizers v0.5.4
   a5390f91 ZipFile v0.8.4
   e88e6eb3 Zygote v0.4.10
   700de1a5 ZygoteRules v0.2.0
   2a0f44e3 Base64
   ade2ca70 Dates
   8bb1440f DelimitedFiles
   8ba89e20 Distributed
   b77e0a4c InteractiveUtils
   76f85450 LibGit2
   8f399da3 Libdl
   37e2e46d LinearAlgebra
   56ddb016 Logging
   d6f4376e Markdown
   a63ad114 Mmap
   44cfe95a Pkg
   de0858da Printf
   9abbd945 Profile
   3fa0cd96 REPL
   9a3f8284 Random
   ea8e919c SHA
   9e88b42a Serialization
   6462fe0b Sockets
   2f01184e SparseArrays
   10745b16 Statistics
   8dfed614 Test
   cf7118a7 UUIDs
   4ec0a83e Unicode
WARNING: could not import Compiler.just_construct_ssa into Wrap
┌ Warning: `@get!(dict, key, default)` at /home/pkgeval/.julia/packages/Requires/9Jse8/src/require.jl:11 is deprecated, use `get!(()->default, dict, key)` instead.
│   caller = include(::Function, ::Module, ::String) at Base.jl:380
└ @ Base ./Base.jl:380
┌ Warning: CuArrays.jl found cuda, but did not find libcudnn. Some functionality will not be available.
└ @ Flux ~/.julia/packages/Flux/NpkMm/src/Flux.jl:48
WARNING: Method definition _pullback(Zygote.Context, typeof(Flux.onehot), Any...) in module Flux at /home/pkgeval/.julia/packages/Zygote/S1EoU/src/lib/grad.jl:14 overwritten in module Basic at /home/pkgeval/.julia/packages/Zygote/S1EoU/src/lib/grad.jl:14.
  ** incremental compilation may be fatally broken for this module **

┌ Warning: Incompatibility detected between CUDA and LLVM 8.0+; disabling debug info emission for CUDA kernels
└ @ CUDAnative ~/.julia/packages/CUDAnative/hfulr/src/CUDAnative.jl:114
┌ Warning: CuArrays.jl found cuda, but did not find libcudnn. Some functionality will not be available.
└ @ Flux ~/.julia/packages/Flux/NpkMm/src/Flux.jl:48
[ Info: Test CUDA
[ Info: Testing CUDA
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
Gather: Error During Test at /home/pkgeval/.julia/packages/Transformers/TNqp3/test/cuda/gather.jl:10
  Test threw exception
  Expression: gather(cuw, todevice([3, 5, 7])) |> collect == hcat(map((i->begin
                    w[:, i]
                end), [3, 5, 7])...)
  TaskFailedException:
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::typeof(GPUArrays.index_kernel), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,1,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Int64,Int64},Tuple{Base.Slice{Base.OneTo{Int64}},Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,1,Nothing}, ::Tuple{CuArray{Float32,1,Nothing},CuArray{Float32,2,Nothing},Tuple{Int64,Int64},Tuple{Base.Slice{Base.OneTo{Int64}},Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Float32,1,Nothing}, ::Tuple{CuArray{Float32,1,Nothing},CuArray{Float32,2,Nothing},Tuple{Int64,Int64},Tuple{Base.Slice{Base.OneTo{Int64}},Int64}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] _unsafe_getindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:96 [inlined]
   [25] _unsafe_getindex at ./multidimensional.jl:749 [inlined]
   [26] _getindex at ./multidimensional.jl:735 [inlined]
   [27] getindex at ./abstractarray.jl:980 [inlined]
   [28] macro expansion at /home/pkgeval/.julia/packages/Transformers/TNqp3/src/basic/embeds/gather.jl:18 [inlined]
   [29] (::Transformers.Basic.var"#225#threadsfor_fun#36"{CuArray{Float32,2,Nothing},Array{Int64,1},CuArray{Float32,2,Nothing},UnitRange{Int64}})(::Bool) at ./threadingconstructs.jl:61
   [30] (::Transformers.Basic.var"#225#threadsfor_fun#36"{CuArray{Float32,2,Nothing},Array{Int64,1},CuArray{Float32,2,Nothing},UnitRange{Int64}})() at ./threadingconstructs.jl:28
  Stacktrace:
   [1] wait(::Task) at ./task.jl:267
   [2] macro expansion at ./threadingconstructs.jl:69 [inlined]
   [3] gather(::CuArray{Float32,2,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Transformers/TNqp3/src/basic/embeds/gather.jl:16
   [4] top-level scope at /home/pkgeval/.julia/packages/Transformers/TNqp3/test/cuda/gather.jl:10
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [6] top-level scope at /home/pkgeval/.julia/packages/Transformers/TNqp3/test/cuda/gather.jl:2
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
Gather: Error During Test at /home/pkgeval/.julia/packages/Transformers/TNqp3/test/cuda/gather.jl:11
  Test threw exception
  Expression: gather(cuw, todevice(ind)) |> collect == cat(map((j->begin
                    hcat(map((i->begin
                                    w[:, i]
                                end), ind[:, j])...)
                end), 1:5)...; dims = 3)
  TaskFailedException:
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::typeof(GPUArrays.index_kernel), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,1,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Int64,Int64},Tuple{Base.Slice{Base.OneTo{Int64}},Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,1,Nothing}, ::Tuple{CuArray{Float32,1,Nothing},CuArray{Float32,2,Nothing},Tuple{Int64,Int64},Tuple{Base.Slice{Base.OneTo{Int64}},Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Float32,1,Nothing}, ::Tuple{CuArray{Float32,1,Nothing},CuArray{Float32,2,Nothing},Tuple{Int64,Int64},Tuple{Base.Slice{Base.OneTo{Int64}},Int64}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] _unsafe_getindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:96 [inlined]
   [25] _unsafe_getindex at ./multidimensional.jl:749 [inlined]
   [26] _getindex at ./multidimensional.jl:735 [inlined]
   [27] getindex at ./abstractarray.jl:980 [inlined]
   [28] macro expansion at /home/pkgeval/.julia/packages/Transformers/TNqp3/src/basic/embeds/gather.jl:18 [inlined]
   [29] (::Transformers.Basic.var"#225#threadsfor_fun#36"{CuArray{Float32,2,Nothing},Array{Int64,2},CuArray{Float32,3,Nothing},UnitRange{Int64}})(::Bool) at ./threadingconstructs.jl:61
   [30] (::Transformers.Basic.var"#225#threadsfor_fun#36"{CuArray{Float32,2,Nothing},Array{Int64,2},CuArray{Float32,3,Nothing},UnitRange{Int64}})() at ./threadingconstructs.jl:28
  Stacktrace:
   [1] wait(::Task) at ./task.jl:267
   [2] macro expansion at ./threadingconstructs.jl:69 [inlined]
   [3] gather(::CuArray{Float32,2,Nothing}, ::Array{Int64,2}) at /home/pkgeval/.julia/packages/Transformers/TNqp3/src/basic/embeds/gather.jl:16
   [4] top-level scope at /home/pkgeval/.julia/packages/Transformers/TNqp3/test/cuda/gather.jl:11
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [6] top-level scope at /home/pkgeval/.julia/packages/Transformers/TNqp3/test/cuda/gather.jl:2
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
Gather: Error During Test at /home/pkgeval/.julia/packages/Transformers/TNqp3/test/cuda/gather.jl:13
  Test threw exception
  Expression: gather(cuwh, todevice([(5, 3, 3) (2, 1, 2); (5, 4, 1) (4, 2, 1)])) |> collect == begin
        a = wh[:, 5, 3, 3]
        b = wh[:, 2, 1, 2]
        c = wh[:, 5, 4, 1]
        d = wh[:, 4, 2, 1]
        A = hcat(a, c)
        B = hcat(b, d)
        Z = cat(A, B; dims = 3)
    end
  TaskFailedException:
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::typeof(GPUArrays.index_kernel), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,1,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float32,4,CUDAnative.AS.Global},NTuple{4,Int64},Tuple{Base.Slice{Base.OneTo{Int64}},Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,1,Nothing}, ::Tuple{CuArray{Float32,1,Nothing},CuArray{Float32,4,Nothing},NTuple{4,Int64},Tuple{Base.Slice{Base.OneTo{Int64}},Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Float32,1,Nothing}, ::Tuple{CuArray{Float32,1,Nothing},CuArray{Float32,4,Nothing},NTuple{4,Int64},Tuple{Base.Slice{Base.OneTo{Int64}},Int64,Int64,Int64}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] _unsafe_getindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:96 [inlined]
   [25] _unsafe_getindex at ./multidimensional.jl:749 [inlined]
   [26] _getindex at ./multidimensional.jl:735 [inlined]
   [27] getindex at ./abstractarray.jl:980 [inlined]
   [28] macro expansion at /home/pkgeval/.julia/packages/Transformers/TNqp3/src/basic/embeds/gather.jl:33 [inlined]
   [29] (::Transformers.Basic.var"#242#threadsfor_fun#37"{CuArray{Float32,4,Nothing},Array{Tuple{Int64,Int64,Int64},2},CuArray{Float32,3,Nothing},UnitRange{Int64}})(::Bool) at ./threadingconstructs.jl:61
   [30] (::Transformers.Basic.var"#242#threadsfor_fun#37"{CuArray{Float32,4,Nothing},Array{Tuple{Int64,Int64,Int64},2},CuArray{Float32,3,Nothing},UnitRange{Int64}})() at ./threadingconstructs.jl:28
  Stacktrace:
   [1] wait(::Task) at ./task.jl:267
   [2] macro expansion at ./threadingconstructs.jl:69 [inlined]
   [3] gather(::CuArray{Float32,4,Nothing}, ::Array{Tuple{Int64,Int64,Int64},2}) at /home/pkgeval/.julia/packages/Transformers/TNqp3/src/basic/embeds/gather.jl:31
   [4] top-level scope at /home/pkgeval/.julia/packages/Transformers/TNqp3/test/cuda/gather.jl:13
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [6] top-level scope at /home/pkgeval/.julia/packages/Transformers/TNqp3/test/cuda/gather.jl:2
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
Gather: Error During Test at /home/pkgeval/.julia/packages/Transformers/TNqp3/test/cuda/gather.jl:1
  Got exception outside of a @test
  TaskFailedException:
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::typeof(GPUArrays.index_kernel), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,1,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Int64,Int64},Tuple{Base.Slice{Base.OneTo{Int64}},Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,1,Nothing}, ::Tuple{CuArray{Float32,1,Nothing},CuArray{Float32,2,Nothing},Tuple{Int64,Int64},Tuple{Base.Slice{Base.OneTo{Int64}},Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Float32,1,Nothing}, ::Tuple{CuArray{Float32,1,Nothing},CuArray{Float32,2,Nothing},Tuple{Int64,Int64},Tuple{Base.Slice{Base.OneTo{Int64}},Int64}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] _unsafe_getindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:96 [inlined]
   [25] _unsafe_getindex at ./multidimensional.jl:749 [inlined]
   [26] _getindex at ./multidimensional.jl:735 [inlined]
   [27] getindex at ./abstractarray.jl:980 [inlined]
   [28] macro expansion at /home/pkgeval/.julia/packages/Transformers/TNqp3/src/basic/embeds/gather.jl:18 [inlined]
   [29] (::Transformers.Basic.var"#225#threadsfor_fun#36"{CuArray{Float32,2,Nothing},Array{Int64,1},CuArray{Float32,2,Nothing},UnitRange{Int64}})(::Bool) at ./threadingconstructs.jl:61
   [30] (::Transformers.Basic.var"#225#threadsfor_fun#36"{CuArray{Float32,2,Nothing},Array{Int64,1},CuArray{Float32,2,Nothing},UnitRange{Int64}})() at ./threadingconstructs.jl:28
  Stacktrace:
   [1] wait(::Task) at ./task.jl:267
   [2] macro expansion at ./threadingconstructs.jl:69 [inlined]
   [3] gather at /home/pkgeval/.julia/packages/Transformers/TNqp3/src/basic/embeds/gather.jl:16 [inlined]
   [4] adjoint at /home/pkgeval/.julia/packages/Transformers/TNqp3/src/basic/embeds/gather.jl:55 [inlined]
   [5] _pullback at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:47 [inlined]
   [6] _pullback at /home/pkgeval/.julia/packages/Transformers/TNqp3/src/basic/embeds/gather.jl:54 [inlined]
   [7] _pullback at /home/pkgeval/.julia/packages/Transformers/TNqp3/src/basic/embeds/onehot.jl:63 [inlined]
   [8] #4 at /home/pkgeval/.julia/packages/Transformers/TNqp3/test/cuda/gather.jl:30 [inlined]
   [9] _pullback(::Zygote.Context, ::var"#4#8"{OneHotArray{2,Array{Flux.OneHotVector,1}}}, ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/S1EoU/src/compiler/interface2.jl:0
   [10] _pullback(::Function, ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/S1EoU/src/compiler/interface.jl:29
   [11] pullback(::Function, ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/S1EoU/src/compiler/interface.jl:35
   [12] gradient(::Function, ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/S1EoU/src/compiler/interface.jl:44
   [13] top-level scope at /home/pkgeval/.julia/packages/Transformers/TNqp3/test/cuda/gather.jl:29
   [14] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [15] top-level scope at /home/pkgeval/.julia/packages/Transformers/TNqp3/test/cuda/gather.jl:2
   [16] include(::String) at ./client.jl:441
   [17] macro expansion at /home/pkgeval/.julia/packages/Transformers/TNqp3/test/test_cuda.jl:7 [inlined]
   [18] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [19] top-level scope at /home/pkgeval/.julia/packages/Transformers/TNqp3/test/test_cuda.jl:4
   [20] include(::String) at ./client.jl:441
   [21] macro expansion at /home/pkgeval/.julia/packages/Transformers/TNqp3/test/runtests.jl:31 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] top-level scope at /home/pkgeval/.julia/packages/Transformers/TNqp3/test/runtests.jl:29
   [24] include(::String) at ./client.jl:441
   [25] top-level scope at none:6
   [26] eval(::Module, ::Any) at ./boot.jl:331
   [27] exec_options(::Base.JLOptions) at ./client.jl:264
   [28] _start() at ./client.jl:490
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
Transformer: Error During Test at /home/pkgeval/.julia/packages/Transformers/TNqp3/test/cuda/transformer.jl:8
  Test threw exception
  Expression: size(t(x)) == (10, 7, 3)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(+),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,1,CUDAnative.AS.Global},Tuple{Bool},Tuple{Int64}}}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(+),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{CuArray{Float32,1,Nothing},Tuple{Bool},Tuple{Int64}}}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(+),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{CuArray{Float32,1,Nothing},Tuple{Bool},Tuple{Int64}}}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(+),Tuple{CuArray{Float32,2,Nothing},CuArray{Float32,1,Nothing}}}}}) at ./broadcast.jl:840
   [27] materialize at ./broadcast.jl:820 [inlined]
   [28] Dense at /home/pkgeval/.julia/packages/Flux/NpkMm/src/layers/basic.jl:115 [inlined]
   [29] (::Flux.Dense{typeof(identity),CuArray{Float32,2,Nothing},CuArray{Float32,1,Nothing}})(::CuArray{Float32,2,CuArray{Float32,3,Nothing}}) at /home/pkgeval/.julia/packages/Flux/NpkMm/src/layers/basic.jl:126
   [30] (::Transformers.Basic.MultiheadAttention)(::CuArray{Float32,3,Nothing}, ::CuArray{Float32,3,Nothing}, ::CuArray{Float32,3,Nothing}; mask::Nothing) at /home/pkgeval/.julia/packages/Transformers/TNqp3/src/basic/mh_atten.jl:85
   [31] (::Transformer)(::CuArray{Float32,3,Nothing}, ::Nothing) at /home/pkgeval/.julia/packages/Transformers/TNqp3/src/basic/transformer.jl:62 (repeats 2 times)
   [32] top-level scope at /home/pkgeval/.julia/packages/Transformers/TNqp3/test/cuda/transformer.jl:8
   [33] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [34] top-level scope at /home/pkgeval/.julia/packages/Transformers/TNqp3/test/cuda/transformer.jl:2
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
Transformer: Error During Test at /home/pkgeval/.julia/packages/Transformers/TNqp3/test/cuda/transformer.jl:9
  Test threw exception
  Expression: size(t(x[:, :, 2])) == (10, 7)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::typeof(GPUArrays.index_kernel), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64},Tuple{Base.Slice{Base.OneTo{Int64}},Base.Slice{Base.OneTo{Int64}},Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},CuArray{Float32,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{Base.Slice{Base.OneTo{Int64}},Base.Slice{Base.OneTo{Int64}},Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},CuArray{Float32,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{Base.Slice{Base.OneTo{Int64}},Base.Slice{Base.OneTo{Int64}},Int64}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] _unsafe_getindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:96 [inlined]
   [25] _unsafe_getindex at ./multidimensional.jl:749 [inlined]
   [26] _getindex at ./multidimensional.jl:735 [inlined]
   [27] getindex(::CuArray{Float32,3,Nothing}, ::Function, ::Function, ::Int64) at ./abstractarray.jl:980
   [28] top-level scope at /home/pkgeval/.julia/packages/Transformers/TNqp3/test/cuda/transformer.jl:9
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [30] top-level scope at /home/pkgeval/.julia/packages/Transformers/TNqp3/test/cuda/transformer.jl:2
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
Transformer: Error During Test at /home/pkgeval/.julia/packages/Transformers/TNqp3/test/cuda/transformer.jl:11
  Test threw exception
  Expression: size(td(y, x)) == (10, 6, 3)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(+),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,1,CUDAnative.AS.Global},Tuple{Bool},Tuple{Int64}}}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(+),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{CuArray{Float32,1,Nothing},Tuple{Bool},Tuple{Int64}}}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(+),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{CuArray{Float32,1,Nothing},Tuple{Bool},Tuple{Int64}}}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(+),Tuple{CuArray{Float32,2,Nothing},CuArray{Float32,1,Nothing}}}}}) at ./broadcast.jl:840
   [27] materialize at ./broadcast.jl:820 [inlined]
   [28] Dense at /home/pkgeval/.julia/packages/Flux/NpkMm/src/layers/basic.jl:115 [inlined]
   [29] (::Flux.Dense{typeof(identity),CuArray{Float32,2,Nothing},CuArray{Float32,1,Nothing}})(::CuArray{Float32,2,CuArray{Float32,3,Nothing}}) at /home/pkgeval/.julia/packages/Flux/NpkMm/src/layers/basic.jl:126
   [30] (::Transformers.Basic.MultiheadAttention)(::CuArray{Float32,3,Nothing}, ::CuArray{Float32,3,Nothing}, ::CuArray{Float32,3,Nothing}; mask::Nothing) at /home/pkgeval/.julia/packages/Transformers/TNqp3/src/basic/mh_atten.jl:85
   [31] MultiheadAttention at /home/pkgeval/.julia/packages/Transformers/TNqp3/src/basic/mh_atten.jl:80 [inlined]
   [32] (::TransformerDecoder)(::CuArray{Float32,3,Nothing}, ::CuArray{Float32,3,Nothing}, ::Nothing) at /home/pkgeval/.julia/packages/Transformers/TNqp3/src/basic/transformer.jl:134 (repeats 2 times)
   [33] top-level scope at /home/pkgeval/.julia/packages/Transformers/TNqp3/test/cuda/transformer.jl:11
   [34] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [35] top-level scope at /home/pkgeval/.julia/packages/Transformers/TNqp3/test/cuda/transformer.jl:2
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
Transformer: Error During Test at /home/pkgeval/.julia/packages/Transformers/TNqp3/test/cuda/transformer.jl:12
  Test threw exception
  Expression: size(td(y[:, :, 2], x[:, :, 2])) == (10, 6)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::typeof(GPUArrays.index_kernel), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64},Tuple{Base.Slice{Base.OneTo{Int64}},Base.Slice{Base.OneTo{Int64}},Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},CuArray{Float32,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{Base.Slice{Base.OneTo{Int64}},Base.Slice{Base.OneTo{Int64}},Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},CuArray{Float32,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{Base.Slice{Base.OneTo{Int64}},Base.Slice{Base.OneTo{Int64}},Int64}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] _unsafe_getindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:96 [inlined]
   [25] _unsafe_getindex at ./multidimensional.jl:749 [inlined]
   [26] _getindex at ./multidimensional.jl:735 [inlined]
   [27] getindex(::CuArray{Float32,3,Nothing}, ::Function, ::Function, ::Int64) at ./abstractarray.jl:980
   [28] top-level scope at /home/pkgeval/.julia/packages/Transformers/TNqp3/test/cuda/transformer.jl:12
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [30] top-level scope at /home/pkgeval/.julia/packages/Transformers/TNqp3/test/cuda/transformer.jl:2
  
[ Info: Test TRANSFORMER
[ Info: Test NNTOPO
[ Info: Test EMBED
[ Info: Test BASIC
[ Info: Test GPT
[ Info: Test BERT
Test Summary:   | Pass  Error  Total
Transformers    |  106      8    114
  CUDA          |           8      8
    Gather      |           4      4
    Transformer |           4      4
  Transformer   |   11            11
  NNTopo        |    9             9
  Embed         |   35            35
  Basic         |    2             2
  Gpt           |   10            10
  Bert          |   39            39
ERROR: LoadError: Some tests did not pass: 106 passed, 0 failed, 8 errored, 0 broken.
in expression starting at /home/pkgeval/.julia/packages/Transformers/TNqp3/test/runtests.jl:28
ERROR: Package Transformers errored during testing
Stacktrace:
 [1] pkgerror(::String, ::Vararg{String,N} where N) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Pkg/src/Types.jl:53
 [2] test(::Pkg.Types.Context, ::Array{Pkg.Types.PackageSpec,1}; coverage::Bool, julia_args::Cmd, test_args::Cmd, test_fn::Nothing) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Pkg/src/Operations.jl:1523
 [3] test(::Pkg.Types.Context, ::Array{Pkg.Types.PackageSpec,1}; coverage::Bool, test_fn::Nothing, julia_args::Cmd, test_args::Cmd, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:316
 [4] test(::Pkg.Types.Context, ::Array{Pkg.Types.PackageSpec,1}) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:303
 [5] #test#68 at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:297 [inlined]
 [6] test at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:297 [inlined]
 [7] #test#67 at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:296 [inlined]
 [8] test at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:296 [inlined]
 [9] test(::String; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:295
 [10] test(::String) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:295
 [11] top-level scope at none:13
