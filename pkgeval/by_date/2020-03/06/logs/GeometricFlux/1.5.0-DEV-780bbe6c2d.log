Julia Version 1.5.0-DEV.399
Commit 780bbe6c2d (2020-03-04 16:59 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-9.0.1 (ORCJIT, skylake)
Environment:
  JULIA_DEPOT_PATH = ::/usr/local/share/julia
  JULIA_NUM_THREADS = 2

  Resolving package versions...
  Installed CEnum ──────────────────────── v0.2.0
  Installed OpenSpecFun_jll ────────────── v0.5.3+2
  Installed Media ──────────────────────── v0.5.0
  Installed Requires ───────────────────── v1.0.1
  Installed CommonSubexpressions ───────── v0.2.0
  Installed FFTW ───────────────────────── v1.2.0
  Installed IRTools ────────────────────── v0.3.1
  Installed GeometricFlux ──────────────── v0.3.0
  Installed DiffRules ──────────────────── v1.0.1
  Installed Adapt ──────────────────────── v1.0.1
  Installed NaNMath ────────────────────── v0.3.3
  Installed Juno ───────────────────────── v0.8.1
  Installed CompilerSupportLibraries_jll ─ v0.2.0+1
  Installed ZipFile ────────────────────── v0.9.1
  Installed NNlib ──────────────────────── v0.6.6
  Installed ColorTypes ─────────────────── v0.9.1
  Installed Zygote ─────────────────────── v0.4.9
  Installed Zlib_jll ───────────────────── v1.2.11+8
  Installed Reexport ───────────────────── v0.2.0
  Installed AbstractFFTs ───────────────── v0.5.0
  Installed FillArrays ─────────────────── v0.8.5
  Installed DataAPI ────────────────────── v1.1.0
  Installed ArrayLayouts ───────────────── v0.1.5
  Installed Flux ───────────────────────── v0.10.3
  Installed FixedPointNumbers ──────────── v0.7.1
  Installed ZygoteRules ────────────────── v0.2.0
  Installed TranscodingStreams ─────────── v0.9.5
  Installed FFTW_jll ───────────────────── v3.3.9+4
  Installed OrderedCollections ─────────── v1.1.0
  Installed StatsBase ──────────────────── v0.32.2
  Installed GPUArrays ──────────────────── v2.0.1
  Installed ForwardDiff ────────────────── v0.10.9
  Installed Colors ─────────────────────── v0.11.2
  Installed LLVM ───────────────────────── v1.3.4
  Installed CuArrays ───────────────────── v1.7.3
  Installed TimerOutputs ───────────────── v0.5.3
  Installed Missings ───────────────────── v0.4.3
  Installed DiffResults ────────────────── v1.0.2
  Installed SpecialFunctions ───────────── v0.10.0
  Installed StaticArrays ───────────────── v0.12.1
  Installed DataStructures ─────────────── v0.17.10
  Installed CUDAdrv ────────────────────── v6.0.0
  Installed CUDAnative ─────────────────── v2.10.2
  Installed IntelOpenMP_jll ────────────── v2018.0.3+0
  Installed CodecZlib ──────────────────── v0.6.0
  Installed SortingAlgorithms ──────────── v0.3.1
  Installed CUDAapi ────────────────────── v3.1.0
  Installed MKL_jll ────────────────────── v2019.0.117+2
  Installed MacroTools ─────────────────── v0.5.4
  Installed AbstractTrees ──────────────── v0.3.2
  Installed BinaryProvider ─────────────── v0.5.8
#=#=#                                                                         ######################################################################## 100.0%
#=#=#                                                                         ##                                                                         3.0%#####                                                                      8.2%##########                                                                14.4%################                                                          23.3%#########################                                                 35.7%#####################################                                     52.5%#####################################################                     74.0%#####################################################################     96.0%######################################################################## 100.0%
#=#=#                                                                         ######################################################################## 100.0%
#=#=#                                                                         #######################                                                   32.0%####################################################################      95.3%######################################################################## 100.0%
#=#=#                                                                         ##############################################                            64.4%######################################################################## 100.0%
   Updating `~/.julia/environments/v1.5/Project.toml`
   7e08b658 + GeometricFlux v0.3.0
   Updating `~/.julia/environments/v1.5/Manifest.toml`
   621f4979 + AbstractFFTs v0.5.0
   1520ce14 + AbstractTrees v0.3.2
   79e6a3ab + Adapt v1.0.1
   4c555306 + ArrayLayouts v0.1.5
   b99e7846 + BinaryProvider v0.5.8
   fa961155 + CEnum v0.2.0
   3895d2a7 + CUDAapi v3.1.0
   c5f51814 + CUDAdrv v6.0.0
   be33ccc6 + CUDAnative v2.10.2
   944b1d66 + CodecZlib v0.6.0
   3da002f7 + ColorTypes v0.9.1
   5ae59095 + Colors v0.11.2
   bbf7d656 + CommonSubexpressions v0.2.0
   e66e0078 + CompilerSupportLibraries_jll v0.2.0+1
   3a865a2d + CuArrays v1.7.3
   9a962f9c + DataAPI v1.1.0
   864edb3b + DataStructures v0.17.10
   163ba53b + DiffResults v1.0.2
   b552c78f + DiffRules v1.0.1
   7a1cc6ca + FFTW v1.2.0
   f5851436 + FFTW_jll v3.3.9+4
   1a297f60 + FillArrays v0.8.5
   53c48c17 + FixedPointNumbers v0.7.1
   587475ba + Flux v0.10.3
   f6369f11 + ForwardDiff v0.10.9
   0c68f7d7 + GPUArrays v2.0.1
   7e08b658 + GeometricFlux v0.3.0
   7869d1d1 + IRTools v0.3.1
   1d5cc7b8 + IntelOpenMP_jll v2018.0.3+0
   e5e0dc1b + Juno v0.8.1
   929cbde3 + LLVM v1.3.4
   856f044c + MKL_jll v2019.0.117+2
   1914dd2f + MacroTools v0.5.4
   e89f7d12 + Media v0.5.0
   e1d29d7a + Missings v0.4.3
   872c559c + NNlib v0.6.6
   77ba4419 + NaNMath v0.3.3
   efe28fd5 + OpenSpecFun_jll v0.5.3+2
   bac558e1 + OrderedCollections v1.1.0
   189a3867 + Reexport v0.2.0
   ae029012 + Requires v1.0.1
   a2af1166 + SortingAlgorithms v0.3.1
   276daf66 + SpecialFunctions v0.10.0
   90137ffa + StaticArrays v0.12.1
   2913bbd2 + StatsBase v0.32.2
   a759f4b9 + TimerOutputs v0.5.3
   3bb67fe8 + TranscodingStreams v0.9.5
   a5390f91 + ZipFile v0.9.1
   83775a58 + Zlib_jll v1.2.11+8
   e88e6eb3 + Zygote v0.4.9
   700de1a5 + ZygoteRules v0.2.0
   2a0f44e3 + Base64
   ade2ca70 + Dates
   8bb1440f + DelimitedFiles
   8ba89e20 + Distributed
   b77e0a4c + InteractiveUtils
   76f85450 + LibGit2
   8f399da3 + Libdl
   37e2e46d + LinearAlgebra
   56ddb016 + Logging
   d6f4376e + Markdown
   a63ad114 + Mmap
   44cfe95a + Pkg
   de0858da + Printf
   9abbd945 + Profile
   3fa0cd96 + REPL
   9a3f8284 + Random
   ea8e919c + SHA
   9e88b42a + Serialization
   6462fe0b + Sockets
   2f01184e + SparseArrays
   10745b16 + Statistics
   8dfed614 + Test
   cf7118a7 + UUIDs
   4ec0a83e + Unicode
   Building FFTW ─────→ `~/.julia/packages/FFTW/qqcBj/deps/build.log`
   Building NNlib ────→ `~/.julia/packages/NNlib/FAI3o/deps/build.log`
   Building CodecZlib → `~/.julia/packages/CodecZlib/5t9zO/deps/build.log`
    Testing GeometricFlux
     Status `/tmp/jl_upUFkb/Project.toml`
   3895d2a7 CUDAapi v3.1.0
   be33ccc6 CUDAnative v2.10.2
   3a865a2d CuArrays v1.7.3
   864edb3b DataStructures v0.17.10
   587475ba Flux v0.10.3
   7e08b658 GeometricFlux v0.3.0
   7869d1d1 IRTools v0.3.1
   093fc24a LightGraphs v1.3.1
   626554b9 MetaGraphs v0.6.5
   ae029012 Requires v1.0.1
   47aef6b3 SimpleWeightedGraphs v1.1.1
   e88e6eb3 Zygote v0.4.9
   700de1a5 ZygoteRules v0.2.0
   37e2e46d LinearAlgebra
   9a3f8284 Random
   2f01184e SparseArrays
   10745b16 Statistics
   8dfed614 Test
     Status `/tmp/jl_upUFkb/Manifest.toml`
   621f4979 AbstractFFTs v0.5.0
   1520ce14 AbstractTrees v0.3.2
   79e6a3ab Adapt v1.0.1
   ec485272 ArnoldiMethod v0.0.4
   4c555306 ArrayLayouts v0.1.5
   b99e7846 BinaryProvider v0.5.8
   fa961155 CEnum v0.2.0
   3895d2a7 CUDAapi v3.1.0
   c5f51814 CUDAdrv v6.0.0
   be33ccc6 CUDAnative v2.10.2
   944b1d66 CodecZlib v0.6.0
   3da002f7 ColorTypes v0.9.1
   5ae59095 Colors v0.11.2
   bbf7d656 CommonSubexpressions v0.2.0
   e66e0078 CompilerSupportLibraries_jll v0.2.0+1
   3a865a2d CuArrays v1.7.3
   9a962f9c DataAPI v1.1.0
   864edb3b DataStructures v0.17.10
   163ba53b DiffResults v1.0.2
   b552c78f DiffRules v1.0.1
   7a1cc6ca FFTW v1.2.0
   f5851436 FFTW_jll v3.3.9+4
   5789e2e9 FileIO v1.2.2
   1a297f60 FillArrays v0.8.5
   53c48c17 FixedPointNumbers v0.7.1
   587475ba Flux v0.10.3
   f6369f11 ForwardDiff v0.10.9
   0c68f7d7 GPUArrays v2.0.1
   7e08b658 GeometricFlux v0.3.0
   7869d1d1 IRTools v0.3.1
   d25df0c9 Inflate v0.1.1
   1d5cc7b8 IntelOpenMP_jll v2018.0.3+0
   033835bb JLD2 v0.1.12
   e5e0dc1b Juno v0.8.1
   929cbde3 LLVM v1.3.4
   093fc24a LightGraphs v1.3.1
   856f044c MKL_jll v2019.0.117+2
   1914dd2f MacroTools v0.5.4
   e89f7d12 Media v0.5.0
   626554b9 MetaGraphs v0.6.5
   e1d29d7a Missings v0.4.3
   872c559c NNlib v0.6.6
   77ba4419 NaNMath v0.3.3
   efe28fd5 OpenSpecFun_jll v0.5.3+2
   bac558e1 OrderedCollections v1.1.0
   189a3867 Reexport v0.2.0
   ae029012 Requires v1.0.1
   699a6c99 SimpleTraits v0.9.1
   47aef6b3 SimpleWeightedGraphs v1.1.1
   a2af1166 SortingAlgorithms v0.3.1
   276daf66 SpecialFunctions v0.10.0
   90137ffa StaticArrays v0.12.1
   2913bbd2 StatsBase v0.32.2
   a759f4b9 TimerOutputs v0.5.3
   3bb67fe8 TranscodingStreams v0.9.5
   a5390f91 ZipFile v0.9.1
   83775a58 Zlib_jll v1.2.11+8
   e88e6eb3 Zygote v0.4.9
   700de1a5 ZygoteRules v0.2.0
   2a0f44e3 Base64
   ade2ca70 Dates
   8bb1440f DelimitedFiles
   8ba89e20 Distributed
   b77e0a4c InteractiveUtils
   76f85450 LibGit2
   8f399da3 Libdl
   37e2e46d LinearAlgebra
   56ddb016 Logging
   d6f4376e Markdown
   a63ad114 Mmap
   44cfe95a Pkg
   de0858da Printf
   9abbd945 Profile
   3fa0cd96 REPL
   9a3f8284 Random
   ea8e919c SHA
   9e88b42a Serialization
   1a1011a3 SharedArrays
   6462fe0b Sockets
   2f01184e SparseArrays
   10745b16 Statistics
   8dfed614 Test
   cf7118a7 UUIDs
   4ec0a83e Unicode
WARNING: could not import Compiler.just_construct_ssa into Wrap
┌ Warning: CuArrays.jl found cuda, but did not find libcudnn. Some functionality will not be available.
└ @ Flux ~/.julia/packages/Flux/NpkMm/src/Flux.jl:48
┌ Warning: Incompatibility detected between CUDA and LLVM 8.0+; disabling debug info emission for CUDA kernels
└ @ CUDAnative ~/.julia/packages/CUDAnative/hfulr/src/CUDAnative.jl:114
┌ Warning: Your CUDA installation does not provide the CUPTI library, CUDAnative.@code_sass will be unavailable
└ @ CUDAnative ~/.julia/packages/CUDAnative/hfulr/src/CUDAnative.jl:160
┌ Warning: CuArrays.jl found cuda, but did not find libcudnn. Some functionality will not be available.
└ @ Flux ~/.julia/packages/Flux/NpkMm/src/Flux.jl:48
┌ Warning: Package GeometricFlux does not have LightGraphs in its dependencies:
│ - If you have GeometricFlux checked out for development and have
│   added LightGraphs as a dependency but haven't updated your primary
│   environment's manifest file, try `Pkg.resolve()`.
│ - Otherwise you may need to report an issue with GeometricFlux
└ Loading LightGraphs into GeometricFlux from project dependency, future warnings for GeometricFlux are suppressed.
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:15
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_add!(x, us, xs))
            end), ys) == (ones(2, 5),)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#3#4"{Array{Int64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/scatter.jl:41
   [2] (::GeometricFlux.var"#36#back#5"{GeometricFlux.var"#3#4"{Array{Int64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #4 at ./none:0 [inlined]
   [4] (::typeof(∂(#4)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#4))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Float64,2}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:15
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:15
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:16
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_add!(copy(ys), x, xs))
            end), us) == (ones(2, 3, 4),)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#3#4"{Array{Int64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/scatter.jl:41
   [2] (::GeometricFlux.var"#36#back#5"{GeometricFlux.var"#3#4"{Array{Int64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #5 at ./none:0 [inlined]
   [4] (::typeof(∂(#5)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#5))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Float64,3}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:16
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:15
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:17
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_add!(copy(ys), us, x))
            end), xs) == (nothing,)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#3#4"{Array{Int64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/scatter.jl:41
   [2] (::GeometricFlux.var"#36#back#5"{GeometricFlux.var"#3#4"{Array{Int64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #6 at ./none:0 [inlined]
   [4] (::typeof(∂(#6)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#6))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Int64,2}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:17
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:15
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:19
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_sub!(x, us, xs))
            end), ys) == (ones(2, 5),)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#7#8"{Array{Int64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/scatter.jl:47
   [2] (::GeometricFlux.var"#48#back#9"{GeometricFlux.var"#7#8"{Array{Int64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #7 at ./none:0 [inlined]
   [4] (::typeof(∂(#7)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#7))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Float64,2}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:19
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:15
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:20
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_sub!(copy(ys), x, xs))
            end), us) == (-(ones(2, 3, 4)),)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#7#8"{Array{Int64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/scatter.jl:47
   [2] (::GeometricFlux.var"#48#back#9"{GeometricFlux.var"#7#8"{Array{Int64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #8 at ./none:0 [inlined]
   [4] (::typeof(∂(#8)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#8))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Float64,3}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:20
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:15
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:21
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_sub!(copy(ys), us, x))
            end), xs) == (nothing,)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#7#8"{Array{Int64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/scatter.jl:47
   [2] (::GeometricFlux.var"#48#back#9"{GeometricFlux.var"#7#8"{Array{Int64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #9 at ./none:0 [inlined]
   [4] (::typeof(∂(#9)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#9))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Int64,2}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:21
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:15
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:23
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_max!(x, us, xs))
            end), ys) == (ones(2, 5),)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#27#28"{Array{Float64,2},Array{Float64,3},Array{Int64,2},Array{Float64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/scatter.jl:99
   [2] (::GeometricFlux.var"#90#back#29"{GeometricFlux.var"#27#28"{Array{Float64,2},Array{Float64,3},Array{Int64,2},Array{Float64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #10 at ./none:0 [inlined]
   [4] (::typeof(∂(#10)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#10))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Float64,2}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:23
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:15
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:24
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_max!(copy(ys), x, xs))
            end), us) == (zeros(2, 3, 4),)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#27#28"{Array{Float64,2},Array{Float64,3},Array{Int64,2},Array{Float64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/scatter.jl:99
   [2] (::GeometricFlux.var"#90#back#29"{GeometricFlux.var"#27#28"{Array{Float64,2},Array{Float64,3},Array{Int64,2},Array{Float64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #11 at ./none:0 [inlined]
   [4] (::typeof(∂(#11)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#11))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Float64,3}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:24
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:15
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:25
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_max!(copy(ys), us, x))
            end), xs) == (nothing,)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#27#28"{Array{Float64,2},Array{Float64,3},Array{Int64,2},Array{Float64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/scatter.jl:99
   [2] (::GeometricFlux.var"#90#back#29"{GeometricFlux.var"#27#28"{Array{Float64,2},Array{Float64,3},Array{Int64,2},Array{Float64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #12 at ./none:0 [inlined]
   [4] (::typeof(∂(#12)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#12))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Int64,2}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:25
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:15
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:27
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_min!(x, us, xs))
            end), ys) == (zeros(2, 5),)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#31#32"{Array{Float64,2},Array{Float64,3},Array{Int64,2},Array{Float64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/scatter.jl:109
   [2] (::GeometricFlux.var"#102#back#33"{GeometricFlux.var"#31#32"{Array{Float64,2},Array{Float64,3},Array{Int64,2},Array{Float64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #13 at ./none:0 [inlined]
   [4] (::typeof(∂(#13)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#13))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Float64,2}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:27
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:15
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:28
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_min!(copy(ys), x, xs))
            end), us) == (ones(2, 3, 4),)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#31#32"{Array{Float64,2},Array{Float64,3},Array{Int64,2},Array{Float64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/scatter.jl:109
   [2] (::GeometricFlux.var"#102#back#33"{GeometricFlux.var"#31#32"{Array{Float64,2},Array{Float64,3},Array{Int64,2},Array{Float64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #14 at ./none:0 [inlined]
   [4] (::typeof(∂(#14)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#14))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Float64,3}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:28
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:15
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:29
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_min!(copy(ys), us, x))
            end), xs) == (nothing,)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#31#32"{Array{Float64,2},Array{Float64,3},Array{Int64,2},Array{Float64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/scatter.jl:109
   [2] (::GeometricFlux.var"#102#back#33"{GeometricFlux.var"#31#32"{Array{Float64,2},Array{Float64,3},Array{Int64,2},Array{Float64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #15 at ./none:0 [inlined]
   [4] (::typeof(∂(#15)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#15))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Int64,2}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:29
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:15
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:31
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_mul!(x, us, xs))
            end), ys) == (∇y_mul,)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#11#14"{Array{Float64,2},Array{Float64,3},Array{Int64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/scatter.jl:57
   [2] (::GeometricFlux.var"#61#back#17"{GeometricFlux.var"#11#14"{Array{Float64,2},Array{Float64,3},Array{Int64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #16 at ./none:0 [inlined]
   [4] (::typeof(∂(#16)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#16))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Float64,2}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:31
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:15
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:32
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_mul!(copy(ys), x, xs))
            end), us) == (2048 * gather(ys, xs),)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#11#14"{Array{Float64,2},Array{Float64,3},Array{Int64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/scatter.jl:57
   [2] (::GeometricFlux.var"#61#back#17"{GeometricFlux.var"#11#14"{Array{Float64,2},Array{Float64,3},Array{Int64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #17 at ./none:0 [inlined]
   [4] (::typeof(∂(#17)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#17))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Float64,3}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:32
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:15
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:33
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_mul!(copy(ys), us, x))
            end), xs) == (nothing,)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#11#14"{Array{Float64,2},Array{Float64,3},Array{Int64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/scatter.jl:57
   [2] (::GeometricFlux.var"#61#back#17"{GeometricFlux.var"#11#14"{Array{Float64,2},Array{Float64,3},Array{Int64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #18 at ./none:0 [inlined]
   [4] (::typeof(∂(#18)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#18))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Int64,2}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:33
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:15
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:35
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_div!(x, us, xs))
            end), ys) == (∇y_div,)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#19#22"{Array{Float64,2},Array{Float64,3},Array{Int64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/scatter.jl:75
   [2] (::GeometricFlux.var"#77#back#25"{GeometricFlux.var"#19#22"{Array{Float64,2},Array{Float64,3},Array{Int64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #19 at ./none:0 [inlined]
   [4] (::typeof(∂(#19)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#19))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Float64,2}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:35
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:15
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:36
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_div!(copy(ys), x, xs))
            end), us) == (-(gather(ys, xs)) / 8192,)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#19#22"{Array{Float64,2},Array{Float64,3},Array{Int64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/scatter.jl:75
   [2] (::GeometricFlux.var"#77#back#25"{GeometricFlux.var"#19#22"{Array{Float64,2},Array{Float64,3},Array{Int64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #20 at ./none:0 [inlined]
   [4] (::typeof(∂(#20)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#20))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Float64,3}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:36
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:15
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:37
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_div!(copy(ys), us, x))
            end), xs) == (nothing,)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#19#22"{Array{Float64,2},Array{Float64,3},Array{Int64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/scatter.jl:75
   [2] (::GeometricFlux.var"#77#back#25"{GeometricFlux.var"#19#22"{Array{Float64,2},Array{Float64,3},Array{Int64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #21 at ./none:0 [inlined]
   [4] (::typeof(∂(#21)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#21))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Int64,2}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:37
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:15
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:39
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_mean!(x, us, xs))
            end), ys) == (ones(2, 5),)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#35#36"{Array{Float64,2},Array{Int64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/scatter.jl:118
   [2] (::GeometricFlux.var"#116#back#37"{GeometricFlux.var"#35#36"{Array{Float64,2},Array{Int64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #22 at ./none:0 [inlined]
   [4] (::typeof(∂(#22)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#22))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Float64,2}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:39
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:15
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:40
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_mean!(copy(ys), x, xs))
            end), us) == (∇u_mean,)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#35#36"{Array{Float64,2},Array{Int64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/scatter.jl:118
   [2] (::GeometricFlux.var"#116#back#37"{GeometricFlux.var"#35#36"{Array{Float64,2},Array{Int64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #23 at ./none:0 [inlined]
   [4] (::typeof(∂(#23)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#23))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Float64,3}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:40
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:15
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:41
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_mean!(copy(ys), us, x))
            end), xs) == (nothing,)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#35#36"{Array{Float64,2},Array{Int64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/scatter.jl:118
   [2] (::GeometricFlux.var"#116#back#37"{GeometricFlux.var"#35#36"{Array{Float64,2},Array{Int64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #24 at ./none:0 [inlined]
   [4] (::typeof(∂(#24)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#24))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Int64,2}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:41
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:15
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:45
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(sumpool(x, us))
            end), xs) == (nothing,)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#88#89"{Array{Int64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/layers/pool.jl:108
   [2] (::GeometricFlux.var"#170#back#90"{GeometricFlux.var"#88#89"{Array{Int64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #25 at ./none:0 [inlined]
   [4] (::typeof(∂(#25)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#25))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Int64,2}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:45
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:45
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:46
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(sumpool(xs, x))
            end), us) == (ones(2, 3, 4),)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#88#89"{Array{Int64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/layers/pool.jl:108
   [2] (::GeometricFlux.var"#170#back#90"{GeometricFlux.var"#88#89"{Array{Int64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #26 at ./none:0 [inlined]
   [4] (::typeof(∂(#26)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#26))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Float64,3}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:46
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:45
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:48
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(subpool(x, us))
            end), xs) == (nothing,)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#92#93"{Array{Int64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/layers/pool.jl:110
   [2] (::GeometricFlux.var"#182#back#94"{GeometricFlux.var"#92#93"{Array{Int64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #27 at ./none:0 [inlined]
   [4] (::typeof(∂(#27)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#27))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Int64,2}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:48
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:45
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:49
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(subpool(xs, x))
            end), us) == (-(ones(2, 3, 4)),)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#92#93"{Array{Int64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/layers/pool.jl:110
   [2] (::GeometricFlux.var"#182#back#94"{GeometricFlux.var"#92#93"{Array{Int64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #28 at ./none:0 [inlined]
   [4] (::typeof(∂(#28)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#28))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Float64,3}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:49
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:45
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:51
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(maxpool(x, us))
            end), xs) == (nothing,)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#112#113"{Array{Int64,2},Array{Float64,3},Array{Float64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/layers/pool.jl:143
   [2] (::GeometricFlux.var"#220#back#114"{GeometricFlux.var"#112#113"{Array{Int64,2},Array{Float64,3},Array{Float64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #29 at ./none:0 [inlined]
   [4] (::typeof(∂(#29)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#29))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Int64,2}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:51
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:45
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:52
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(maxpool(xs, x))
            end), us) == (ones(2, 3, 4),)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#112#113"{Array{Int64,2},Array{Float64,3},Array{Float64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/layers/pool.jl:143
   [2] (::GeometricFlux.var"#220#back#114"{GeometricFlux.var"#112#113"{Array{Int64,2},Array{Float64,3},Array{Float64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #30 at ./none:0 [inlined]
   [4] (::typeof(∂(#30)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#30))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Float64,3}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:52
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:45
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:54
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(minpool(x, us))
            end), xs) == (nothing,)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#116#117"{Array{Int64,2},Array{Float64,3},Array{Float64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/layers/pool.jl:151
   [2] (::GeometricFlux.var"#232#back#118"{GeometricFlux.var"#116#117"{Array{Int64,2},Array{Float64,3},Array{Float64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #31 at ./none:0 [inlined]
   [4] (::typeof(∂(#31)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#31))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Int64,2}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:54
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:45
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:55
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(minpool(xs, x))
            end), us) == (ones(2, 3, 4),)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#116#117"{Array{Int64,2},Array{Float64,3},Array{Float64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/layers/pool.jl:151
   [2] (::GeometricFlux.var"#232#back#118"{GeometricFlux.var"#116#117"{Array{Int64,2},Array{Float64,3},Array{Float64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #32 at ./none:0 [inlined]
   [4] (::typeof(∂(#32)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#32))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Float64,3}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:55
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:45
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:57
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(prodpool(x, us))
            end), xs) == (nothing,)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#96#99"{Array{Int64,2},Array{Float64,3}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/layers/pool.jl:115
   [2] (::GeometricFlux.var"#195#back#102"{GeometricFlux.var"#96#99"{Array{Int64,2},Array{Float64,3}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #33 at ./none:0 [inlined]
   [4] (::typeof(∂(#33)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#33))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Int64,2}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:57
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:45
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:58
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(prodpool(xs, x))
            end), us) == (2048 * ones(2, 3, 4),)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#96#99"{Array{Int64,2},Array{Float64,3}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/layers/pool.jl:115
   [2] (::GeometricFlux.var"#195#back#102"{GeometricFlux.var"#96#99"{Array{Int64,2},Array{Float64,3}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #34 at ./none:0 [inlined]
   [4] (::typeof(∂(#34)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#34))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Float64,3}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:58
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:45
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:60
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(divpool(x, us))
            end), xs) == (nothing,)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#104#107"{Array{Int64,2},Array{Float64,3}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/layers/pool.jl:129
   [2] (::GeometricFlux.var"#208#back#110"{GeometricFlux.var"#104#107"{Array{Int64,2},Array{Float64,3}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #35 at ./none:0 [inlined]
   [4] (::typeof(∂(#35)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#35))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Int64,2}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:60
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:45
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:61
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(divpool(xs, x))
            end), us) == (-(ones(2, 3, 4)) / 8192,)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#104#107"{Array{Int64,2},Array{Float64,3}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/layers/pool.jl:129
   [2] (::GeometricFlux.var"#208#back#110"{GeometricFlux.var"#104#107"{Array{Int64,2},Array{Float64,3}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #36 at ./none:0 [inlined]
   [4] (::typeof(∂(#36)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#36))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Float64,3}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:61
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:45
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:63
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(meanpool(x, us))
            end), xs) == (nothing,)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#120#121"{Array{Int64,2},Array{Float64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/layers/pool.jl:159
   [2] (::GeometricFlux.var"#246#back#122"{GeometricFlux.var"#120#121"{Array{Int64,2},Array{Float64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #37 at ./none:0 [inlined]
   [4] (::typeof(∂(#37)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#37))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Int64,2}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:63
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:45
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:64
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(meanpool(xs, x))
            end), us) == (∇u_mean,)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#120#121"{Array{Int64,2},Array{Float64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/layers/pool.jl:159
   [2] (::GeometricFlux.var"#246#back#122"{GeometricFlux.var"#120#121"{Array{Int64,2},Array{Float64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #38 at ./none:0 [inlined]
   [4] (::typeof(∂(#38)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#38))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Float64,3}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:64
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:45
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
add: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:17
  Test threw exception
  Expression: scatter_add!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:17 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:15 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
add: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:18
  Test threw exception
  Expression: scatter!(:add, T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:18 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:15 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
sub: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:24
  Test threw exception
  Expression: scatter_sub!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:24 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:22 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
sub: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:25
  Test threw exception
  Expression: scatter!(:sub, T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:25 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:22 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
max: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:31
  Test threw exception
  Expression: scatter_max!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:31 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:29 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
max: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:32
  Test threw exception
  Expression: scatter!(:max, T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:32 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:29 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
min: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:38
  Test threw exception
  Expression: scatter_min!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:38 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:36 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
min: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:39
  Test threw exception
  Expression: scatter!(:min, T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:39 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:36 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
add: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:17
  Test threw exception
  Expression: scatter_add!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:17 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:15 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
add: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:18
  Test threw exception
  Expression: scatter!(:add, T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:18 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:15 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
sub: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:24
  Test threw exception
  Expression: scatter_sub!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:24 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:22 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
sub: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:25
  Test threw exception
  Expression: scatter!(:sub, T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:25 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:22 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
max: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:31
  Test threw exception
  Expression: scatter_max!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:31 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:29 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
max: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:32
  Test threw exception
  Expression: scatter!(:max, T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:32 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:29 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
min: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:38
  Test threw exception
  Expression: scatter_min!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:38 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:36 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
min: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:39
  Test threw exception
  Expression: scatter!(:min, T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:39 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:36 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
add: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:17
  Test threw exception
  Expression: scatter_add!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:17 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:15 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
add: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:18
  Test threw exception
  Expression: scatter!(:add, T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:18 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:15 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
sub: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:24
  Test threw exception
  Expression: scatter_sub!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:24 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:22 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
sub: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:25
  Test threw exception
  Expression: scatter!(:sub, T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:25 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:22 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
max: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:31
  Test threw exception
  Expression: scatter_max!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:31 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:29 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
max: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:32
  Test threw exception
  Expression: scatter!(:max, T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:32 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:29 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
min: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:38
  Test threw exception
  Expression: scatter_min!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:38 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:36 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
min: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:39
  Test threw exception
  Expression: scatter!(:min, T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:39 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:36 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
add: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:17
  Test threw exception
  Expression: scatter_add!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:17 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:15 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
add: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:18
  Test threw exception
  Expression: scatter!(:add, T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:18 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:15 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
sub: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:24
  Test threw exception
  Expression: scatter_sub!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:24 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:22 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
sub: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:25
  Test threw exception
  Expression: scatter!(:sub, T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:25 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:22 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
max: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:31
  Test threw exception
  Expression: scatter_max!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:31 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:29 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
max: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:32
  Test threw exception
  Expression: scatter!(:max, T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:32 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:29 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
min: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:38
  Test threw exception
  Expression: scatter_min!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:38 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:36 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
min: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:39
  Test threw exception
  Expression: scatter!(:min, T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:39 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:36 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
add: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:50
  Test threw exception
  Expression: scatter_add!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:50 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:48 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
add: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:51
  Test threw exception
  Expression: scatter!(:add, T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:51 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:48 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
sub: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:57
  Test threw exception
  Expression: scatter_sub!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:57 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:55 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
sub: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:58
  Test threw exception
  Expression: scatter!(:sub, T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:58 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:55 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
max: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:64
  Test threw exception
  Expression: scatter_max!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:64 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:62 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
max: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:65
  Test threw exception
  Expression: scatter!(:max, T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:65 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:62 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
min: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:71
  Test threw exception
  Expression: scatter_min!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:71 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:69 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
min: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:72
  Test threw exception
  Expression: scatter!(:min, T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:72 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:69 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
mul: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:78
  Test threw exception
  Expression: scatter_mul!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:78 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:76 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
mul: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:79
  Test threw exception
  Expression: scatter!(:mul, T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:79 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:76 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
div: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:82
  Got exception outside of a @test
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},typeof(*),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}},Int64}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},typeof(*),Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}},Int64}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},typeof(*),Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}},Int64}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},typeof(*),Tuple{CuArray{Int64,3,Nothing},Int64}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(*),Tuple{CuArray{Int64,3,Nothing},Int64}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:83 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:83 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
   [27] include(::String) at ./client.jl:441
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/runtests.jl:52 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/runtests.jl:51
   [31] include(::String) at ./client.jl:441
   [32] top-level scope at none:6
   [33] eval(::Module, ::Any) at ./boot.jl:331
   [34] exec_options(::Base.JLOptions) at ./client.jl:264
   [35] _start() at ./client.jl:490
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
mean: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:93
  Test threw exception
  Expression: scatter_mean!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:93 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:91 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
mean: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:94
  Test threw exception
  Expression: scatter!(:mean, T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:94 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:91 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
add: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:50
  Test threw exception
  Expression: scatter_add!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:50 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:48 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
add: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:51
  Test threw exception
  Expression: scatter!(:add, T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:51 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:48 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
sub: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:57
  Test threw exception
  Expression: scatter_sub!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:57 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:55 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
sub: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:58
  Test threw exception
  Expression: scatter!(:sub, T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:58 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:55 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
max: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:64
  Test threw exception
  Expression: scatter_max!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:64 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:62 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
max: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:65
  Test threw exception
  Expression: scatter!(:max, T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:65 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:62 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
min: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:71
  Test threw exception
  Expression: scatter_min!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:71 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:69 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
min: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:72
  Test threw exception
  Expression: scatter!(:min, T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:72 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:69 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
mul: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:78
  Test threw exception
  Expression: scatter_mul!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:78 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:76 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
mul: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:79
  Test threw exception
  Expression: scatter!(:mul, T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:79 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:76 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
div: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:82
  Got exception outside of a @test
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},typeof(*),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}},Int64}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},typeof(*),Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}},Int64}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},typeof(*),Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}},Int64}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},typeof(*),Tuple{CuArray{Int64,3,Nothing},Int64}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(*),Tuple{CuArray{Int64,3,Nothing},Int64}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:83 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:83 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
   [27] include(::String) at ./client.jl:441
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/runtests.jl:52 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/runtests.jl:51
   [31] include(::String) at ./client.jl:441
   [32] top-level scope at none:6
   [33] eval(::Module, ::Any) at ./boot.jl:331
   [34] exec_options(::Base.JLOptions) at ./client.jl:264
   [35] _start() at ./client.jl:490
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
mean: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:93
  Test threw exception
  Expression: scatter_mean!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:93 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:91 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
mean: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:94
  Test threw exception
  Expression: scatter!(:mean, T.(copy(ys)), T.(us), xs) == T.(ys_)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [21] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:94 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [23] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:91 [inlined]
   [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
sumpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:12
  Test threw exception
  Expression: sumpool(CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{UInt32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{UInt32,3,Nothing}, ::Tuple{CuArray{UInt32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{UInt32,3,Nothing}, ::Tuple{CuArray{UInt32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{UInt32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:12 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:10 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:9 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
sumpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:13
  Test threw exception
  Expression: pool(:add, CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{UInt32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{UInt32,3,Nothing}, ::Tuple{CuArray{UInt32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{UInt32,3,Nothing}, ::Tuple{CuArray{UInt32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{UInt32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:13 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:10 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:9 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
sumpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:14
  Test threw exception
  Expression: sumpool(cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{UInt32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{UInt32,3,Nothing}, ::Tuple{CuArray{UInt32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{UInt32,3,Nothing}, ::Tuple{CuArray{UInt32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{UInt32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:14 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:10 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:9 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
sumpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:15
  Test threw exception
  Expression: pool(:add, cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{UInt32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{UInt32,3,Nothing}, ::Tuple{CuArray{UInt32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{UInt32,3,Nothing}, ::Tuple{CuArray{UInt32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{UInt32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:15 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:10 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:9 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
maxpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:21
  Test threw exception
  Expression: maxpool(CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{UInt32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{UInt32,3,Nothing}, ::Tuple{CuArray{UInt32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{UInt32,3,Nothing}, ::Tuple{CuArray{UInt32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{UInt32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:21 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:19 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:9 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
maxpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:22
  Test threw exception
  Expression: pool(:max, CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{UInt32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{UInt32,3,Nothing}, ::Tuple{CuArray{UInt32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{UInt32,3,Nothing}, ::Tuple{CuArray{UInt32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{UInt32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:22 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:19 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:9 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
maxpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:23
  Test threw exception
  Expression: maxpool(cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{UInt32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{UInt32,3,Nothing}, ::Tuple{CuArray{UInt32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{UInt32,3,Nothing}, ::Tuple{CuArray{UInt32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{UInt32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:23 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:19 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:9 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
maxpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:24
  Test threw exception
  Expression: pool(:max, cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{UInt32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{UInt32,3,Nothing}, ::Tuple{CuArray{UInt32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{UInt32,3,Nothing}, ::Tuple{CuArray{UInt32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{UInt32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:24 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:19 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:9 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
minpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:30
  Test threw exception
  Expression: minpool(CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{UInt32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{UInt32,3,Nothing}, ::Tuple{CuArray{UInt32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{UInt32,3,Nothing}, ::Tuple{CuArray{UInt32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{UInt32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:30 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:28 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:9 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
minpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:31
  Test threw exception
  Expression: pool(:min, CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{UInt32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{UInt32,3,Nothing}, ::Tuple{CuArray{UInt32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{UInt32,3,Nothing}, ::Tuple{CuArray{UInt32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{UInt32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:31 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:28 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:9 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
minpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:32
  Test threw exception
  Expression: minpool(cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{UInt32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{UInt32,3,Nothing}, ::Tuple{CuArray{UInt32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{UInt32,3,Nothing}, ::Tuple{CuArray{UInt32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{UInt32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:32 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:28 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:9 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
minpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:33
  Test threw exception
  Expression: pool(:min, cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{UInt32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{UInt32,3,Nothing}, ::Tuple{CuArray{UInt32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{UInt32,3,Nothing}, ::Tuple{CuArray{UInt32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{UInt32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:33 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:28 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:9 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
sumpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:12
  Test threw exception
  Expression: sumpool(CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{UInt64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{UInt64,3,Nothing}, ::Tuple{CuArray{UInt64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{UInt64,3,Nothing}, ::Tuple{CuArray{UInt64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{UInt64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:12 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:10 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:9 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
sumpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:13
  Test threw exception
  Expression: pool(:add, CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{UInt64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{UInt64,3,Nothing}, ::Tuple{CuArray{UInt64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{UInt64,3,Nothing}, ::Tuple{CuArray{UInt64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{UInt64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:13 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:10 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:9 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
sumpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:14
  Test threw exception
  Expression: sumpool(cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{UInt64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{UInt64,3,Nothing}, ::Tuple{CuArray{UInt64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{UInt64,3,Nothing}, ::Tuple{CuArray{UInt64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{UInt64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:14 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:10 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:9 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
sumpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:15
  Test threw exception
  Expression: pool(:add, cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{UInt64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{UInt64,3,Nothing}, ::Tuple{CuArray{UInt64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{UInt64,3,Nothing}, ::Tuple{CuArray{UInt64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{UInt64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:15 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:10 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:9 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
maxpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:21
  Test threw exception
  Expression: maxpool(CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{UInt64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{UInt64,3,Nothing}, ::Tuple{CuArray{UInt64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{UInt64,3,Nothing}, ::Tuple{CuArray{UInt64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{UInt64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:21 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:19 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:9 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
maxpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:22
  Test threw exception
  Expression: pool(:max, CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{UInt64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{UInt64,3,Nothing}, ::Tuple{CuArray{UInt64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{UInt64,3,Nothing}, ::Tuple{CuArray{UInt64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{UInt64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:22 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:19 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:9 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
maxpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:23
  Test threw exception
  Expression: maxpool(cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{UInt64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{UInt64,3,Nothing}, ::Tuple{CuArray{UInt64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{UInt64,3,Nothing}, ::Tuple{CuArray{UInt64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{UInt64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:23 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:19 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:9 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
maxpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:24
  Test threw exception
  Expression: pool(:max, cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{UInt64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{UInt64,3,Nothing}, ::Tuple{CuArray{UInt64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{UInt64,3,Nothing}, ::Tuple{CuArray{UInt64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{UInt64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:24 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:19 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:9 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
minpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:30
  Test threw exception
  Expression: minpool(CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{UInt64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{UInt64,3,Nothing}, ::Tuple{CuArray{UInt64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{UInt64,3,Nothing}, ::Tuple{CuArray{UInt64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{UInt64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:30 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:28 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:9 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
minpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:31
  Test threw exception
  Expression: pool(:min, CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{UInt64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{UInt64,3,Nothing}, ::Tuple{CuArray{UInt64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{UInt64,3,Nothing}, ::Tuple{CuArray{UInt64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{UInt64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:31 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:28 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:9 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
minpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:32
  Test threw exception
  Expression: minpool(cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{UInt64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{UInt64,3,Nothing}, ::Tuple{CuArray{UInt64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{UInt64,3,Nothing}, ::Tuple{CuArray{UInt64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{UInt64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:32 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:28 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:9 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
minpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:33
  Test threw exception
  Expression: pool(:min, cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{UInt64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{UInt64,3,Nothing}, ::Tuple{CuArray{UInt64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{UInt64,3,Nothing}, ::Tuple{CuArray{UInt64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{UInt64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:33 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:28 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:9 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
sumpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:43
  Test threw exception
  Expression: sumpool(CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:43 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:41 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
sumpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:44
  Test threw exception
  Expression: pool(:add, CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:44 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:41 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
sumpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:45
  Test threw exception
  Expression: sumpool(cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:45 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:41 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
sumpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:46
  Test threw exception
  Expression: pool(:add, cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:46 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:41 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
subpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:52
  Test threw exception
  Expression: subpool(CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:52 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:50 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
subpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:53
  Test threw exception
  Expression: pool(:sub, CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:53 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:50 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
subpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:54
  Test threw exception
  Expression: subpool(cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:54 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:50 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
subpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:55
  Test threw exception
  Expression: pool(:sub, cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:55 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:50 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
maxpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:61
  Test threw exception
  Expression: maxpool(CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:61 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:59 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
maxpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:62
  Test threw exception
  Expression: pool(:max, CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:62 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:59 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
maxpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:63
  Test threw exception
  Expression: maxpool(cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:63 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:59 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
maxpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:64
  Test threw exception
  Expression: pool(:max, cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:64 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:59 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
minpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:70
  Test threw exception
  Expression: minpool(CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:70 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:68 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
minpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:71
  Test threw exception
  Expression: pool(:min, CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:71 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:68 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
minpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:72
  Test threw exception
  Expression: minpool(cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:72 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:68 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
minpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:73
  Test threw exception
  Expression: pool(:min, cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:73 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:68 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
sumpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:43
  Test threw exception
  Expression: sumpool(CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:43 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:41 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
sumpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:44
  Test threw exception
  Expression: pool(:add, CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:44 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:41 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
sumpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:45
  Test threw exception
  Expression: sumpool(cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:45 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:41 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
sumpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:46
  Test threw exception
  Expression: pool(:add, cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:46 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:41 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
subpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:52
  Test threw exception
  Expression: subpool(CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:52 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:50 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
subpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:53
  Test threw exception
  Expression: pool(:sub, CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:53 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:50 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
subpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:54
  Test threw exception
  Expression: subpool(cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:54 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:50 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
subpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:55
  Test threw exception
  Expression: pool(:sub, cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:55 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:50 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
maxpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:61
  Test threw exception
  Expression: maxpool(CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:61 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:59 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
maxpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:62
  Test threw exception
  Expression: pool(:max, CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:62 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:59 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
maxpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:63
  Test threw exception
  Expression: maxpool(cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:63 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:59 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
maxpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:64
  Test threw exception
  Expression: pool(:max, cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:64 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:59 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
minpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:70
  Test threw exception
  Expression: minpool(CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:70 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:68 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
minpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:71
  Test threw exception
  Expression: pool(:min, CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:71 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:68 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
minpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:72
  Test threw exception
  Expression: minpool(cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:72 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:68 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
minpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:73
  Test threw exception
  Expression: pool(:min, cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:73 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:68 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
sumpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:83
  Test threw exception
  Expression: sumpool(CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:83 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:81 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
sumpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:84
  Test threw exception
  Expression: pool(:add, CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:84 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:81 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
sumpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:85
  Test threw exception
  Expression: sumpool(cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:85 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:81 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
sumpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:86
  Test threw exception
  Expression: pool(:add, cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:86 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:81 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
subpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:92
  Test threw exception
  Expression: subpool(CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:92 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:90 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
subpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:93
  Test threw exception
  Expression: pool(:sub, CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:93 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:90 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
subpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:94
  Test threw exception
  Expression: subpool(cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:94 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:90 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
subpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:95
  Test threw exception
  Expression: pool(:sub, cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:95 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:90 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
maxpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:101
  Test threw exception
  Expression: maxpool(CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:101 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:99 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
maxpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:102
  Test threw exception
  Expression: pool(:max, CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:102 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:99 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
maxpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:103
  Test threw exception
  Expression: maxpool(cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:103 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:99 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
maxpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:104
  Test threw exception
  Expression: pool(:max, cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:104 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:99 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
minpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:110
  Test threw exception
  Expression: minpool(CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:110 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:108 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
minpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:111
  Test threw exception
  Expression: pool(:min, CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:111 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:108 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
minpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:112
  Test threw exception
  Expression: minpool(cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:112 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:108 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
minpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:113
  Test threw exception
  Expression: pool(:min, cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:113 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:108 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
prodpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:119
  Test threw exception
  Expression: prodpool(CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:119 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:117 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
prodpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:120
  Test threw exception
  Expression: pool(:mul, CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:120 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:117 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
prodpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:121
  Test threw exception
  Expression: prodpool(cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:121 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:117 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
prodpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:122
  Test threw exception
  Expression: pool(:mul, cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:122 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:117 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
divpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:128
  Test threw exception
  Expression: divpool(CuArray{Int64}(cluster), T.(X)) ≈ T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:128 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:126 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
divpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:129
  Test threw exception
  Expression: pool(:div, CuArray{Int64}(cluster), T.(X)) ≈ T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:129 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:126 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
divpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:130
  Test threw exception
  Expression: divpool(cluster, T.(X)) ≈ T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:130 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:126 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
divpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:131
  Test threw exception
  Expression: pool(:div, cluster, T.(X)) ≈ T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:131 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:126 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
meanpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:137
  Test threw exception
  Expression: meanpool(CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:137 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:135 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
meanpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:138
  Test threw exception
  Expression: pool(:mean, CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:138 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:135 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
meanpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:139
  Test threw exception
  Expression: meanpool(cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:139 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:135 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
meanpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:140
  Test threw exception
  Expression: pool(:mean, cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:140 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:135 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
sumpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:83
  Test threw exception
  Expression: sumpool(CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:83 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:81 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
sumpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:84
  Test threw exception
  Expression: pool(:add, CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:84 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:81 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
sumpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:85
  Test threw exception
  Expression: sumpool(cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:85 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:81 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
sumpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:86
  Test threw exception
  Expression: pool(:add, cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:86 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:81 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
subpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:92
  Test threw exception
  Expression: subpool(CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:92 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:90 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
subpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:93
  Test threw exception
  Expression: pool(:sub, CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:93 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:90 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
subpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:94
  Test threw exception
  Expression: subpool(cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:94 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:90 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
subpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:95
  Test threw exception
  Expression: pool(:sub, cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:95 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:90 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
maxpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:101
  Test threw exception
  Expression: maxpool(CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:101 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:99 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
maxpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:102
  Test threw exception
  Expression: pool(:max, CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:102 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:99 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
maxpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:103
  Test threw exception
  Expression: maxpool(cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:103 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:99 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
maxpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:104
  Test threw exception
  Expression: pool(:max, cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:104 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:99 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
minpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:110
  Test threw exception
  Expression: minpool(CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:110 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:108 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
minpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:111
  Test threw exception
  Expression: pool(:min, CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:111 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:108 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
minpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:112
  Test threw exception
  Expression: minpool(cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:112 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:108 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
minpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:113
  Test threw exception
  Expression: pool(:min, cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:113 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:108 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
prodpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:119
  Test threw exception
  Expression: prodpool(CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:119 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:117 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
prodpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:120
  Test threw exception
  Expression: pool(:mul, CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:120 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:117 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
prodpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:121
  Test threw exception
  Expression: prodpool(cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:121 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:117 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
prodpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:122
  Test threw exception
  Expression: pool(:mul, cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:122 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:117 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
divpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:128
  Test threw exception
  Expression: divpool(CuArray{Int64}(cluster), T.(X)) ≈ T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:128 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:126 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
divpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:129
  Test threw exception
  Expression: pool(:div, CuArray{Int64}(cluster), T.(X)) ≈ T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:129 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:126 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
divpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:130
  Test threw exception
  Expression: divpool(cluster, T.(X)) ≈ T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:130 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:126 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
divpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:131
  Test threw exception
  Expression: pool(:div, cluster, T.(X)) ≈ T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:131 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:126 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
meanpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:137
  Test threw exception
  Expression: meanpool(CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:137 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:135 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
meanpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:138
  Test threw exception
  Expression: pool(:mean, CuArray{Int64}(cluster), T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:138 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:135 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
meanpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:139
  Test threw exception
  Expression: meanpool(cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:139 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:135 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
meanpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:140
  Test threw exception
  Expression: pool(:mean, cluster, T.(X)) == T.(y)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float64,3,Nothing}, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [17] copyto! at ./broadcast.jl:864 [inlined]
   [18] copy at ./broadcast.jl:840 [inlined]
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [20] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:140 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [22] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:135 [inlined]
   [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:17
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_add!(x, us, xs))
            end), ys) == (ones(2, 5),)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Float32,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Float32,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180 [inlined]
   [21] adjoint at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/scatter.jl:39 [inlined]
   [22] _pullback(::Zygote.Context, ::typeof(scatter_add!), ::CuArray{Float32,2,Nothing}, ::CuArray{Float32,3,Nothing}, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:47
   [23] #74 at ./none:0 [inlined]
   [24] _pullback(::Zygote.Context, ::var"#74#109", ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [25] _pullback(::Function, ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:29
   [26] pullback(::Function, ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:35
   [27] gradient(::Function, ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:44
   [28] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:17
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [30] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:17
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:16
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:18
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_add!(copy(ys), x, xs))
            end), us) == (ones(2, 3, 4),)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Float32,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Float32,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180 [inlined]
   [21] adjoint at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/lib/array.jl:17 [inlined]
   [22] _pullback(::Zygote.Context, ::typeof(copy), ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:47
   [23] #75 at ./none:0 [inlined]
   [24] _pullback(::Zygote.Context, ::var"#75#110", ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [25] _pullback(::Function, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:29
   [26] pullback(::Function, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:35
   [27] gradient(::Function, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:44
   [28] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:18
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [30] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:17
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:16
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:19
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_add!(copy(ys), us, x))
            end), xs) == (nothing,)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Float32,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Float32,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180 [inlined]
   [21] adjoint at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/lib/array.jl:17 [inlined]
   [22] _pullback(::Zygote.Context, ::typeof(copy), ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:47
   [23] #76 at ./none:0 [inlined]
   [24] _pullback(::Zygote.Context, ::var"#76#111", ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [25] _pullback(::Function, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:29
   [26] pullback(::Function, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:35
   [27] gradient(::Function, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:44
   [28] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:19
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [30] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:17
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:16
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:21
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_sub!(x, us, xs))
            end), ys) == (ones(2, 5),)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Float32,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Float32,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180 [inlined]
   [21] adjoint at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/scatter.jl:45 [inlined]
   [22] _pullback(::Zygote.Context, ::typeof(scatter_sub!), ::CuArray{Float32,2,Nothing}, ::CuArray{Float32,3,Nothing}, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:47
   [23] #77 at ./none:0 [inlined]
   [24] _pullback(::Zygote.Context, ::var"#77#112", ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [25] _pullback(::Function, ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:29
   [26] pullback(::Function, ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:35
   [27] gradient(::Function, ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:44
   [28] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:21
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [30] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:17
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:16
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:22
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_sub!(copy(ys), x, xs))
            end), us) == (-(ones(2, 3, 4)),)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Float32,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Float32,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180 [inlined]
   [21] adjoint at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/lib/array.jl:17 [inlined]
   [22] _pullback(::Zygote.Context, ::typeof(copy), ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:47
   [23] #78 at ./none:0 [inlined]
   [24] _pullback(::Zygote.Context, ::var"#78#113", ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [25] _pullback(::Function, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:29
   [26] pullback(::Function, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:35
   [27] gradient(::Function, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:44
   [28] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:22
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [30] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:17
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:16
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:23
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_sub!(copy(ys), us, x))
            end), xs) == (nothing,)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Float32,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Float32,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180 [inlined]
   [21] adjoint at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/lib/array.jl:17 [inlined]
   [22] _pullback(::Zygote.Context, ::typeof(copy), ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:47
   [23] #79 at ./none:0 [inlined]
   [24] _pullback(::Zygote.Context, ::var"#79#114", ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [25] _pullback(::Function, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:29
   [26] pullback(::Function, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:35
   [27] gradient(::Function, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:44
   [28] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:23
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [30] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:17
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:16
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:25
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_max!(x, us, xs))
            end), ys) == (ones(2, 5),)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Float32,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Float32,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180 [inlined]
   [21] adjoint at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/scatter.jl:161 [inlined]
   [22] _pullback(::Zygote.Context, ::typeof(scatter_max!), ::CuArray{Float32,2,Nothing}, ::CuArray{Float32,3,Nothing}, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:47
   [23] #80 at ./none:0 [inlined]
   [24] _pullback(::Zygote.Context, ::var"#80#115", ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [25] _pullback(::Function, ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:29
   [26] pullback(::Function, ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:35
   [27] gradient(::Function, ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:44
   [28] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:25
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [30] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:17
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:16
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:26
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_max!(copy(ys), x, xs))
            end), us) == (zeros(2, 3, 4),)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Float32,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Float32,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180 [inlined]
   [21] adjoint at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/lib/array.jl:17 [inlined]
   [22] _pullback(::Zygote.Context, ::typeof(copy), ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:47
   [23] #81 at ./none:0 [inlined]
   [24] _pullback(::Zygote.Context, ::var"#81#116", ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [25] _pullback(::Function, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:29
   [26] pullback(::Function, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:35
   [27] gradient(::Function, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:44
   [28] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:26
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [30] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:17
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:16
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:27
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_max!(copy(ys), us, x))
            end), xs) == (nothing,)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Float32,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Float32,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180 [inlined]
   [21] adjoint at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/lib/array.jl:17 [inlined]
   [22] _pullback(::Zygote.Context, ::typeof(copy), ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:47
   [23] #82 at ./none:0 [inlined]
   [24] _pullback(::Zygote.Context, ::var"#82#117", ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [25] _pullback(::Function, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:29
   [26] pullback(::Function, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:35
   [27] gradient(::Function, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:44
   [28] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:27
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [30] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:17
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:16
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:29
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_min!(x, us, xs))
            end), ys) == (zeros(2, 5),)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Float32,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Float32,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180 [inlined]
   [21] adjoint at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/scatter.jl:173 [inlined]
   [22] _pullback(::Zygote.Context, ::typeof(scatter_min!), ::CuArray{Float32,2,Nothing}, ::CuArray{Float32,3,Nothing}, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:47
   [23] #83 at ./none:0 [inlined]
   [24] _pullback(::Zygote.Context, ::var"#83#118", ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [25] _pullback(::Function, ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:29
   [26] pullback(::Function, ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:35
   [27] gradient(::Function, ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:44
   [28] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:29
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [30] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:17
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:16
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:30
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_min!(copy(ys), x, xs))
            end), us) == (ones(2, 3, 4),)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Float32,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Float32,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180 [inlined]
   [21] adjoint at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/lib/array.jl:17 [inlined]
   [22] _pullback(::Zygote.Context, ::typeof(copy), ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:47
   [23] #84 at ./none:0 [inlined]
   [24] _pullback(::Zygote.Context, ::var"#84#119", ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [25] _pullback(::Function, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:29
   [26] pullback(::Function, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:35
   [27] gradient(::Function, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:44
   [28] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:30
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [30] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:17
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:16
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:31
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_min!(copy(ys), us, x))
            end), xs) == (nothing,)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Float32,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Float32,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180 [inlined]
   [21] adjoint at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/lib/array.jl:17 [inlined]
   [22] _pullback(::Zygote.Context, ::typeof(copy), ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:47
   [23] #85 at ./none:0 [inlined]
   [24] _pullback(::Zygote.Context, ::var"#85#120", ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [25] _pullback(::Function, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:29
   [26] pullback(::Function, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:35
   [27] gradient(::Function, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:44
   [28] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:31
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [30] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:17
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:16
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:33
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_mul!(x, us, xs))
            end), ys) == (∇y_mul,)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Float32,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Float32,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180 [inlined]
   [21] adjoint at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/scatter.jl:115 [inlined]
   [22] _pullback(::Zygote.Context, ::typeof(scatter_mul!), ::CuArray{Float32,2,Nothing}, ::CuArray{Float32,3,Nothing}, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:47
   [23] #86 at ./none:0 [inlined]
   [24] _pullback(::Zygote.Context, ::var"#86#121", ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [25] _pullback(::Function, ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:29
   [26] pullback(::Function, ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:35
   [27] gradient(::Function, ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:44
   [28] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:33
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [30] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:17
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:16
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:34
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_mul!(copy(ys), x, xs))
            end), us) == (2048 * gather(ys, xs),)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Float32,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Float32,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180 [inlined]
   [21] adjoint at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/lib/array.jl:17 [inlined]
   [22] _pullback(::Zygote.Context, ::typeof(copy), ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:47
   [23] #87 at ./none:0 [inlined]
   [24] _pullback(::Zygote.Context, ::var"#87#122", ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [25] _pullback(::Function, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:29
   [26] pullback(::Function, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:35
   [27] gradient(::Function, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:44
   [28] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:34
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [30] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:17
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:16
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:35
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_mul!(copy(ys), us, x))
            end), xs) == (nothing,)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Float32,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Float32,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180 [inlined]
   [21] adjoint at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/lib/array.jl:17 [inlined]
   [22] _pullback(::Zygote.Context, ::typeof(copy), ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:47
   [23] #88 at ./none:0 [inlined]
   [24] _pullback(::Zygote.Context, ::var"#88#123", ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [25] _pullback(::Function, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:29
   [26] pullback(::Function, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:35
   [27] gradient(::Function, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:44
   [28] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:35
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [30] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:17
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:16
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:37
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_div!(x, us, xs))
            end), ys) == (∇y_div,)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Float32,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Float32,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180 [inlined]
   [21] adjoint at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/scatter.jl:134 [inlined]
   [22] _pullback(::Zygote.Context, ::typeof(scatter_div!), ::CuArray{Float32,2,Nothing}, ::CuArray{Float32,3,Nothing}, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:47
   [23] #89 at ./none:0 [inlined]
   [24] _pullback(::Zygote.Context, ::var"#89#124", ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [25] _pullback(::Function, ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:29
   [26] pullback(::Function, ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:35
   [27] gradient(::Function, ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:44
   [28] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:37
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [30] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:17
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:16
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:38
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_div!(copy(ys), x, xs))
            end), us) == (-(gather(ys, xs)) / 8192,)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Float32,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Float32,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180 [inlined]
   [21] adjoint at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/lib/array.jl:17 [inlined]
   [22] _pullback(::Zygote.Context, ::typeof(copy), ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:47
   [23] #90 at ./none:0 [inlined]
   [24] _pullback(::Zygote.Context, ::var"#90#125", ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [25] _pullback(::Function, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:29
   [26] pullback(::Function, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:35
   [27] gradient(::Function, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:44
   [28] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:38
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [30] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:17
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:16
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:39
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_div!(copy(ys), us, x))
            end), xs) == (nothing,)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Float32,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Float32,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180 [inlined]
   [21] adjoint at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/lib/array.jl:17 [inlined]
   [22] _pullback(::Zygote.Context, ::typeof(copy), ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:47
   [23] #91 at ./none:0 [inlined]
   [24] _pullback(::Zygote.Context, ::var"#91#126", ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [25] _pullback(::Function, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:29
   [26] pullback(::Function, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:35
   [27] gradient(::Function, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:44
   [28] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:39
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [30] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:17
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:16
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:41
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_mean!(x, us, xs))
            end), ys) == (ones(2, 5),)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Float32,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Float32,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180 [inlined]
   [21] adjoint at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/scatter.jl:115 [inlined]
   [22] _pullback(::Zygote.Context, ::typeof(scatter_mean!), ::CuArray{Float32,2,Nothing}, ::CuArray{Float32,3,Nothing}, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:47
   [23] #92 at ./none:0 [inlined]
   [24] _pullback(::Zygote.Context, ::var"#92#127", ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [25] _pullback(::Function, ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:29
   [26] pullback(::Function, ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:35
   [27] gradient(::Function, ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:44
   [28] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:41
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [30] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:17
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:16
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:42
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_mean!(copy(ys), x, xs))
            end), us) == (∇u_mean,)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Float32,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Float32,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180 [inlined]
   [21] adjoint at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/lib/array.jl:17 [inlined]
   [22] _pullback(::Zygote.Context, ::typeof(copy), ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:47
   [23] #93 at ./none:0 [inlined]
   [24] _pullback(::Zygote.Context, ::var"#93#128", ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [25] _pullback(::Function, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:29
   [26] pullback(::Function, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:35
   [27] gradient(::Function, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:44
   [28] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:42
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [30] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:17
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:16
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:43
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_mean!(copy(ys), us, x))
            end), xs) == (nothing,)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] copyto!(::CuArray{Float32,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [17] copyto!(::CuArray{Float32,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:864
   [18] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:840
   [19] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Float32,2,Nothing}}}) at ./broadcast.jl:820
   [20] copy at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180 [inlined]
   [21] adjoint at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/lib/array.jl:17 [inlined]
   [22] _pullback(::Zygote.Context, ::typeof(copy), ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:47
   [23] #94 at ./none:0 [inlined]
   [24] _pullback(::Zygote.Context, ::var"#94#129", ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [25] _pullback(::Function, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:29
   [26] pullback(::Function, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:35
   [27] gradient(::Function, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:44
   [28] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:43
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [30] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:17
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:16
  
┌ Warning: Performing scalar operations on GPU arrays: This is very slow, consider disallowing these operations with `allowscalar(false)`
└ @ GPUArrays ~/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:16
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:47
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(sumpool(x, us))
            end), xs) == (nothing,)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GeometricFlux.var"#kernel!#170"{Float32}, ::Type{Tuple{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] scatter_add!(::CuArray{Float32,2,Nothing}, ::CuArray{Float32,3,Nothing}, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/scatter.jl:75
   [14] sumpool(::CuArray{Int64,2,Nothing}, ::CuArray{Float32,3,Nothing}, ::Int64) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/pool.jl:5
   [15] sumpool at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/pool.jl:3 [inlined]
   [16] adjoint at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/layers/pool.jl:107 [inlined]
   [17] _pullback(::Zygote.Context, ::typeof(sumpool), ::CuArray{Int64,2,Nothing}, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:47
   [18] #95 at ./none:0 [inlined]
   [19] _pullback(::Zygote.Context, ::var"#95#130", ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [20] _pullback(::Function, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:29
   [21] pullback(::Function, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:35
   [22] gradient(::Function, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:44
   [23] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:47
   [24] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [25] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:47
   [26] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:16
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:48
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(sumpool(xs, x))
            end), us) == (ones(2, 3, 4),)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GeometricFlux.var"#kernel!#170"{Float32}, ::Type{Tuple{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] scatter_add!(::CuArray{Float32,2,Nothing}, ::CuArray{Float32,3,Nothing}, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/scatter.jl:75
   [14] sumpool(::CuArray{Int64,2,Nothing}, ::CuArray{Float32,3,Nothing}, ::Int64) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/pool.jl:5
   [15] sumpool at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/pool.jl:3 [inlined]
   [16] adjoint at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/layers/pool.jl:107 [inlined]
   [17] _pullback(::Zygote.Context, ::typeof(sumpool), ::CuArray{Int64,2,Nothing}, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:47
   [18] #96 at ./none:0 [inlined]
   [19] _pullback(::Zygote.Context, ::var"#96#131", ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [20] _pullback(::Function, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:29
   [21] pullback(::Function, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:35
   [22] gradient(::Function, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:44
   [23] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:48
   [24] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [25] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:47
   [26] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:16
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:50
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(subpool(x, us))
            end), xs) == (nothing,)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GeometricFlux.var"#kernel!#172"{Float32}, ::Type{Tuple{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] scatter_sub!(::CuArray{Float32,2,Nothing}, ::CuArray{Float32,3,Nothing}, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/scatter.jl:75
   [14] subpool(::CuArray{Int64,2,Nothing}, ::CuArray{Float32,3,Nothing}, ::Int64) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/pool.jl:16
   [15] subpool at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/pool.jl:14 [inlined]
   [16] adjoint at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/layers/pool.jl:109 [inlined]
   [17] _pullback(::Zygote.Context, ::typeof(subpool), ::CuArray{Int64,2,Nothing}, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:47
   [18] #97 at ./none:0 [inlined]
   [19] _pullback(::Zygote.Context, ::var"#97#132", ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [20] _pullback(::Function, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:29
   [21] pullback(::Function, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:35
   [22] gradient(::Function, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:44
   [23] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:50
   [24] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [25] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:47
   [26] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:16
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:51
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(subpool(xs, x))
            end), us) == (-(ones(2, 3, 4)),)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GeometricFlux.var"#kernel!#172"{Float32}, ::Type{Tuple{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] scatter_sub!(::CuArray{Float32,2,Nothing}, ::CuArray{Float32,3,Nothing}, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/scatter.jl:75
   [14] subpool(::CuArray{Int64,2,Nothing}, ::CuArray{Float32,3,Nothing}, ::Int64) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/pool.jl:16
   [15] subpool at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/pool.jl:14 [inlined]
   [16] adjoint at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/layers/pool.jl:109 [inlined]
   [17] _pullback(::Zygote.Context, ::typeof(subpool), ::CuArray{Int64,2,Nothing}, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:47
   [18] #98 at ./none:0 [inlined]
   [19] _pullback(::Zygote.Context, ::var"#98#133", ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [20] _pullback(::Function, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:29
   [21] pullback(::Function, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:35
   [22] gradient(::Function, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:44
   [23] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:51
   [24] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [25] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:47
   [26] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:16
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:53
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(maxpool(x, us))
            end), xs) == (nothing,)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GeometricFlux.var"#kernel!#178"{Float32}, ::Type{Tuple{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] scatter_max!(::CuArray{Float32,2,Nothing}, ::CuArray{Float32,3,Nothing}, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/scatter.jl:75
   [14] maxpool(::CuArray{Int64,2,Nothing}, ::CuArray{Float32,3,Nothing}, ::Int64) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/pool.jl:50
   [15] maxpool at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/pool.jl:48 [inlined]
   [16] adjoint at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/pool.jl:112 [inlined]
   [17] _pullback(::Zygote.Context, ::typeof(maxpool), ::CuArray{Int64,2,Nothing}, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:47
   [18] #99 at ./none:0 [inlined]
   [19] _pullback(::Zygote.Context, ::var"#99#134", ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [20] _pullback(::Function, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:29
   [21] pullback(::Function, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:35
   [22] gradient(::Function, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:44
   [23] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:53
   [24] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [25] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:47
   [26] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:16
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:54
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(maxpool(xs, x))
            end), us) == (ones(2, 3, 4),)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GeometricFlux.var"#kernel!#178"{Float32}, ::Type{Tuple{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] scatter_max!(::CuArray{Float32,2,Nothing}, ::CuArray{Float32,3,Nothing}, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/scatter.jl:75
   [14] maxpool(::CuArray{Int64,2,Nothing}, ::CuArray{Float32,3,Nothing}, ::Int64) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/pool.jl:50
   [15] maxpool at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/pool.jl:48 [inlined]
   [16] adjoint at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/pool.jl:112 [inlined]
   [17] _pullback(::Zygote.Context, ::typeof(maxpool), ::CuArray{Int64,2,Nothing}, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:47
   [18] #100 at ./none:0 [inlined]
   [19] _pullback(::Zygote.Context, ::var"#100#135", ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [20] _pullback(::Function, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:29
   [21] pullback(::Function, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:35
   [22] gradient(::Function, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:44
   [23] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:54
   [24] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [25] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:47
   [26] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:16
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:56
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(minpool(x, us))
            end), xs) == (nothing,)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GeometricFlux.var"#kernel!#180"{Float32}, ::Type{Tuple{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] scatter_min!(::CuArray{Float32,2,Nothing}, ::CuArray{Float32,3,Nothing}, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/scatter.jl:75
   [14] minpool(::CuArray{Int64,2,Nothing}, ::CuArray{Float32,3,Nothing}, ::Int64) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/pool.jl:61
   [15] minpool at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/pool.jl:59 [inlined]
   [16] adjoint at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/pool.jl:121 [inlined]
   [17] _pullback(::Zygote.Context, ::typeof(minpool), ::CuArray{Int64,2,Nothing}, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:47
   [18] #101 at ./none:0 [inlined]
   [19] _pullback(::Zygote.Context, ::var"#101#136", ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [20] _pullback(::Function, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:29
   [21] pullback(::Function, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:35
   [22] gradient(::Function, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:44
   [23] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:56
   [24] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [25] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:47
   [26] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:16
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:57
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(minpool(xs, x))
            end), us) == (ones(2, 3, 4),)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GeometricFlux.var"#kernel!#180"{Float32}, ::Type{Tuple{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] scatter_min!(::CuArray{Float32,2,Nothing}, ::CuArray{Float32,3,Nothing}, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/scatter.jl:75
   [14] minpool(::CuArray{Int64,2,Nothing}, ::CuArray{Float32,3,Nothing}, ::Int64) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/pool.jl:61
   [15] minpool at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/pool.jl:59 [inlined]
   [16] adjoint at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/pool.jl:121 [inlined]
   [17] _pullback(::Zygote.Context, ::typeof(minpool), ::CuArray{Int64,2,Nothing}, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:47
   [18] #102 at ./none:0 [inlined]
   [19] _pullback(::Zygote.Context, ::var"#102#137", ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [20] _pullback(::Function, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:29
   [21] pullback(::Function, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:35
   [22] gradient(::Function, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:44
   [23] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:57
   [24] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [25] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:47
   [26] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:16
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:59
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(prodpool(x, us))
            end), xs) == (nothing,)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GeometricFlux.var"#kernel!#174"{Float32}, ::Type{Tuple{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] scatter_mul!(::CuArray{Float32,2,Nothing}, ::CuArray{Float32,3,Nothing}, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/scatter.jl:75
   [14] prodpool(::CuArray{Int64,2,Nothing}, ::CuArray{Float32,3,Nothing}, ::Int64) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/pool.jl:27
   [15] prodpool at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/pool.jl:25 [inlined]
   [16] adjoint at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/pool.jl:81 [inlined]
   [17] _pullback(::Zygote.Context, ::typeof(prodpool), ::CuArray{Int64,2,Nothing}, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:47
   [18] #103 at ./none:0 [inlined]
   [19] _pullback(::Zygote.Context, ::var"#103#138", ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [20] _pullback(::Function, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:29
   [21] pullback(::Function, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:35
   [22] gradient(::Function, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:44
   [23] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:59
   [24] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [25] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:47
   [26] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:16
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:60
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(prodpool(xs, x))
            end), us) == (2048 * ones(2, 3, 4),)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GeometricFlux.var"#kernel!#174"{Float32}, ::Type{Tuple{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] scatter_mul!(::CuArray{Float32,2,Nothing}, ::CuArray{Float32,3,Nothing}, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/scatter.jl:75
   [14] prodpool(::CuArray{Int64,2,Nothing}, ::CuArray{Float32,3,Nothing}, ::Int64) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/pool.jl:27
   [15] prodpool at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/pool.jl:25 [inlined]
   [16] adjoint at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/pool.jl:81 [inlined]
   [17] _pullback(::Zygote.Context, ::typeof(prodpool), ::CuArray{Int64,2,Nothing}, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:47
   [18] #104 at ./none:0 [inlined]
   [19] _pullback(::Zygote.Context, ::var"#104#139", ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [20] _pullback(::Function, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:29
   [21] pullback(::Function, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:35
   [22] gradient(::Function, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:44
   [23] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:60
   [24] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [25] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:47
   [26] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [27] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/grad.jl:16
  

signal (15): Terminated
in expression starting at none:13
