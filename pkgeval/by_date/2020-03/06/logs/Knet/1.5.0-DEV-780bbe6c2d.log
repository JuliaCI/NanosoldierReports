Julia Version 1.5.0-DEV.399
Commit 780bbe6c2d (2020-03-04 16:59 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-9.0.1 (ORCJIT, skylake)
Environment:
  JULIA_DEPOT_PATH = ::/usr/local/share/julia
  JULIA_NUM_THREADS = 2

  Resolving package versions...
  Installed CEnum ──────────────────────── v0.2.0
  Installed Requires ───────────────────── v1.0.1
  Installed OpenSpecFun_jll ────────────── v0.5.3+2
  Installed Adapt ──────────────────────── v1.0.1
  Installed JLD2 ───────────────────────── v0.1.12
  Installed AutoGrad ───────────────────── v1.2.1
  Installed NNlib ──────────────────────── v0.6.6
  Installed CompilerSupportLibraries_jll ─ v0.2.0+1
  Installed AbstractFFTs ───────────────── v0.5.0
  Installed TranscodingStreams ─────────── v0.9.5
  Installed FileIO ─────────────────────── v1.2.2
  Installed GPUArrays ──────────────────── v2.0.1
  Installed LLVM ───────────────────────── v1.3.4
  Installed CuArrays ───────────────────── v1.7.3
  Installed Knet ───────────────────────── v1.3.4
  Installed OrderedCollections ─────────── v1.1.0
  Installed TimerOutputs ───────────────── v0.5.3
  Installed CodecZlib ──────────────────── v0.6.0
  Installed MacroTools ─────────────────── v0.5.4
  Installed SpecialFunctions ───────────── v0.10.0
  Installed DataStructures ─────────────── v0.17.10
  Installed CUDAdrv ────────────────────── v6.0.0
  Installed CUDAnative ─────────────────── v2.10.2
  Installed CUDAapi ────────────────────── v3.1.0
  Installed BinaryProvider ─────────────── v0.5.8
#=#=#                                                                         ######################################################################## 100.0%
#=#=#                                                                         ##                                                                         3.1%######                                                                     9.1%############                                                              17.3%####################                                                      28.0%##############################                                            42.6%############################################                              61.6%###########################################################               83.0%######################################################################## 100.0%
   Updating `~/.julia/environments/v1.5/Project.toml`
   1902f260 + Knet v1.3.4
   Updating `~/.julia/environments/v1.5/Manifest.toml`
   621f4979 + AbstractFFTs v0.5.0
   79e6a3ab + Adapt v1.0.1
   6710c13c + AutoGrad v1.2.1
   b99e7846 + BinaryProvider v0.5.8
   fa961155 + CEnum v0.2.0
   3895d2a7 + CUDAapi v3.1.0
   c5f51814 + CUDAdrv v6.0.0
   be33ccc6 + CUDAnative v2.10.2
   944b1d66 + CodecZlib v0.6.0
   e66e0078 + CompilerSupportLibraries_jll v0.2.0+1
   3a865a2d + CuArrays v1.7.3
   864edb3b + DataStructures v0.17.10
   5789e2e9 + FileIO v1.2.2
   0c68f7d7 + GPUArrays v2.0.1
   033835bb + JLD2 v0.1.12
   1902f260 + Knet v1.3.4
   929cbde3 + LLVM v1.3.4
   1914dd2f + MacroTools v0.5.4
   872c559c + NNlib v0.6.6
   efe28fd5 + OpenSpecFun_jll v0.5.3+2
   bac558e1 + OrderedCollections v1.1.0
   ae029012 + Requires v1.0.1
   276daf66 + SpecialFunctions v0.10.0
   a759f4b9 + TimerOutputs v0.5.3
   3bb67fe8 + TranscodingStreams v0.9.5
   2a0f44e3 + Base64
   ade2ca70 + Dates
   8ba89e20 + Distributed
   b77e0a4c + InteractiveUtils
   76f85450 + LibGit2
   8f399da3 + Libdl
   37e2e46d + LinearAlgebra
   56ddb016 + Logging
   d6f4376e + Markdown
   a63ad114 + Mmap
   44cfe95a + Pkg
   de0858da + Printf
   3fa0cd96 + REPL
   9a3f8284 + Random
   ea8e919c + SHA
   9e88b42a + Serialization
   6462fe0b + Sockets
   2f01184e + SparseArrays
   10745b16 + Statistics
   8dfed614 + Test
   cf7118a7 + UUIDs
   4ec0a83e + Unicode
   Building NNlib ────→ `~/.julia/packages/NNlib/FAI3o/deps/build.log`
   Building CodecZlib → `~/.julia/packages/CodecZlib/5t9zO/deps/build.log`
   Building Knet ─────→ `~/.julia/packages/Knet/vxHRi/deps/build.log`
    Testing Knet
     Status `/tmp/jl_KwUdnS/Project.toml`
   6710c13c AutoGrad v1.2.1
   3895d2a7 CUDAapi v3.1.0
   3a865a2d CuArrays v1.7.3
   864edb3b DataStructures v0.17.10
   5789e2e9 FileIO v1.2.2
   033835bb JLD2 v0.1.12
   1902f260 Knet v1.3.4
   872c559c NNlib v0.6.6
   276daf66 SpecialFunctions v0.10.0
   a759f4b9 TimerOutputs v0.5.3
   8f399da3 Libdl
   37e2e46d LinearAlgebra
   de0858da Printf
   9a3f8284 Random
   10745b16 Statistics
   8dfed614 Test
     Status `/tmp/jl_KwUdnS/Manifest.toml`
   621f4979 AbstractFFTs v0.5.0
   79e6a3ab Adapt v1.0.1
   6710c13c AutoGrad v1.2.1
   b99e7846 BinaryProvider v0.5.8
   fa961155 CEnum v0.2.0
   3895d2a7 CUDAapi v3.1.0
   c5f51814 CUDAdrv v6.0.0
   be33ccc6 CUDAnative v2.10.2
   944b1d66 CodecZlib v0.6.0
   e66e0078 CompilerSupportLibraries_jll v0.2.0+1
   3a865a2d CuArrays v1.7.3
   864edb3b DataStructures v0.17.10
   5789e2e9 FileIO v1.2.2
   0c68f7d7 GPUArrays v2.0.1
   033835bb JLD2 v0.1.12
   1902f260 Knet v1.3.4
   929cbde3 LLVM v1.3.4
   1914dd2f MacroTools v0.5.4
   872c559c NNlib v0.6.6
   efe28fd5 OpenSpecFun_jll v0.5.3+2
   bac558e1 OrderedCollections v1.1.0
   ae029012 Requires v1.0.1
   276daf66 SpecialFunctions v0.10.0
   a759f4b9 TimerOutputs v0.5.3
   3bb67fe8 TranscodingStreams v0.9.5
   2a0f44e3 Base64
   ade2ca70 Dates
   8ba89e20 Distributed
   b77e0a4c InteractiveUtils
   76f85450 LibGit2
   8f399da3 Libdl
   37e2e46d LinearAlgebra
   56ddb016 Logging
   d6f4376e Markdown
   a63ad114 Mmap
   44cfe95a Pkg
   de0858da Printf
   3fa0cd96 REPL
   9a3f8284 Random
   ea8e919c SHA
   9e88b42a Serialization
   6462fe0b Sockets
   2f01184e SparseArrays
   10745b16 Statistics
   8dfed614 Test
   cf7118a7 UUIDs
   4ec0a83e Unicode
┌ Warning: Incompatibility detected between CUDA and LLVM 8.0+; disabling debug info emission for CUDA kernels
└ @ CUDAnative ~/.julia/packages/CUDAnative/hfulr/src/CUDAnative.jl:114
┌ Warning: Your CUDA installation does not provide the CUPTI library, CUDAnative.@code_sass will be unavailable
└ @ CUDAnative ~/.julia/packages/CUDAnative/hfulr/src/CUDAnative.jl:160
  0.688244 seconds (359.51 k allocations: 18.601 MiB, 26.58% gc time)
Knet.gpuCount() = 1
Knet.gpu() = 0
Knet.tk = ["/usr", "/usr/lib/x86_64-linux-gnu", "/usr/lib/nvidia-cuda-toolkit"]
Knet.libknet8 = "/home/pkgeval/.julia/packages/Knet/vxHRi/deps/libknet8"
Knet.cudartfound = true
Knet.cudaRuntimeVersion = 9010
Knet.cudaDriverVersion = 10020
Knet.cudaGetDeviceCount() = 1
Knet.cudaGetDevice() = 0
Knet.cudaMemGetInfo() = (15623192576, 15843721216)
Knet.cudaDeviceSynchronize() = nothing
Knet.nvmlfound = true
Knet.nvmlDriverVersion = "440.33.01"
Knet.nvmlVersion = "10.440.33.01"
Knet.nvmlDeviceGetMemoryInfo() = (15843721216, 15623192576, 220528640)
Knet.cublashandle() = Ptr{Nothing} @0x000000000be4bf50
Knet.cublasVersion = 9010
gpu: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/gpu.jl:3
  Got exception outside of a @test
  Cannot find cudnn
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
   [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
   [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
   [5] top-level scope at show.jl:613
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/gpu.jl:28
   [7] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/gpu.jl:5
   [9] include(::String) at ./client.jl:441
   [10] macro expansion at ./util.jl:175 [inlined]
   [11] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:7 [inlined]
   [12] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
   [14] include(::String) at ./client.jl:441
   [15] top-level scope at none:6
   [16] eval(::Module, ::Any) at ./boot.jl:331
   [17] exec_options(::Base.JLOptions) at ./client.jl:264
   [18] _start() at ./client.jl:490
  
 12.892599 seconds (6.80 M allocations: 334.243 MiB, 1.26% gc time)
  3.106629 seconds (4.83 M allocations: 237.013 MiB, 3.95% gc time)
 17.227953 seconds (6.18 M allocations: 316.717 MiB, 0.89% gc time)
serialize: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/serialize.jl:5
  Got exception outside of a @test
  Cannot find cudnn
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
   [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
   [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
   [5] gethandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/rnn.jl:384
   [6] RNN(::Int64, ::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/rnn.jl:147
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/serialize.jl:6
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/serialize.jl:6
   [10] include(::String) at ./client.jl:441
   [11] macro expansion at ./util.jl:175 [inlined]
   [12] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:10 [inlined]
   [13] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
   [15] include(::String) at ./client.jl:441
   [16] top-level scope at none:6
   [17] eval(::Module, ::Any) at ./boot.jl:331
   [18] exec_options(::Base.JLOptions) at ./client.jl:264
   [19] _start() at ./client.jl:490
  
  1.556409 seconds (1.47 M allocations: 75.879 MiB, 3.64% gc time)
JLD: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/jld.jl:3
  Got exception outside of a @test
  Cannot find cudnn
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
   [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
   [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
   [5] gethandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/rnn.jl:384
   [6] RNN(::Int64, ::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/rnn.jl:147
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/jld.jl:7
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/jld.jl:6
   [10] include(::String) at ./client.jl:441
   [11] macro expansion at ./util.jl:175 [inlined]
   [12] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:11 [inlined]
   [13] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
   [15] include(::String) at ./client.jl:441
   [16] top-level scope at none:6
   [17] eval(::Module, ::Any) at ./boot.jl:331
   [18] exec_options(::Base.JLOptions) at ./client.jl:264
   [19] _start() at ./client.jl:490
  
  0.295052 seconds (5.80 k allocations: 355.641 KiB)
gcnode: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/gcnode.jl:3
  Got exception outside of a @test
  Cannot find cudnn
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
   [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
   [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
   [5] gethandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/rnn.jl:384
   [6] RNN(::Int64, ::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/rnn.jl:147
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/gcnode.jl:8
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/gcnode.jl:6
   [10] include(::String) at ./client.jl:441
   [11] macro expansion at ./util.jl:175 [inlined]
   [12] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:12 [inlined]
   [13] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
   [15] include(::String) at ./client.jl:441
   [16] top-level scope at none:6
   [17] eval(::Module, ::Any) at ./boot.jl:331
   [18] exec_options(::Base.JLOptions) at ./client.jl:264
   [19] _start() at ./client.jl:490
  
  0.214627 seconds (5.48 k allocations: 333.166 KiB)
 24.769522 seconds (22.44 M allocations: 1.121 GiB, 2.39% gc time)
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,1,CUDAnative.AS.Global},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,1,Nothing},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call(::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,1,Nothing},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
 [15] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
 [16] _setindex! at ./multidimensional.jl:777 [inlined]
 [17] setindex! at ./abstractarray.jl:1073 [inlined]
 [18] __cat(::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{Int64,Int64}, ::Tuple{Bool,Bool}, ::CuArrays.CuArray{Float64,1,Nothing}, ::Vararg{CuArrays.CuArray{Float64,1,Nothing},N} where N) at ./abstractarray.jl:1463
 [19] cat(::KnetArray{Float64,1}, ::Vararg{KnetArray{Float64,1},N} where N; dims::Val{2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
 [20] forw(::Function, ::Param{KnetArray{Float64,1}}, ::Vararg{Param{KnetArray{Float64,1}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Val{2},Tuple{Symbol},NamedTuple{(:dims,),Tuple{Val{2}}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [21] #cat#199 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:30 [inlined]
 [22] hcat at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:119 [inlined]
 [23] (::var"#75#95"{Param{KnetArray{Float64,1}},Param{KnetArray{Float64,1}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [24] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [25] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [26] (::AutoGrad.var"#234#239"{Tuple{},var"#75#95"{Param{KnetArray{Float64,1}},Param{KnetArray{Float64,1}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [27] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [28] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [29] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [30] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [31] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:37
 [32] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [33] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
 [34] include(::String) at ./client.jl:441
 [35] macro expansion at ./util.jl:175 [inlined]
 [36] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:14 [inlined]
 [37] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [38] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [39] include(::String) at ./client.jl:441
 [40] top-level scope at none:6
 [41] eval(::Module, ::Any) at ./boot.jl:331
 [42] exec_options(::Base.JLOptions) at ./client.jl:264
 [43] _start() at ./client.jl:490
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:37
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:37 =# @gcheck hcat(a3, b3)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:37
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,1,CUDAnative.AS.Global},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,1,Nothing},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,1,Nothing},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
   [16] _setindex! at ./multidimensional.jl:777 [inlined]
   [17] setindex! at ./abstractarray.jl:1073 [inlined]
   [18] __cat(::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{Int64,Int64}, ::Tuple{Bool,Bool}, ::CuArrays.CuArray{Float64,1,Nothing}, ::Vararg{CuArrays.CuArray{Float64,1,Nothing},N} where N) at ./abstractarray.jl:1463
   [19] cat(::KnetArray{Float64,1}, ::Vararg{KnetArray{Float64,1},N} where N; dims::Val{2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
   [20] forw(::Function, ::Param{KnetArray{Float64,1}}, ::Vararg{Param{KnetArray{Float64,1}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Val{2},Tuple{Symbol},NamedTuple{(:dims,),Tuple{Val{2}}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [21] #cat#199 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:30 [inlined]
   [22] hcat at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:119 [inlined]
   [23] (::var"#75#95"{Param{KnetArray{Float64,1}},Param{KnetArray{Float64,1}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [24] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [25] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [26] (::AutoGrad.var"#234#239"{Tuple{},var"#75#95"{Param{KnetArray{Float64,1}},Param{KnetArray{Float64,1}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [27] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [28] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [29] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [30] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [31] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:37
   [32] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [33] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,1,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,1,CUDAnative.AS.Global},Tuple{Int64},Tuple{UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,1,Nothing}, ::Tuple{CuArrays.CuArray{Float64,1,Nothing},CuArrays.CuArray{Float64,1,Nothing},Tuple{Int64},Tuple{UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call(::Function, ::CuArrays.CuArray{Float64,1,Nothing}, ::Tuple{CuArrays.CuArray{Float64,1,Nothing},CuArrays.CuArray{Float64,1,Nothing},Tuple{Int64},Tuple{UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
 [15] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
 [16] _setindex! at ./multidimensional.jl:777 [inlined]
 [17] setindex! at ./abstractarray.jl:1073 [inlined]
 [18] __cat(::CuArrays.CuArray{Float64,1,Nothing}, ::Tuple{Int64}, ::Tuple{Bool}, ::CuArrays.CuArray{Float64,1,Nothing}, ::Vararg{CuArrays.CuArray{Float64,1,Nothing},N} where N) at ./abstractarray.jl:1463
 [19] cat(::KnetArray{Float64,1}, ::Vararg{KnetArray{Float64,1},N} where N; dims::Val{1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
 [20] forw(::Function, ::Param{KnetArray{Float64,1}}, ::Vararg{Param{KnetArray{Float64,1}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Val{1},Tuple{Symbol},NamedTuple{(:dims,),Tuple{Val{1}}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [21] #cat#199 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:30 [inlined]
 [22] vcat at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:118 [inlined]
 [23] (::var"#76#96"{Param{KnetArray{Float64,1}},Param{KnetArray{Float64,1}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [24] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [25] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [26] (::AutoGrad.var"#234#239"{Tuple{},var"#76#96"{Param{KnetArray{Float64,1}},Param{KnetArray{Float64,1}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [27] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [28] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [29] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [30] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [31] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:38
 [32] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [33] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
 [34] include(::String) at ./client.jl:441
 [35] macro expansion at ./util.jl:175 [inlined]
 [36] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:14 [inlined]
 [37] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [38] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [39] include(::String) at ./client.jl:441
 [40] top-level scope at none:6
 [41] eval(::Module, ::Any) at ./boot.jl:331
 [42] exec_options(::Base.JLOptions) at ./client.jl:264
 [43] _start() at ./client.jl:490
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:38
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:38 =# @gcheck vcat(a3, b3)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:38
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,1,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,1,CUDAnative.AS.Global},Tuple{Int64},Tuple{UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,1,Nothing}, ::Tuple{CuArrays.CuArray{Float64,1,Nothing},CuArrays.CuArray{Float64,1,Nothing},Tuple{Int64},Tuple{UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArrays.CuArray{Float64,1,Nothing}, ::Tuple{CuArrays.CuArray{Float64,1,Nothing},CuArrays.CuArray{Float64,1,Nothing},Tuple{Int64},Tuple{UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
   [16] _setindex! at ./multidimensional.jl:777 [inlined]
   [17] setindex! at ./abstractarray.jl:1073 [inlined]
   [18] __cat(::CuArrays.CuArray{Float64,1,Nothing}, ::Tuple{Int64}, ::Tuple{Bool}, ::CuArrays.CuArray{Float64,1,Nothing}, ::Vararg{CuArrays.CuArray{Float64,1,Nothing},N} where N) at ./abstractarray.jl:1463
   [19] cat(::KnetArray{Float64,1}, ::Vararg{KnetArray{Float64,1},N} where N; dims::Val{1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
   [20] forw(::Function, ::Param{KnetArray{Float64,1}}, ::Vararg{Param{KnetArray{Float64,1}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Val{1},Tuple{Symbol},NamedTuple{(:dims,),Tuple{Val{1}}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [21] #cat#199 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:30 [inlined]
   [22] vcat at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:118 [inlined]
   [23] (::var"#76#96"{Param{KnetArray{Float64,1}},Param{KnetArray{Float64,1}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [24] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [25] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [26] (::AutoGrad.var"#234#239"{Tuple{},var"#76#96"{Param{KnetArray{Float64,1}},Param{KnetArray{Float64,1}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [27] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [28] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [29] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [30] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [31] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:38
   [32] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [33] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:40
  Test threw exception
  Expression: cat(a0, b0, dims = i) == cat(a1, b1, dims = i)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,1,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,1,CUDAnative.AS.Global},Tuple{Int64},Tuple{UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,1,Nothing}, ::Tuple{CuArrays.CuArray{Float64,1,Nothing},CuArrays.CuArray{Float64,1,Nothing},Tuple{Int64},Tuple{UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArrays.CuArray{Float64,1,Nothing}, ::Tuple{CuArrays.CuArray{Float64,1,Nothing},CuArrays.CuArray{Float64,1,Nothing},Tuple{Int64},Tuple{UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
   [16] _setindex! at ./multidimensional.jl:777 [inlined]
   [17] setindex! at ./abstractarray.jl:1073 [inlined]
   [18] __cat(::CuArrays.CuArray{Float64,1,Nothing}, ::Tuple{Int64}, ::Tuple{Bool}, ::CuArrays.CuArray{Float64,1,Nothing}, ::Vararg{CuArrays.CuArray{Float64,1,Nothing},N} where N) at ./abstractarray.jl:1463
   [19] cat(::KnetArray{Float64,1}, ::Vararg{KnetArray{Float64,1},N} where N; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:40
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,1,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,1,CUDAnative.AS.Global},Tuple{Int64},Tuple{UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,1,Nothing}, ::Tuple{CuArrays.CuArray{Float64,1,Nothing},CuArrays.CuArray{Float64,1,Nothing},Tuple{Int64},Tuple{UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call(::Function, ::CuArrays.CuArray{Float64,1,Nothing}, ::Tuple{CuArrays.CuArray{Float64,1,Nothing},CuArrays.CuArray{Float64,1,Nothing},Tuple{Int64},Tuple{UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
 [15] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
 [16] _setindex! at ./multidimensional.jl:777 [inlined]
 [17] setindex! at ./abstractarray.jl:1073 [inlined]
 [18] __cat(::CuArrays.CuArray{Float64,1,Nothing}, ::Tuple{Int64}, ::Tuple{Bool}, ::CuArrays.CuArray{Float64,1,Nothing}, ::Vararg{CuArrays.CuArray{Float64,1,Nothing},N} where N) at ./abstractarray.jl:1463
 [19] cat(::KnetArray{Float64,1}, ::Vararg{KnetArray{Float64,1},N} where N; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
 [20] forw(::Function, ::Param{KnetArray{Float64,1}}, ::Vararg{Param{KnetArray{Float64,1}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [21] #cat#199 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:30 [inlined]
 [22] (::var"#78#98"{Param{KnetArray{Float64,1}},Param{KnetArray{Float64,1}},Int64})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [23] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [25] (::AutoGrad.var"#234#239"{Tuple{},var"#78#98"{Param{KnetArray{Float64,1}},Param{KnetArray{Float64,1}},Int64},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [29] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:14 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42 =# @gcheck cat(a3, b3, dims = i)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,1,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,1,CUDAnative.AS.Global},Tuple{Int64},Tuple{UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,1,Nothing}, ::Tuple{CuArrays.CuArray{Float64,1,Nothing},CuArrays.CuArray{Float64,1,Nothing},Tuple{Int64},Tuple{UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArrays.CuArray{Float64,1,Nothing}, ::Tuple{CuArrays.CuArray{Float64,1,Nothing},CuArrays.CuArray{Float64,1,Nothing},Tuple{Int64},Tuple{UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
   [16] _setindex! at ./multidimensional.jl:777 [inlined]
   [17] setindex! at ./abstractarray.jl:1073 [inlined]
   [18] __cat(::CuArrays.CuArray{Float64,1,Nothing}, ::Tuple{Int64}, ::Tuple{Bool}, ::CuArrays.CuArray{Float64,1,Nothing}, ::Vararg{CuArrays.CuArray{Float64,1,Nothing},N} where N) at ./abstractarray.jl:1463
   [19] cat(::KnetArray{Float64,1}, ::Vararg{KnetArray{Float64,1},N} where N; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
   [20] forw(::Function, ::Param{KnetArray{Float64,1}}, ::Vararg{Param{KnetArray{Float64,1}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [21] #cat#199 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:30 [inlined]
   [22] (::var"#78#98"{Param{KnetArray{Float64,1}},Param{KnetArray{Float64,1}},Int64})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [23] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [25] (::AutoGrad.var"#234#239"{Tuple{},var"#78#98"{Param{KnetArray{Float64,1}},Param{KnetArray{Float64,1}},Int64},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [29] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:20
  Test threw exception
  Expression: permutedims(a0, (2, 1)) == permutedims(a1, (2, 1))
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] permutedims!(::CuArrays.CuArray{Float64,2,Nothing}, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{Int64,Int64}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,2}, ::KnetArray{Float64,2}, ::Tuple{Int64,Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,2}, ::Tuple{Int64,Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:20
   [20] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:21
  Test threw exception
  Expression: permutedims(a0, (1, 2)) == permutedims(a1, (1, 2))
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] permutedims!(::CuArrays.CuArray{Float64,2,Nothing}, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{Int64,Int64}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,2}, ::KnetArray{Float64,2}, ::Tuple{Int64,Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,2}, ::Tuple{Int64,Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:21
   [20] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call(::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
 [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
 [16] permutedims!(::CuArrays.CuArray{Float64,2,Nothing}, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{Int64,Int64}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [17] permutedims!(::KnetArray{Float64,2}, ::KnetArray{Float64,2}, ::Tuple{Int64,Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [18] permutedims(::KnetArray{Float64,2}, ::Tuple{Int64,Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [19] forw(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [21] permutedims at ./none:0 [inlined]
 [22] (::var"#69#89"{Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [23] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [25] (::AutoGrad.var"#234#239"{Tuple{},var"#69#89"{Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [29] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:26
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:14 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:26
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:26 =# @gcheck permutedims(a3, (2, 1))
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:26
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] permutedims!(::CuArrays.CuArray{Float64,2,Nothing}, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{Int64,Int64}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,2}, ::KnetArray{Float64,2}, ::Tuple{Int64,Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,2}, ::Tuple{Int64,Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] forw(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [21] permutedims at ./none:0 [inlined]
   [22] (::var"#69#89"{Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [23] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [25] (::AutoGrad.var"#234#239"{Tuple{},var"#69#89"{Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [29] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:26
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call(::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
 [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
 [16] permutedims!(::CuArrays.CuArray{Float64,2,Nothing}, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{Int64,Int64}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [17] permutedims!(::KnetArray{Float64,2}, ::KnetArray{Float64,2}, ::Tuple{Int64,Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [18] permutedims(::KnetArray{Float64,2}, ::Tuple{Int64,Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [19] forw(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [21] permutedims at ./none:0 [inlined]
 [22] (::var"#70#90"{Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [23] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [25] (::AutoGrad.var"#234#239"{Tuple{},var"#70#90"{Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [29] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:27
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:14 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:27
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:27 =# @gcheck permutedims(a3, (1, 2))
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:27
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] permutedims!(::CuArrays.CuArray{Float64,2,Nothing}, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{Int64,Int64}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,2}, ::KnetArray{Float64,2}, ::Tuple{Int64,Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,2}, ::Tuple{Int64,Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] forw(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [21] permutedims at ./none:0 [inlined]
   [22] (::var"#70#90"{Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [23] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [25] (::AutoGrad.var"#234#239"{Tuple{},var"#70#90"{Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [29] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:27
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call(::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
 [15] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
 [16] _setindex! at ./multidimensional.jl:777 [inlined]
 [17] setindex! at ./abstractarray.jl:1073 [inlined]
 [18] __cat(::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{Int64,Int64}, ::Tuple{Bool,Bool}, ::CuArrays.CuArray{Float64,2,Nothing}, ::Vararg{CuArrays.CuArray{Float64,2,Nothing},N} where N) at ./abstractarray.jl:1463
 [19] cat(::KnetArray{Float64,2}, ::Vararg{KnetArray{Float64,2},N} where N; dims::Val{2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
 [20] forw(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Param{KnetArray{Float64,2}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Val{2},Tuple{Symbol},NamedTuple{(:dims,),Tuple{Val{2}}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [21] #cat#199 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:30 [inlined]
 [22] hcat at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:119 [inlined]
 [23] (::var"#75#95"{Param{KnetArray{Float64,2}},Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [24] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [25] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [26] (::AutoGrad.var"#234#239"{Tuple{},var"#75#95"{Param{KnetArray{Float64,2}},Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [27] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [28] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [29] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [30] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [31] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:37
 [32] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [33] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
 [34] include(::String) at ./client.jl:441
 [35] macro expansion at ./util.jl:175 [inlined]
 [36] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:14 [inlined]
 [37] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [38] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [39] include(::String) at ./client.jl:441
 [40] top-level scope at none:6
 [41] eval(::Module, ::Any) at ./boot.jl:331
 [42] exec_options(::Base.JLOptions) at ./client.jl:264
 [43] _start() at ./client.jl:490
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:37
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:37 =# @gcheck hcat(a3, b3)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:37
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
   [16] _setindex! at ./multidimensional.jl:777 [inlined]
   [17] setindex! at ./abstractarray.jl:1073 [inlined]
   [18] __cat(::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{Int64,Int64}, ::Tuple{Bool,Bool}, ::CuArrays.CuArray{Float64,2,Nothing}, ::Vararg{CuArrays.CuArray{Float64,2,Nothing},N} where N) at ./abstractarray.jl:1463
   [19] cat(::KnetArray{Float64,2}, ::Vararg{KnetArray{Float64,2},N} where N; dims::Val{2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
   [20] forw(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Param{KnetArray{Float64,2}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Val{2},Tuple{Symbol},NamedTuple{(:dims,),Tuple{Val{2}}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [21] #cat#199 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:30 [inlined]
   [22] hcat at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:119 [inlined]
   [23] (::var"#75#95"{Param{KnetArray{Float64,2}},Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [24] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [25] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [26] (::AutoGrad.var"#234#239"{Tuple{},var"#75#95"{Param{KnetArray{Float64,2}},Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [27] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [28] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [29] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [30] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [31] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:37
   [32] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [33] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call(::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
 [15] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
 [16] _setindex! at ./multidimensional.jl:777 [inlined]
 [17] setindex! at ./abstractarray.jl:1073 [inlined]
 [18] __cat(::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{Int64,Int64}, ::Tuple{Bool}, ::CuArrays.CuArray{Float64,2,Nothing}, ::Vararg{CuArrays.CuArray{Float64,2,Nothing},N} where N) at ./abstractarray.jl:1463
 [19] cat(::KnetArray{Float64,2}, ::Vararg{KnetArray{Float64,2},N} where N; dims::Val{1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
 [20] forw(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Param{KnetArray{Float64,2}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Val{1},Tuple{Symbol},NamedTuple{(:dims,),Tuple{Val{1}}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [21] #cat#199 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:30 [inlined]
 [22] vcat at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:118 [inlined]
 [23] (::var"#76#96"{Param{KnetArray{Float64,2}},Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [24] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [25] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [26] (::AutoGrad.var"#234#239"{Tuple{},var"#76#96"{Param{KnetArray{Float64,2}},Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [27] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [28] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [29] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [30] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [31] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:38
 [32] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [33] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
 [34] include(::String) at ./client.jl:441
 [35] macro expansion at ./util.jl:175 [inlined]
 [36] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:14 [inlined]
 [37] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [38] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [39] include(::String) at ./client.jl:441
 [40] top-level scope at none:6
 [41] eval(::Module, ::Any) at ./boot.jl:331
 [42] exec_options(::Base.JLOptions) at ./client.jl:264
 [43] _start() at ./client.jl:490
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:38
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:38 =# @gcheck vcat(a3, b3)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:38
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
   [16] _setindex! at ./multidimensional.jl:777 [inlined]
   [17] setindex! at ./abstractarray.jl:1073 [inlined]
   [18] __cat(::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{Int64,Int64}, ::Tuple{Bool}, ::CuArrays.CuArray{Float64,2,Nothing}, ::Vararg{CuArrays.CuArray{Float64,2,Nothing},N} where N) at ./abstractarray.jl:1463
   [19] cat(::KnetArray{Float64,2}, ::Vararg{KnetArray{Float64,2},N} where N; dims::Val{1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
   [20] forw(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Param{KnetArray{Float64,2}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Val{1},Tuple{Symbol},NamedTuple{(:dims,),Tuple{Val{1}}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [21] #cat#199 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:30 [inlined]
   [22] vcat at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:118 [inlined]
   [23] (::var"#76#96"{Param{KnetArray{Float64,2}},Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [24] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [25] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [26] (::AutoGrad.var"#234#239"{Tuple{},var"#76#96"{Param{KnetArray{Float64,2}},Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [27] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [28] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [29] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [30] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [31] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:38
   [32] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [33] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:40
  Test threw exception
  Expression: cat(a0, b0, dims = i) == cat(a1, b1, dims = i)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
   [16] _setindex! at ./multidimensional.jl:777 [inlined]
   [17] setindex! at ./abstractarray.jl:1073 [inlined]
   [18] __cat(::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{Int64,Int64}, ::Tuple{Bool}, ::CuArrays.CuArray{Float64,2,Nothing}, ::Vararg{CuArrays.CuArray{Float64,2,Nothing},N} where N) at ./abstractarray.jl:1463
   [19] cat(::KnetArray{Float64,2}, ::Vararg{KnetArray{Float64,2},N} where N; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:40
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call(::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
 [15] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
 [16] _setindex! at ./multidimensional.jl:777 [inlined]
 [17] setindex! at ./abstractarray.jl:1073 [inlined]
 [18] __cat(::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{Int64,Int64}, ::Tuple{Bool}, ::CuArrays.CuArray{Float64,2,Nothing}, ::Vararg{CuArrays.CuArray{Float64,2,Nothing},N} where N) at ./abstractarray.jl:1463
 [19] cat(::KnetArray{Float64,2}, ::Vararg{KnetArray{Float64,2},N} where N; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
 [20] forw(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Param{KnetArray{Float64,2}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [21] #cat#199 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:30 [inlined]
 [22] (::var"#78#98"{Param{KnetArray{Float64,2}},Param{KnetArray{Float64,2}},Int64})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [23] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [25] (::AutoGrad.var"#234#239"{Tuple{},var"#78#98"{Param{KnetArray{Float64,2}},Param{KnetArray{Float64,2}},Int64},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [29] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:14 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42 =# @gcheck cat(a3, b3, dims = i)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
   [16] _setindex! at ./multidimensional.jl:777 [inlined]
   [17] setindex! at ./abstractarray.jl:1073 [inlined]
   [18] __cat(::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{Int64,Int64}, ::Tuple{Bool}, ::CuArrays.CuArray{Float64,2,Nothing}, ::Vararg{CuArrays.CuArray{Float64,2,Nothing},N} where N) at ./abstractarray.jl:1463
   [19] cat(::KnetArray{Float64,2}, ::Vararg{KnetArray{Float64,2},N} where N; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
   [20] forw(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Param{KnetArray{Float64,2}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [21] #cat#199 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:30 [inlined]
   [22] (::var"#78#98"{Param{KnetArray{Float64,2}},Param{KnetArray{Float64,2}},Int64})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [23] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [25] (::AutoGrad.var"#234#239"{Tuple{},var"#78#98"{Param{KnetArray{Float64,2}},Param{KnetArray{Float64,2}},Int64},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [29] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:40
  Test threw exception
  Expression: cat(a0, b0, dims = i) == cat(a1, b1, dims = i)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
   [16] _setindex! at ./multidimensional.jl:777 [inlined]
   [17] setindex! at ./abstractarray.jl:1073 [inlined]
   [18] __cat(::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{Int64,Int64}, ::Tuple{Bool,Bool}, ::CuArrays.CuArray{Float64,2,Nothing}, ::Vararg{CuArrays.CuArray{Float64,2,Nothing},N} where N) at ./abstractarray.jl:1463
   [19] cat(::KnetArray{Float64,2}, ::Vararg{KnetArray{Float64,2},N} where N; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:40
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call(::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
 [15] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
 [16] _setindex! at ./multidimensional.jl:777 [inlined]
 [17] setindex! at ./abstractarray.jl:1073 [inlined]
 [18] __cat(::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{Int64,Int64}, ::Tuple{Bool,Bool}, ::CuArrays.CuArray{Float64,2,Nothing}, ::Vararg{CuArrays.CuArray{Float64,2,Nothing},N} where N) at ./abstractarray.jl:1463
 [19] cat(::KnetArray{Float64,2}, ::Vararg{KnetArray{Float64,2},N} where N; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
 [20] forw(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Param{KnetArray{Float64,2}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [21] #cat#199 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:30 [inlined]
 [22] (::var"#78#98"{Param{KnetArray{Float64,2}},Param{KnetArray{Float64,2}},Int64})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [23] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [25] (::AutoGrad.var"#234#239"{Tuple{},var"#78#98"{Param{KnetArray{Float64,2}},Param{KnetArray{Float64,2}},Int64},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [29] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:14 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42 =# @gcheck cat(a3, b3, dims = i)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
   [16] _setindex! at ./multidimensional.jl:777 [inlined]
   [17] setindex! at ./abstractarray.jl:1073 [inlined]
   [18] __cat(::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{Int64,Int64}, ::Tuple{Bool,Bool}, ::CuArrays.CuArray{Float64,2,Nothing}, ::Vararg{CuArrays.CuArray{Float64,2,Nothing},N} where N) at ./abstractarray.jl:1463
   [19] cat(::KnetArray{Float64,2}, ::Vararg{KnetArray{Float64,2},N} where N; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
   [20] forw(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Param{KnetArray{Float64,2}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [21] #cat#199 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:30 [inlined]
   [22] (::var"#78#98"{Param{KnetArray{Float64,2}},Param{KnetArray{Float64,2}},Int64})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [23] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [25] (::AutoGrad.var"#234#239"{Tuple{},var"#78#98"{Param{KnetArray{Float64,2}},Param{KnetArray{Float64,2}},Int64},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [29] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:11
  Test threw exception
  Expression: getindex(a0, idx...) == getindex(a1, idx...)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::typeof(GPUArrays.index_kernel), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] _unsafe_getindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:96 [inlined]
   [17] getindex(::KnetArray{Float64,3}, ::UnitRange{Int64}, ::UnitRange{Int64}, ::UnitRange{Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:28
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:11
   [19] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::typeof(GPUArrays.index_kernel), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
 [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
 [16] _unsafe_getindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:96 [inlined]
 [17] getindex(::KnetArray{Float64,3}, ::UnitRange{Int64}, ::UnitRange{Int64}, ::UnitRange{Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:28
 [18] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [19] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [20] getindex at ./none:0 [inlined]
 [21] (::var"#62#82"{Param{KnetArray{Float64,3}},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [22] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [23] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [24] (::AutoGrad.var"#234#239"{Tuple{},var"#62#82"{Param{KnetArray{Float64,3}},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [25] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [26] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [27] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [28] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:13
 [30] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [31] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
 [32] include(::String) at ./client.jl:441
 [33] macro expansion at ./util.jl:175 [inlined]
 [34] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:14 [inlined]
 [35] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [36] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [37] include(::String) at ./client.jl:441
 [38] top-level scope at none:6
 [39] eval(::Module, ::Any) at ./boot.jl:331
 [40] exec_options(::Base.JLOptions) at ./client.jl:264
 [41] _start() at ./client.jl:490
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:13
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:13 =# @gcheck getindex(a3, idx...)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:13
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::typeof(GPUArrays.index_kernel), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] _unsafe_getindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:96 [inlined]
   [17] getindex(::KnetArray{Float64,3}, ::UnitRange{Int64}, ::UnitRange{Int64}, ::UnitRange{Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:28
   [18] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [19] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [20] getindex at ./none:0 [inlined]
   [21] (::var"#62#82"{Param{KnetArray{Float64,3}},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [22] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [23] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [24] (::AutoGrad.var"#234#239"{Tuple{},var"#62#82"{Param{KnetArray{Float64,3}},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [25] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [26] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [27] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [28] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [29] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:13
   [30] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [31] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:29
  Test threw exception
  Expression: permutedims(a0, (1, 3, 2)) == permutedims(a1, (1, 3, 2))
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{Int64,Int64,Int64}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Tuple{Int64,Int64,Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,3}, ::Tuple{Int64,Int64,Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:29
   [20] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
 [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
 [16] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{Int64,Int64,Int64}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [17] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Tuple{Int64,Int64,Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [18] permutedims(::KnetArray{Float64,3}, ::Tuple{Int64,Int64,Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [19] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [21] permutedims at ./none:0 [inlined]
 [22] (::var"#72#92"{Param{KnetArray{Float64,3}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [23] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [25] (::AutoGrad.var"#234#239"{Tuple{},var"#72#92"{Param{KnetArray{Float64,3}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [29] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:31
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:14 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:31
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:31 =# @gcheck permutedims(a3, (1, 3, 2))
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:31
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{Int64,Int64,Int64}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Tuple{Int64,Int64,Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,3}, ::Tuple{Int64,Int64,Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [21] permutedims at ./none:0 [inlined]
   [22] (::var"#72#92"{Param{KnetArray{Float64,3}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [23] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [25] (::AutoGrad.var"#234#239"{Tuple{},var"#72#92"{Param{KnetArray{Float64,3}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [29] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:31
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:33
  Test threw exception
  Expression: hcat(a0, b0) == hcat(a1, b1)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
   [16] _setindex! at ./multidimensional.jl:777 [inlined]
   [17] setindex! at ./abstractarray.jl:1073 [inlined]
   [18] __cat(::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{Int64,Int64,Int64}, ::Tuple{Bool,Bool}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Vararg{CuArrays.CuArray{Float64,3,Nothing},N} where N) at ./abstractarray.jl:1463
   [19] cat(::KnetArray{Float64,3}, ::Vararg{KnetArray{Float64,3},N} where N; dims::Val{2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
   [20] hcat(::KnetArray{Float64,3}, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:59
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:33
   [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
 [15] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
 [16] _setindex! at ./multidimensional.jl:777 [inlined]
 [17] setindex! at ./abstractarray.jl:1073 [inlined]
 [18] __cat(::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{Int64,Int64,Int64}, ::Tuple{Bool,Bool}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Vararg{CuArrays.CuArray{Float64,3,Nothing},N} where N) at ./abstractarray.jl:1463
 [19] cat(::KnetArray{Float64,3}, ::Vararg{KnetArray{Float64,3},N} where N; dims::Val{2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
 [20] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Param{KnetArray{Float64,3}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Val{2},Tuple{Symbol},NamedTuple{(:dims,),Tuple{Val{2}}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [21] #cat#199 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:30 [inlined]
 [22] hcat at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:119 [inlined]
 [23] (::var"#75#95"{Param{KnetArray{Float64,3}},Param{KnetArray{Float64,3}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [24] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [25] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [26] (::AutoGrad.var"#234#239"{Tuple{},var"#75#95"{Param{KnetArray{Float64,3}},Param{KnetArray{Float64,3}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [27] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [28] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [29] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [30] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [31] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:37
 [32] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [33] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
 [34] include(::String) at ./client.jl:441
 [35] macro expansion at ./util.jl:175 [inlined]
 [36] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:14 [inlined]
 [37] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [38] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [39] include(::String) at ./client.jl:441
 [40] top-level scope at none:6
 [41] eval(::Module, ::Any) at ./boot.jl:331
 [42] exec_options(::Base.JLOptions) at ./client.jl:264
 [43] _start() at ./client.jl:490
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:37
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:37 =# @gcheck hcat(a3, b3)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:37
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
   [16] _setindex! at ./multidimensional.jl:777 [inlined]
   [17] setindex! at ./abstractarray.jl:1073 [inlined]
   [18] __cat(::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{Int64,Int64,Int64}, ::Tuple{Bool,Bool}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Vararg{CuArrays.CuArray{Float64,3,Nothing},N} where N) at ./abstractarray.jl:1463
   [19] cat(::KnetArray{Float64,3}, ::Vararg{KnetArray{Float64,3},N} where N; dims::Val{2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
   [20] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Param{KnetArray{Float64,3}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Val{2},Tuple{Symbol},NamedTuple{(:dims,),Tuple{Val{2}}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [21] #cat#199 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:30 [inlined]
   [22] hcat at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:119 [inlined]
   [23] (::var"#75#95"{Param{KnetArray{Float64,3}},Param{KnetArray{Float64,3}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [24] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [25] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [26] (::AutoGrad.var"#234#239"{Tuple{},var"#75#95"{Param{KnetArray{Float64,3}},Param{KnetArray{Float64,3}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [27] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [28] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [29] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [30] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [31] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:37
   [32] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [33] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
 [15] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
 [16] _setindex! at ./multidimensional.jl:777 [inlined]
 [17] setindex! at ./abstractarray.jl:1073 [inlined]
 [18] __cat(::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{Int64,Int64,Int64}, ::Tuple{Bool}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Vararg{CuArrays.CuArray{Float64,3,Nothing},N} where N) at ./abstractarray.jl:1463
 [19] cat(::KnetArray{Float64,3}, ::Vararg{KnetArray{Float64,3},N} where N; dims::Val{1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
 [20] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Param{KnetArray{Float64,3}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Val{1},Tuple{Symbol},NamedTuple{(:dims,),Tuple{Val{1}}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [21] #cat#199 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:30 [inlined]
 [22] vcat at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:118 [inlined]
 [23] (::var"#76#96"{Param{KnetArray{Float64,3}},Param{KnetArray{Float64,3}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [24] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [25] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [26] (::AutoGrad.var"#234#239"{Tuple{},var"#76#96"{Param{KnetArray{Float64,3}},Param{KnetArray{Float64,3}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [27] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [28] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [29] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [30] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [31] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:38
 [32] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [33] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
 [34] include(::String) at ./client.jl:441
 [35] macro expansion at ./util.jl:175 [inlined]
 [36] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:14 [inlined]
 [37] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [38] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [39] include(::String) at ./client.jl:441
 [40] top-level scope at none:6
 [41] eval(::Module, ::Any) at ./boot.jl:331
 [42] exec_options(::Base.JLOptions) at ./client.jl:264
 [43] _start() at ./client.jl:490
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:38
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:38 =# @gcheck vcat(a3, b3)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:38
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
   [16] _setindex! at ./multidimensional.jl:777 [inlined]
   [17] setindex! at ./abstractarray.jl:1073 [inlined]
   [18] __cat(::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{Int64,Int64,Int64}, ::Tuple{Bool}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Vararg{CuArrays.CuArray{Float64,3,Nothing},N} where N) at ./abstractarray.jl:1463
   [19] cat(::KnetArray{Float64,3}, ::Vararg{KnetArray{Float64,3},N} where N; dims::Val{1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
   [20] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Param{KnetArray{Float64,3}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Val{1},Tuple{Symbol},NamedTuple{(:dims,),Tuple{Val{1}}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [21] #cat#199 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:30 [inlined]
   [22] vcat at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:118 [inlined]
   [23] (::var"#76#96"{Param{KnetArray{Float64,3}},Param{KnetArray{Float64,3}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [24] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [25] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [26] (::AutoGrad.var"#234#239"{Tuple{},var"#76#96"{Param{KnetArray{Float64,3}},Param{KnetArray{Float64,3}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [27] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [28] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [29] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [30] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [31] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:38
   [32] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [33] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:40
  Test threw exception
  Expression: cat(a0, b0, dims = i) == cat(a1, b1, dims = i)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
   [16] _setindex! at ./multidimensional.jl:777 [inlined]
   [17] setindex! at ./abstractarray.jl:1073 [inlined]
   [18] __cat(::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{Int64,Int64,Int64}, ::Tuple{Bool}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Vararg{CuArrays.CuArray{Float64,3,Nothing},N} where N) at ./abstractarray.jl:1463
   [19] cat(::KnetArray{Float64,3}, ::Vararg{KnetArray{Float64,3},N} where N; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:40
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
 [15] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
 [16] _setindex! at ./multidimensional.jl:777 [inlined]
 [17] setindex! at ./abstractarray.jl:1073 [inlined]
 [18] __cat(::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{Int64,Int64,Int64}, ::Tuple{Bool}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Vararg{CuArrays.CuArray{Float64,3,Nothing},N} where N) at ./abstractarray.jl:1463
 [19] cat(::KnetArray{Float64,3}, ::Vararg{KnetArray{Float64,3},N} where N; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
 [20] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Param{KnetArray{Float64,3}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [21] #cat#199 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:30 [inlined]
 [22] (::var"#78#98"{Param{KnetArray{Float64,3}},Param{KnetArray{Float64,3}},Int64})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [23] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [25] (::AutoGrad.var"#234#239"{Tuple{},var"#78#98"{Param{KnetArray{Float64,3}},Param{KnetArray{Float64,3}},Int64},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [29] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:14 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42 =# @gcheck cat(a3, b3, dims = i)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
   [16] _setindex! at ./multidimensional.jl:777 [inlined]
   [17] setindex! at ./abstractarray.jl:1073 [inlined]
   [18] __cat(::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{Int64,Int64,Int64}, ::Tuple{Bool}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Vararg{CuArrays.CuArray{Float64,3,Nothing},N} where N) at ./abstractarray.jl:1463
   [19] cat(::KnetArray{Float64,3}, ::Vararg{KnetArray{Float64,3},N} where N; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
   [20] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Param{KnetArray{Float64,3}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [21] #cat#199 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:30 [inlined]
   [22] (::var"#78#98"{Param{KnetArray{Float64,3}},Param{KnetArray{Float64,3}},Int64})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [23] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [25] (::AutoGrad.var"#234#239"{Tuple{},var"#78#98"{Param{KnetArray{Float64,3}},Param{KnetArray{Float64,3}},Int64},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [29] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:40
  Test threw exception
  Expression: cat(a0, b0, dims = i) == cat(a1, b1, dims = i)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
   [16] _setindex! at ./multidimensional.jl:777 [inlined]
   [17] setindex! at ./abstractarray.jl:1073 [inlined]
   [18] __cat(::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{Int64,Int64,Int64}, ::Tuple{Bool,Bool}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Vararg{CuArrays.CuArray{Float64,3,Nothing},N} where N) at ./abstractarray.jl:1463
   [19] cat(::KnetArray{Float64,3}, ::Vararg{KnetArray{Float64,3},N} where N; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:40
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
 [15] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
 [16] _setindex! at ./multidimensional.jl:777 [inlined]
 [17] setindex! at ./abstractarray.jl:1073 [inlined]
 [18] __cat(::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{Int64,Int64,Int64}, ::Tuple{Bool,Bool}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Vararg{CuArrays.CuArray{Float64,3,Nothing},N} where N) at ./abstractarray.jl:1463
 [19] cat(::KnetArray{Float64,3}, ::Vararg{KnetArray{Float64,3},N} where N; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
 [20] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Param{KnetArray{Float64,3}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [21] #cat#199 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:30 [inlined]
 [22] (::var"#78#98"{Param{KnetArray{Float64,3}},Param{KnetArray{Float64,3}},Int64})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [23] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [25] (::AutoGrad.var"#234#239"{Tuple{},var"#78#98"{Param{KnetArray{Float64,3}},Param{KnetArray{Float64,3}},Int64},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [29] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:14 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42 =# @gcheck cat(a3, b3, dims = i)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
   [16] _setindex! at ./multidimensional.jl:777 [inlined]
   [17] setindex! at ./abstractarray.jl:1073 [inlined]
   [18] __cat(::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{Int64,Int64,Int64}, ::Tuple{Bool,Bool}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Vararg{CuArrays.CuArray{Float64,3,Nothing},N} where N) at ./abstractarray.jl:1463
   [19] cat(::KnetArray{Float64,3}, ::Vararg{KnetArray{Float64,3},N} where N; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
   [20] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Param{KnetArray{Float64,3}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [21] #cat#199 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:30 [inlined]
   [22] (::var"#78#98"{Param{KnetArray{Float64,3}},Param{KnetArray{Float64,3}},Int64})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [23] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [25] (::AutoGrad.var"#234#239"{Tuple{},var"#78#98"{Param{KnetArray{Float64,3}},Param{KnetArray{Float64,3}},Int64},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [29] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:40
  Test threw exception
  Expression: cat(a0, b0, dims = i) == cat(a1, b1, dims = i)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
   [16] _setindex! at ./multidimensional.jl:777 [inlined]
   [17] setindex! at ./abstractarray.jl:1073 [inlined]
   [18] __cat(::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{Int64,Int64,Int64}, ::Tuple{Bool,Bool,Bool}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Vararg{CuArrays.CuArray{Float64,3,Nothing},N} where N) at ./abstractarray.jl:1463
   [19] cat(::KnetArray{Float64,3}, ::Vararg{KnetArray{Float64,3},N} where N; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:40
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
 [15] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
 [16] _setindex! at ./multidimensional.jl:777 [inlined]
 [17] setindex! at ./abstractarray.jl:1073 [inlined]
 [18] __cat(::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{Int64,Int64,Int64}, ::Tuple{Bool,Bool,Bool}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Vararg{CuArrays.CuArray{Float64,3,Nothing},N} where N) at ./abstractarray.jl:1463
 [19] cat(::KnetArray{Float64,3}, ::Vararg{KnetArray{Float64,3},N} where N; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
 [20] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Param{KnetArray{Float64,3}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [21] #cat#199 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:30 [inlined]
 [22] (::var"#78#98"{Param{KnetArray{Float64,3}},Param{KnetArray{Float64,3}},Int64})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [23] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [25] (::AutoGrad.var"#234#239"{Tuple{},var"#78#98"{Param{KnetArray{Float64,3}},Param{KnetArray{Float64,3}},Int64},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [29] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:14 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42 =# @gcheck cat(a3, b3, dims = i)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
   [16] _setindex! at ./multidimensional.jl:777 [inlined]
   [17] setindex! at ./abstractarray.jl:1073 [inlined]
   [18] __cat(::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{Int64,Int64,Int64}, ::Tuple{Bool,Bool,Bool}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Vararg{CuArrays.CuArray{Float64,3,Nothing},N} where N) at ./abstractarray.jl:1463
   [19] cat(::KnetArray{Float64,3}, ::Vararg{KnetArray{Float64,3},N} where N; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
   [20] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Param{KnetArray{Float64,3}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [21] #cat#199 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:30 [inlined]
   [22] (::var"#78#98"{Param{KnetArray{Float64,3}},Param{KnetArray{Float64,3}},Int64})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [23] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [25] (::AutoGrad.var"#234#239"{Tuple{},var"#78#98"{Param{KnetArray{Float64,3}},Param{KnetArray{Float64,3}},Int64},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [29] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:44
  Test threw exception
  Expression: setindex!(a0, b0[idx...], idx...) == setindex!(a1, b1[idx...], idx...)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::typeof(GPUArrays.index_kernel), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] _unsafe_getindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:96 [inlined]
   [17] getindex(::KnetArray{Float64,3}, ::UnitRange{Int64}, ::UnitRange{Int64}, ::UnitRange{Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:28
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:44
   [19] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
cuarray: Test Failed at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:48
  Expression: argmin(a0) == argmin(a1)
   Evaluated: CartesianIndex(6, 6, 2) == CartesianIndex(4, 3, 3)
Stacktrace:
 [1] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:48
 [2] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [3] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
cuarray: Test Failed at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:50
  Expression: findmin(a0) == findmin(a1)
   Evaluated: (0.0040776875568471205, CartesianIndex(6, 6, 2)) == (0.0014251310450574817, CartesianIndex(4, 3, 3))
Stacktrace:
 [1] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:50
 [2] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [3] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
cuarray: Test Failed at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:52
  Expression: argmax(a0, dims = i) == argmax(a1, dims = i)
   Evaluated: CartesianIndex{3}[CartesianIndex(2, 1, 1) CartesianIndex(5, 2, 1) … CartesianIndex(7, 7, 1) CartesianIndex(8, 8, 1)]

CartesianIndex{3}[CartesianIndex(1, 1, 2) CartesianIndex(8, 2, 2) … CartesianIndex(3, 7, 2) CartesianIndex(1, 8, 2)]

CartesianIndex{3}[CartesianIndex(7, 1, 3) CartesianIndex(6, 2, 3) … CartesianIndex(7, 7, 3) CartesianIndex(4, 8, 3)]

CartesianIndex{3}[CartesianIndex(6, 1, 4) CartesianIndex(8, 2, 4) … CartesianIndex(5, 7, 4) CartesianIndex(4, 8, 4)]

CartesianIndex{3}[CartesianIndex(3, 1, 5) CartesianIndex(4, 2, 5) … CartesianIndex(3, 7, 5) CartesianIndex(3, 8, 5)]

CartesianIndex{3}[CartesianIndex(2, 1, 6) CartesianIndex(2, 2, 6) … CartesianIndex(5, 7, 6) CartesianIndex(5, 8, 6)]

CartesianIndex{3}[CartesianIndex(7, 1, 7) CartesianIndex(6, 2, 7) … CartesianIndex(3, 7, 7) CartesianIndex(1, 8, 7)]

CartesianIndex{3}[CartesianIndex(7, 1, 8) CartesianIndex(7, 2, 8) … CartesianIndex(5, 7, 8) CartesianIndex(3, 8, 8)] == CartesianIndex{3}[CartesianIndex(2, 1, 1) CartesianIndex(5, 2, 1) … CartesianIndex(7, 7, 1) CartesianIndex(8, 8, 1)]

CartesianIndex{3}[CartesianIndex(1, 1, 2) CartesianIndex(8, 2, 2) … CartesianIndex(3, 7, 2) CartesianIndex(1, 8, 2)]

CartesianIndex{3}[CartesianIndex(7, 1, 3) CartesianIndex(2, 2, 3) … CartesianIndex(7, 7, 3) CartesianIndex(4, 8, 3)]

CartesianIndex{3}[CartesianIndex(6, 1, 4) CartesianIndex(3, 2, 4) … CartesianIndex(5, 7, 4) CartesianIndex(4, 8, 4)]

CartesianIndex{3}[CartesianIndex(3, 1, 5) CartesianIndex(4, 2, 5) … CartesianIndex(3, 7, 5) CartesianIndex(3, 8, 5)]

CartesianIndex{3}[CartesianIndex(2, 1, 6) CartesianIndex(2, 2, 6) … CartesianIndex(5, 7, 6) CartesianIndex(5, 8, 6)]

CartesianIndex{3}[CartesianIndex(7, 1, 7) CartesianIndex(6, 2, 7) … CartesianIndex(3, 7, 7) CartesianIndex(1, 8, 7)]

CartesianIndex{3}[CartesianIndex(7, 1, 8) CartesianIndex(7, 2, 8) … CartesianIndex(5, 7, 8) CartesianIndex(3, 8, 8)]
Stacktrace:
 [1] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:52
 [2] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [3] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
cuarray: Test Failed at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:53
  Expression: argmin(a0, dims = i) == argmin(a1, dims = i)
   Evaluated: CartesianIndex{3}[CartesianIndex(1, 1, 1) CartesianIndex(7, 2, 1) … CartesianIndex(2, 7, 1) CartesianIndex(2, 8, 1)]

CartesianIndex{3}[CartesianIndex(3, 1, 2) CartesianIndex(1, 2, 2) … CartesianIndex(2, 7, 2) CartesianIndex(2, 8, 2)]

CartesianIndex{3}[CartesianIndex(8, 1, 3) CartesianIndex(3, 2, 3) … CartesianIndex(4, 7, 3) CartesianIndex(7, 8, 3)]

CartesianIndex{3}[CartesianIndex(8, 1, 4) CartesianIndex(2, 2, 4) … CartesianIndex(2, 7, 4) CartesianIndex(6, 8, 4)]

CartesianIndex{3}[CartesianIndex(8, 1, 5) CartesianIndex(7, 2, 5) … CartesianIndex(4, 7, 5) CartesianIndex(2, 8, 5)]

CartesianIndex{3}[CartesianIndex(3, 1, 6) CartesianIndex(6, 2, 6) … CartesianIndex(4, 7, 6) CartesianIndex(6, 8, 6)]

CartesianIndex{3}[CartesianIndex(1, 1, 7) CartesianIndex(3, 2, 7) … CartesianIndex(7, 7, 7) CartesianIndex(6, 8, 7)]

CartesianIndex{3}[CartesianIndex(8, 1, 8) CartesianIndex(4, 2, 8) … CartesianIndex(3, 7, 8) CartesianIndex(1, 8, 8)] == CartesianIndex{3}[CartesianIndex(1, 1, 1) CartesianIndex(7, 2, 1) … CartesianIndex(2, 7, 1) CartesianIndex(2, 8, 1)]

CartesianIndex{3}[CartesianIndex(3, 1, 2) CartesianIndex(1, 2, 2) … CartesianIndex(2, 7, 2) CartesianIndex(2, 8, 2)]

CartesianIndex{3}[CartesianIndex(8, 1, 3) CartesianIndex(7, 2, 3) … CartesianIndex(4, 7, 3) CartesianIndex(7, 8, 3)]

CartesianIndex{3}[CartesianIndex(8, 1, 4) CartesianIndex(4, 2, 4) … CartesianIndex(2, 7, 4) CartesianIndex(6, 8, 4)]

CartesianIndex{3}[CartesianIndex(8, 1, 5) CartesianIndex(7, 2, 5) … CartesianIndex(4, 7, 5) CartesianIndex(2, 8, 5)]

CartesianIndex{3}[CartesianIndex(3, 1, 6) CartesianIndex(6, 2, 6) … CartesianIndex(4, 7, 6) CartesianIndex(6, 8, 6)]

CartesianIndex{3}[CartesianIndex(1, 1, 7) CartesianIndex(3, 2, 7) … CartesianIndex(7, 7, 7) CartesianIndex(6, 8, 7)]

CartesianIndex{3}[CartesianIndex(8, 1, 8) CartesianIndex(4, 2, 8) … CartesianIndex(3, 7, 8) CartesianIndex(1, 8, 8)]
Stacktrace:
 [1] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:53
 [2] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [3] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
cuarray: Test Failed at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:54
  Expression: findmax(a0, dims = i) == findmax(a1, dims = i)
   Evaluated: ([0.9632560818069502 0.8829646573881613 … 0.9345108810417633 0.886664030440691]

[0.9448844156721377 0.987646110931272 … 0.9995336379658828 0.7934950357565262]

[0.8723719857201679 0.8038904201916817 … 0.9592281288373083 0.9399745593136906]

[0.9886632317816504 0.9346871452194352 … 0.9171822131933736 0.9814996163868117]

[0.9065504285081698 0.9407904694981475 … 0.945141646848709 0.9192369148483313]

[0.9210726090364629 0.94707914779281 … 0.9816818748500351 0.9723829099272989]

[0.9899497603429668 0.9030548211716845 … 0.9563271234995396 0.9021176809078981]

[0.9994658721908023 0.9460495715122235 … 0.7994612581940006 0.7173323218458298], CartesianIndex{3}[CartesianIndex(2, 1, 1) CartesianIndex(5, 2, 1) … CartesianIndex(7, 7, 1) CartesianIndex(8, 8, 1)]

CartesianIndex{3}[CartesianIndex(1, 1, 2) CartesianIndex(8, 2, 2) … CartesianIndex(3, 7, 2) CartesianIndex(1, 8, 2)]

CartesianIndex{3}[CartesianIndex(7, 1, 3) CartesianIndex(6, 2, 3) … CartesianIndex(7, 7, 3) CartesianIndex(4, 8, 3)]

CartesianIndex{3}[CartesianIndex(6, 1, 4) CartesianIndex(8, 2, 4) … CartesianIndex(5, 7, 4) CartesianIndex(4, 8, 4)]

CartesianIndex{3}[CartesianIndex(3, 1, 5) CartesianIndex(4, 2, 5) … CartesianIndex(3, 7, 5) CartesianIndex(3, 8, 5)]

CartesianIndex{3}[CartesianIndex(2, 1, 6) CartesianIndex(2, 2, 6) … CartesianIndex(5, 7, 6) CartesianIndex(5, 8, 6)]

CartesianIndex{3}[CartesianIndex(7, 1, 7) CartesianIndex(6, 2, 7) … CartesianIndex(3, 7, 7) CartesianIndex(1, 8, 7)]

CartesianIndex{3}[CartesianIndex(7, 1, 8) CartesianIndex(7, 2, 8) … CartesianIndex(5, 7, 8) CartesianIndex(3, 8, 8)]) == ([0.9632560818069502 0.8829646573881613 … 0.9345108810417633 0.886664030440691]

[0.9448844156721377 0.987646110931272 … 0.9995336379658828 0.7934950357565262]

[0.8723719857201679 0.9967841054120943 … 0.9592281288373083 0.9399745593136906]

[0.9886632317816504 0.9723095836501838 … 0.9171822131933736 0.9814996163868117]

[0.9065504285081698 0.9407904694981475 … 0.945141646848709 0.9192369148483313]

[0.9210726090364629 0.94707914779281 … 0.9816818748500351 0.9723829099272989]

[0.9899497603429668 0.9030548211716845 … 0.9563271234995396 0.9021176809078981]

[0.9994658721908023 0.9460495715122235 … 0.7994612581940006 0.7173323218458298], CartesianIndex{3}[CartesianIndex(2, 1, 1) CartesianIndex(5, 2, 1) … CartesianIndex(7, 7, 1) CartesianIndex(8, 8, 1)]

CartesianIndex{3}[CartesianIndex(1, 1, 2) CartesianIndex(8, 2, 2) … CartesianIndex(3, 7, 2) CartesianIndex(1, 8, 2)]

CartesianIndex{3}[CartesianIndex(7, 1, 3) CartesianIndex(2, 2, 3) … CartesianIndex(7, 7, 3) CartesianIndex(4, 8, 3)]

CartesianIndex{3}[CartesianIndex(6, 1, 4) CartesianIndex(3, 2, 4) … CartesianIndex(5, 7, 4) CartesianIndex(4, 8, 4)]

CartesianIndex{3}[CartesianIndex(3, 1, 5) CartesianIndex(4, 2, 5) … CartesianIndex(3, 7, 5) CartesianIndex(3, 8, 5)]

CartesianIndex{3}[CartesianIndex(2, 1, 6) CartesianIndex(2, 2, 6) … CartesianIndex(5, 7, 6) CartesianIndex(5, 8, 6)]

CartesianIndex{3}[CartesianIndex(7, 1, 7) CartesianIndex(6, 2, 7) … CartesianIndex(3, 7, 7) CartesianIndex(1, 8, 7)]

CartesianIndex{3}[CartesianIndex(7, 1, 8) CartesianIndex(7, 2, 8) … CartesianIndex(5, 7, 8) CartesianIndex(3, 8, 8)])
Stacktrace:
 [1] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:54
 [2] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [3] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
cuarray: Test Failed at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:55
  Expression: findmin(a0, dims = i) == findmin(a1, dims = i)
   Evaluated: ([0.007405722779616486 0.16286886719801275 … 0.15763313324806671 0.15844376669882254]

[0.03599234211788205 0.08532517246251059 … 0.05898830340371797 0.04737756686902905]

[0.0880309678391773 0.009109171746720213 … 0.03825860278473714 0.2592926138525731]

[0.029455842591826986 0.15983721020182617 … 0.22708687838419706 0.09710408045909324]

[0.054220367390030555 0.015072845574038984 … 0.148868095462094 0.13608767897100327]

[0.30631243548633336 0.008771835886351775 … 0.012866190551789503 0.055522499633746]

[0.06780268891522434 0.10682709177383387 … 0.18138326313079012 0.1404072491516284]

[0.005596455051920746 0.039368407324267274 … 0.04063403724556114 0.021425892155901627], CartesianIndex{3}[CartesianIndex(1, 1, 1) CartesianIndex(7, 2, 1) … CartesianIndex(2, 7, 1) CartesianIndex(2, 8, 1)]

CartesianIndex{3}[CartesianIndex(3, 1, 2) CartesianIndex(1, 2, 2) … CartesianIndex(2, 7, 2) CartesianIndex(2, 8, 2)]

CartesianIndex{3}[CartesianIndex(8, 1, 3) CartesianIndex(3, 2, 3) … CartesianIndex(4, 7, 3) CartesianIndex(7, 8, 3)]

CartesianIndex{3}[CartesianIndex(8, 1, 4) CartesianIndex(2, 2, 4) … CartesianIndex(2, 7, 4) CartesianIndex(6, 8, 4)]

CartesianIndex{3}[CartesianIndex(8, 1, 5) CartesianIndex(7, 2, 5) … CartesianIndex(4, 7, 5) CartesianIndex(2, 8, 5)]

CartesianIndex{3}[CartesianIndex(3, 1, 6) CartesianIndex(6, 2, 6) … CartesianIndex(4, 7, 6) CartesianIndex(6, 8, 6)]

CartesianIndex{3}[CartesianIndex(1, 1, 7) CartesianIndex(3, 2, 7) … CartesianIndex(7, 7, 7) CartesianIndex(6, 8, 7)]

CartesianIndex{3}[CartesianIndex(8, 1, 8) CartesianIndex(4, 2, 8) … CartesianIndex(3, 7, 8) CartesianIndex(1, 8, 8)]) == ([0.007405722779616486 0.16286886719801275 … 0.15763313324806671 0.15844376669882254]

[0.03599234211788205 0.08532517246251059 … 0.05898830340371797 0.04737756686902905]

[0.0880309678391773 0.22514872539772757 … 0.03825860278473714 0.2592926138525731]

[0.029455842591826986 0.23797704384998086 … 0.22708687838419706 0.09710408045909324]

[0.054220367390030555 0.015072845574038984 … 0.148868095462094 0.13608767897100327]

[0.30631243548633336 0.008771835886351775 … 0.012866190551789503 0.055522499633746]

[0.06780268891522434 0.10682709177383387 … 0.18138326313079012 0.1404072491516284]

[0.005596455051920746 0.039368407324267274 … 0.04063403724556114 0.021425892155901627], CartesianIndex{3}[CartesianIndex(1, 1, 1) CartesianIndex(7, 2, 1) … CartesianIndex(2, 7, 1) CartesianIndex(2, 8, 1)]

CartesianIndex{3}[CartesianIndex(3, 1, 2) CartesianIndex(1, 2, 2) … CartesianIndex(2, 7, 2) CartesianIndex(2, 8, 2)]

CartesianIndex{3}[CartesianIndex(8, 1, 3) CartesianIndex(7, 2, 3) … CartesianIndex(4, 7, 3) CartesianIndex(7, 8, 3)]

CartesianIndex{3}[CartesianIndex(8, 1, 4) CartesianIndex(4, 2, 4) … CartesianIndex(2, 7, 4) CartesianIndex(6, 8, 4)]

CartesianIndex{3}[CartesianIndex(8, 1, 5) CartesianIndex(7, 2, 5) … CartesianIndex(4, 7, 5) CartesianIndex(2, 8, 5)]

CartesianIndex{3}[CartesianIndex(3, 1, 6) CartesianIndex(6, 2, 6) … CartesianIndex(4, 7, 6) CartesianIndex(6, 8, 6)]

CartesianIndex{3}[CartesianIndex(1, 1, 7) CartesianIndex(3, 2, 7) … CartesianIndex(7, 7, 7) CartesianIndex(6, 8, 7)]

CartesianIndex{3}[CartesianIndex(8, 1, 8) CartesianIndex(4, 2, 8) … CartesianIndex(3, 7, 8) CartesianIndex(1, 8, 8)])
Stacktrace:
 [1] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:55
 [2] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [3] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
cuarray: Test Failed at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:52
  Expression: argmax(a0, dims = i) == argmax(a1, dims = i)
   Evaluated: CartesianIndex{3}[CartesianIndex(1, 6, 1); CartesianIndex(2, 1, 1); … ; CartesianIndex(7, 7, 1); CartesianIndex(8, 4, 1)]

CartesianIndex{3}[CartesianIndex(1, 1, 2); CartesianIndex(2, 6, 2); … ; CartesianIndex(7, 7, 2); CartesianIndex(8, 2, 2)]

CartesianIndex{3}[CartesianIndex(1, 8, 3); CartesianIndex(2, 7, 3); … ; CartesianIndex(7, 5, 3); CartesianIndex(8, 7, 3)]

CartesianIndex{3}[CartesianIndex(1, 8, 4); CartesianIndex(2, 6, 4); … ; CartesianIndex(7, 4, 4); CartesianIndex(8, 2, 4)]

CartesianIndex{3}[CartesianIndex(1, 7, 5); CartesianIndex(2, 4, 5); … ; CartesianIndex(7, 8, 5); CartesianIndex(8, 6, 5)]

CartesianIndex{3}[CartesianIndex(1, 2, 6); CartesianIndex(2, 2, 6); … ; CartesianIndex(7, 4, 6); CartesianIndex(8, 6, 6)]

CartesianIndex{3}[CartesianIndex(1, 8, 7); CartesianIndex(2, 3, 7); … ; CartesianIndex(7, 1, 7); CartesianIndex(8, 2, 7)]

CartesianIndex{3}[CartesianIndex(1, 1, 8); CartesianIndex(2, 1, 8); … ; CartesianIndex(7, 1, 8); CartesianIndex(8, 6, 8)] == CartesianIndex{3}[CartesianIndex(1, 6, 1); CartesianIndex(2, 1, 1); … ; CartesianIndex(7, 7, 1); CartesianIndex(8, 4, 1)]

CartesianIndex{3}[CartesianIndex(1, 1, 2); CartesianIndex(2, 6, 2); … ; CartesianIndex(7, 7, 2); CartesianIndex(8, 2, 2)]

CartesianIndex{3}[CartesianIndex(1, 8, 3); CartesianIndex(2, 2, 3); … ; CartesianIndex(7, 5, 3); CartesianIndex(8, 7, 3)]

CartesianIndex{3}[CartesianIndex(1, 8, 4); CartesianIndex(2, 6, 4); … ; CartesianIndex(7, 4, 4); CartesianIndex(8, 2, 4)]

CartesianIndex{3}[CartesianIndex(1, 7, 5); CartesianIndex(2, 4, 5); … ; CartesianIndex(7, 8, 5); CartesianIndex(8, 6, 5)]

CartesianIndex{3}[CartesianIndex(1, 2, 6); CartesianIndex(2, 2, 6); … ; CartesianIndex(7, 4, 6); CartesianIndex(8, 6, 6)]

CartesianIndex{3}[CartesianIndex(1, 8, 7); CartesianIndex(2, 3, 7); … ; CartesianIndex(7, 1, 7); CartesianIndex(8, 2, 7)]

CartesianIndex{3}[CartesianIndex(1, 1, 8); CartesianIndex(2, 1, 8); … ; CartesianIndex(7, 1, 8); CartesianIndex(8, 6, 8)]
Stacktrace:
 [1] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:52
 [2] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [3] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
cuarray: Test Failed at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:53
  Expression: argmin(a0, dims = i) == argmin(a1, dims = i)
   Evaluated: CartesianIndex{3}[CartesianIndex(1, 1, 1); CartesianIndex(2, 7, 1); … ; CartesianIndex(7, 2, 1); CartesianIndex(8, 2, 1)]

CartesianIndex{3}[CartesianIndex(1, 2, 2); CartesianIndex(2, 8, 2); … ; CartesianIndex(7, 3, 2); CartesianIndex(8, 4, 2)]

CartesianIndex{3}[CartesianIndex(1, 3, 3); CartesianIndex(2, 2, 3); … ; CartesianIndex(7, 4, 3); CartesianIndex(8, 3, 3)]

CartesianIndex{3}[CartesianIndex(1, 6, 4); CartesianIndex(2, 3, 4); … ; CartesianIndex(7, 6, 4); CartesianIndex(8, 1, 4)]

CartesianIndex{3}[CartesianIndex(1, 2, 5); CartesianIndex(2, 3, 5); … ; CartesianIndex(7, 2, 5); CartesianIndex(8, 4, 5)]

CartesianIndex{3}[CartesianIndex(1, 8, 6); CartesianIndex(2, 4, 6); … ; CartesianIndex(7, 8, 6); CartesianIndex(8, 3, 6)]

CartesianIndex{3}[CartesianIndex(1, 3, 7); CartesianIndex(2, 8, 7); … ; CartesianIndex(7, 7, 7); CartesianIndex(8, 5, 7)]

CartesianIndex{3}[CartesianIndex(1, 8, 8); CartesianIndex(2, 8, 8); … ; CartesianIndex(7, 7, 8); CartesianIndex(8, 1, 8)] == CartesianIndex{3}[CartesianIndex(1, 1, 1); CartesianIndex(2, 7, 1); … ; CartesianIndex(7, 2, 1); CartesianIndex(8, 2, 1)]

CartesianIndex{3}[CartesianIndex(1, 2, 2); CartesianIndex(2, 8, 2); … ; CartesianIndex(7, 3, 2); CartesianIndex(8, 4, 2)]

CartesianIndex{3}[CartesianIndex(1, 3, 3); CartesianIndex(2, 1, 3); … ; CartesianIndex(7, 4, 3); CartesianIndex(8, 3, 3)]

CartesianIndex{3}[CartesianIndex(1, 6, 4); CartesianIndex(2, 5, 4); … ; CartesianIndex(7, 6, 4); CartesianIndex(8, 1, 4)]

CartesianIndex{3}[CartesianIndex(1, 2, 5); CartesianIndex(2, 3, 5); … ; CartesianIndex(7, 2, 5); CartesianIndex(8, 4, 5)]

CartesianIndex{3}[CartesianIndex(1, 8, 6); CartesianIndex(2, 4, 6); … ; CartesianIndex(7, 8, 6); CartesianIndex(8, 3, 6)]

CartesianIndex{3}[CartesianIndex(1, 3, 7); CartesianIndex(2, 8, 7); … ; CartesianIndex(7, 7, 7); CartesianIndex(8, 5, 7)]

CartesianIndex{3}[CartesianIndex(1, 8, 8); CartesianIndex(2, 8, 8); … ; CartesianIndex(7, 7, 8); CartesianIndex(8, 1, 8)]
Stacktrace:
 [1] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:53
 [2] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [3] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
cuarray: Test Failed at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:54
  Expression: findmax(a0, dims = i) == findmax(a1, dims = i)
   Evaluated: ([0.7352979984019468; 0.9632560818069502; … ; 0.9345108810417633; 0.9437670078835736]

[0.9448844156721377; 0.7835622829683142; … ; 0.7940175419844551; 0.987646110931272]

[0.7944135150939102; 0.9160278860282953; … ; 0.9860711428164819; 0.939393370638721]

[0.9473570678965917; 0.8879323598974469; … ; 0.8787331630428767; 0.9346871452194352]

[0.8628554432353899; 0.9653929967625863; … ; 0.8490069427917282; 0.5677684105851251]

[0.8551205134246376; 0.94707914779281; … ; 0.7937169091379752; 0.9247128653655328]

[0.9021176809078981; 0.9984422746152597; … ; 0.9899497603429668; 0.8827636232381277]

[0.9644229149334453; 0.9630830274903257; … ; 0.9994658721908023; 0.7273780983412501], CartesianIndex{3}[CartesianIndex(1, 6, 1); CartesianIndex(2, 1, 1); … ; CartesianIndex(7, 7, 1); CartesianIndex(8, 4, 1)]

CartesianIndex{3}[CartesianIndex(1, 1, 2); CartesianIndex(2, 6, 2); … ; CartesianIndex(7, 7, 2); CartesianIndex(8, 2, 2)]

CartesianIndex{3}[CartesianIndex(1, 8, 3); CartesianIndex(2, 7, 3); … ; CartesianIndex(7, 5, 3); CartesianIndex(8, 7, 3)]

CartesianIndex{3}[CartesianIndex(1, 8, 4); CartesianIndex(2, 6, 4); … ; CartesianIndex(7, 4, 4); CartesianIndex(8, 2, 4)]

CartesianIndex{3}[CartesianIndex(1, 7, 5); CartesianIndex(2, 4, 5); … ; CartesianIndex(7, 8, 5); CartesianIndex(8, 6, 5)]

CartesianIndex{3}[CartesianIndex(1, 2, 6); CartesianIndex(2, 2, 6); … ; CartesianIndex(7, 4, 6); CartesianIndex(8, 6, 6)]

CartesianIndex{3}[CartesianIndex(1, 8, 7); CartesianIndex(2, 3, 7); … ; CartesianIndex(7, 1, 7); CartesianIndex(8, 2, 7)]

CartesianIndex{3}[CartesianIndex(1, 1, 8); CartesianIndex(2, 1, 8); … ; CartesianIndex(7, 1, 8); CartesianIndex(8, 6, 8)]) == ([0.7352979984019468; 0.9632560818069502; … ; 0.9345108810417633; 0.9437670078835736]

[0.9448844156721377; 0.7835622829683142; … ; 0.7940175419844551; 0.987646110931272]

[0.7944135150939102; 0.9967841054120943; … ; 0.9860711428164819; 0.939393370638721]

[0.9473570678965917; 0.8879323598974469; … ; 0.8787331630428767; 0.9346871452194352]

[0.8628554432353899; 0.9653929967625863; … ; 0.8490069427917282; 0.5677684105851251]

[0.8551205134246376; 0.94707914779281; … ; 0.7937169091379752; 0.9247128653655328]

[0.9021176809078981; 0.9984422746152597; … ; 0.9899497603429668; 0.8827636232381277]

[0.9644229149334453; 0.9630830274903257; … ; 0.9994658721908023; 0.7273780983412501], CartesianIndex{3}[CartesianIndex(1, 6, 1); CartesianIndex(2, 1, 1); … ; CartesianIndex(7, 7, 1); CartesianIndex(8, 4, 1)]

CartesianIndex{3}[CartesianIndex(1, 1, 2); CartesianIndex(2, 6, 2); … ; CartesianIndex(7, 7, 2); CartesianIndex(8, 2, 2)]

CartesianIndex{3}[CartesianIndex(1, 8, 3); CartesianIndex(2, 2, 3); … ; CartesianIndex(7, 5, 3); CartesianIndex(8, 7, 3)]

CartesianIndex{3}[CartesianIndex(1, 8, 4); CartesianIndex(2, 6, 4); … ; CartesianIndex(7, 4, 4); CartesianIndex(8, 2, 4)]

CartesianIndex{3}[CartesianIndex(1, 7, 5); CartesianIndex(2, 4, 5); … ; CartesianIndex(7, 8, 5); CartesianIndex(8, 6, 5)]

CartesianIndex{3}[CartesianIndex(1, 2, 6); CartesianIndex(2, 2, 6); … ; CartesianIndex(7, 4, 6); CartesianIndex(8, 6, 6)]

CartesianIndex{3}[CartesianIndex(1, 8, 7); CartesianIndex(2, 3, 7); … ; CartesianIndex(7, 1, 7); CartesianIndex(8, 2, 7)]

CartesianIndex{3}[CartesianIndex(1, 1, 8); CartesianIndex(2, 1, 8); … ; CartesianIndex(7, 1, 8); CartesianIndex(8, 6, 8)])
Stacktrace:
 [1] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:54
 [2] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [3] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
cuarray: Test Failed at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:55
  Expression: findmin(a0, dims = i) == findmin(a1, dims = i)
   Evaluated: ([0.007405722779616486; 0.15763313324806671; … ; 0.16286886719801275; 0.3573181073137939]

[0.08532517246251059; 0.04737756686902905; … ; 0.007407125431612149; 0.018575125007873616]

[0.0871950622778912; 0.19878169337101514; … ; 0.012916452125250277; 0.07664256669331349]

[0.11906303579838928; 0.0358401250337661; … ; 0.015297735444532057; 0.029455842591826986]

[0.13123437222884782; 0.1330687345243442; … ; 0.015072845574038984; 0.04123600691535767]

[0.2716030564185845; 0.2112416000018793; … ; 0.1102593445212765; 0.02387139362158508]

[0.05975565690333573; 0.2560678391125142; … ; 0.18138326313079012; 0.08173877404892704]

[0.021425892155901627; 0.09320268029039225; … ; 0.1871099571333965; 0.005596455051920746], CartesianIndex{3}[CartesianIndex(1, 1, 1); CartesianIndex(2, 7, 1); … ; CartesianIndex(7, 2, 1); CartesianIndex(8, 2, 1)]

CartesianIndex{3}[CartesianIndex(1, 2, 2); CartesianIndex(2, 8, 2); … ; CartesianIndex(7, 3, 2); CartesianIndex(8, 4, 2)]

CartesianIndex{3}[CartesianIndex(1, 3, 3); CartesianIndex(2, 2, 3); … ; CartesianIndex(7, 4, 3); CartesianIndex(8, 3, 3)]

CartesianIndex{3}[CartesianIndex(1, 6, 4); CartesianIndex(2, 3, 4); … ; CartesianIndex(7, 6, 4); CartesianIndex(8, 1, 4)]

CartesianIndex{3}[CartesianIndex(1, 2, 5); CartesianIndex(2, 3, 5); … ; CartesianIndex(7, 2, 5); CartesianIndex(8, 4, 5)]

CartesianIndex{3}[CartesianIndex(1, 8, 6); CartesianIndex(2, 4, 6); … ; CartesianIndex(7, 8, 6); CartesianIndex(8, 3, 6)]

CartesianIndex{3}[CartesianIndex(1, 3, 7); CartesianIndex(2, 8, 7); … ; CartesianIndex(7, 7, 7); CartesianIndex(8, 5, 7)]

CartesianIndex{3}[CartesianIndex(1, 8, 8); CartesianIndex(2, 8, 8); … ; CartesianIndex(7, 7, 8); CartesianIndex(8, 1, 8)]) == ([0.007405722779616486; 0.15763313324806671; … ; 0.16286886719801275; 0.3573181073137939]

[0.08532517246251059; 0.04737756686902905; … ; 0.007407125431612149; 0.018575125007873616]

[0.0871950622778912; 0.29043973819984314; … ; 0.012916452125250277; 0.07664256669331349]

[0.11906303579838928; 0.07855827644056879; … ; 0.015297735444532057; 0.029455842591826986]

[0.13123437222884782; 0.1330687345243442; … ; 0.015072845574038984; 0.04123600691535767]

[0.2716030564185845; 0.2112416000018793; … ; 0.1102593445212765; 0.02387139362158508]

[0.05975565690333573; 0.2560678391125142; … ; 0.18138326313079012; 0.08173877404892704]

[0.021425892155901627; 0.09320268029039225; … ; 0.1871099571333965; 0.005596455051920746], CartesianIndex{3}[CartesianIndex(1, 1, 1); CartesianIndex(2, 7, 1); … ; CartesianIndex(7, 2, 1); CartesianIndex(8, 2, 1)]

CartesianIndex{3}[CartesianIndex(1, 2, 2); CartesianIndex(2, 8, 2); … ; CartesianIndex(7, 3, 2); CartesianIndex(8, 4, 2)]

CartesianIndex{3}[CartesianIndex(1, 3, 3); CartesianIndex(2, 1, 3); … ; CartesianIndex(7, 4, 3); CartesianIndex(8, 3, 3)]

CartesianIndex{3}[CartesianIndex(1, 6, 4); CartesianIndex(2, 5, 4); … ; CartesianIndex(7, 6, 4); CartesianIndex(8, 1, 4)]

CartesianIndex{3}[CartesianIndex(1, 2, 5); CartesianIndex(2, 3, 5); … ; CartesianIndex(7, 2, 5); CartesianIndex(8, 4, 5)]

CartesianIndex{3}[CartesianIndex(1, 8, 6); CartesianIndex(2, 4, 6); … ; CartesianIndex(7, 8, 6); CartesianIndex(8, 3, 6)]

CartesianIndex{3}[CartesianIndex(1, 3, 7); CartesianIndex(2, 8, 7); … ; CartesianIndex(7, 7, 7); CartesianIndex(8, 5, 7)]

CartesianIndex{3}[CartesianIndex(1, 8, 8); CartesianIndex(2, 8, 8); … ; CartesianIndex(7, 7, 8); CartesianIndex(8, 1, 8)])
Stacktrace:
 [1] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:55
 [2] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [3] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
cuarray: Test Failed at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:52
  Expression: argmax(a0, dims = i) == argmax(a1, dims = i)
   Evaluated: CartesianIndex{3}[CartesianIndex(1, 1, 8) CartesianIndex(1, 2, 6) … CartesianIndex(1, 7, 5) CartesianIndex(1, 8, 4); CartesianIndex(2, 1, 1) CartesianIndex(2, 2, 6) … CartesianIndex(2, 7, 3) CartesianIndex(2, 8, 4); … ; CartesianIndex(7, 1, 8) CartesianIndex(7, 2, 8) … CartesianIndex(7, 7, 3) CartesianIndex(7, 8, 4); CartesianIndex(8, 1, 2) CartesianIndex(8, 2, 2) … CartesianIndex(8, 7, 3) CartesianIndex(8, 8, 1)] == CartesianIndex{3}[CartesianIndex(1, 1, 8) CartesianIndex(1, 2, 6) … CartesianIndex(1, 7, 5) CartesianIndex(1, 8, 4); CartesianIndex(2, 1, 1) CartesianIndex(2, 2, 3) … CartesianIndex(2, 7, 3) CartesianIndex(2, 8, 4); … ; CartesianIndex(7, 1, 8) CartesianIndex(7, 2, 8) … CartesianIndex(7, 7, 3) CartesianIndex(7, 8, 4); CartesianIndex(8, 1, 2) CartesianIndex(8, 2, 2) … CartesianIndex(8, 7, 3) CartesianIndex(8, 8, 1)]
Stacktrace:
 [1] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:52
 [2] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [3] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
cuarray: Test Failed at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:53
  Expression: argmin(a0, dims = i) == argmin(a1, dims = i)
   Evaluated: CartesianIndex{3}[CartesianIndex(1, 1, 1) CartesianIndex(1, 2, 2) … CartesianIndex(1, 7, 2) CartesianIndex(1, 8, 8); CartesianIndex(2, 1, 2) CartesianIndex(2, 2, 4) … CartesianIndex(2, 7, 2) CartesianIndex(2, 8, 2); … ; CartesianIndex(7, 1, 4) CartesianIndex(7, 2, 5) … CartesianIndex(7, 7, 6) CartesianIndex(7, 8, 6); CartesianIndex(8, 1, 8) CartesianIndex(8, 2, 8) … CartesianIndex(8, 7, 8) CartesianIndex(8, 8, 6)] == CartesianIndex{3}[CartesianIndex(1, 1, 1) CartesianIndex(1, 2, 2) … CartesianIndex(1, 7, 2) CartesianIndex(1, 8, 8); CartesianIndex(2, 1, 2) CartesianIndex(2, 2, 1) … CartesianIndex(2, 7, 2) CartesianIndex(2, 8, 2); … ; CartesianIndex(7, 1, 4) CartesianIndex(7, 2, 5) … CartesianIndex(7, 7, 6) CartesianIndex(7, 8, 6); CartesianIndex(8, 1, 8) CartesianIndex(8, 2, 8) … CartesianIndex(8, 7, 8) CartesianIndex(8, 8, 6)]
Stacktrace:
 [1] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:53
 [2] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [3] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
cuarray: Test Failed at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:54
  Expression: findmax(a0, dims = i) == findmax(a1, dims = i)
   Evaluated: ([0.9644229149334453 0.8551205134246376 … 0.8628554432353899 0.9473570678965917; 0.9632560818069502 0.94707914779281 … 0.9160278860282953 0.845898206580801; … ; 0.9994658721908023 0.9460495715122235 … 0.9592281288373083 0.8573219381258419; 0.9068187716440272 0.987646110931272 … 0.939393370638721 0.886664030440691], CartesianIndex{3}[CartesianIndex(1, 1, 8) CartesianIndex(1, 2, 6) … CartesianIndex(1, 7, 5) CartesianIndex(1, 8, 4); CartesianIndex(2, 1, 1) CartesianIndex(2, 2, 6) … CartesianIndex(2, 7, 3) CartesianIndex(2, 8, 4); … ; CartesianIndex(7, 1, 8) CartesianIndex(7, 2, 8) … CartesianIndex(7, 7, 3) CartesianIndex(7, 8, 4); CartesianIndex(8, 1, 2) CartesianIndex(8, 2, 2) … CartesianIndex(8, 7, 3) CartesianIndex(8, 8, 1)]) == ([0.9644229149334453 0.8551205134246376 … 0.8628554432353899 0.9473570678965917; 0.9632560818069502 0.9967841054120943 … 0.9160278860282953 0.845898206580801; … ; 0.9994658721908023 0.9460495715122235 … 0.9592281288373083 0.8573219381258419; 0.9068187716440272 0.987646110931272 … 0.939393370638721 0.886664030440691], CartesianIndex{3}[CartesianIndex(1, 1, 8) CartesianIndex(1, 2, 6) … CartesianIndex(1, 7, 5) CartesianIndex(1, 8, 4); CartesianIndex(2, 1, 1) CartesianIndex(2, 2, 3) … CartesianIndex(2, 7, 3) CartesianIndex(2, 8, 4); … ; CartesianIndex(7, 1, 8) CartesianIndex(7, 2, 8) … CartesianIndex(7, 7, 3) CartesianIndex(7, 8, 4); CartesianIndex(8, 1, 2) CartesianIndex(8, 2, 2) … CartesianIndex(8, 7, 3) CartesianIndex(8, 8, 1)])
Stacktrace:
 [1] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:54
 [2] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [3] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
cuarray: Test Failed at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:55
  Expression: findmin(a0, dims = i) == findmin(a1, dims = i)
   Evaluated: ([0.007405722779616486 0.08532517246251059 … 0.13115273340150457 0.021425892155901627; 0.24074903259654024 0.15983721020182617 … 0.05898830340371797 0.04737756686902905; … ; 0.06993926295757635 0.015072845574038984 … 0.12271944194146633 0.1102593445212765; 0.005596455051920746 0.13592605918987166 … 0.07559545407163037 0.30688820667457106], CartesianIndex{3}[CartesianIndex(1, 1, 1) CartesianIndex(1, 2, 2) … CartesianIndex(1, 7, 2) CartesianIndex(1, 8, 8); CartesianIndex(2, 1, 2) CartesianIndex(2, 2, 4) … CartesianIndex(2, 7, 2) CartesianIndex(2, 8, 2); … ; CartesianIndex(7, 1, 4) CartesianIndex(7, 2, 5) … CartesianIndex(7, 7, 6) CartesianIndex(7, 8, 6); CartesianIndex(8, 1, 8) CartesianIndex(8, 2, 8) … CartesianIndex(8, 7, 8) CartesianIndex(8, 8, 6)]) == ([0.007405722779616486 0.08532517246251059 … 0.13115273340150457 0.021425892155901627; 0.24074903259654024 0.23966477382937512 … 0.05898830340371797 0.04737756686902905; … ; 0.06993926295757635 0.015072845574038984 … 0.12271944194146633 0.1102593445212765; 0.005596455051920746 0.13592605918987166 … 0.07559545407163037 0.30688820667457106], CartesianIndex{3}[CartesianIndex(1, 1, 1) CartesianIndex(1, 2, 2) … CartesianIndex(1, 7, 2) CartesianIndex(1, 8, 8); CartesianIndex(2, 1, 2) CartesianIndex(2, 2, 1) … CartesianIndex(2, 7, 2) CartesianIndex(2, 8, 2); … ; CartesianIndex(7, 1, 4) CartesianIndex(7, 2, 5) … CartesianIndex(7, 7, 6) CartesianIndex(7, 8, 6); CartesianIndex(8, 1, 8) CartesianIndex(8, 2, 8) … CartesianIndex(8, 7, 8) CartesianIndex(8, 8, 6)])
Stacktrace:
 [1] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:55
 [2] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [3] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
 60.682940 seconds (49.72 M allocations: 2.430 GiB, 3.22% gc time)
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
bmm: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/bmm.jl:5
  Got exception outside of a @test
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [15] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [16] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{Int64,Int64,Int64}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Tuple{Int64,Int64,Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,3}, ::Tuple{Int64,Int64,Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/bmm.jl:38
   [20] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/bmm.jl:6
   [22] include(::String) at ./client.jl:441
   [23] macro expansion at ./util.jl:175 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:15 [inlined]
   [25] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
   [27] include(::String) at ./client.jl:441
   [28] top-level scope at none:6
   [29] eval(::Module, ::Any) at ./boot.jl:331
   [30] exec_options(::Base.JLOptions) at ./client.jl:264
   [31] _start() at ./client.jl:490
  
 29.623604 seconds (25.27 M allocations: 1.244 GiB, 2.32% gc time)
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:84
  Test threw exception
  Expression: isapprox(p2(a), Array(p2(ka)))
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float32,2,Nothing}, ::Tuple{CuArrays.CuArray{Float32,2,Nothing},CuArrays.CuArray{Float32,2,Nothing},Tuple{Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float32,2,Nothing}, ::Tuple{CuArrays.CuArray{Float32,2,Nothing},CuArrays.CuArray{Float32,2,Nothing},Tuple{Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float32,2,Nothing}, ::CuArrays.CuArray{Float32,2,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float32,2}, ::KnetArray{Float32,2}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49 [inlined]
   [19] (::var"#p2#115"{Array{Int64,1}})(::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:81
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:84
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float32,2,Nothing}, ::Tuple{CuArrays.CuArray{Float32,2,Nothing},CuArrays.CuArray{Float32,2,Nothing},Tuple{Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [15] gpu_call(::Function, ::CuArrays.CuArray{Float32,2,Nothing}, ::Tuple{CuArrays.CuArray{Float32,2,Nothing},CuArrays.CuArray{Float32,2,Nothing},Tuple{Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [16] permutedims!(::CuArrays.CuArray{Float32,2,Nothing}, ::CuArrays.CuArray{Float32,2,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [17] permutedims!(::KnetArray{Float32,2}, ::KnetArray{Float32,2}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [18] permutedims(::KnetArray{Float32,2}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [19] forw(::Function, ::Param{KnetArray{Float32,2}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [21] permutedims at ./none:0 [inlined]
 [22] (::var"#p2#115"{Array{Int64,1}})(::Param{KnetArray{Float32,2}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:81
 [23] gcsum(::Function, ::Param{KnetArray{Float32,2}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum(::Function, ::Param{KnetArray{Float32,2}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p2#115"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gradcheck(::var"#p2#115"{Array{Int64,1}}, ::KnetArray{Float32,2}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [29] gradcheck(::Function, ::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:85
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:85
  Test threw exception
  Expression: gradcheck(p2, ka)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p2#115"{Array{Int64,1}}, ::KnetArray{Float32,2}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:85
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float32,2,Nothing}, ::Tuple{CuArrays.CuArray{Float32,2,Nothing},CuArrays.CuArray{Float32,2,Nothing},Tuple{Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float32,2,Nothing}, ::Tuple{CuArrays.CuArray{Float32,2,Nothing},CuArrays.CuArray{Float32,2,Nothing},Tuple{Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float32,2,Nothing}, ::CuArrays.CuArray{Float32,2,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float32,2}, ::KnetArray{Float32,2}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float32,2}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] forw(::Function, ::Param{KnetArray{Float32,2}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [21] permutedims at ./none:0 [inlined]
   [22] (::var"#p2#115"{Array{Int64,1}})(::Param{KnetArray{Float32,2}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:81
   [23] gcsum(::Function, ::Param{KnetArray{Float32,2}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum(::Function, ::Param{KnetArray{Float32,2}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p2#115"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gradcheck(::var"#p2#115"{Array{Int64,1}}, ::KnetArray{Float32,2}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [29] gradcheck(::Function, ::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:85
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:84
  Test threw exception
  Expression: isapprox(p2(a), Array(p2(ka)))
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float32,2,Nothing}, ::Tuple{CuArrays.CuArray{Float32,2,Nothing},CuArrays.CuArray{Float32,2,Nothing},Tuple{Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float32,2,Nothing}, ::Tuple{CuArrays.CuArray{Float32,2,Nothing},CuArrays.CuArray{Float32,2,Nothing},Tuple{Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float32,2,Nothing}, ::CuArrays.CuArray{Float32,2,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float32,2}, ::KnetArray{Float32,2}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49 [inlined]
   [19] (::var"#p2#115"{Array{Int64,1}})(::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:81
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:84
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float32,2,Nothing}, ::Tuple{CuArrays.CuArray{Float32,2,Nothing},CuArrays.CuArray{Float32,2,Nothing},Tuple{Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [15] gpu_call(::Function, ::CuArrays.CuArray{Float32,2,Nothing}, ::Tuple{CuArrays.CuArray{Float32,2,Nothing},CuArrays.CuArray{Float32,2,Nothing},Tuple{Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [16] permutedims!(::CuArrays.CuArray{Float32,2,Nothing}, ::CuArrays.CuArray{Float32,2,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [17] permutedims!(::KnetArray{Float32,2}, ::KnetArray{Float32,2}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [18] permutedims(::KnetArray{Float32,2}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [19] forw(::Function, ::Param{KnetArray{Float32,2}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [21] permutedims at ./none:0 [inlined]
 [22] (::var"#p2#115"{Array{Int64,1}})(::Param{KnetArray{Float32,2}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:81
 [23] gcsum(::Function, ::Param{KnetArray{Float32,2}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum(::Function, ::Param{KnetArray{Float32,2}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p2#115"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gradcheck(::var"#p2#115"{Array{Int64,1}}, ::KnetArray{Float32,2}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [29] gradcheck(::Function, ::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:85
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:85
  Test threw exception
  Expression: gradcheck(p2, ka)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p2#115"{Array{Int64,1}}, ::KnetArray{Float32,2}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:85
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float32,2,Nothing}, ::Tuple{CuArrays.CuArray{Float32,2,Nothing},CuArrays.CuArray{Float32,2,Nothing},Tuple{Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float32,2,Nothing}, ::Tuple{CuArrays.CuArray{Float32,2,Nothing},CuArrays.CuArray{Float32,2,Nothing},Tuple{Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float32,2,Nothing}, ::CuArrays.CuArray{Float32,2,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float32,2}, ::KnetArray{Float32,2}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float32,2}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] forw(::Function, ::Param{KnetArray{Float32,2}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [21] permutedims at ./none:0 [inlined]
   [22] (::var"#p2#115"{Array{Int64,1}})(::Param{KnetArray{Float32,2}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:81
   [23] gcsum(::Function, ::Param{KnetArray{Float32,2}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum(::Function, ::Param{KnetArray{Float32,2}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p2#115"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gradcheck(::var"#p2#115"{Array{Int64,1}}, ::KnetArray{Float32,2}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [29] gradcheck(::Function, ::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:85
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:95
  Test threw exception
  Expression: isapprox(p3(a3), Array(p3(k3)))
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49 [inlined]
   [19] (::var"#p3#116"{Array{Int64,1}})(::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:95
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [16] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [17] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [18] permutedims(::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [19] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [21] permutedims at ./none:0 [inlined]
 [22] (::var"#p3#116"{Array{Int64,1}})(::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
 [23] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum(::Function, ::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p3#116"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [29] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
  Test threw exception
  Expression: gradcheck(p3, k3)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [21] permutedims at ./none:0 [inlined]
   [22] (::var"#p3#116"{Array{Int64,1}})(::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
   [23] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum(::Function, ::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p3#116"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [29] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:95
  Test threw exception
  Expression: isapprox(p3(a3), Array(p3(k3)))
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49 [inlined]
   [19] (::var"#p3#116"{Array{Int64,1}})(::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:95
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [16] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [17] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [18] permutedims(::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [19] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [21] permutedims at ./none:0 [inlined]
 [22] (::var"#p3#116"{Array{Int64,1}})(::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
 [23] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum(::Function, ::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p3#116"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [29] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
  Test threw exception
  Expression: gradcheck(p3, k3)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [21] permutedims at ./none:0 [inlined]
   [22] (::var"#p3#116"{Array{Int64,1}})(::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
   [23] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum(::Function, ::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p3#116"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [29] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:95
  Test threw exception
  Expression: isapprox(p3(a3), Array(p3(k3)))
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49 [inlined]
   [19] (::var"#p3#116"{Array{Int64,1}})(::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:95
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [16] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [17] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [18] permutedims(::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [19] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [21] permutedims at ./none:0 [inlined]
 [22] (::var"#p3#116"{Array{Int64,1}})(::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
 [23] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum(::Function, ::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p3#116"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [29] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
  Test threw exception
  Expression: gradcheck(p3, k3)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [21] permutedims at ./none:0 [inlined]
   [22] (::var"#p3#116"{Array{Int64,1}})(::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
   [23] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum(::Function, ::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p3#116"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [29] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:95
  Test threw exception
  Expression: isapprox(p3(a3), Array(p3(k3)))
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49 [inlined]
   [19] (::var"#p3#116"{Array{Int64,1}})(::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:95
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [16] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [17] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [18] permutedims(::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [19] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [21] permutedims at ./none:0 [inlined]
 [22] (::var"#p3#116"{Array{Int64,1}})(::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
 [23] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum(::Function, ::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p3#116"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [29] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
  Test threw exception
  Expression: gradcheck(p3, k3)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [21] permutedims at ./none:0 [inlined]
   [22] (::var"#p3#116"{Array{Int64,1}})(::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
   [23] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum(::Function, ::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p3#116"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [29] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:95
  Test threw exception
  Expression: isapprox(p3(a3), Array(p3(k3)))
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49 [inlined]
   [19] (::var"#p3#116"{Array{Int64,1}})(::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:95
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [16] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [17] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [18] permutedims(::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [19] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [21] permutedims at ./none:0 [inlined]
 [22] (::var"#p3#116"{Array{Int64,1}})(::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
 [23] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum(::Function, ::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p3#116"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [29] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
  Test threw exception
  Expression: gradcheck(p3, k3)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [21] permutedims at ./none:0 [inlined]
   [22] (::var"#p3#116"{Array{Int64,1}})(::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
   [23] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum(::Function, ::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p3#116"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [29] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:95
  Test threw exception
  Expression: isapprox(p3(a3), Array(p3(k3)))
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49 [inlined]
   [19] (::var"#p3#116"{Array{Int64,1}})(::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:95
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [16] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [17] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [18] permutedims(::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [19] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [21] permutedims at ./none:0 [inlined]
 [22] (::var"#p3#116"{Array{Int64,1}})(::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
 [23] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum(::Function, ::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p3#116"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [29] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
  Test threw exception
  Expression: gradcheck(p3, k3)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [21] permutedims at ./none:0 [inlined]
   [22] (::var"#p3#116"{Array{Int64,1}})(::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
   [23] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum(::Function, ::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p3#116"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [29] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:106
  Test threw exception
  Expression: isapprox(p4(a4), Array(p4(k4)))
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,4,Nothing}, ::CuArrays.CuArray{Float64,4,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,4}, ::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49 [inlined]
   [19] (::var"#p4#117"{Array{Int64,1}})(::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:103
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:106
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [16] permutedims!(::CuArrays.CuArray{Float64,4,Nothing}, ::CuArrays.CuArray{Float64,4,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [17] permutedims!(::KnetArray{Float64,4}, ::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [18] permutedims(::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [19] forw(::Function, ::Param{KnetArray{Float64,4}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [21] permutedims at ./none:0 [inlined]
 [22] (::var"#p4#117"{Array{Int64,1}})(::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:103
 [23] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum(::Function, ::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p4#117"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gradcheck(::var"#p4#117"{Array{Int64,1}}, ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [29] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
  Test threw exception
  Expression: gradcheck(p4, k4)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p4#117"{Array{Int64,1}}, ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,4,Nothing}, ::CuArrays.CuArray{Float64,4,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,4}, ::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] forw(::Function, ::Param{KnetArray{Float64,4}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [21] permutedims at ./none:0 [inlined]
   [22] (::var"#p4#117"{Array{Int64,1}})(::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:103
   [23] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum(::Function, ::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p4#117"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gradcheck(::var"#p4#117"{Array{Int64,1}}, ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [29] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:106
  Test threw exception
  Expression: isapprox(p4(a4), Array(p4(k4)))
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,4,Nothing}, ::CuArrays.CuArray{Float64,4,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,4}, ::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49 [inlined]
   [19] (::var"#p4#117"{Array{Int64,1}})(::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:103
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:106
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [16] permutedims!(::CuArrays.CuArray{Float64,4,Nothing}, ::CuArrays.CuArray{Float64,4,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [17] permutedims!(::KnetArray{Float64,4}, ::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [18] permutedims(::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [19] forw(::Function, ::Param{KnetArray{Float64,4}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [21] permutedims at ./none:0 [inlined]
 [22] (::var"#p4#117"{Array{Int64,1}})(::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:103
 [23] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum(::Function, ::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p4#117"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gradcheck(::var"#p4#117"{Array{Int64,1}}, ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [29] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
  Test threw exception
  Expression: gradcheck(p4, k4)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p4#117"{Array{Int64,1}}, ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,4,Nothing}, ::CuArrays.CuArray{Float64,4,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,4}, ::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] forw(::Function, ::Param{KnetArray{Float64,4}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [21] permutedims at ./none:0 [inlined]
   [22] (::var"#p4#117"{Array{Int64,1}})(::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:103
   [23] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum(::Function, ::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p4#117"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gradcheck(::var"#p4#117"{Array{Int64,1}}, ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [29] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:106
  Test threw exception
  Expression: isapprox(p4(a4), Array(p4(k4)))
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,4,Nothing}, ::CuArrays.CuArray{Float64,4,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,4}, ::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49 [inlined]
   [19] (::var"#p4#117"{Array{Int64,1}})(::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:103
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:106
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [16] permutedims!(::CuArrays.CuArray{Float64,4,Nothing}, ::CuArrays.CuArray{Float64,4,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [17] permutedims!(::KnetArray{Float64,4}, ::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [18] permutedims(::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [19] forw(::Function, ::Param{KnetArray{Float64,4}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [21] permutedims at ./none:0 [inlined]
 [22] (::var"#p4#117"{Array{Int64,1}})(::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:103
 [23] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum(::Function, ::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p4#117"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gradcheck(::var"#p4#117"{Array{Int64,1}}, ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [29] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
  Test threw exception
  Expression: gradcheck(p4, k4)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p4#117"{Array{Int64,1}}, ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,4,Nothing}, ::CuArrays.CuArray{Float64,4,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,4}, ::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] forw(::Function, ::Param{KnetArray{Float64,4}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [21] permutedims at ./none:0 [inlined]
   [22] (::var"#p4#117"{Array{Int64,1}})(::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:103
   [23] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum(::Function, ::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p4#117"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gradcheck(::var"#p4#117"{Array{Int64,1}}, ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [29] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:106
  Test threw exception
  Expression: isapprox(p4(a4), Array(p4(k4)))
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,4,Nothing}, ::CuArrays.CuArray{Float64,4,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,4}, ::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49 [inlined]
   [19] (::var"#p4#117"{Array{Int64,1}})(::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:103
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:106
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [16] permutedims!(::CuArrays.CuArray{Float64,4,Nothing}, ::CuArrays.CuArray{Float64,4,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [17] permutedims!(::KnetArray{Float64,4}, ::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [18] permutedims(::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [19] forw(::Function, ::Param{KnetArray{Float64,4}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [21] permutedims at ./none:0 [inlined]
 [22] (::var"#p4#117"{Array{Int64,1}})(::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:103
 [23] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum(::Function, ::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p4#117"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gradcheck(::var"#p4#117"{Array{Int64,1}}, ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [29] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
  Test threw exception
  Expression: gradcheck(p4, k4)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p4#117"{Array{Int64,1}}, ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,4,Nothing}, ::CuArrays.CuArray{Float64,4,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,4}, ::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] forw(::Function, ::Param{KnetArray{Float64,4}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [21] permutedims at ./none:0 [inlined]
   [22] (::var"#p4#117"{Array{Int64,1}})(::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:103
   [23] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum(::Function, ::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p4#117"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gradcheck(::var"#p4#117"{Array{Int64,1}}, ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [29] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:106
  Test threw exception
  Expression: isapprox(p4(a4), Array(p4(k4)))
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,4,Nothing}, ::CuArrays.CuArray{Float64,4,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,4}, ::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49 [inlined]
   [19] (::var"#p4#117"{Array{Int64,1}})(::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:103
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:106
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [16] permutedims!(::CuArrays.CuArray{Float64,4,Nothing}, ::CuArrays.CuArray{Float64,4,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [17] permutedims!(::KnetArray{Float64,4}, ::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [18] permutedims(::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [19] forw(::Function, ::Param{KnetArray{Float64,4}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [21] permutedims at ./none:0 [inlined]
 [22] (::var"#p4#117"{Array{Int64,1}})(::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:103
 [23] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum(::Function, ::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p4#117"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gradcheck(::var"#p4#117"{Array{Int64,1}}, ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [29] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
  Test threw exception
  Expression: gradcheck(p4, k4)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p4#117"{Array{Int64,1}}, ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,4,Nothing}, ::CuArrays.CuArray{Float64,4,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,4}, ::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] forw(::Function, ::Param{KnetArray{Float64,4}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [21] permutedims at ./none:0 [inlined]
   [22] (::var"#p4#117"{Array{Int64,1}})(::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:103
   [23] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum(::Function, ::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p4#117"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gradcheck(::var"#p4#117"{Array{Int64,1}}, ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [29] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:106
  Test threw exception
  Expression: isapprox(p4(a4), Array(p4(k4)))
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,4,Nothing}, ::CuArrays.CuArray{Float64,4,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,4}, ::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49 [inlined]
   [19] (::var"#p4#117"{Array{Int64,1}})(::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:103
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:106
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [16] permutedims!(::CuArrays.CuArray{Float64,4,Nothing}, ::CuArrays.CuArray{Float64,4,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [17] permutedims!(::KnetArray{Float64,4}, ::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [18] permutedims(::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [19] forw(::Function, ::Param{KnetArray{Float64,4}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [21] permutedims at ./none:0 [inlined]
 [22] (::var"#p4#117"{Array{Int64,1}})(::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:103
 [23] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum(::Function, ::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p4#117"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gradcheck(::var"#p4#117"{Array{Int64,1}}, ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [29] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
  Test threw exception
  Expression: gradcheck(p4, k4)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p4#117"{Array{Int64,1}}, ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,4,Nothing}, ::CuArrays.CuArray{Float64,4,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,4}, ::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] forw(::Function, ::Param{KnetArray{Float64,4}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [21] permutedims at ./none:0 [inlined]
   [22] (::var"#p4#117"{Array{Int64,1}})(::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:103
   [23] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum(::Function, ::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p4#117"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gradcheck(::var"#p4#117"{Array{Int64,1}}, ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [29] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:117
  Test threw exception
  Expression: isapprox(p5(a5), Array(p5(k5)))
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},NTuple{5,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,5,Nothing}, ::CuArrays.CuArray{Float64,5,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,5}, ::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] (::var"#p5#118"{Array{Int64,1}})(::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:114
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:117
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},NTuple{5,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [16] permutedims!(::CuArrays.CuArray{Float64,5,Nothing}, ::CuArrays.CuArray{Float64,5,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [17] permutedims!(::KnetArray{Float64,5}, ::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [18] permutedims(::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [19] forw(::Function, ::Param{KnetArray{Float64,5}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [21] permutedims at ./none:0 [inlined]
 [22] (::var"#p5#118"{Array{Int64,1}})(::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:114
 [23] gcsum(::Function, ::Param{KnetArray{Float64,5}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum(::Function, ::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p5#118"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gradcheck(::var"#p5#118"{Array{Int64,1}}, ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [29] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:118
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:118
  Test threw exception
  Expression: gradcheck(p5, k5)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p5#118"{Array{Int64,1}}, ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:118
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},NTuple{5,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,5,Nothing}, ::CuArrays.CuArray{Float64,5,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,5}, ::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] forw(::Function, ::Param{KnetArray{Float64,5}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [21] permutedims at ./none:0 [inlined]
   [22] (::var"#p5#118"{Array{Int64,1}})(::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:114
   [23] gcsum(::Function, ::Param{KnetArray{Float64,5}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum(::Function, ::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p5#118"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gradcheck(::var"#p5#118"{Array{Int64,1}}, ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [29] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:118
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:117
  Test threw exception
  Expression: isapprox(p5(a5), Array(p5(k5)))
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},NTuple{5,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,5,Nothing}, ::CuArrays.CuArray{Float64,5,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,5}, ::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] (::var"#p5#118"{Array{Int64,1}})(::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:114
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:117
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},NTuple{5,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [16] permutedims!(::CuArrays.CuArray{Float64,5,Nothing}, ::CuArrays.CuArray{Float64,5,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [17] permutedims!(::KnetArray{Float64,5}, ::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [18] permutedims(::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [19] forw(::Function, ::Param{KnetArray{Float64,5}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [21] permutedims at ./none:0 [inlined]
 [22] (::var"#p5#118"{Array{Int64,1}})(::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:114
 [23] gcsum(::Function, ::Param{KnetArray{Float64,5}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum(::Function, ::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p5#118"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gradcheck(::var"#p5#118"{Array{Int64,1}}, ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [29] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:118
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:118
  Test threw exception
  Expression: gradcheck(p5, k5)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p5#118"{Array{Int64,1}}, ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:118
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},NTuple{5,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,5,Nothing}, ::CuArrays.CuArray{Float64,5,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,5}, ::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] forw(::Function, ::Param{KnetArray{Float64,5}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [21] permutedims at ./none:0 [inlined]
   [22] (::var"#p5#118"{Array{Int64,1}})(::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:114
   [23] gcsum(::Function, ::Param{KnetArray{Float64,5}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum(::Function, ::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p5#118"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gradcheck(::var"#p5#118"{Array{Int64,1}}, ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [29] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:118
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:117
  Test threw exception
  Expression: isapprox(p5(a5), Array(p5(k5)))
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},NTuple{5,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,5,Nothing}, ::CuArrays.CuArray{Float64,5,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,5}, ::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] (::var"#p5#118"{Array{Int64,1}})(::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:114
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:117
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},NTuple{5,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [16] permutedims!(::CuArrays.CuArray{Float64,5,Nothing}, ::CuArrays.CuArray{Float64,5,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [17] permutedims!(::KnetArray{Float64,5}, ::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [18] permutedims(::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [19] forw(::Function, ::Param{KnetArray{Float64,5}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [21] permutedims at ./none:0 [inlined]
 [22] (::var"#p5#118"{Array{Int64,1}})(::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:114
 [23] gcsum(::Function, ::Param{KnetArray{Float64,5}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum(::Function, ::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p5#118"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gradcheck(::var"#p5#118"{Array{Int64,1}}, ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [29] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:118
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:118
  Test threw exception
  Expression: gradcheck(p5, k5)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p5#118"{Array{Int64,1}}, ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:118
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},NTuple{5,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,5,Nothing}, ::CuArrays.CuArray{Float64,5,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,5}, ::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] forw(::Function, ::Param{KnetArray{Float64,5}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [21] permutedims at ./none:0 [inlined]
   [22] (::var"#p5#118"{Array{Int64,1}})(::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:114
   [23] gcsum(::Function, ::Param{KnetArray{Float64,5}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum(::Function, ::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p5#118"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gradcheck(::var"#p5#118"{Array{Int64,1}}, ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [29] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:118
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:117
  Test threw exception
  Expression: isapprox(p5(a5), Array(p5(k5)))
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},NTuple{5,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,5,Nothing}, ::CuArrays.CuArray{Float64,5,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,5}, ::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] (::var"#p5#118"{Array{Int64,1}})(::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:114
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:117
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},NTuple{5,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [16] permutedims!(::CuArrays.CuArray{Float64,5,Nothing}, ::CuArrays.CuArray{Float64,5,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [17] permutedims!(::KnetArray{Float64,5}, ::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [18] permutedims(::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [19] forw(::Function, ::Param{KnetArray{Float64,5}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [21] permutedims at ./none:0 [inlined]
 [22] (::var"#p5#118"{Array{Int64,1}})(::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:114
 [23] gcsum(::Function, ::Param{KnetArray{Float64,5}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum(::Function, ::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p5#118"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gradcheck(::var"#p5#118"{Array{Int64,1}}, ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [29] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:118
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:118
  Test threw exception
  Expression: gradcheck(p5, k5)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p5#118"{Array{Int64,1}}, ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:118
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},NTuple{5,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,5,Nothing}, ::CuArrays.CuArray{Float64,5,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,5}, ::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] forw(::Function, ::Param{KnetArray{Float64,5}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [21] permutedims at ./none:0 [inlined]
   [22] (::var"#p5#118"{Array{Int64,1}})(::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:114
   [23] gcsum(::Function, ::Param{KnetArray{Float64,5}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum(::Function, ::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p5#118"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gradcheck(::var"#p5#118"{Array{Int64,1}}, ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [29] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:118
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:117
  Test threw exception
  Expression: isapprox(p5(a5), Array(p5(k5)))
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},NTuple{5,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,5,Nothing}, ::CuArrays.CuArray{Float64,5,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,5}, ::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] (::var"#p5#118"{Array{Int64,1}})(::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:114
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:117
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},NTuple{5,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [16] permutedims!(::CuArrays.CuArray{Float64,5,Nothing}, ::CuArrays.CuArray{Float64,5,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [17] permutedims!(::KnetArray{Float64,5}, ::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [18] permutedims(::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [19] forw(::Function, ::Param{KnetArray{Float64,5}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [21] permutedims at ./none:0 [inlined]
 [22] (::var"#p5#118"{Array{Int64,1}})(::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:114
 [23] gcsum(::Function, ::Param{KnetArray{Float64,5}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum(::Function, ::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p5#118"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gradcheck(::var"#p5#118"{Array{Int64,1}}, ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [29] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:118
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:118
  Test threw exception
  Expression: gradcheck(p5, k5)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p5#118"{Array{Int64,1}}, ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:118
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},NTuple{5,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,5,Nothing}, ::CuArrays.CuArray{Float64,5,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,5}, ::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] forw(::Function, ::Param{KnetArray{Float64,5}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [21] permutedims at ./none:0 [inlined]
   [22] (::var"#p5#118"{Array{Int64,1}})(::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:114
   [23] gcsum(::Function, ::Param{KnetArray{Float64,5}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum(::Function, ::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p5#118"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gradcheck(::var"#p5#118"{Array{Int64,1}}, ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [29] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:118
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:117
  Test threw exception
  Expression: isapprox(p5(a5), Array(p5(k5)))
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},NTuple{5,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,5,Nothing}, ::CuArrays.CuArray{Float64,5,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,5}, ::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] (::var"#p5#118"{Array{Int64,1}})(::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:114
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:117
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},NTuple{5,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [16] permutedims!(::CuArrays.CuArray{Float64,5,Nothing}, ::CuArrays.CuArray{Float64,5,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [17] permutedims!(::KnetArray{Float64,5}, ::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [18] permutedims(::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [19] forw(::Function, ::Param{KnetArray{Float64,5}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [21] permutedims at ./none:0 [inlined]
 [22] (::var"#p5#118"{Array{Int64,1}})(::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:114
 [23] gcsum(::Function, ::Param{KnetArray{Float64,5}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum(::Function, ::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p5#118"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gradcheck(::var"#p5#118"{Array{Int64,1}}, ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [29] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:118
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:118
  Test threw exception
  Expression: gradcheck(p5, k5)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p5#118"{Array{Int64,1}}, ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:118
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},NTuple{5,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,5,Nothing}, ::CuArrays.CuArray{Float64,5,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,5}, ::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] forw(::Function, ::Param{KnetArray{Float64,5}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [21] permutedims at ./none:0 [inlined]
   [22] (::var"#p5#118"{Array{Int64,1}})(::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:114
   [23] gcsum(::Function, ::Param{KnetArray{Float64,5}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum(::Function, ::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p5#118"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gradcheck(::var"#p5#118"{Array{Int64,1}}, ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [29] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:118
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:84
  Test threw exception
  Expression: isapprox(p2(a), Array(p2(ka)))
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,2,Nothing}, ::CuArrays.CuArray{Float64,2,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,2}, ::KnetArray{Float64,2}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49 [inlined]
   [19] (::var"#p2#115"{Array{Int64,1}})(::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:81
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:84
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [16] permutedims!(::CuArrays.CuArray{Float64,2,Nothing}, ::CuArrays.CuArray{Float64,2,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [17] permutedims!(::KnetArray{Float64,2}, ::KnetArray{Float64,2}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [18] permutedims(::KnetArray{Float64,2}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [19] forw(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [21] permutedims at ./none:0 [inlined]
 [22] (::var"#p2#115"{Array{Int64,1}})(::Param{KnetArray{Float64,2}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:81
 [23] gcsum(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p2#115"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gradcheck(::var"#p2#115"{Array{Int64,1}}, ::KnetArray{Float64,2}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [29] gradcheck(::Function, ::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:85
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:85
  Test threw exception
  Expression: gradcheck(p2, ka)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p2#115"{Array{Int64,1}}, ::KnetArray{Float64,2}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:85
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,2,Nothing}, ::CuArrays.CuArray{Float64,2,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,2}, ::KnetArray{Float64,2}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,2}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] forw(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [21] permutedims at ./none:0 [inlined]
   [22] (::var"#p2#115"{Array{Int64,1}})(::Param{KnetArray{Float64,2}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:81
   [23] gcsum(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p2#115"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gradcheck(::var"#p2#115"{Array{Int64,1}}, ::KnetArray{Float64,2}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [29] gradcheck(::Function, ::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:85
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:84
  Test threw exception
  Expression: isapprox(p2(a), Array(p2(ka)))
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,2,Nothing}, ::CuArrays.CuArray{Float64,2,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,2}, ::KnetArray{Float64,2}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49 [inlined]
   [19] (::var"#p2#115"{Array{Int64,1}})(::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:81
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:84
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [16] permutedims!(::CuArrays.CuArray{Float64,2,Nothing}, ::CuArrays.CuArray{Float64,2,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [17] permutedims!(::KnetArray{Float64,2}, ::KnetArray{Float64,2}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [18] permutedims(::KnetArray{Float64,2}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [19] forw(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [21] permutedims at ./none:0 [inlined]
 [22] (::var"#p2#115"{Array{Int64,1}})(::Param{KnetArray{Float64,2}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:81
 [23] gcsum(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p2#115"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gradcheck(::var"#p2#115"{Array{Int64,1}}, ::KnetArray{Float64,2}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [29] gradcheck(::Function, ::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:85
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:85
  Test threw exception
  Expression: gradcheck(p2, ka)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p2#115"{Array{Int64,1}}, ::KnetArray{Float64,2}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:85
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,2,Nothing}, ::CuArrays.CuArray{Float64,2,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,2}, ::KnetArray{Float64,2}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,2}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] forw(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [21] permutedims at ./none:0 [inlined]
   [22] (::var"#p2#115"{Array{Int64,1}})(::Param{KnetArray{Float64,2}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:81
   [23] gcsum(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p2#115"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gradcheck(::var"#p2#115"{Array{Int64,1}}, ::KnetArray{Float64,2}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [29] gradcheck(::Function, ::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:85
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:95
  Test threw exception
  Expression: isapprox(p3(a3), Array(p3(k3)))
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49 [inlined]
   [19] (::var"#p3#116"{Array{Int64,1}})(::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:95
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [16] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [17] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [18] permutedims(::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [19] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [21] permutedims at ./none:0 [inlined]
 [22] (::var"#p3#116"{Array{Int64,1}})(::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
 [23] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum(::Function, ::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p3#116"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [29] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
  Test threw exception
  Expression: gradcheck(p3, k3)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [21] permutedims at ./none:0 [inlined]
   [22] (::var"#p3#116"{Array{Int64,1}})(::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
   [23] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum(::Function, ::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p3#116"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [29] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:95
  Test threw exception
  Expression: isapprox(p3(a3), Array(p3(k3)))
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49 [inlined]
   [19] (::var"#p3#116"{Array{Int64,1}})(::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:95
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [16] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [17] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [18] permutedims(::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [19] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [21] permutedims at ./none:0 [inlined]
 [22] (::var"#p3#116"{Array{Int64,1}})(::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
 [23] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum(::Function, ::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p3#116"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [29] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
  Test threw exception
  Expression: gradcheck(p3, k3)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [21] permutedims at ./none:0 [inlined]
   [22] (::var"#p3#116"{Array{Int64,1}})(::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
   [23] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum(::Function, ::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p3#116"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [29] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:95
  Test threw exception
  Expression: isapprox(p3(a3), Array(p3(k3)))
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49 [inlined]
   [19] (::var"#p3#116"{Array{Int64,1}})(::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:95
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [16] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [17] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [18] permutedims(::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [19] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [21] permutedims at ./none:0 [inlined]
 [22] (::var"#p3#116"{Array{Int64,1}})(::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
 [23] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum(::Function, ::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p3#116"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [29] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
  Test threw exception
  Expression: gradcheck(p3, k3)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [21] permutedims at ./none:0 [inlined]
   [22] (::var"#p3#116"{Array{Int64,1}})(::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
   [23] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum(::Function, ::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p3#116"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [29] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:95
  Test threw exception
  Expression: isapprox(p3(a3), Array(p3(k3)))
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49 [inlined]
   [19] (::var"#p3#116"{Array{Int64,1}})(::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:95
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [16] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [17] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [18] permutedims(::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [19] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [21] permutedims at ./none:0 [inlined]
 [22] (::var"#p3#116"{Array{Int64,1}})(::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
 [23] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum(::Function, ::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p3#116"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [29] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
  Test threw exception
  Expression: gradcheck(p3, k3)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [21] permutedims at ./none:0 [inlined]
   [22] (::var"#p3#116"{Array{Int64,1}})(::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
   [23] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum(::Function, ::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p3#116"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [29] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:95
  Test threw exception
  Expression: isapprox(p3(a3), Array(p3(k3)))
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49 [inlined]
   [19] (::var"#p3#116"{Array{Int64,1}})(::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:95
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [16] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [17] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [18] permutedims(::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [19] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [21] permutedims at ./none:0 [inlined]
 [22] (::var"#p3#116"{Array{Int64,1}})(::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
 [23] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum(::Function, ::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p3#116"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [29] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
  Test threw exception
  Expression: gradcheck(p3, k3)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [21] permutedims at ./none:0 [inlined]
   [22] (::var"#p3#116"{Array{Int64,1}})(::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
   [23] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum(::Function, ::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p3#116"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [29] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:95
  Test threw exception
  Expression: isapprox(p3(a3), Array(p3(k3)))
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49 [inlined]
   [19] (::var"#p3#116"{Array{Int64,1}})(::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:95
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [16] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [17] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [18] permutedims(::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [19] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [21] permutedims at ./none:0 [inlined]
 [22] (::var"#p3#116"{Array{Int64,1}})(::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
 [23] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum(::Function, ::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p3#116"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [29] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
  Test threw exception
  Expression: gradcheck(p3, k3)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [21] permutedims at ./none:0 [inlined]
   [22] (::var"#p3#116"{Array{Int64,1}})(::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
   [23] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum(::Function, ::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p3#116"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [29] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:106
  Test threw exception
  Expression: isapprox(p4(a4), Array(p4(k4)))
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,4,Nothing}, ::CuArrays.CuArray{Float64,4,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,4}, ::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49 [inlined]
   [19] (::var"#p4#117"{Array{Int64,1}})(::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:103
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:106
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [16] permutedims!(::CuArrays.CuArray{Float64,4,Nothing}, ::CuArrays.CuArray{Float64,4,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [17] permutedims!(::KnetArray{Float64,4}, ::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [18] permutedims(::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [19] forw(::Function, ::Param{KnetArray{Float64,4}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [21] permutedims at ./none:0 [inlined]
 [22] (::var"#p4#117"{Array{Int64,1}})(::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:103
 [23] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum(::Function, ::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p4#117"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gradcheck(::var"#p4#117"{Array{Int64,1}}, ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [29] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
  Test threw exception
  Expression: gradcheck(p4, k4)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p4#117"{Array{Int64,1}}, ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,4,Nothing}, ::CuArrays.CuArray{Float64,4,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,4}, ::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] forw(::Function, ::Param{KnetArray{Float64,4}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [21] permutedims at ./none:0 [inlined]
   [22] (::var"#p4#117"{Array{Int64,1}})(::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:103
   [23] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum(::Function, ::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p4#117"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gradcheck(::var"#p4#117"{Array{Int64,1}}, ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [29] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:106
  Test threw exception
  Expression: isapprox(p4(a4), Array(p4(k4)))
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,4,Nothing}, ::CuArrays.CuArray{Float64,4,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,4}, ::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49 [inlined]
   [19] (::var"#p4#117"{Array{Int64,1}})(::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:103
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:106
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [16] permutedims!(::CuArrays.CuArray{Float64,4,Nothing}, ::CuArrays.CuArray{Float64,4,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [17] permutedims!(::KnetArray{Float64,4}, ::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [18] permutedims(::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [19] forw(::Function, ::Param{KnetArray{Float64,4}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [21] permutedims at ./none:0 [inlined]
 [22] (::var"#p4#117"{Array{Int64,1}})(::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:103
 [23] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum(::Function, ::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p4#117"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gradcheck(::var"#p4#117"{Array{Int64,1}}, ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [29] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
  Test threw exception
  Expression: gradcheck(p4, k4)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p4#117"{Array{Int64,1}}, ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,4,Nothing}, ::CuArrays.CuArray{Float64,4,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,4}, ::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] forw(::Function, ::Param{KnetArray{Float64,4}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [21] permutedims at ./none:0 [inlined]
   [22] (::var"#p4#117"{Array{Int64,1}})(::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:103
   [23] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum(::Function, ::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p4#117"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gradcheck(::var"#p4#117"{Array{Int64,1}}, ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [29] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:106
  Test threw exception
  Expression: isapprox(p4(a4), Array(p4(k4)))
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,4,Nothing}, ::CuArrays.CuArray{Float64,4,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,4}, ::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49 [inlined]
   [19] (::var"#p4#117"{Array{Int64,1}})(::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:103
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:106
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [16] permutedims!(::CuArrays.CuArray{Float64,4,Nothing}, ::CuArrays.CuArray{Float64,4,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [17] permutedims!(::KnetArray{Float64,4}, ::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [18] permutedims(::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [19] forw(::Function, ::Param{KnetArray{Float64,4}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [21] permutedims at ./none:0 [inlined]
 [22] (::var"#p4#117"{Array{Int64,1}})(::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:103
 [23] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum(::Function, ::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p4#117"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gradcheck(::var"#p4#117"{Array{Int64,1}}, ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [29] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
  Test threw exception
  Expression: gradcheck(p4, k4)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p4#117"{Array{Int64,1}}, ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,4,Nothing}, ::CuArrays.CuArray{Float64,4,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,4}, ::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] forw(::Function, ::Param{KnetArray{Float64,4}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [21] permutedims at ./none:0 [inlined]
   [22] (::var"#p4#117"{Array{Int64,1}})(::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:103
   [23] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum(::Function, ::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p4#117"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gradcheck(::var"#p4#117"{Array{Int64,1}}, ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [29] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:106
  Test threw exception
  Expression: isapprox(p4(a4), Array(p4(k4)))
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,4,Nothing}, ::CuArrays.CuArray{Float64,4,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,4}, ::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49 [inlined]
   [19] (::var"#p4#117"{Array{Int64,1}})(::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:103
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:106
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [16] permutedims!(::CuArrays.CuArray{Float64,4,Nothing}, ::CuArrays.CuArray{Float64,4,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [17] permutedims!(::KnetArray{Float64,4}, ::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [18] permutedims(::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [19] forw(::Function, ::Param{KnetArray{Float64,4}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [21] permutedims at ./none:0 [inlined]
 [22] (::var"#p4#117"{Array{Int64,1}})(::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:103
 [23] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum(::Function, ::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p4#117"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gradcheck(::var"#p4#117"{Array{Int64,1}}, ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [29] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
  Test threw exception
  Expression: gradcheck(p4, k4)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p4#117"{Array{Int64,1}}, ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,4,Nothing}, ::CuArrays.CuArray{Float64,4,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,4}, ::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] forw(::Function, ::Param{KnetArray{Float64,4}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [21] permutedims at ./none:0 [inlined]
   [22] (::var"#p4#117"{Array{Int64,1}})(::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:103
   [23] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum(::Function, ::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p4#117"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gradcheck(::var"#p4#117"{Array{Int64,1}}, ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [29] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:106
  Test threw exception
  Expression: isapprox(p4(a4), Array(p4(k4)))
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,4,Nothing}, ::CuArrays.CuArray{Float64,4,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,4}, ::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49 [inlined]
   [19] (::var"#p4#117"{Array{Int64,1}})(::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:103
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:106
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [16] permutedims!(::CuArrays.CuArray{Float64,4,Nothing}, ::CuArrays.CuArray{Float64,4,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [17] permutedims!(::KnetArray{Float64,4}, ::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [18] permutedims(::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [19] forw(::Function, ::Param{KnetArray{Float64,4}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [21] permutedims at ./none:0 [inlined]
 [22] (::var"#p4#117"{Array{Int64,1}})(::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:103
 [23] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum(::Function, ::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p4#117"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gradcheck(::var"#p4#117"{Array{Int64,1}}, ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [29] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
  Test threw exception
  Expression: gradcheck(p4, k4)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p4#117"{Array{Int64,1}}, ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,4,Nothing}, ::CuArrays.CuArray{Float64,4,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,4}, ::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] forw(::Function, ::Param{KnetArray{Float64,4}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [21] permutedims at ./none:0 [inlined]
   [22] (::var"#p4#117"{Array{Int64,1}})(::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:103
   [23] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum(::Function, ::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p4#117"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gradcheck(::var"#p4#117"{Array{Int64,1}}, ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [29] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:106
  Test threw exception
  Expression: isapprox(p4(a4), Array(p4(k4)))
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,4,Nothing}, ::CuArrays.CuArray{Float64,4,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,4}, ::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49 [inlined]
   [19] (::var"#p4#117"{Array{Int64,1}})(::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:103
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:106
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [16] permutedims!(::CuArrays.CuArray{Float64,4,Nothing}, ::CuArrays.CuArray{Float64,4,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [17] permutedims!(::KnetArray{Float64,4}, ::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [18] permutedims(::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [19] forw(::Function, ::Param{KnetArray{Float64,4}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [21] permutedims at ./none:0 [inlined]
 [22] (::var"#p4#117"{Array{Int64,1}})(::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:103
 [23] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum(::Function, ::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p4#117"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gradcheck(::var"#p4#117"{Array{Int64,1}}, ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [29] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
  Test threw exception
  Expression: gradcheck(p4, k4)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p4#117"{Array{Int64,1}}, ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,4,Nothing}, ::CuArrays.CuArray{Float64,4,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,4}, ::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] forw(::Function, ::Param{KnetArray{Float64,4}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [21] permutedims at ./none:0 [inlined]
   [22] (::var"#p4#117"{Array{Int64,1}})(::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:103
   [23] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum(::Function, ::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p4#117"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gradcheck(::var"#p4#117"{Array{Int64,1}}, ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [29] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:117
  Test threw exception
  Expression: isapprox(p5(a5), Array(p5(k5)))
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},NTuple{5,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,5,Nothing}, ::CuArrays.CuArray{Float64,5,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,5}, ::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] (::var"#p5#118"{Array{Int64,1}})(::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:114
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:117
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},NTuple{5,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [16] permutedims!(::CuArrays.CuArray{Float64,5,Nothing}, ::CuArrays.CuArray{Float64,5,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [17] permutedims!(::KnetArray{Float64,5}, ::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [18] permutedims(::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [19] forw(::Function, ::Param{KnetArray{Float64,5}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [21] permutedims at ./none:0 [inlined]
 [22] (::var"#p5#118"{Array{Int64,1}})(::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:114
 [23] gcsum(::Function, ::Param{KnetArray{Float64,5}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum(::Function, ::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p5#118"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gradcheck(::var"#p5#118"{Array{Int64,1}}, ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [29] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:118
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:118
  Test threw exception
  Expression: gradcheck(p5, k5)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p5#118"{Array{Int64,1}}, ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:118
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},NTuple{5,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,5,Nothing}, ::CuArrays.CuArray{Float64,5,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,5}, ::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] forw(::Function, ::Param{KnetArray{Float64,5}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [21] permutedims at ./none:0 [inlined]
   [22] (::var"#p5#118"{Array{Int64,1}})(::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:114
   [23] gcsum(::Function, ::Param{KnetArray{Float64,5}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum(::Function, ::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p5#118"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gradcheck(::var"#p5#118"{Array{Int64,1}}, ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [29] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:118
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:117
  Test threw exception
  Expression: isapprox(p5(a5), Array(p5(k5)))
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},NTuple{5,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,5,Nothing}, ::CuArrays.CuArray{Float64,5,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,5}, ::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] (::var"#p5#118"{Array{Int64,1}})(::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:114
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:117
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},NTuple{5,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [16] permutedims!(::CuArrays.CuArray{Float64,5,Nothing}, ::CuArrays.CuArray{Float64,5,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [17] permutedims!(::KnetArray{Float64,5}, ::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [18] permutedims(::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [19] forw(::Function, ::Param{KnetArray{Float64,5}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [21] permutedims at ./none:0 [inlined]
 [22] (::var"#p5#118"{Array{Int64,1}})(::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:114
 [23] gcsum(::Function, ::Param{KnetArray{Float64,5}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum(::Function, ::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p5#118"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gradcheck(::var"#p5#118"{Array{Int64,1}}, ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [29] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:118
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:118
  Test threw exception
  Expression: gradcheck(p5, k5)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p5#118"{Array{Int64,1}}, ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:118
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},NTuple{5,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,5,Nothing}, ::CuArrays.CuArray{Float64,5,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,5}, ::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] forw(::Function, ::Param{KnetArray{Float64,5}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [21] permutedims at ./none:0 [inlined]
   [22] (::var"#p5#118"{Array{Int64,1}})(::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:114
   [23] gcsum(::Function, ::Param{KnetArray{Float64,5}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum(::Function, ::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p5#118"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gradcheck(::var"#p5#118"{Array{Int64,1}}, ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [29] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:118
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:117
  Test threw exception
  Expression: isapprox(p5(a5), Array(p5(k5)))
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},NTuple{5,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,5,Nothing}, ::CuArrays.CuArray{Float64,5,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,5}, ::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] (::var"#p5#118"{Array{Int64,1}})(::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:114
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:117
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},NTuple{5,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [16] permutedims!(::CuArrays.CuArray{Float64,5,Nothing}, ::CuArrays.CuArray{Float64,5,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [17] permutedims!(::KnetArray{Float64,5}, ::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [18] permutedims(::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [19] forw(::Function, ::Param{KnetArray{Float64,5}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [21] permutedims at ./none:0 [inlined]
 [22] (::var"#p5#118"{Array{Int64,1}})(::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:114
 [23] gcsum(::Function, ::Param{KnetArray{Float64,5}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum(::Function, ::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p5#118"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gradcheck(::var"#p5#118"{Array{Int64,1}}, ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [29] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:118
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:118
  Test threw exception
  Expression: gradcheck(p5, k5)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p5#118"{Array{Int64,1}}, ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:118
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},NTuple{5,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,5,Nothing}, ::CuArrays.CuArray{Float64,5,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,5}, ::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] forw(::Function, ::Param{KnetArray{Float64,5}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [21] permutedims at ./none:0 [inlined]
   [22] (::var"#p5#118"{Array{Int64,1}})(::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:114
   [23] gcsum(::Function, ::Param{KnetArray{Float64,5}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum(::Function, ::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p5#118"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gradcheck(::var"#p5#118"{Array{Int64,1}}, ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [29] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:118
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:117
  Test threw exception
  Expression: isapprox(p5(a5), Array(p5(k5)))
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},NTuple{5,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,5,Nothing}, ::CuArrays.CuArray{Float64,5,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,5}, ::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] (::var"#p5#118"{Array{Int64,1}})(::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:114
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:117
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},NTuple{5,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [16] permutedims!(::CuArrays.CuArray{Float64,5,Nothing}, ::CuArrays.CuArray{Float64,5,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [17] permutedims!(::KnetArray{Float64,5}, ::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [18] permutedims(::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [19] forw(::Function, ::Param{KnetArray{Float64,5}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [21] permutedims at ./none:0 [inlined]
 [22] (::var"#p5#118"{Array{Int64,1}})(::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:114
 [23] gcsum(::Function, ::Param{KnetArray{Float64,5}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum(::Function, ::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p5#118"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gradcheck(::var"#p5#118"{Array{Int64,1}}, ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [29] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:118
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:118
  Test threw exception
  Expression: gradcheck(p5, k5)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p5#118"{Array{Int64,1}}, ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:118
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},NTuple{5,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,5,Nothing}, ::CuArrays.CuArray{Float64,5,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,5}, ::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] forw(::Function, ::Param{KnetArray{Float64,5}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [21] permutedims at ./none:0 [inlined]
   [22] (::var"#p5#118"{Array{Int64,1}})(::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:114
   [23] gcsum(::Function, ::Param{KnetArray{Float64,5}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum(::Function, ::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p5#118"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gradcheck(::var"#p5#118"{Array{Int64,1}}, ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [29] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:118
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:117
  Test threw exception
  Expression: isapprox(p5(a5), Array(p5(k5)))
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},NTuple{5,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,5,Nothing}, ::CuArrays.CuArray{Float64,5,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,5}, ::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] (::var"#p5#118"{Array{Int64,1}})(::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:114
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:117
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},NTuple{5,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [16] permutedims!(::CuArrays.CuArray{Float64,5,Nothing}, ::CuArrays.CuArray{Float64,5,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [17] permutedims!(::KnetArray{Float64,5}, ::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [18] permutedims(::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [19] forw(::Function, ::Param{KnetArray{Float64,5}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [21] permutedims at ./none:0 [inlined]
 [22] (::var"#p5#118"{Array{Int64,1}})(::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:114
 [23] gcsum(::Function, ::Param{KnetArray{Float64,5}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum(::Function, ::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p5#118"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gradcheck(::var"#p5#118"{Array{Int64,1}}, ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [29] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:118
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:118
  Test threw exception
  Expression: gradcheck(p5, k5)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p5#118"{Array{Int64,1}}, ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:118
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},NTuple{5,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,5,Nothing}, ::CuArrays.CuArray{Float64,5,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,5}, ::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] forw(::Function, ::Param{KnetArray{Float64,5}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [21] permutedims at ./none:0 [inlined]
   [22] (::var"#p5#118"{Array{Int64,1}})(::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:114
   [23] gcsum(::Function, ::Param{KnetArray{Float64,5}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum(::Function, ::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p5#118"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gradcheck(::var"#p5#118"{Array{Int64,1}}, ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [29] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:118
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:117
  Test threw exception
  Expression: isapprox(p5(a5), Array(p5(k5)))
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},NTuple{5,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,5,Nothing}, ::CuArrays.CuArray{Float64,5,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,5}, ::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] (::var"#p5#118"{Array{Int64,1}})(::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:114
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:117
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...

Stacktrace:
 [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
 [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
 [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},NTuple{5,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [16] permutedims!(::CuArrays.CuArray{Float64,5,Nothing}, ::CuArrays.CuArray{Float64,5,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [17] permutedims!(::KnetArray{Float64,5}, ::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [18] permutedims(::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [19] forw(::Function, ::Param{KnetArray{Float64,5}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [21] permutedims at ./none:0 [inlined]
 [22] (::var"#p5#118"{Array{Int64,1}})(::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:114
 [23] gcsum(::Function, ::Param{KnetArray{Float64,5}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [24] gcsum(::Function, ::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p5#118"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [28] gradcheck(::var"#p5#118"{Array{Int64,1}}, ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [29] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:118
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [33] include(::String) at ./client.jl:441
 [34] macro expansion at ./util.jl:175 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [38] include(::String) at ./client.jl:441
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:264
 [42] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:118
  Test threw exception
  Expression: gradcheck(p5, k5)
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p5#118"{Array{Int64,1}}, ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:118
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  could not load symbol "jl_LLVMContext":
  /opt/julia/bin/julia: undefined symbol: jl_LLVMContext
  Stacktrace:
   [1] JuliaContext at /home/pkgeval/.julia/packages/LLVM/pINgj/src/interop/base.jl:9 [inlined]
   [2] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:151
   [3] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [4] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [5] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [6] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [7] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [8] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [9] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [10] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},NTuple{5,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [11] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [12] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [13] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [14] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [15] gpu_call(::Function, ::CuArrays.CuArray{Float64,5,Nothing}, ::Tuple{CuArrays.CuArray{Float64,5,Nothing},CuArrays.CuArray{Float64,5,Nothing},NTuple{5,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [16] permutedims!(::CuArrays.CuArray{Float64,5,Nothing}, ::CuArrays.CuArray{Float64,5,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [17] permutedims!(::KnetArray{Float64,5}, ::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [18] permutedims(::KnetArray{Float64,5}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [19] forw(::Function, ::Param{KnetArray{Float64,5}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [20] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [21] permutedims at ./none:0 [inlined]
   [22] (::var"#p5#118"{Array{Int64,1}})(::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:114
   [23] gcsum(::Function, ::Param{KnetArray{Float64,5}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [24] gcsum(::Function, ::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [25] (::AutoGrad.var"#217#219"{Tuple{},var"#p5#118"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [26] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [27] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [28] gradcheck(::var"#p5#118"{Array{Int64,1}}, ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [29] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:118
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
 72.583587 seconds (49.00 M allocations: 2.412 GiB, 2.34% gc time)
gpu-stats: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:54
  Test threw exception
  Expression: abs(mean(batchnorm(kax; training = true))) < 0.001
  Cannot find cudnn
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
   [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
   [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
   [5] batchnorm4(::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:259
   [6] batchnorm2(::Nothing, ::Nothing, ::KnetArray{Float64,2}; moments::Nothing, training::Bool, o::Base.Iterators.Pairs{Symbol,Knet.BNCache,Tuple{Symbol},NamedTuple{(:cache,),Tuple{Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:496
   [7] batchnorm2(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:507
   [8] batchnorm(::KnetArray{Float64,2}, ::Nothing, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:90
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:54
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:54
   [12] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:40
   [14] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:8
  
gpu-stats: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:55
  Test threw exception
  Expression: isapprox(std2(batchnorm(kax; training = true)), 1.0; rtol = 0.01)
  Cannot find cudnn
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
   [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
   [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
   [5] batchnorm4(::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:259
   [6] batchnorm2(::Nothing, ::Nothing, ::KnetArray{Float64,2}; moments::Nothing, training::Bool, o::Base.Iterators.Pairs{Symbol,Knet.BNCache,Tuple{Symbol},NamedTuple{(:cache,),Tuple{Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:496
   [7] batchnorm2(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:507
   [8] batchnorm(::KnetArray{Float64,2}, ::Nothing, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:90
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:55
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:54
   [12] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:40
   [14] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:8
  

Stacktrace:
 [1] error(::String) at ./error.jl:33
 [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
 [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
 [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
 [5] batchnorm4(::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:259
 [6] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [7] #batchnorm4#480 at ./none:0 [inlined]
 [8] batchnorm2(::Nothing, ::Nothing, ::Param{KnetArray{Float64,2}}; moments::Nothing, training::Bool, o::Base.Iterators.Pairs{Symbol,Knet.BNCache,Tuple{Symbol},NamedTuple{(:cache,),Tuple{Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:496
 [9] #batchnorm2#596 at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:507 [inlined]
 [10] #batchnorm#464 at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:90 [inlined]
 [11] (::var"#bn1#121")(::Param{KnetArray{Float64,2}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:14
 [12] gcsum(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [13] gcsum(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [14] (::AutoGrad.var"#217#219"{Tuple{},var"#bn1#121",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [15] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [16] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [17] gradcheck(::var"#bn1#121", ::KnetArray{Float64,2}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [18] gradcheck(::Function, ::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [19] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:59
 [20] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:59
 [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:40
 [24] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:8
 [26] include(::String) at ./client.jl:441
 [27] macro expansion at ./util.jl:175 [inlined]
 [28] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:17 [inlined]
 [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [31] include(::String) at ./client.jl:441
 [32] top-level scope at none:6
 [33] eval(::Module, ::Any) at ./boot.jl:331
 [34] exec_options(::Base.JLOptions) at ./client.jl:264
 [35] _start() at ./client.jl:490
gpu-grads: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:59
  Test threw exception
  Expression: gradcheck(bn1, kax)
  Cannot find cudnn
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#bn1#121", ::KnetArray{Float64,2}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:59
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:59
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:40
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:8
  caused by [exception 1]
  Cannot find cudnn
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
   [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
   [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
   [5] batchnorm4(::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:259
   [6] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [7] #batchnorm4#480 at ./none:0 [inlined]
   [8] batchnorm2(::Nothing, ::Nothing, ::Param{KnetArray{Float64,2}}; moments::Nothing, training::Bool, o::Base.Iterators.Pairs{Symbol,Knet.BNCache,Tuple{Symbol},NamedTuple{(:cache,),Tuple{Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:496
   [9] #batchnorm2#596 at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:507 [inlined]
   [10] #batchnorm#464 at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:90 [inlined]
   [11] (::var"#bn1#121")(::Param{KnetArray{Float64,2}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:14
   [12] gcsum(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [13] gcsum(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [14] (::AutoGrad.var"#217#219"{Tuple{},var"#bn1#121",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [15] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [16] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [17] gradcheck(::var"#bn1#121", ::KnetArray{Float64,2}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [18] gradcheck(::Function, ::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:59
   [20] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:59
   [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:40
   [24] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:8
  

Stacktrace:
 [1] error(::String) at ./error.jl:33
 [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
 [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
 [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
 [5] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [6] #batchnorm4#493 at ./none:0 [inlined]
 [7] batchnorm2(::AutoGrad.Result{KnetArray{Float64,2}}, ::AutoGrad.Result{KnetArray{Float64,2}}, ::AutoGrad.Result{KnetArray{Float64,2}}; moments::Nothing, training::Bool, o::Base.Iterators.Pairs{Symbol,Knet.BNCache,Tuple{Symbol},NamedTuple{(:cache,),Tuple{Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:496
 [8] batchnorm(::AutoGrad.Result{KnetArray{Float64,2}}, ::Nothing, ::AutoGrad.Result{KnetArray{Float64,1}}; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:90
 [9] (::var"#bn3#120")(::Param{Tuple{KnetArray{Float64,2},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:13
 [10] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,2},KnetArray{Float64,1}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [11] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,2},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [12] (::AutoGrad.var"#217#219"{Tuple{},var"#bn3#120",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [13] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [14] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [15] gradcheck(::var"#bn3#120", ::Tuple{KnetArray{Float64,2},KnetArray{Float64,1}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [16] gradcheck(::Function, ::Tuple{KnetArray{Float64,2},KnetArray{Float64,1}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [17] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:60
 [18] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [19] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:59
 [20] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:40
 [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:8
 [24] include(::String) at ./client.jl:441
 [25] macro expansion at ./util.jl:175 [inlined]
 [26] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:17 [inlined]
 [27] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [28] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [29] include(::String) at ./client.jl:441
 [30] top-level scope at none:6
 [31] eval(::Module, ::Any) at ./boot.jl:331
 [32] exec_options(::Base.JLOptions) at ./client.jl:264
 [33] _start() at ./client.jl:490
gpu-grads: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:60
  Test threw exception
  Expression: gradcheck(bn3, (kax, kaw))
  Cannot find cudnn
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#bn3#120", ::Tuple{KnetArray{Float64,2},KnetArray{Float64,1}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::Tuple{KnetArray{Float64,2},KnetArray{Float64,1}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:60
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:59
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:40
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:8
  caused by [exception 1]
  Cannot find cudnn
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
   [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
   [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
   [5] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [6] #batchnorm4#493 at ./none:0 [inlined]
   [7] batchnorm2(::AutoGrad.Result{KnetArray{Float64,2}}, ::AutoGrad.Result{KnetArray{Float64,2}}, ::AutoGrad.Result{KnetArray{Float64,2}}; moments::Nothing, training::Bool, o::Base.Iterators.Pairs{Symbol,Knet.BNCache,Tuple{Symbol},NamedTuple{(:cache,),Tuple{Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:496
   [8] batchnorm(::AutoGrad.Result{KnetArray{Float64,2}}, ::Nothing, ::AutoGrad.Result{KnetArray{Float64,1}}; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:90
   [9] (::var"#bn3#120")(::Param{Tuple{KnetArray{Float64,2},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:13
   [10] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,2},KnetArray{Float64,1}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [11] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,2},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [12] (::AutoGrad.var"#217#219"{Tuple{},var"#bn3#120",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [13] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [14] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [15] gradcheck(::var"#bn3#120", ::Tuple{KnetArray{Float64,2},KnetArray{Float64,1}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [16] gradcheck(::Function, ::Tuple{KnetArray{Float64,2},KnetArray{Float64,1}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:60
   [18] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:59
   [20] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:40
   [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:8
  
dev-consistency: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:63
  Got exception outside of a @test
  Cannot find cudnn
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
   [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
   [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
   [5] batchnorm4(::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.BNMoments,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:259
   [6] batchnorm2(::Nothing, ::Nothing, ::KnetArray{Float64,2}; moments::Knet.BNMoments, training::Bool, o::Base.Iterators.Pairs{Symbol,Knet.BNCache,Tuple{Symbol},NamedTuple{(:cache,),Tuple{Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:496
   [7] batchnorm2(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.BNMoments,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:507
   [8] batchnorm(::KnetArray{Float64,2}, ::Knet.BNMoments, ::Nothing; training::Nothing, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:90
   [9] batchnorm(::KnetArray{Float64,2}, ::Knet.BNMoments, ::Nothing) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:79 (repeats 2 times)
   [10] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:67
   [11] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [12] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:64
   [13] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:40
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:8
   [17] include(::String) at ./client.jl:441
   [18] macro expansion at ./util.jl:175 [inlined]
   [19] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:17 [inlined]
   [20] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
   [22] include(::String) at ./client.jl:441
   [23] top-level scope at none:6
   [24] eval(::Module, ::Any) at ./boot.jl:331
   [25] exec_options(::Base.JLOptions) at ./client.jl:264
   [26] _start() at ./client.jl:490
  
test-moments: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:75
  Got exception outside of a @test
  Cannot find cudnn
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
   [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
   [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
   [5] batchnorm4(::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.BNMoments,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:259
   [6] batchnorm2(::Nothing, ::Nothing, ::KnetArray{Float64,2}; moments::Knet.BNMoments, training::Bool, o::Base.Iterators.Pairs{Symbol,Knet.BNCache,Tuple{Symbol},NamedTuple{(:cache,),Tuple{Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:496
   [7] batchnorm2(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.BNMoments,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:507
   [8] batchnorm(::KnetArray{Float64,2}, ::Knet.BNMoments, ::Nothing; training::Nothing, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:90
   [9] batchnorm(::KnetArray{Float64,2}, ::Knet.BNMoments, ::Nothing) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:79 (repeats 2 times)
   [10] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:84
   [11] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [12] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:76
   [13] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:40
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:8
   [17] include(::String) at ./client.jl:441
   [18] macro expansion at ./util.jl:175 [inlined]
   [19] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:17 [inlined]
   [20] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
   [22] include(::String) at ./client.jl:441
   [23] top-level scope at none:6
   [24] eval(::Module, ::Any) at ./boot.jl:331
   [25] exec_options(::Base.JLOptions) at ./client.jl:264
   [26] _start() at ./client.jl:490
  
training-moments: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:92
  Got exception outside of a @test
  Cannot find cudnn
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
   [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
   [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
   [5] batchnorm4(::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.BNMoments,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:259
   [6] batchnorm2(::Nothing, ::Nothing, ::KnetArray{Float64,2}; moments::Knet.BNMoments, training::Bool, o::Base.Iterators.Pairs{Symbol,Knet.BNCache,Tuple{Symbol},NamedTuple{(:cache,),Tuple{Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:496
   [7] batchnorm2(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.BNMoments,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:507
   [8] batchnorm(::KnetArray{Float64,2}, ::Knet.BNMoments, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:90
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:101
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:93
   [12] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:40
   [14] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:8
   [16] include(::String) at ./client.jl:441
   [17] macro expansion at ./util.jl:175 [inlined]
   [18] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:17 [inlined]
   [19] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
   [21] include(::String) at ./client.jl:441
   [22] top-level scope at none:6
   [23] eval(::Module, ::Any) at ./boot.jl:331
   [24] exec_options(::Base.JLOptions) at ./client.jl:264
   [25] _start() at ./client.jl:490
  
gpu-stats: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:54
  Test threw exception
  Expression: abs(mean(batchnorm(kax; training = true))) < 0.001
  Cannot find cudnn
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
   [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
   [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
   [5] batchnorm4(::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:259
   [6] batchnorm(::KnetArray{Float64,4}, ::Nothing, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:95
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:54
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:54
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:40
   [12] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:8
  
gpu-stats: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:55
  Test threw exception
  Expression: isapprox(std2(batchnorm(kax; training = true)), 1.0; rtol = 0.01)
  Cannot find cudnn
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
   [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
   [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
   [5] batchnorm4(::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:259
   [6] batchnorm(::KnetArray{Float64,4}, ::Nothing, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:95
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:55
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:54
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:40
   [12] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:8
  

Stacktrace:
 [1] error(::String) at ./error.jl:33
 [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
 [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
 [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
 [5] batchnorm4(::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:259
 [6] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [7] #batchnorm4#480 at ./none:0 [inlined]
 [8] batchnorm(::Param{KnetArray{Float64,4}}, ::Nothing, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:95
 [9] (::var"#bn1#121")(::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:14
 [10] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [11] gcsum(::Function, ::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [12] (::AutoGrad.var"#217#219"{Tuple{},var"#bn1#121",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [13] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [14] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [15] gradcheck(::var"#bn1#121", ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [16] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [17] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:59
 [18] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [19] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:59
 [20] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:40
 [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:8
 [24] include(::String) at ./client.jl:441
 [25] macro expansion at ./util.jl:175 [inlined]
 [26] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:17 [inlined]
 [27] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [28] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [29] include(::String) at ./client.jl:441
 [30] top-level scope at none:6
 [31] eval(::Module, ::Any) at ./boot.jl:331
 [32] exec_options(::Base.JLOptions) at ./client.jl:264
 [33] _start() at ./client.jl:490
gpu-grads: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:59
  Test threw exception
  Expression: gradcheck(bn1, kax)
  Cannot find cudnn
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#bn1#121", ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:59
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:59
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:40
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:8
  caused by [exception 1]
  Cannot find cudnn
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
   [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
   [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
   [5] batchnorm4(::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:259
   [6] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [7] #batchnorm4#480 at ./none:0 [inlined]
   [8] batchnorm(::Param{KnetArray{Float64,4}}, ::Nothing, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:95
   [9] (::var"#bn1#121")(::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:14
   [10] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [11] gcsum(::Function, ::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [12] (::AutoGrad.var"#217#219"{Tuple{},var"#bn1#121",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [13] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [14] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [15] gradcheck(::var"#bn1#121", ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [16] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:59
   [18] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:59
   [20] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:40
   [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:8
  

Stacktrace:
 [1] error(::String) at ./error.jl:33
 [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
 [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
 [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
 [5] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [6] #batchnorm4#493 at ./none:0 [inlined]
 [7] batchnorm(::AutoGrad.Result{KnetArray{Float64,4}}, ::Nothing, ::AutoGrad.Result{KnetArray{Float64,1}}; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:95
 [8] (::var"#bn3#120")(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:13
 [9] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [10] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [11] (::AutoGrad.var"#217#219"{Tuple{},var"#bn3#120",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [12] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [13] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [14] gradcheck(::var"#bn3#120", ::Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [15] gradcheck(::Function, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:60
 [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [18] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:59
 [19] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:40
 [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:8
 [23] include(::String) at ./client.jl:441
 [24] macro expansion at ./util.jl:175 [inlined]
 [25] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:17 [inlined]
 [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [27] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [28] include(::String) at ./client.jl:441
 [29] top-level scope at none:6
 [30] eval(::Module, ::Any) at ./boot.jl:331
 [31] exec_options(::Base.JLOptions) at ./client.jl:264
 [32] _start() at ./client.jl:490
gpu-grads: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:60
  Test threw exception
  Expression: gradcheck(bn3, (kax, kaw))
  Cannot find cudnn
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#bn3#120", ::Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:60
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:59
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:40
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:8
  caused by [exception 1]
  Cannot find cudnn
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
   [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
   [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
   [5] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [6] #batchnorm4#493 at ./none:0 [inlined]
   [7] batchnorm(::AutoGrad.Result{KnetArray{Float64,4}}, ::Nothing, ::AutoGrad.Result{KnetArray{Float64,1}}; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:95
   [8] (::var"#bn3#120")(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:13
   [9] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [10] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [11] (::AutoGrad.var"#217#219"{Tuple{},var"#bn3#120",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [12] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [13] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [14] gradcheck(::var"#bn3#120", ::Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [15] gradcheck(::Function, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:60
   [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:59
   [19] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:40
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:8
  
dev-consistency: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:63
  Got exception outside of a @test
  Cannot find cudnn
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
   [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
   [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
   [5] batchnorm4(::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.BNMoments,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:259
   [6] batchnorm(::KnetArray{Float64,4}, ::Knet.BNMoments, ::Nothing; training::Nothing, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:95
   [7] batchnorm(::KnetArray{Float64,4}, ::Knet.BNMoments, ::Nothing) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:79 (repeats 2 times)
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:67
   [9] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [10] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:64
   [11] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [12] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:40
   [13] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:8
   [15] include(::String) at ./client.jl:441
   [16] macro expansion at ./util.jl:175 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:17 [inlined]
   [18] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
   [20] include(::String) at ./client.jl:441
   [21] top-level scope at none:6
   [22] eval(::Module, ::Any) at ./boot.jl:331
   [23] exec_options(::Base.JLOptions) at ./client.jl:264
   [24] _start() at ./client.jl:490
  
test-moments: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:75
  Got exception outside of a @test
  Cannot find cudnn
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
   [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
   [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
   [5] batchnorm4(::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.BNMoments,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:259
   [6] batchnorm(::KnetArray{Float64,4}, ::Knet.BNMoments, ::Nothing; training::Nothing, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:95
   [7] batchnorm(::KnetArray{Float64,4}, ::Knet.BNMoments, ::Nothing) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:79 (repeats 2 times)
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:84
   [9] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [10] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:76
   [11] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [12] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:40
   [13] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:8
   [15] include(::String) at ./client.jl:441
   [16] macro expansion at ./util.jl:175 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:17 [inlined]
   [18] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
   [20] include(::String) at ./client.jl:441
   [21] top-level scope at none:6
   [22] eval(::Module, ::Any) at ./boot.jl:331
   [23] exec_options(::Base.JLOptions) at ./client.jl:264
   [24] _start() at ./client.jl:490
  
training-moments: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:92
  Got exception outside of a @test
  Cannot find cudnn
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
   [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
   [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
   [5] batchnorm4(::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.BNMoments,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:259
   [6] batchnorm(::KnetArray{Float64,4}, ::Knet.BNMoments, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:95
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:101
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:93
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:40
   [12] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:8
   [14] include(::String) at ./client.jl:441
   [15] macro expansion at ./util.jl:175 [inlined]
   [16] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:17 [inlined]
   [17] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
   [19] include(::String) at ./client.jl:441
   [20] top-level scope at none:6
   [21] eval(::Module, ::Any) at ./boot.jl:331
   [22] exec_options(::Base.JLOptions) at ./client.jl:264
   [23] _start() at ./client.jl:490
  

Stacktrace:
 [1] error(::String) at ./error.jl:33
 [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
 [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
 [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
 [5] batchnorm4(::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.BNMoments,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:259
 [6] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.BNMoments,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [7] #batchnorm4#480 at ./none:0 [inlined]
 [8] batchnorm(::Param{KnetArray{Float64,4}}, ::Knet.BNMoments, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:95
 [9] (::var"#bn1ts#123")(::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:16
 [10] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [11] gcsum(::Function, ::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [12] (::AutoGrad.var"#217#219"{Tuple{},var"#bn1ts#123",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [13] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [14] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [15] gradcheck(::var"#bn1ts#123", ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [16] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [17] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:118
 [18] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [19] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:118
 [20] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:40
 [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:8
 [24] include(::String) at ./client.jl:441
 [25] macro expansion at ./util.jl:175 [inlined]
 [26] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:17 [inlined]
 [27] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [28] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [29] include(::String) at ./client.jl:441
 [30] top-level scope at none:6
 [31] eval(::Module, ::Any) at ./boot.jl:331
 [32] exec_options(::Base.JLOptions) at ./client.jl:264
 [33] _start() at ./client.jl:490
gpu-grads-testing: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:118
  Test threw exception
  Expression: gradcheck(bn1ts, kax)
  Cannot find cudnn
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#bn1ts#123", ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:118
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:118
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:40
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:8
  caused by [exception 1]
  Cannot find cudnn
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
   [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
   [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
   [5] batchnorm4(::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.BNMoments,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:259
   [6] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.BNMoments,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [7] #batchnorm4#480 at ./none:0 [inlined]
   [8] batchnorm(::Param{KnetArray{Float64,4}}, ::Knet.BNMoments, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:95
   [9] (::var"#bn1ts#123")(::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:16
   [10] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [11] gcsum(::Function, ::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [12] (::AutoGrad.var"#217#219"{Tuple{},var"#bn1ts#123",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [13] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [14] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [15] gradcheck(::var"#bn1ts#123", ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [16] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:118
   [18] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:118
   [20] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:40
   [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:8
  

Stacktrace:
 [1] error(::String) at ./error.jl:33
 [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
 [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
 [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
 [5] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.BNMoments,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [6] #batchnorm4#493 at ./none:0 [inlined]
 [7] batchnorm(::AutoGrad.Result{KnetArray{Float64,4}}, ::Knet.BNMoments, ::AutoGrad.Result{KnetArray{Float64,1}}; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:95
 [8] (::var"#bn3ts#122")(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:15
 [9] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [10] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [11] (::AutoGrad.var"#217#219"{Tuple{},var"#bn3ts#122",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [12] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [13] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [14] gradcheck(::var"#bn3ts#122", ::Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [15] gradcheck(::Function, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:119
 [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [18] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:118
 [19] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:40
 [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:8
 [23] include(::String) at ./client.jl:441
 [24] macro expansion at ./util.jl:175 [inlined]
 [25] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:17 [inlined]
 [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [27] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [28] include(::String) at ./client.jl:441
 [29] top-level scope at none:6
 [30] eval(::Module, ::Any) at ./boot.jl:331
 [31] exec_options(::Base.JLOptions) at ./client.jl:264
 [32] _start() at ./client.jl:490
gpu-grads-testing: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:119
  Test threw exception
  Expression: gradcheck(bn3ts, (kax, kaw))
  Cannot find cudnn
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#bn3ts#122", ::Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:119
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:118
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:40
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:8
  caused by [exception 1]
  Cannot find cudnn
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
   [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
   [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
   [5] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.BNMoments,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [6] #batchnorm4#493 at ./none:0 [inlined]
   [7] batchnorm(::AutoGrad.Result{KnetArray{Float64,4}}, ::Knet.BNMoments, ::AutoGrad.Result{KnetArray{Float64,1}}; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:95
   [8] (::var"#bn3ts#122")(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:15
   [9] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [10] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [11] (::AutoGrad.var"#217#219"{Tuple{},var"#bn3ts#122",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [12] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [13] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [14] gradcheck(::var"#bn3ts#122", ::Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [15] gradcheck(::Function, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:119
   [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:118
   [19] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:40
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:8
  
gpu-stats: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:54
  Test threw exception
  Expression: abs(mean(batchnorm(kax; training = true))) < 0.001
  Cannot find cudnn
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
   [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
   [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
   [5] batchnorm4(::KnetArray{Float64,5}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:259
   [6] batchnorm(::KnetArray{Float64,5}, ::Nothing, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:95
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:54
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:54
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:40
   [12] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:8
  
gpu-stats: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:55
  Test threw exception
  Expression: isapprox(std2(batchnorm(kax; training = true)), 1.0; rtol = 0.01)
  Cannot find cudnn
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
   [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
   [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
   [5] batchnorm4(::KnetArray{Float64,5}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:259
   [6] batchnorm(::KnetArray{Float64,5}, ::Nothing, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:95
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:55
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:54
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:40
   [12] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:8
  

Stacktrace:
 [1] error(::String) at ./error.jl:33
 [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
 [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
 [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
 [5] batchnorm4(::KnetArray{Float64,5}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:259
 [6] forw(::Function, ::Param{KnetArray{Float64,5}}; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [7] #batchnorm4#480 at ./none:0 [inlined]
 [8] batchnorm(::Param{KnetArray{Float64,5}}, ::Nothing, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:95
 [9] (::var"#bn1#121")(::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:14
 [10] gcsum(::Function, ::Param{KnetArray{Float64,5}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [11] gcsum(::Function, ::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [12] (::AutoGrad.var"#217#219"{Tuple{},var"#bn1#121",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [13] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [14] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [15] gradcheck(::var"#bn1#121", ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [16] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [17] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:59
 [18] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [19] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:59
 [20] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:40
 [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:8
 [24] include(::String) at ./client.jl:441
 [25] macro expansion at ./util.jl:175 [inlined]
 [26] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:17 [inlined]
 [27] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [28] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [29] include(::String) at ./client.jl:441
 [30] top-level scope at none:6
 [31] eval(::Module, ::Any) at ./boot.jl:331
 [32] exec_options(::Base.JLOptions) at ./client.jl:264
 [33] _start() at ./client.jl:490
gpu-grads: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:59
  Test threw exception
  Expression: gradcheck(bn1, kax)
  Cannot find cudnn
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#bn1#121", ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:59
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:59
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:40
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:8
  caused by [exception 1]
  Cannot find cudnn
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
   [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
   [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
   [5] batchnorm4(::KnetArray{Float64,5}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:259
   [6] forw(::Function, ::Param{KnetArray{Float64,5}}; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [7] #batchnorm4#480 at ./none:0 [inlined]
   [8] batchnorm(::Param{KnetArray{Float64,5}}, ::Nothing, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:95
   [9] (::var"#bn1#121")(::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:14
   [10] gcsum(::Function, ::Param{KnetArray{Float64,5}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [11] gcsum(::Function, ::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [12] (::AutoGrad.var"#217#219"{Tuple{},var"#bn1#121",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [13] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [14] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [15] gradcheck(::var"#bn1#121", ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [16] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:59
   [18] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:59
   [20] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:40
   [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:8
  

Stacktrace:
 [1] error(::String) at ./error.jl:33
 [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
 [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
 [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
 [5] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,5}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,5}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [6] #batchnorm4#493 at ./none:0 [inlined]
 [7] batchnorm(::AutoGrad.Result{KnetArray{Float64,5}}, ::Nothing, ::AutoGrad.Result{KnetArray{Float64,1}}; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:95
 [8] (::var"#bn3#120")(::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:13
 [9] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [10] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [11] (::AutoGrad.var"#217#219"{Tuple{},var"#bn3#120",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [12] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [13] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [14] gradcheck(::var"#bn3#120", ::Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [15] gradcheck(::Function, ::Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:60
 [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [18] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:59
 [19] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:40
 [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:8
 [23] include(::String) at ./client.jl:441
 [24] macro expansion at ./util.jl:175 [inlined]
 [25] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:17 [inlined]
 [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [27] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [28] include(::String) at ./client.jl:441
 [29] top-level scope at none:6
 [30] eval(::Module, ::Any) at ./boot.jl:331
 [31] exec_options(::Base.JLOptions) at ./client.jl:264
 [32] _start() at ./client.jl:490
gpu-grads: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:60
  Test threw exception
  Expression: gradcheck(bn3, (kax, kaw))
  Cannot find cudnn
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#bn3#120", ::Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:60
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:59
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:40
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:8
  caused by [exception 1]
  Cannot find cudnn
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
   [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
   [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
   [5] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,5}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,5}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [6] #batchnorm4#493 at ./none:0 [inlined]
   [7] batchnorm(::AutoGrad.Result{KnetArray{Float64,5}}, ::Nothing, ::AutoGrad.Result{KnetArray{Float64,1}}; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:95
   [8] (::var"#bn3#120")(::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:13
   [9] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [10] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [11] (::AutoGrad.var"#217#219"{Tuple{},var"#bn3#120",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [12] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [13] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [14] gradcheck(::var"#bn3#120", ::Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [15] gradcheck(::Function, ::Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:60
   [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:59
   [19] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:40
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:8
  
dev-consistency: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:63
  Got exception outside of a @test
  Cannot find cudnn
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
   [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
   [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
   [5] batchnorm4(::KnetArray{Float64,5}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.BNMoments,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:259
   [6] batchnorm(::KnetArray{Float64,5}, ::Knet.BNMoments, ::Nothing; training::Nothing, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:95
   [7] batchnorm(::KnetArray{Float64,5}, ::Knet.BNMoments, ::Nothing) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:79 (repeats 2 times)
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:67
   [9] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [10] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:64
   [11] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [12] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:40
   [13] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:8
   [15] include(::String) at ./client.jl:441
   [16] macro expansion at ./util.jl:175 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:17 [inlined]
   [18] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
   [20] include(::String) at ./client.jl:441
   [21] top-level scope at none:6
   [22] eval(::Module, ::Any) at ./boot.jl:331
   [23] exec_options(::Base.JLOptions) at ./client.jl:264
   [24] _start() at ./client.jl:490
  
test-moments: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:75
  Got exception outside of a @test
  Cannot find cudnn
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
   [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
   [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
   [5] batchnorm4(::KnetArray{Float64,5}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.BNMoments,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:259
   [6] batchnorm(::KnetArray{Float64,5}, ::Knet.BNMoments, ::Nothing; training::Nothing, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:95
   [7] batchnorm(::KnetArray{Float64,5}, ::Knet.BNMoments, ::Nothing) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:79 (repeats 2 times)
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:84
   [9] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [10] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:76
   [11] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [12] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:40
   [13] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:8
   [15] include(::String) at ./client.jl:441
   [16] macro expansion at ./util.jl:175 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:17 [inlined]
   [18] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
   [20] include(::String) at ./client.jl:441
   [21] top-level scope at none:6
   [22] eval(::Module, ::Any) at ./boot.jl:331
   [23] exec_options(::Base.JLOptions) at ./client.jl:264
   [24] _start() at ./client.jl:490
  
training-moments: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:92
  Got exception outside of a @test
  Cannot find cudnn
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
   [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
   [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
   [5] batchnorm4(::KnetArray{Float64,5}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.BNMoments,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:259
   [6] batchnorm(::KnetArray{Float64,5}, ::Knet.BNMoments, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:95
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:101
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:93
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:40
   [12] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:8
   [14] include(::String) at ./client.jl:441
   [15] macro expansion at ./util.jl:175 [inlined]
   [16] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:17 [inlined]
   [17] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
   [19] include(::String) at ./client.jl:441
   [20] top-level scope at none:6
   [21] eval(::Module, ::Any) at ./boot.jl:331
   [22] exec_options(::Base.JLOptions) at ./client.jl:264
   [23] _start() at ./client.jl:490
  

Stacktrace:
 [1] error(::String) at ./error.jl:33
 [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
 [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
 [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
 [5] batchnorm4(::KnetArray{Float64,5}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.BNMoments,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:259
 [6] forw(::Function, ::Param{KnetArray{Float64,5}}; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.BNMoments,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [7] #batchnorm4#480 at ./none:0 [inlined]
 [8] batchnorm(::Param{KnetArray{Float64,5}}, ::Knet.BNMoments, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:95
 [9] (::var"#bn1ts#123")(::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:16
 [10] gcsum(::Function, ::Param{KnetArray{Float64,5}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [11] gcsum(::Function, ::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [12] (::AutoGrad.var"#217#219"{Tuple{},var"#bn1ts#123",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [13] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [14] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [15] gradcheck(::var"#bn1ts#123", ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [16] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [17] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:118
 [18] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [19] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:118
 [20] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:40
 [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:8
 [24] include(::String) at ./client.jl:441
 [25] macro expansion at ./util.jl:175 [inlined]
 [26] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:17 [inlined]
 [27] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [28] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [29] include(::String) at ./client.jl:441
 [30] top-level scope at none:6
 [31] eval(::Module, ::Any) at ./boot.jl:331
 [32] exec_options(::Base.JLOptions) at ./client.jl:264
 [33] _start() at ./client.jl:490
gpu-grads-testing: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:118
  Test threw exception
  Expression: gradcheck(bn1ts, kax)
  Cannot find cudnn
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#bn1ts#123", ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:118
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:118
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:40
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:8
  caused by [exception 1]
  Cannot find cudnn
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
   [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
   [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
   [5] batchnorm4(::KnetArray{Float64,5}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.BNMoments,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:259
   [6] forw(::Function, ::Param{KnetArray{Float64,5}}; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.BNMoments,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [7] #batchnorm4#480 at ./none:0 [inlined]
   [8] batchnorm(::Param{KnetArray{Float64,5}}, ::Knet.BNMoments, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:95
   [9] (::var"#bn1ts#123")(::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:16
   [10] gcsum(::Function, ::Param{KnetArray{Float64,5}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [11] gcsum(::Function, ::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [12] (::AutoGrad.var"#217#219"{Tuple{},var"#bn1ts#123",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [13] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [14] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [15] gradcheck(::var"#bn1ts#123", ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [16] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:118
   [18] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:118
   [20] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:40
   [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:8
  

Stacktrace:
 [1] error(::String) at ./error.jl:33
 [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
 [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
 [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
 [5] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,5}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,5}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.BNMoments,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [6] #batchnorm4#493 at ./none:0 [inlined]
 [7] batchnorm(::AutoGrad.Result{KnetArray{Float64,5}}, ::Knet.BNMoments, ::AutoGrad.Result{KnetArray{Float64,1}}; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:95
 [8] (::var"#bn3ts#122")(::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:15
 [9] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [10] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [11] (::AutoGrad.var"#217#219"{Tuple{},var"#bn3ts#122",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [12] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [13] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [14] gradcheck(::var"#bn3ts#122", ::Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [15] gradcheck(::Function, ::Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:119
 [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [18] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:118
 [19] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:40
 [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:8
 [23] include(::String) at ./client.jl:441
 [24] macro expansion at ./util.jl:175 [inlined]
 [25] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:17 [inlined]
 [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [27] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [28] include(::String) at ./client.jl:441
 [29] top-level scope at none:6
 [30] eval(::Module, ::Any) at ./boot.jl:331
 [31] exec_options(::Base.JLOptions) at ./client.jl:264
 [32] _start() at ./client.jl:490
gpu-grads-testing: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:119
  Test threw exception
  Expression: gradcheck(bn3ts, (kax, kaw))
  Cannot find cudnn
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#bn3ts#122", ::Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:119
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:118
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:40
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:8
  caused by [exception 1]
  Cannot find cudnn
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
   [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
   [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
   [5] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,5}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,5}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.BNMoments,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [6] #batchnorm4#493 at ./none:0 [inlined]
   [7] batchnorm(::AutoGrad.Result{KnetArray{Float64,5}}, ::Knet.BNMoments, ::AutoGrad.Result{KnetArray{Float64,1}}; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/batchnorm.jl:95
   [8] (::var"#bn3ts#122")(::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:15
   [9] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [10] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [11] (::AutoGrad.var"#217#219"{Tuple{},var"#bn3ts#122",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [12] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [13] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [14] gradcheck(::var"#bn3ts#122", ::Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [15] gradcheck(::Function, ::Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:119
   [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:118
   [19] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:40
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/batchnorm.jl:8
  
 48.938314 seconds (39.04 M allocations: 1.916 GiB, 2.58% gc time)

Stacktrace:
 [1] error(::String) at ./error.jl:33
 [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
 [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
 [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
 [5] generic_softmax(::Param{KnetArray{Float64,2}}, ::Int64, ::Function; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/loss.jl:138
 [6] #logp#705 at /home/pkgeval/.julia/packages/Knet/vxHRi/src/loss.jl:14 [inlined]
 [7] logp(::Param{KnetArray{Float64,2}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/loss.jl:14
 [8] gcsum(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [9] gcsum(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [10] (::AutoGrad.var"#217#219"{Tuple{},typeof(logp),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [11] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [12] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [13] gradcheck(::typeof(logp), ::KnetArray{Float64,2}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [14] gradcheck(::Function, ::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [15] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:11
 [16] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [17] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:4
 [18] include(::String) at ./client.jl:441
 [19] macro expansion at ./util.jl:175 [inlined]
 [20] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:18 [inlined]
 [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [23] include(::String) at ./client.jl:441
 [24] top-level scope at none:6
 [25] eval(::Module, ::Any) at ./boot.jl:331
 [26] exec_options(::Base.JLOptions) at ./client.jl:264
 [27] _start() at ./client.jl:490
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:11
  Test threw exception
  Expression: gradcheck(f, k)
  Cannot find cudnn
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(logp), ::KnetArray{Float64,2}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:11
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:4
  caused by [exception 1]
  Cannot find cudnn
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
   [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
   [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
   [5] generic_softmax(::Param{KnetArray{Float64,2}}, ::Int64, ::Function; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/loss.jl:138
   [6] #logp#705 at /home/pkgeval/.julia/packages/Knet/vxHRi/src/loss.jl:14 [inlined]
   [7] logp(::Param{KnetArray{Float64,2}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/loss.jl:14
   [8] gcsum(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [9] gcsum(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [10] (::AutoGrad.var"#217#219"{Tuple{},typeof(logp),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [11] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [12] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [13] gradcheck(::typeof(logp), ::KnetArray{Float64,2}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [14] gradcheck(::Function, ::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:11
   [16] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:4
  

Stacktrace:
 [1] error(::String) at ./error.jl:33
 [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
 [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
 [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
 [5] generic_softmax(::Param{KnetArray{Float64,2}}, ::Int64, ::Function; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/loss.jl:138
 [6] #logp#705 at /home/pkgeval/.julia/packages/Knet/vxHRi/src/loss.jl:14 [inlined]
 [7] gcsum(::Function, ::Param{KnetArray{Float64,2}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [8] (::AutoGrad.var"#217#219"{Tuple{Pair{Symbol,Int64}},typeof(logp),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [9] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [10] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [11] gradcheck(::typeof(logp), ::KnetArray{Float64,2}; kw::Tuple{Pair{Symbol,Int64}}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [12] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:12
 [13] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:4
 [15] include(::String) at ./client.jl:441
 [16] macro expansion at ./util.jl:175 [inlined]
 [17] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:18 [inlined]
 [18] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [19] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [20] include(::String) at ./client.jl:441
 [21] top-level scope at none:6
 [22] eval(::Module, ::Any) at ./boot.jl:331
 [23] exec_options(::Base.JLOptions) at ./client.jl:264
 [24] _start() at ./client.jl:490
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:12
  Test threw exception
  Expression: gradcheck(f, k, kw = (:dims => 1,))
  Cannot find cudnn
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(logp), ::KnetArray{Float64,2}; kw::Tuple{Pair{Symbol,Int64}}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:12
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:4
  caused by [exception 1]
  Cannot find cudnn
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
   [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
   [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
   [5] generic_softmax(::Param{KnetArray{Float64,2}}, ::Int64, ::Function; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/loss.jl:138
   [6] #logp#705 at /home/pkgeval/.julia/packages/Knet/vxHRi/src/loss.jl:14 [inlined]
   [7] gcsum(::Function, ::Param{KnetArray{Float64,2}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [8] (::AutoGrad.var"#217#219"{Tuple{Pair{Symbol,Int64}},typeof(logp),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [9] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [10] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [11] gradcheck(::typeof(logp), ::KnetArray{Float64,2}; kw::Tuple{Pair{Symbol,Int64}}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [12] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:12
   [13] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:4
  

Stacktrace:
 [1] error(::String) at ./error.jl:33
 [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
 [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
 [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
 [5] generic_softmax(::Param{KnetArray{Float64,2}}, ::Int64, ::Function; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/loss.jl:138
 [6] #logp#705 at /home/pkgeval/.julia/packages/Knet/vxHRi/src/loss.jl:14 [inlined]
 [7] gcsum(::Function, ::Param{KnetArray{Float64,2}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [8] (::AutoGrad.var"#217#219"{Tuple{Pair{Symbol,Int64}},typeof(logp),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [9] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [10] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [11] gradcheck(::typeof(logp), ::KnetArray{Float64,2}; kw::Tuple{Pair{Symbol,Int64}}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [12] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:13
 [13] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:4
 [15] include(::String) at ./client.jl:441
 [16] macro expansion at ./util.jl:175 [inlined]
 [17] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:18 [inlined]
 [18] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [19] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [20] include(::String) at ./client.jl:441
 [21] top-level scope at none:6
 [22] eval(::Module, ::Any) at ./boot.jl:331
 [23] exec_options(::Base.JLOptions) at ./client.jl:264
 [24] _start() at ./client.jl:490
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:13
  Test threw exception
  Expression: gradcheck(f, k, kw = (:dims => 2,))
  Cannot find cudnn
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(logp), ::KnetArray{Float64,2}; kw::Tuple{Pair{Symbol,Int64}}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:13
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:4
  caused by [exception 1]
  Cannot find cudnn
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
   [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
   [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
   [5] generic_softmax(::Param{KnetArray{Float64,2}}, ::Int64, ::Function; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/loss.jl:138
   [6] #logp#705 at /home/pkgeval/.julia/packages/Knet/vxHRi/src/loss.jl:14 [inlined]
   [7] gcsum(::Function, ::Param{KnetArray{Float64,2}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [8] (::AutoGrad.var"#217#219"{Tuple{Pair{Symbol,Int64}},typeof(logp),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [9] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [10] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [11] gradcheck(::typeof(logp), ::KnetArray{Float64,2}; kw::Tuple{Pair{Symbol,Int64}}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [12] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:13
   [13] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:4
  
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:14
  Test threw exception
  Expression: isapprox(f(a), f(k))
  Cannot find cudnn
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
   [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
   [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
   [5] generic_softmax(::KnetArray{Float64,2}, ::Int64, ::Function; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/loss.jl:138
   [6] #logp#705 at /home/pkgeval/.julia/packages/Knet/vxHRi/src/loss.jl:14 [inlined]
   [7] logp(::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/loss.jl:14
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:14
   [9] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [10] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:4
  
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:15
  Test threw exception
  Expression: isapprox(f(a, dims = 1), f(k, dims = 1))
  Cannot find cudnn
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
   [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
   [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
   [5] generic_softmax(::KnetArray{Float64,2}, ::Int64, ::Function; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/loss.jl:138
   [6] #logp#705 at /home/pkgeval/.julia/packages/Knet/vxHRi/src/loss.jl:14 [inlined]
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:15
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:4
  
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:16
  Test threw exception
  Expression: isapprox(f(a, dims = 2), f(k, dims = 2))
  Cannot find cudnn
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
   [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
   [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
   [5] generic_softmax(::KnetArray{Float64,2}, ::Int64, ::Function; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/loss.jl:138
   [6] #logp#705 at /home/pkgeval/.julia/packages/Knet/vxHRi/src/loss.jl:14 [inlined]
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:16
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:4
  

Stacktrace:
 [1] error(::String) at ./error.jl:33
 [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
 [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
 [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
 [5] generic_softmax(::Param{KnetArray{Float64,3}}, ::Int64, ::Function; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/loss.jl:138
 [6] #logp#705 at /home/pkgeval/.julia/packages/Knet/vxHRi/src/loss.jl:14 [inlined]
 [7] logp(::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/loss.jl:14
 [8] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [9] gcsum(::Function, ::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [10] (::AutoGrad.var"#217#219"{Tuple{},typeof(logp),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [11] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [12] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [13] gradcheck(::typeof(logp), ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [14] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [15] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:30
 [16] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [17] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:4
 [18] include(::String) at ./client.jl:441
 [19] macro expansion at ./util.jl:175 [inlined]
 [20] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:18 [inlined]
 [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [23] include(::String) at ./client.jl:441
 [24] top-level scope at none:6
 [25] eval(::Module, ::Any) at ./boot.jl:331
 [26] exec_options(::Base.JLOptions) at ./client.jl:264
 [27] _start() at ./client.jl:490
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:30
  Test threw exception
  Expression: gradcheck(f, k)
  Cannot find cudnn
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(logp), ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:30
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:4
  caused by [exception 1]
  Cannot find cudnn
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
   [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
   [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
   [5] generic_softmax(::Param{KnetArray{Float64,3}}, ::Int64, ::Function; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/loss.jl:138
   [6] #logp#705 at /home/pkgeval/.julia/packages/Knet/vxHRi/src/loss.jl:14 [inlined]
   [7] logp(::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/loss.jl:14
   [8] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [9] gcsum(::Function, ::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [10] (::AutoGrad.var"#217#219"{Tuple{},typeof(logp),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [11] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [12] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [13] gradcheck(::typeof(logp), ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [14] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:30
   [16] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:4
  
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:31
  Test threw exception
  Expression: isapprox(f(a), f(k))
  Cannot find cudnn
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
   [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
   [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
   [5] generic_softmax(::KnetArray{Float64,3}, ::Int64, ::Function; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/loss.jl:138
   [6] #logp#705 at /home/pkgeval/.julia/packages/Knet/vxHRi/src/loss.jl:14 [inlined]
   [7] logp(::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/loss.jl:14
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:31
   [9] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [10] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:4
  

Stacktrace:
 [1] error(::String) at ./error.jl:33
 [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
 [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
 [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
 [5] generic_softmax(::Param{KnetArray{Float64,3}}, ::Int64, ::Function; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/loss.jl:138
 [6] #logp#705 at /home/pkgeval/.julia/packages/Knet/vxHRi/src/loss.jl:14 [inlined]
 [7] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [8] (::AutoGrad.var"#217#219"{Tuple{Pair{Symbol,Int64}},typeof(logp),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [9] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [10] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [11] gradcheck(::typeof(logp), ::KnetArray{Float64,3}; kw::Tuple{Pair{Symbol,Int64}}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [12] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:33
 [13] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:4
 [15] include(::String) at ./client.jl:441
 [16] macro expansion at ./util.jl:175 [inlined]
 [17] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:18 [inlined]
 [18] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [19] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [20] include(::String) at ./client.jl:441
 [21] top-level scope at none:6
 [22] eval(::Module, ::Any) at ./boot.jl:331
 [23] exec_options(::Base.JLOptions) at ./client.jl:264
 [24] _start() at ./client.jl:490
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:33
  Test threw exception
  Expression: gradcheck(f, k, kw = (:dims => d,))
  Cannot find cudnn
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(logp), ::KnetArray{Float64,3}; kw::Tuple{Pair{Symbol,Int64}}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:33
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:4
  caused by [exception 1]
  Cannot find cudnn
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
   [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
   [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
   [5] generic_softmax(::Param{KnetArray{Float64,3}}, ::Int64, ::Function; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/loss.jl:138
   [6] #logp#705 at /home/pkgeval/.julia/packages/Knet/vxHRi/src/loss.jl:14 [inlined]
   [7] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [8] (::AutoGrad.var"#217#219"{Tuple{Pair{Symbol,Int64}},typeof(logp),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [9] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [10] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [11] gradcheck(::typeof(logp), ::KnetArray{Float64,3}; kw::Tuple{Pair{Symbol,Int64}}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [12] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:33
   [13] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:4
  
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:34
  Test threw exception
  Expression: isapprox(f(a, dims = d), f(k, dims = d))
  Cannot find cudnn
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
   [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
   [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
   [5] generic_softmax(::KnetArray{Float64,3}, ::Int64, ::Function; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/loss.jl:138
   [6] #logp#705 at /home/pkgeval/.julia/packages/Knet/vxHRi/src/loss.jl:14 [inlined]
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:34
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:4
  

Stacktrace:
 [1] error(::String) at ./error.jl:33
 [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
 [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
 [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
 [5] generic_softmax(::Param{KnetArray{Float64,3}}, ::Int64, ::Function; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/loss.jl:138
 [6] #logp#705 at /home/pkgeval/.julia/packages/Knet/vxHRi/src/loss.jl:14 [inlined]
 [7] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [8] (::AutoGrad.var"#217#219"{Tuple{Pair{Symbol,Int64}},typeof(logp),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [9] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [10] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [11] gradcheck(::typeof(logp), ::KnetArray{Float64,3}; kw::Tuple{Pair{Symbol,Int64}}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [12] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:33
 [13] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/loss.jl:4
 [15] include(::String) at ./client.jl:441
 [16] macro expansion at ./util.jl:175 [inlined]
 [17] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:18 [inlined]
 [18] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [19] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [20] include(::String) at ./client.jl:441
 [21] top-level scope at none:6
 [22] eval(::Module, ::Any) at ./boot.jl:331
 [23] exec_options(::Base.JLOptions) at ./client.jl:264
 [24] _start() at ./client.jl:490

signal (15): Terminated
in expression starting at none:13
pthread_cond_wait at /lib/x86_64-linux-gnu/libpthread.so.0 (unknown line)
