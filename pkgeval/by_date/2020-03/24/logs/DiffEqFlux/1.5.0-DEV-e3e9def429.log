Julia Version 1.5.0-DEV.505
Commit e3e9def429 (2020-03-24 17:47 UTC)
Platform Info:
  OS: Linux (x86_64-linux-gnu)
  CPU: Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-9.0.1 (ORCJIT, skylake)
Environment:
  JULIA_DEPOT_PATH = ::/usr/local/share/julia
  JULIA_NUM_THREADS = 2

  Resolving package versions...
  Installed SortingAlgorithms ──────────── v0.3.1
  Installed PositiveFactorizations ─────── v0.2.3
  Installed DiffEqFlux ─────────────────── v1.7.1
  Installed ExponentialUtilities ───────── v1.6.0
  Installed SpecialFunctions ───────────── v0.10.0
  Installed ChainRulesCore ─────────────── v0.7.1
  Installed Distributions ──────────────── v0.23.1
  Installed Optim ──────────────────────── v0.20.4
  Installed RecursiveFactorization ─────── v0.1.0
  Installed UnPack ─────────────────────── v0.1.0
  Installed PDMats ─────────────────────── v0.9.12
  Installed Reexport ───────────────────── v0.2.0
  Installed ArrayInterface ─────────────── v2.6.1
  Installed StatsFuns ──────────────────── v0.9.4
  Installed FixedPointNumbers ──────────── v0.7.1
  Installed TranscodingStreams ─────────── v0.9.5
  Installed VertexSafeGraphs ───────────── v0.1.1
  Installed IteratorInterfaceExtensions ── v1.0.0
  Installed CEnum ──────────────────────── v0.2.0
  Installed GenericSVD ─────────────────── v0.3.0
  Installed Inflate ────────────────────── v0.1.2
  Installed FFTW ───────────────────────── v1.2.0
  Installed ColorTypes ─────────────────── v0.9.1
  Installed Roots ──────────────────────── v1.0.1
  Installed IterativeSolvers ───────────── v0.8.3
  Installed TimerOutputs ───────────────── v0.5.3
  Installed RecipesBase ────────────────── v0.8.0
  Installed SparseDiffTools ────────────── v1.4.0
  Installed NNlib ──────────────────────── v0.6.6
  Installed Parameters ─────────────────── v0.12.0
  Installed OrdinaryDiffEq ─────────────── v5.32.0
  Installed CUDAapi ────────────────────── v3.1.0
  Installed CommonSubexpressions ───────── v0.2.0
  Installed CompilerSupportLibraries_jll ─ v0.3.1+0
  Installed CuArrays ───────────────────── v1.7.3
  Installed DiffRules ──────────────────── v1.0.1
  Installed LightGraphs ────────────────── v1.3.1
  Installed LineSearches ───────────────── v7.0.1
  Installed FunctionWrappers ───────────── v1.1.1
  Installed Tracker ────────────────────── v0.2.6
  Installed Missings ───────────────────── v0.4.3
  Installed Media ──────────────────────── v0.5.0
  Installed StaticArrays ───────────────── v0.12.1
  Installed IRTools ────────────────────── v0.3.1
  Installed Zygote ─────────────────────── v0.4.12
  Installed TreeViews ──────────────────── v0.3.0
  Installed QuasiMonteCarlo ────────────── v0.1.2
  Installed CodecZlib ──────────────────── v0.6.0
  Installed ProgressMeter ──────────────── v1.2.0
  Installed IntelOpenMP_jll ────────────── v2018.0.3+0
  Installed Requires ───────────────────── v1.0.1
  Installed OrderedCollections ─────────── v1.1.0
  Installed FiniteDiff ─────────────────── v2.3.0
  Installed ArnoldiMethod ──────────────── v0.0.4
  Installed Arpack ─────────────────────── v0.4.0
  Installed CUDAnative ─────────────────── v2.10.2
  Installed ProgressLogging ────────────── v0.1.2
  Installed MKL_jll ────────────────────── v2019.0.117+2
  Installed Rmath ──────────────────────── v0.6.1
  Installed NLSolversBase ──────────────── v7.6.1
  Installed ReverseDiff ────────────────── v1.1.0
  Installed GPUArrays ──────────────────── v2.0.1
  Installed AbstractTrees ──────────────── v0.3.2
  Installed LLVM ───────────────────────── v1.3.4
  Installed Juno ───────────────────────── v0.8.1
  Installed DiffResults ────────────────── v1.0.2
  Installed OpenBLAS_jll ───────────────── v0.3.7+8
  Installed Colors ─────────────────────── v0.11.2
  Installed ForwardDiff ────────────────── v0.10.9
  Installed Compat ─────────────────────── v3.8.0
  Installed NLsolve ────────────────────── v4.3.0
  Installed RecursiveArrayTools ────────── v2.1.0
  Installed DiffEqCallbacks ────────────── v2.12.1
  Installed MuladdMacro ────────────────── v0.2.2
  Installed CUDAdrv ────────────────────── v6.0.0
  Installed ZygoteRules ────────────────── v0.2.0
  Installed FillArrays ─────────────────── v0.8.5
  Installed Distances ──────────────────── v0.8.2
  Installed DataStructures ─────────────── v0.17.10
  Installed ConsoleProgressMonitor ─────── v0.1.2
  Installed Flux ───────────────────────── v0.10.3
  Installed Arpack_jll ─────────────────── v3.5.0+2
  Installed DiffEqBase ─────────────────── v6.25.0
  Installed LoggingExtras ──────────────── v0.4.0
  Installed Adapt ──────────────────────── v1.0.1
  Installed TableTraits ────────────────── v1.0.0
  Installed AbstractFFTs ───────────────── v0.5.0
  Installed Rmath_jll ──────────────────── v0.2.2+0
  Installed MacroTools ─────────────────── v0.5.5
  Installed DataAPI ────────────────────── v1.1.0
  Installed QuadGK ─────────────────────── v2.3.1
  Installed FFTW_jll ───────────────────── v3.3.9+4
  Installed DiffEqSensitivity ──────────── v6.9.0
  Installed DocStringExtensions ────────── v0.8.1
  Installed ZipFile ────────────────────── v0.9.1
  Installed LeftChildRightSiblingTrees ─── v0.1.2
  Installed StatsBase ──────────────────── v0.32.2
  Installed ArrayLayouts ───────────────── v0.1.5
  Installed SimpleTraits ───────────────── v0.9.1
  Installed BinaryProvider ─────────────── v0.5.8
  Installed Zlib_jll ───────────────────── v1.2.11+8
  Installed NaNMath ────────────────────── v0.3.3
  Installed TerminalLoggers ────────────── v0.1.1
  Installed OpenSpecFun_jll ────────────── v0.5.3+3
  Installed Sobol ──────────────────────── v1.3.0
  Installed LatinHypercubeSampling ─────── v1.6.3
   Updating `~/.julia/environments/v1.5/Project.toml`
   aae7a2af + DiffEqFlux v1.7.1
   Updating `~/.julia/environments/v1.5/Manifest.toml`
   621f4979 + AbstractFFTs v0.5.0
   1520ce14 + AbstractTrees v0.3.2
   79e6a3ab + Adapt v1.0.1
   ec485272 + ArnoldiMethod v0.0.4
   7d9fca2a + Arpack v0.4.0
   68821587 + Arpack_jll v3.5.0+2
   4fba245c + ArrayInterface v2.6.1
   4c555306 + ArrayLayouts v0.1.5
   b99e7846 + BinaryProvider v0.5.8
   fa961155 + CEnum v0.2.0
   3895d2a7 + CUDAapi v3.1.0
   c5f51814 + CUDAdrv v6.0.0
   be33ccc6 + CUDAnative v2.10.2
   d360d2e6 + ChainRulesCore v0.7.1
   944b1d66 + CodecZlib v0.6.0
   3da002f7 + ColorTypes v0.9.1
   5ae59095 + Colors v0.11.2
   bbf7d656 + CommonSubexpressions v0.2.0
   34da2185 + Compat v3.8.0
   e66e0078 + CompilerSupportLibraries_jll v0.3.1+0
   88cd18e8 + ConsoleProgressMonitor v0.1.2
   3a865a2d + CuArrays v1.7.3
   9a962f9c + DataAPI v1.1.0
   864edb3b + DataStructures v0.17.10
   2b5f629d + DiffEqBase v6.25.0
   459566f4 + DiffEqCallbacks v2.12.1
   aae7a2af + DiffEqFlux v1.7.1
   41bf760c + DiffEqSensitivity v6.9.0
   163ba53b + DiffResults v1.0.2
   b552c78f + DiffRules v1.0.1
   b4f34e82 + Distances v0.8.2
   31c24e10 + Distributions v0.23.1
   ffbed154 + DocStringExtensions v0.8.1
   d4d017d3 + ExponentialUtilities v1.6.0
   7a1cc6ca + FFTW v1.2.0
   f5851436 + FFTW_jll v3.3.9+4
   1a297f60 + FillArrays v0.8.5
   6a86dc24 + FiniteDiff v2.3.0
   53c48c17 + FixedPointNumbers v0.7.1
   587475ba + Flux v0.10.3
   f6369f11 + ForwardDiff v0.10.9
   069b7b12 + FunctionWrappers v1.1.1
   0c68f7d7 + GPUArrays v2.0.1
   01680d73 + GenericSVD v0.3.0
   7869d1d1 + IRTools v0.3.1
   d25df0c9 + Inflate v0.1.2
   1d5cc7b8 + IntelOpenMP_jll v2018.0.3+0
   42fd0dbc + IterativeSolvers v0.8.3
   82899510 + IteratorInterfaceExtensions v1.0.0
   e5e0dc1b + Juno v0.8.1
   929cbde3 + LLVM v1.3.4
   a5e1c1ea + LatinHypercubeSampling v1.6.3
   1d6d02ad + LeftChildRightSiblingTrees v0.1.2
   093fc24a + LightGraphs v1.3.1
   d3d80556 + LineSearches v7.0.1
   e6f89c97 + LoggingExtras v0.4.0
   856f044c + MKL_jll v2019.0.117+2
   1914dd2f + MacroTools v0.5.5
   e89f7d12 + Media v0.5.0
   e1d29d7a + Missings v0.4.3
   46d2c3a1 + MuladdMacro v0.2.2
   d41bc354 + NLSolversBase v7.6.1
   2774e3e8 + NLsolve v4.3.0
   872c559c + NNlib v0.6.6
   77ba4419 + NaNMath v0.3.3
   4536629a + OpenBLAS_jll v0.3.7+8
   efe28fd5 + OpenSpecFun_jll v0.5.3+3
   429524aa + Optim v0.20.4
   bac558e1 + OrderedCollections v1.1.0
   1dea7af3 + OrdinaryDiffEq v5.32.0
   90014a1f + PDMats v0.9.12
   d96e819e + Parameters v0.12.0
   85a6dd25 + PositiveFactorizations v0.2.3
   33c8b6b6 + ProgressLogging v0.1.2
   92933f4c + ProgressMeter v1.2.0
   1fd47b50 + QuadGK v2.3.1
   8a4e6c94 + QuasiMonteCarlo v0.1.2
   3cdcf5f2 + RecipesBase v0.8.0
   731186ca + RecursiveArrayTools v2.1.0
   f2c3362d + RecursiveFactorization v0.1.0
   189a3867 + Reexport v0.2.0
   ae029012 + Requires v1.0.1
   37e2e3b7 + ReverseDiff v1.1.0
   79098fc4 + Rmath v0.6.1
   f50d1b31 + Rmath_jll v0.2.2+0
   f2b01f46 + Roots v1.0.1
   699a6c99 + SimpleTraits v0.9.1
   ed01d8cd + Sobol v1.3.0
   a2af1166 + SortingAlgorithms v0.3.1
   47a9eef4 + SparseDiffTools v1.4.0
   276daf66 + SpecialFunctions v0.10.0
   90137ffa + StaticArrays v0.12.1
   2913bbd2 + StatsBase v0.32.2
   4c63d2b9 + StatsFuns v0.9.4
   3783bdb8 + TableTraits v1.0.0
   5d786b92 + TerminalLoggers v0.1.1
   a759f4b9 + TimerOutputs v0.5.3
   9f7883ad + Tracker v0.2.6
   3bb67fe8 + TranscodingStreams v0.9.5
   a2a6695c + TreeViews v0.3.0
   3a884ed6 + UnPack v0.1.0
   19fa3120 + VertexSafeGraphs v0.1.1
   a5390f91 + ZipFile v0.9.1
   83775a58 + Zlib_jll v1.2.11+8
   e88e6eb3 + Zygote v0.4.12
   700de1a5 + ZygoteRules v0.2.0
   2a0f44e3 + Base64
   ade2ca70 + Dates
   8bb1440f + DelimitedFiles
   8ba89e20 + Distributed
   b77e0a4c + InteractiveUtils
   76f85450 + LibGit2
   8f399da3 + Libdl
   37e2e46d + LinearAlgebra
   56ddb016 + Logging
   d6f4376e + Markdown
   a63ad114 + Mmap
   44cfe95a + Pkg
   de0858da + Printf
   9abbd945 + Profile
   3fa0cd96 + REPL
   9a3f8284 + Random
   ea8e919c + SHA
   9e88b42a + Serialization
   1a1011a3 + SharedArrays
   6462fe0b + Sockets
   2f01184e + SparseArrays
   10745b16 + Statistics
   4607b0f0 + SuiteSparse
   8dfed614 + Test
   cf7118a7 + UUIDs
   4ec0a83e + Unicode
   Building FFTW ─────→ `~/.julia/packages/FFTW/qqcBj/deps/build.log`
   Building NNlib ────→ `~/.julia/packages/NNlib/FAI3o/deps/build.log`
   Building CodecZlib → `~/.julia/packages/CodecZlib/5t9zO/deps/build.log`
    Testing DiffEqFlux
     Status `/tmp/jl_FOXrKI/Project.toml`
   79e6a3ab Adapt v1.0.1
   bcd4f6db DelayDiffEq v5.23.0
   2b5f629d DiffEqBase v6.25.0
   aae7a2af DiffEqFlux v1.7.1
   41bf760c DiffEqSensitivity v6.9.0
   163ba53b DiffResults v1.0.2
   587475ba Flux v0.10.3
   f6369f11 ForwardDiff v0.10.9
   76087f3c NLopt v0.5.1
   429524aa Optim v0.20.4
   1dea7af3 OrdinaryDiffEq v5.32.0
   33c8b6b6 ProgressLogging v0.1.2
   731186ca RecursiveArrayTools v2.1.0
   ae029012 Requires v1.0.1
   37e2e3b7 ReverseDiff v1.1.0
   1bc83da4 SafeTestsets v0.0.1
   90137ffa StaticArrays v0.12.1
   789caeaf StochasticDiffEq v6.19.1
   9f7883ad Tracker v0.2.6
   e88e6eb3 Zygote v0.4.12
   700de1a5 ZygoteRules v0.2.0
   37e2e46d LinearAlgebra
   56ddb016 Logging
   10745b16 Statistics
   8dfed614 Test
     Status `/tmp/jl_FOXrKI/Manifest.toml`
   621f4979 AbstractFFTs v0.5.0
   1520ce14 AbstractTrees v0.3.2
   79e6a3ab Adapt v1.0.1
   ec485272 ArnoldiMethod v0.0.4
   7d9fca2a Arpack v0.4.0
   68821587 Arpack_jll v3.5.0+2
   4fba245c ArrayInterface v2.6.1
   4c555306 ArrayLayouts v0.1.5
   9e28174c BinDeps v1.0.0
   b99e7846 BinaryProvider v0.5.8
   fa961155 CEnum v0.2.0
   631607c0 CMake v1.2.0
   d5fb7624 CMakeWrapper v0.2.3
   3895d2a7 CUDAapi v3.1.0
   c5f51814 CUDAdrv v6.0.0
   be33ccc6 CUDAnative v2.10.2
   d360d2e6 ChainRulesCore v0.7.1
   944b1d66 CodecZlib v0.6.0
   3da002f7 ColorTypes v0.9.1
   5ae59095 Colors v0.11.2
   bbf7d656 CommonSubexpressions v0.2.0
   34da2185 Compat v3.8.0
   e66e0078 CompilerSupportLibraries_jll v0.3.1+0
   88cd18e8 ConsoleProgressMonitor v0.1.2
   3a865a2d CuArrays v1.7.3
   9a962f9c DataAPI v1.1.0
   864edb3b DataStructures v0.17.10
   bcd4f6db DelayDiffEq v5.23.0
   2b5f629d DiffEqBase v6.25.0
   459566f4 DiffEqCallbacks v2.12.1
   aae7a2af DiffEqFlux v1.7.1
   77a26b50 DiffEqNoiseProcess v3.9.0
   41bf760c DiffEqSensitivity v6.9.0
   163ba53b DiffResults v1.0.2
   b552c78f DiffRules v1.0.1
   b4f34e82 Distances v0.8.2
   31c24e10 Distributions v0.23.1
   ffbed154 DocStringExtensions v0.8.1
   d4d017d3 ExponentialUtilities v1.6.0
   7a1cc6ca FFTW v1.2.0
   f5851436 FFTW_jll v3.3.9+4
   1a297f60 FillArrays v0.8.5
   6a86dc24 FiniteDiff v2.3.0
   53c48c17 FixedPointNumbers v0.7.1
   587475ba Flux v0.10.3
   f6369f11 ForwardDiff v0.10.9
   069b7b12 FunctionWrappers v1.1.1
   0c68f7d7 GPUArrays v2.0.1
   01680d73 GenericSVD v0.3.0
   7869d1d1 IRTools v0.3.1
   d25df0c9 Inflate v0.1.2
   1d5cc7b8 IntelOpenMP_jll v2018.0.3+0
   42fd0dbc IterativeSolvers v0.8.3
   82899510 IteratorInterfaceExtensions v1.0.0
   e5e0dc1b Juno v0.8.1
   929cbde3 LLVM v1.3.4
   a5e1c1ea LatinHypercubeSampling v1.6.3
   1d6d02ad LeftChildRightSiblingTrees v0.1.2
   093fc24a LightGraphs v1.3.1
   d3d80556 LineSearches v7.0.1
   e6f89c97 LoggingExtras v0.4.0
   856f044c MKL_jll v2019.0.117+2
   1914dd2f MacroTools v0.5.5
   fdba3010 MathProgBase v0.7.8
   e89f7d12 Media v0.5.0
   e1d29d7a Missings v0.4.3
   46d2c3a1 MuladdMacro v0.2.2
   d41bc354 NLSolversBase v7.6.1
   76087f3c NLopt v0.5.1
   2774e3e8 NLsolve v4.3.0
   872c559c NNlib v0.6.6
   77ba4419 NaNMath v0.3.3
   4536629a OpenBLAS_jll v0.3.7+8
   efe28fd5 OpenSpecFun_jll v0.5.3+3
   429524aa Optim v0.20.4
   bac558e1 OrderedCollections v1.1.0
   1dea7af3 OrdinaryDiffEq v5.32.0
   90014a1f PDMats v0.9.12
   d96e819e Parameters v0.12.0
   85a6dd25 PositiveFactorizations v0.2.3
   33c8b6b6 ProgressLogging v0.1.2
   92933f4c ProgressMeter v1.2.0
   1fd47b50 QuadGK v2.3.1
   8a4e6c94 QuasiMonteCarlo v0.1.2
   e6cf234a RandomNumbers v1.4.0
   3cdcf5f2 RecipesBase v0.8.0
   731186ca RecursiveArrayTools v2.1.0
   f2c3362d RecursiveFactorization v0.1.0
   189a3867 Reexport v0.2.0
   ae029012 Requires v1.0.1
   ae5879a3 ResettableStacks v1.0.0
   37e2e3b7 ReverseDiff v1.1.0
   79098fc4 Rmath v0.6.1
   f50d1b31 Rmath_jll v0.2.2+0
   f2b01f46 Roots v1.0.1
   1bc83da4 SafeTestsets v0.0.1
   699a6c99 SimpleTraits v0.9.1
   ed01d8cd Sobol v1.3.0
   a2af1166 SortingAlgorithms v0.3.1
   47a9eef4 SparseDiffTools v1.4.0
   276daf66 SpecialFunctions v0.10.0
   90137ffa StaticArrays v0.12.1
   2913bbd2 StatsBase v0.32.2
   4c63d2b9 StatsFuns v0.9.4
   789caeaf StochasticDiffEq v6.19.1
   3783bdb8 TableTraits v1.0.0
   5d786b92 TerminalLoggers v0.1.1
   a759f4b9 TimerOutputs v0.5.3
   9f7883ad Tracker v0.2.6
   3bb67fe8 TranscodingStreams v0.9.5
   a2a6695c TreeViews v0.3.0
   30578b45 URIParser v0.4.0
   3a884ed6 UnPack v0.1.0
   19fa3120 VertexSafeGraphs v0.1.1
   a5390f91 ZipFile v0.9.1
   83775a58 Zlib_jll v1.2.11+8
   e88e6eb3 Zygote v0.4.12
   700de1a5 ZygoteRules v0.2.0
   2a0f44e3 Base64
   ade2ca70 Dates
   8bb1440f DelimitedFiles
   8ba89e20 Distributed
   b77e0a4c InteractiveUtils
   76f85450 LibGit2
   8f399da3 Libdl
   37e2e46d LinearAlgebra
   56ddb016 Logging
   d6f4376e Markdown
   a63ad114 Mmap
   44cfe95a Pkg
   de0858da Printf
   9abbd945 Profile
   3fa0cd96 REPL
   9a3f8284 Random
   ea8e919c SHA
   9e88b42a Serialization
   1a1011a3 SharedArrays
   6462fe0b Sockets
   2f01184e SparseArrays
   10745b16 Statistics
   4607b0f0 SuiteSparse
   8dfed614 Test
   cf7118a7 UUIDs
   4ec0a83e Unicode
WARNING: could not import Compiler.just_construct_ssa into Wrap
┌ Warning: CuArrays.jl found cuda, but did not find libcudnn. Some functionality will not be available.
└ @ Flux ~/.julia/packages/Flux/NpkMm/src/Flux.jl:48
┌ Warning: Incompatibility detected between CUDA and LLVM 8.0+; disabling debug info emission for CUDA kernels
└ @ CUDAnative ~/.julia/packages/CUDAnative/hfulr/src/CUDAnative.jl:114
┌ Warning: CuArrays.jl found cuda, but did not find libcudnn. Some functionality will not be available.
└ @ Flux ~/.julia/packages/Flux/NpkMm/src/Flux.jl:48
416.7086995514811301.86465432569827213.21697213391016149.3929796071194115.2247517913104296.3438136311290279.3936123278511765.5625775751988754.73408970504229446.2626159904189439.55562467607837434.29338639796445430.54262397471921328.93102825992030330.78230280655101637.5616921386252547.3899178101139352.31771747076819448.8559023431533241.1409918445188933.4998573173513127.7136171838481823.8213532431337521.2574969683678819.43364473086034417.9080234863235616.40169427585258414.75827680393393112.9155083938401710.8813206837866058.7182734287630626.5317427624672214.45717155975515762.6440088213855261.23592071815229730.34657206429314730.032376452495214510.26367482940779230.905567097641851.72975726545156162.4738072183786732.93352765725007243.03618679634409672.84498125290393672.49864919108605132.1348949032518721.84171784634708581.64735382952383951.53548374223780541.46787860302471571.40348733006078131.31057624087927451.1723646987133380.98792068128521930.77000396330597080.54099857867771020.327773302321376360.156017818275280080.0445653704883056740.00091322139101779030.0190239786206280320.080410475436539270.159197271761418070.229589835455646730.27324885152407110.28364198270243850.26573545746272670.231707259387955280.195075966066423880.16585180213779310.148215404734272720.140797690163480650.13867239213660060.135911948945067140.127791420961522520.112133203758811030.089657382441687130.063454110359997350.037871264121909730.017167018654110830.0042911560636956050.000132331621225468740.0034254726393664930.0112719546828020260.020185633126746330.027176635651557520.0305475552150724330.0301433293399415230.027042574606087690.0228944748810245740.0192068480124802340.0168492892211720180.0158974341901012840.0157962049318957630.0157121461393411320.0149121397453862370.013034469259305310.0101844853779321530.0068527418229991870.00371107376408517140.4165147233362722.05749907210333312.0370229088210226.5135197460485113.4493268314854941.75595334745193780.84094112859804390.37442848437232390.168534450628907680.113993987252263160.146123478717827850.226085226249190860.330333604729165340.444534373326884170.55998839915566550.67148736086149520.77600647672168410.87189532854251120.95837015434837681.03519180224068191.10245973190405681.1604782310606711.20967136813183121.2505271011940631.28356141199421251.30929540278075081.32824101618583621.34089244428137851.34772122926775031.34917388070383741.34567112361001651.33760820437140931.32535586876996651.30926175199351081.2896520048541721.26683303750284851.2410933007046591.21270505766819881.18192610099156161.14900136156496921.11416445784295411.077639125372521.03964052899088681.00037646561251430.96004837917381170.9188522812487860.87697958751648160.83461759464484320.79194990569877520.74915676965992480.70641524364564860.6638991331799460.6217785606913130.58021964301761350.53938387483431670.499427333461248070.460499739708821740.42274337822788430.386291899150562330.35126903336119110.317787254515817130.28594642748894370.2558324889077440.22751621015976570.201052096409242430.17647747594999640.153811832936124440.13305643256661220.114194281298790780.097190454764755960.081992813256698670.068533108927257250.0567284800940232450.046483294056760050.037691293225716780.0302379803044163580.0240031626280547870.018863561059381280.0146954113248494580.0113769316606047150.0087905797423000960.0068250187158452330.0053767511287227460.0043513374548950640.0036642088220860870.0032410697523287770.0030179127204255330.00294069263292744520.002964718794096250.0030538328905185820.00317944571738445650.00331950398506860.00345745205301092220.00358124310142483430.00368244138331800960.00375544316037069530.0037968299933879190.003804855491655320.0037790561592185770.0037199694116348114416.70869955148316301.87186997430365213.22801705361297149.40648614478837115.2240412249364696.3454391345538679.3977976779125865.5678714713974554.73969630607274446.2681431647925939.5608383606454234.2980229596167530.54633410011867428.9332381463373130.78208250143966437.5587566610162847.3870414634704752.3184906512701248.8589544323429841.14383255919601433.5012140860317727.71329342894746323.81952936942857221.2543767187669819.4294856183711817.90303100693667616.39608500849614714.75230897079328312.90945584534311610.8754625577971288.712882525457966.5270686024001344.4534149599774862.6412962392444671.2342795663056390.3459158046638280.032506748634095760.264310414146053160.90639787468961761.7305156642179892.4743407762289472.9338239544033673.0363286740031682.84506488225275162.49872238818174442.13494430274363721.8416920826943291.6471951659831451.53515198186780831.46736343542057451.40280885504569251.30977827422533061.1715059590677530.98706641658549010.76921762520158130.54033333964074280.32726603243509670.155685347538229820.044402053227207340.00089265358725995560.0191039242369983630.080534769824929170.159307303717266780.229641600525029880.273224851436197740.283552996888048250.26561206968046450.23158584804701470.194985613398158870.165806707487119760.14821387238102470.140826097601411540.13871147379454670.135943057155115480.12780127946737040.112116466759106760.089616889144780110.063399129505176940.037814459743258160.017120978004210560.00426522788223897450.00013060182132885350.00344585479851722280.011306698777051870.0202248123961467770.02721155913652230.0305730930624841660.030158440940405340.027049241017987240.022895782224433470.0192051454721658860.016845112348903880.0158894783750984330.015782221974403340.015690315650227170.0148822708785586060.0129985681020147110.0101464819170533940.0068175358724189420.003683289147449469Test Summary: | Pass  Total
Layers Tests  |    6      6
Test Summary: | Pass  Total
Fast Layers   |    9      9
Training   0%|                                          |  ETA: N/A
494.2089032199091Training   1%|▍                                         |  ETA: 0:07:57
416.7086995514831Training   2%|▉                                         |  ETA: 0:04:02
301.87186997430365Training   3%|█▎                                        |  ETA: 0:02:42
213.22801705361297Training   4%|█▋                                        |  ETA: 0:02:01
149.40648614478843Training   5%|██▏                                       |  ETA: 0:01:37
115.22404122493646Training   6%|██▌                                       |  ETA: 0:01:23
96.34543913455384Training   7%|███                                       |  ETA: 0:01:11
79.39779767791259Training   8%|███▍                                      |  ETA: 0:01:02
65.56787147139745Training   9%|███▊                                      |  ETA: 0:00:55
54.739696306072744Training  10%|████▎                                     |  ETA: 0:00:49
46.26814316479258Training  11%|████▋                                     |  ETA: 0:00:44
39.560838360645405Training  12%|█████                                     |  ETA: 0:00:41
34.298022959616745Training  13%|█████▌                                    |  ETA: 0:00:37
30.54633410011867Training  14%|█████▉                                    |  ETA: 0:00:35
28.933238146337313Training  15%|██████▎                                   |  ETA: 0:00:32
30.78208250143967Training  16%|██████▊                                   |  ETA: 0:00:30
37.55875666101628Training  17%|███████▏                                  |  ETA: 0:00:28
47.38704146347047Training  18%|███████▌                                  |  ETA: 0:00:27
52.318490651270146Training  19%|████████                                  |  ETA: 0:00:25
48.85895443234298Training  20%|████████▍                                 |  ETA: 0:00:24
41.143832559196Training  21%|████████▉                                 |  ETA: 0:00:23
33.50121408603177Training  22%|█████████▎                                |  ETA: 0:00:22
27.713293428947466Training  23%|█████████▋                                |  ETA: 0:00:20
23.819529369428558Training  24%|██████████▏                               |  ETA: 0:00:20
21.254376718766984Training  25%|██████████▌                               |  ETA: 0:00:19
19.42948561837119Training  26%|██████████▉                               |  ETA: 0:00:18
17.90303100693668Training  27%|███████████▍                              |  ETA: 0:00:17
16.396085008496147Training  28%|███████████▊                              |  ETA: 0:00:16
14.752308970793283Training  29%|████████████▏                             |  ETA: 0:00:16
12.909455845343116Training  30%|████████████▋                             |  ETA: 0:00:15
10.875462557797128Training  31%|█████████████                             |  ETA: 0:00:14
8.71288252545796Training  32%|█████████████▌                            |  ETA: 0:00:14
6.527068602400136Training  33%|█████████████▉                            |  ETA: 0:00:13
4.4534149599774855Training  34%|██████████████▎                           |  ETA: 0:00:13
2.6412962392444665Training  35%|██████████████▊                           |  ETA: 0:00:12
1.2342795663056394Training  36%|███████████████▏                          |  ETA: 0:00:12
0.34591580466382804Training  37%|███████████████▌                          |  ETA: 0:00:11
0.03250674863409575Training  38%|████████████████                          |  ETA: 0:00:11
0.2643104141460533Training  39%|████████████████▍                         |  ETA: 0:00:11
0.9063978746896175Training  40%|████████████████▊                         |  ETA: 0:00:10
1.7305156642179889Training  41%|█████████████████▎                        |  ETA: 0:00:10
2.474340776228947Training  42%|█████████████████▋                        |  ETA: 0:00:10
2.933823954403367Training  43%|██████████████████                        |  ETA: 0:00:09
3.036328674003168Training  44%|██████████████████▌                       |  ETA: 0:00:09
2.845064882252751Training  45%|██████████████████▉                       |  ETA: 0:00:09
2.498722388181745Training  46%|███████████████████▍                      |  ETA: 0:00:09
2.1349443027436377Training  47%|███████████████████▊                      |  ETA: 0:00:08
1.8416920826943293Training  48%|████████████████████▏                     |  ETA: 0:00:08
1.647195165983145Training  49%|████████████████████▋                     |  ETA: 0:00:08
1.5351519818678083Training  50%|█████████████████████                     |  ETA: 0:00:07
1.467363435420574Training  51%|█████████████████████▍                    |  ETA: 0:00:07
1.402808855045693Training  52%|█████████████████████▉                    |  ETA: 0:00:07
1.3097782742253306Training  53%|██████████████████████▎                   |  ETA: 0:00:07
1.171505959067753Training  54%|██████████████████████▋                   |  ETA: 0:00:07
0.9870664165854897Training  55%|███████████████████████▏                  |  ETA: 0:00:06
0.7692176252015813Training  56%|███████████████████████▌                  |  ETA: 0:00:06
0.5403333396407428Training  57%|████████████████████████                  |  ETA: 0:00:06
0.3272660324350968Training  58%|████████████████████████▍                 |  ETA: 0:00:06
0.1556853475382298Training  59%|████████████████████████▊                 |  ETA: 0:00:06
0.04440205322720733Training  60%|█████████████████████████▎                |  ETA: 0:00:05
0.0008926535872599556Training  61%|█████████████████████████▋                |  ETA: 0:00:05
0.019103924236998377Training  62%|██████████████████████████                |  ETA: 0:00:05
0.08053476982492919Training  63%|██████████████████████████▌               |  ETA: 0:00:05
0.15930730371726676Training  64%|██████████████████████████▉               |  ETA: 0:00:05
0.22964160052502988Training  65%|███████████████████████████▎              |  ETA: 0:00:04
0.2732248514361978Training  66%|███████████████████████████▊              |  ETA: 0:00:04
0.2835529968880482Training  67%|████████████████████████████▏             |  ETA: 0:00:04
0.2656120696804645Training  68%|████████████████████████████▌             |  ETA: 0:00:04
0.23158584804701468Training  69%|█████████████████████████████             |  ETA: 0:00:04
0.19498561339815892Training  70%|█████████████████████████████▍            |  ETA: 0:00:04
0.1658067074871198Training  71%|█████████████████████████████▉            |  ETA: 0:00:03
0.14821387238102476Training  72%|██████████████████████████████▎           |  ETA: 0:00:03
0.14082609760141152Training  73%|██████████████████████████████▋           |  ETA: 0:00:03
0.13871147379454676Training  74%|███████████████████████████████▏          |  ETA: 0:00:03
0.13594305715511545Training  75%|███████████████████████████████▌          |  ETA: 0:00:03
0.1278012794673704Training  76%|███████████████████████████████▉          |  ETA: 0:00:03
0.11211646675910676Training  77%|████████████████████████████████▍         |  ETA: 0:00:03
0.0896168891447801Training  78%|████████████████████████████████▊         |  ETA: 0:00:02
0.06339912950517693Training  79%|█████████████████████████████████▏        |  ETA: 0:00:02
0.03781445974325816Training  80%|█████████████████████████████████▋        |  ETA: 0:00:02
0.017120978004210557Training  81%|██████████████████████████████████        |  ETA: 0:00:02
0.004265227882238975Training  82%|██████████████████████████████████▌       |  ETA: 0:00:02
0.00013060182132885353Training  83%|██████████████████████████████████▉       |  ETA: 0:00:02
0.0034458547985172228Training  84%|███████████████████████████████████▎      |  ETA: 0:00:02
0.01130669877705187Training  85%|███████████████████████████████████▊      |  ETA: 0:00:02
0.020224812396146763Training  86%|████████████████████████████████████▏     |  ETA: 0:00:02
0.0272115591365223Training  87%|████████████████████████████████████▌     |  ETA: 0:00:01
0.030573093062484166Training  88%|█████████████████████████████████████     |  ETA: 0:00:01
0.03015844094040534Training  89%|█████████████████████████████████████▍    |  ETA: 0:00:01
0.027049241017987252Training  90%|█████████████████████████████████████▊    |  ETA: 0:00:01
0.02289578222443347Training  91%|██████████████████████████████████████▎   |  ETA: 0:00:01
0.019205145472165897Training  92%|██████████████████████████████████████▋   |  ETA: 0:00:01
0.01684511234890388Training  93%|███████████████████████████████████████   |  ETA: 0:00:01
0.01588947837509843Training  94%|███████████████████████████████████████▌  |  ETA: 0:00:01
0.015782221974403335Training  95%|███████████████████████████████████████▉  |  ETA: 0:00:01
0.01569031565022717Training  96%|████████████████████████████████████████▍ |  ETA: 0:00:00
0.014882270878558595Training  97%|████████████████████████████████████████▊ |  ETA: 0:00:00
0.01299856810201471Training  98%|█████████████████████████████████████████▏|  ETA: 0:00:00
0.010146481917053394Training  99%|█████████████████████████████████████████▋|  ETA: 0:00:00
0.0068175358724189405Training 100%|██████████████████████████████████████████| Time: 0:00:09
Training 100%|██████████████████████████████████████████| Time: 0:00:10
494.2089032199091473.8759825703607567.1074818633421318.3855315564830028.9648899689183876.117368644930780.16955330167694750.00127452976279778434.4571905150957083e-72.8044413366351072e-82.7293451721924503e-82.729347900600587e-8494.2089032199091473.8237904460742767.225842645127621.36751439899320811.4350691360731788.0109706995672490.32915359623862510.0036303407653235878.934029714091469e-56.246060306129264e-50.00031047251577495510.0058696171661135780.000109832425486265727.733918863503838e-64.2537266437804656e-78.726103760581099e-74.2537266437804656e-72.72458808565546e-72.766701471525958e-72.72458808565546e-72.6483736867625894e-72.648412606822959e-72.6483736867625894e-72.5881397656192965e-72.588142311285918e-72.5881397656192965e-72.508740247077843e-72.508742514801445e-72.508740247077843e-72.508742104639958e-7Training   0%|                                          |  ETA: N/A
74.23333058147179Training   1%|▍                                         |  ETA: 0:00:23
40.41651472333627Training   2%|▉                                         |  ETA: 0:00:11
22.057499072103333Training   3%|█▎                                        |  ETA: 0:00:07
12.037022908821022Training   4%|█▋                                        |  ETA: 0:00:06
6.513519746048511Training   5%|██▏                                       |  ETA: 0:00:04
3.449326831485494Training   6%|██▌                                       |  ETA: 0:00:04
1.7559533474519378Training   7%|███                                       |  ETA: 0:00:03
0.8409411285980439Training   8%|███▍                                      |  ETA: 0:00:03
0.3744284843723239Training   9%|███▊                                      |  ETA: 0:00:02
0.16853445062890768Training  10%|████▎                                     |  ETA: 0:00:02
0.11399398725226316Training  11%|████▋                                     |  ETA: 0:00:02
0.14612347871782785Training  12%|█████                                     |  ETA: 0:00:02
0.22608522624919086Training  13%|█████▌                                    |  ETA: 0:00:02
0.33033360472916534Training  14%|█████▉                                    |  ETA: 0:00:01
0.44453437332688417Training  15%|██████▎                                   |  ETA: 0:00:01
0.5599883991556655Training  16%|██████▊                                   |  ETA: 0:00:01
0.6714873608614952Training  17%|███████▏                                  |  ETA: 0:00:01
0.7760064767216841Training  18%|███████▌                                  |  ETA: 0:00:01
0.8718953285425112Training  19%|████████                                  |  ETA: 0:00:01
0.9583701543483768Training  20%|████████▍                                 |  ETA: 0:00:01
1.0351918022406819Training  21%|████████▉                                 |  ETA: 0:00:01
1.1024597319040568Training  22%|█████████▎                                |  ETA: 0:00:01
1.160478231060671Training  23%|█████████▋                                |  ETA: 0:00:01
1.2096713681318312Training  24%|██████████▏                               |  ETA: 0:00:01
1.250527101194063Training  25%|██████████▌                               |  ETA: 0:00:01
1.2835614119942125Training  26%|██████████▉                               |  ETA: 0:00:01
1.3092954027807508Training  27%|███████████▍                              |  ETA: 0:00:01
1.3282410161858362Training  28%|███████████▊                              |  ETA: 0:00:01
1.3408924442813785Training  29%|████████████▏                             |  ETA: 0:00:01
1.3477212292677503Training  30%|████████████▋                             |  ETA: 0:00:01
1.3491738807038374Training  31%|█████████████                             |  ETA: 0:00:01
1.3456711236100165Training  32%|█████████████▌                            |  ETA: 0:00:01
1.3376082043714093Training  33%|█████████████▉                            |  ETA: 0:00:01
1.3253558687699665Training  34%|██████████████▎                           |  ETA: 0:00:01
1.3092617519935108Training  35%|██████████████▊                           |  ETA: 0:00:01
1.289652004854172Training  36%|███████████████▏                          |  ETA: 0:00:00
1.2668330375028485Training  37%|███████████████▌                          |  ETA: 0:00:00
1.241093300704659Training  38%|████████████████                          |  ETA: 0:00:00
1.2127050576681988Training  39%|████████████████▍                         |  ETA: 0:00:00
1.1819261009915616Training  40%|████████████████▊                         |  ETA: 0:00:00
1.1490013615649692Training  41%|█████████████████▎                        |  ETA: 0:00:00
1.1141644578429541Training  42%|█████████████████▋                        |  ETA: 0:00:00
1.07763912537252Training  43%|██████████████████                        |  ETA: 0:00:00
1.0396405289908868Training  44%|██████████████████▌                       |  ETA: 0:00:00
1.0003764656125143Training  45%|██████████████████▉                       |  ETA: 0:00:00
0.9600483791738117Training  46%|███████████████████▍                      |  ETA: 0:00:00
0.918852281248786Training  47%|███████████████████▊                      |  ETA: 0:00:00
0.8769795875164816Training  48%|████████████████████▏                     |  ETA: 0:00:00
0.8346175946448432Training  49%|████████████████████▋                     |  ETA: 0:00:00
0.7919499056987752Training  50%|█████████████████████                     |  ETA: 0:00:00
0.7491567696599248Training  51%|█████████████████████▍                    |  ETA: 0:00:00
0.7064152436456486Training  52%|█████████████████████▉                    |  ETA: 0:00:00
0.663899133179946Training  53%|██████████████████████▎                   |  ETA: 0:00:00
0.621778560691313Training  54%|██████████████████████▋                   |  ETA: 0:00:00
0.5802196430176135Training  55%|███████████████████████▏                  |  ETA: 0:00:00
0.5393838748343167Training  56%|███████████████████████▌                  |  ETA: 0:00:00
0.49942733346124807Training  57%|████████████████████████                  |  ETA: 0:00:00
0.46049973970882174Training  58%|████████████████████████▍                 |  ETA: 0:00:00
0.4227433782278843Training  59%|████████████████████████▊                 |  ETA: 0:00:00
0.38629189915056233Training  60%|█████████████████████████▎                |  ETA: 0:00:00
0.3512690333611911Training  61%|█████████████████████████▋                |  ETA: 0:00:00
0.31778725451581713Training  62%|██████████████████████████                |  ETA: 0:00:00
0.2859464274889437Training  63%|██████████████████████████▌               |  ETA: 0:00:00
0.255832488907744Training  64%|██████████████████████████▉               |  ETA: 0:00:00
0.2275162101597657Training  65%|███████████████████████████▎              |  ETA: 0:00:00
0.20105209640924243Training  66%|███████████████████████████▊              |  ETA: 0:00:00
0.1764774759499964Training  67%|████████████████████████████▏             |  ETA: 0:00:00
0.15381183293612444Training  68%|████████████████████████████▌             |  ETA: 0:00:00
0.1330564325666122Training  69%|█████████████████████████████             |  ETA: 0:00:00
0.11419428129879078Training  70%|█████████████████████████████▍            |  ETA: 0:00:00
0.09719045476475596Training  71%|█████████████████████████████▉            |  ETA: 0:00:00
0.08199281325669867Training  72%|██████████████████████████████▎           |  ETA: 0:00:00
0.06853310892725725Training  73%|██████████████████████████████▋           |  ETA: 0:00:00
0.056728480094023245Training  74%|███████████████████████████████▏          |  ETA: 0:00:00
0.04648329405676005Training  75%|███████████████████████████████▌          |  ETA: 0:00:00
0.03769129322571678Training  76%|███████████████████████████████▉          |  ETA: 0:00:00
0.030237980304416358Training  77%|████████████████████████████████▍         |  ETA: 0:00:00
0.024003162628054787Training  78%|████████████████████████████████▊         |  ETA: 0:00:00
0.01886356105938128Training  79%|█████████████████████████████████▏        |  ETA: 0:00:00
0.014695411324849458Training  80%|█████████████████████████████████▋        |  ETA: 0:00:00
0.011376931660604715Training  81%|██████████████████████████████████        |  ETA: 0:00:00
0.008790579742300096Training  82%|██████████████████████████████████▌       |  ETA: 0:00:00
0.006825018715845233Training  83%|██████████████████████████████████▉       |  ETA: 0:00:00
0.005376751128722746Training  84%|███████████████████████████████████▎      |  ETA: 0:00:00
0.004351337454895064Training  85%|███████████████████████████████████▊      |  ETA: 0:00:00
0.003664208822086087Training  86%|████████████████████████████████████▏     |  ETA: 0:00:00
0.003241069752328777Training  87%|████████████████████████████████████▌     |  ETA: 0:00:00
0.003017912720425533Training  88%|█████████████████████████████████████     |  ETA: 0:00:00
0.0029406926329274452Training  89%|█████████████████████████████████████▍    |  ETA: 0:00:00
0.00296471879409625Training  90%|█████████████████████████████████████▊    |  ETA: 0:00:00
0.003053832890518582Training  91%|██████████████████████████████████████▎   |  ETA: 0:00:00
0.0031794457173844565Training  92%|██████████████████████████████████████▋   |  ETA: 0:00:00
0.0033195039850686Training  93%|███████████████████████████████████████   |  ETA: 0:00:00
0.0034574520530109222Training  94%|███████████████████████████████████████▌  |  ETA: 0:00:00
0.0035812431014248343Training  95%|███████████████████████████████████████▉  |  ETA: 0:00:00
0.0036824413833180096Training  96%|████████████████████████████████████████▍ |  ETA: 0:00:00
0.0037554431603706953Training  97%|████████████████████████████████████████▊ |  ETA: 0:00:00
0.003796829993387919Training  98%|█████████████████████████████████████████▏|  ETA: 0:00:00
0.00380485549165532Training  99%|█████████████████████████████████████████▋|  ETA: 0:00:00
0.003779056159218577Training 100%|██████████████████████████████████████████| Time: 0:00:00
Training 100%|██████████████████████████████████████████| Time: 0:00:00
74.233330581471791.56135771993787721.51943291733128061.51473100623542180.68242629663728870.191372695910355050.00019358396316548111.7955751251456144e-61.0477433167206356e-107.924036678678786e-177.924038666328425e-17Training   0%|                                          |  ETA: N/A
494.2089032199091Training   1%|▍                                         |  ETA: 0:00:32
416.7086995514831Training   2%|▉                                         |  ETA: 0:00:18
301.87186997430365Training   3%|█▎                                        |  ETA: 0:00:14
213.22801705361297Training   4%|█▋                                        |  ETA: 0:00:11
149.40648614478843Training   5%|██▏                                       |  ETA: 0:00:10
115.22404122493646Training   6%|██▌                                       |  ETA: 0:00:09
96.34543913455384Training   7%|███                                       |  ETA: 0:00:08
79.39779767791259Training   8%|███▍                                      |  ETA: 0:00:08
65.56787147139745Training   9%|███▊                                      |  ETA: 0:00:08
54.739696306072744Training  10%|████▎                                     |  ETA: 0:00:07
46.26814316479258Training  11%|████▋                                     |  ETA: 0:00:07
39.560838360645405Training  12%|█████                                     |  ETA: 0:00:06
34.298022959616745Training  13%|█████▌                                    |  ETA: 0:00:06
30.54633410011867Training  14%|█████▉                                    |  ETA: 0:00:06
28.933238146337313Training  15%|██████▎                                   |  ETA: 0:00:05
30.78208250143967Training  16%|██████▊                                   |  ETA: 0:00:05
37.55875666101628Training  17%|███████▏                                  |  ETA: 0:00:05
47.38704146347047Training  18%|███████▌                                  |  ETA: 0:00:05
52.318490651270146Training  19%|████████                                  |  ETA: 0:00:05
48.85895443234298Training  20%|████████▍                                 |  ETA: 0:00:04
41.143832559196Training  21%|████████▉                                 |  ETA: 0:00:04
33.50121408603177Training  22%|█████████▎                                |  ETA: 0:00:04
27.713293428947466Training  23%|█████████▋                                |  ETA: 0:00:04
23.819529369428558Training  24%|██████████▏                               |  ETA: 0:00:04
21.254376718766984Training  25%|██████████▌                               |  ETA: 0:00:04
19.42948561837119Training  26%|██████████▉                               |  ETA: 0:00:04
17.90303100693668Training  27%|███████████▍                              |  ETA: 0:00:04
16.396085008496147Training  28%|███████████▊                              |  ETA: 0:00:04
14.752308970793283Training  29%|████████████▏                             |  ETA: 0:00:04
12.909455845343116Training  30%|████████████▋                             |  ETA: 0:00:04
10.875462557797128Training  31%|█████████████                             |  ETA: 0:00:04
8.71288252545796Training  32%|█████████████▌                            |  ETA: 0:00:03
6.527068602400136Training  33%|█████████████▉                            |  ETA: 0:00:03
4.4534149599774855Training  34%|██████████████▎                           |  ETA: 0:00:03
2.6412962392444665Training  35%|██████████████▊                           |  ETA: 0:00:03
1.2342795663056394Training  36%|███████████████▏                          |  ETA: 0:00:03
0.34591580466382804Training  37%|███████████████▌                          |  ETA: 0:00:03
0.03250674863409575Training  38%|████████████████                          |  ETA: 0:00:03
0.2643104141460533Training  39%|████████████████▍                         |  ETA: 0:00:03
0.9063978746896175Training  40%|████████████████▊                         |  ETA: 0:00:03
1.7305156642179889Training  41%|█████████████████▎                        |  ETA: 0:00:03
2.474340776228947Training  42%|█████████████████▋                        |  ETA: 0:00:03
2.933823954403367Training  43%|██████████████████                        |  ETA: 0:00:03
3.036328674003168Training  44%|██████████████████▌                       |  ETA: 0:00:03
2.845064882252751Training  45%|██████████████████▉                       |  ETA: 0:00:03
2.498722388181745Training  46%|███████████████████▍                      |  ETA: 0:00:03
2.1349443027436377Training  47%|███████████████████▊                      |  ETA: 0:00:03
1.8416920826943293Training  48%|████████████████████▏                     |  ETA: 0:00:03
1.647195165983145Training  49%|████████████████████▋                     |  ETA: 0:00:03
1.5351519818678083Training  50%|█████████████████████                     |  ETA: 0:00:02
1.467363435420574Training  51%|█████████████████████▍                    |  ETA: 0:00:02
1.402808855045693Training  52%|█████████████████████▉                    |  ETA: 0:00:02
1.3097782742253306Training  53%|██████████████████████▎                   |  ETA: 0:00:02
1.171505959067753Training  54%|██████████████████████▋                   |  ETA: 0:00:02
0.9870664165854897Training  55%|███████████████████████▏                  |  ETA: 0:00:02
0.7692176252015813Training  56%|███████████████████████▌                  |  ETA: 0:00:02
0.5403333396407428Training  57%|████████████████████████                  |  ETA: 0:00:02
0.3272660324350968Training  58%|████████████████████████▍                 |  ETA: 0:00:02
0.1556853475382298Training  59%|████████████████████████▊                 |  ETA: 0:00:02
0.04440205322720733Training  60%|█████████████████████████▎                |  ETA: 0:00:02
0.0008926535872599556Training  61%|█████████████████████████▋                |  ETA: 0:00:02
0.019103924236998377Training  62%|██████████████████████████                |  ETA: 0:00:02
0.08053476982492919Training  63%|██████████████████████████▌               |  ETA: 0:00:02
0.15930730371726676Training  64%|██████████████████████████▉               |  ETA: 0:00:02
0.22964160052502988Training  65%|███████████████████████████▎              |  ETA: 0:00:02
0.2732248514361978Training  66%|███████████████████████████▊              |  ETA: 0:00:02
0.2835529968880482Training  67%|████████████████████████████▏             |  ETA: 0:00:02
0.2656120696804645Training  68%|████████████████████████████▌             |  ETA: 0:00:02
0.23158584804701468Training  69%|█████████████████████████████             |  ETA: 0:00:01
0.19498561339815892Training  70%|█████████████████████████████▍            |  ETA: 0:00:01
0.1658067074871198Training  71%|█████████████████████████████▉            |  ETA: 0:00:01
0.14821387238102476Training  72%|██████████████████████████████▎           |  ETA: 0:00:01
0.14082609760141152Training  73%|██████████████████████████████▋           |  ETA: 0:00:01
0.13871147379454676Training  74%|███████████████████████████████▏          |  ETA: 0:00:01
0.13594305715511545Training  75%|███████████████████████████████▌          |  ETA: 0:00:01
0.1278012794673704Training  76%|███████████████████████████████▉          |  ETA: 0:00:01
0.11211646675910676Training  77%|████████████████████████████████▍         |  ETA: 0:00:01
0.0896168891447801Training  78%|████████████████████████████████▊         |  ETA: 0:00:01
0.06339912950517693Training  79%|█████████████████████████████████▏        |  ETA: 0:00:01
0.03781445974325816Training  80%|█████████████████████████████████▋        |  ETA: 0:00:01
0.017120978004210557Training  81%|██████████████████████████████████        |  ETA: 0:00:01
0.004265227882238975Training  82%|██████████████████████████████████▌       |  ETA: 0:00:01
0.00013060182132885353Training  83%|██████████████████████████████████▉       |  ETA: 0:00:01
0.0034458547985172228Training  84%|███████████████████████████████████▎      |  ETA: 0:00:01
0.01130669877705187Training  85%|███████████████████████████████████▊      |  ETA: 0:00:01
0.020224812396146763Training  86%|████████████████████████████████████▏     |  ETA: 0:00:01
0.0272115591365223Training  87%|████████████████████████████████████▌     |  ETA: 0:00:01
0.030573093062484166Training  88%|█████████████████████████████████████     |  ETA: 0:00:01
0.03015844094040534Training  89%|█████████████████████████████████████▍    |  ETA: 0:00:01
0.027049241017987252Training  90%|█████████████████████████████████████▊    |  ETA: 0:00:00
0.02289578222443347Training  91%|██████████████████████████████████████▎   |  ETA: 0:00:00
0.019205145472165897Training  92%|██████████████████████████████████████▋   |  ETA: 0:00:00
0.01684511234890388Training  93%|███████████████████████████████████████   |  ETA: 0:00:00
0.01588947837509843Training  94%|███████████████████████████████████████▌  |  ETA: 0:00:00
0.015782221974403335Training  95%|███████████████████████████████████████▉  |  ETA: 0:00:00
0.01569031565022717Training  96%|████████████████████████████████████████▍ |  ETA: 0:00:00
0.014882270878558595Training  97%|████████████████████████████████████████▊ |  ETA: 0:00:00
0.01299856810201471Training  98%|█████████████████████████████████████████▏|  ETA: 0:00:00
0.010146481917053394Training  99%|█████████████████████████████████████████▋|  ETA: 0:00:00
0.0068175358724189405Training 100%|██████████████████████████████████████████| Time: 0:00:04
Training 100%|██████████████████████████████████████████| Time: 0:00:04
494.2089032199091473.8759825703607567.1074818633421318.3855315564830028.9648899689183876.117368644930780.16955330167694750.00127452976279778434.4571905150957083e-72.8044413366351072e-82.7293451721924503e-82.729347900600587e-8494.2089032199091473.8237904460742767.225842645127621.36751439899320811.4350691360731788.0109706995672490.32915359623862510.0036303407653235878.934029714091469e-56.246060306129264e-50.00031047251577495510.0058696171661135780.000109832425486265727.733918863503838e-64.2537266437804656e-78.726103760581099e-74.2537266437804656e-72.72458808565546e-72.766701471525958e-72.72458808565546e-72.6483736867625894e-72.648412606822959e-72.6483736867625894e-72.5881397656192965e-72.588142311285918e-72.5881397656192965e-72.508740247077843e-72.508742514801445e-72.508740247077843e-72.508742104639958e-7┌ Warning: dt <= dtmin. Aborting. There is either an error in your model specification or the true solution is unstable.
└ @ DiffEqBase ~/.julia/packages/DiffEqBase/ytZvl/src/integrator_interface.jl:343
┌ Warning: Instability detected. Aborting
└ @ DiffEqBase ~/.julia/packages/DiffEqBase/ytZvl/src/integrator_interface.jl:349
┌ Warning: Interrupted. Larger maxiters is needed.
└ @ DiffEqBase ~/.julia/packages/DiffEqBase/ytZvl/src/integrator_interface.jl:329
┌ Warning: Instability detected. Aborting
└ @ DiffEqBase ~/.julia/packages/DiffEqBase/ytZvl/src/integrator_interface.jl:349
┌ Warning: Interrupted. Larger maxiters is needed.
└ @ DiffEqBase ~/.julia/packages/DiffEqBase/ytZvl/src/integrator_interface.jl:329
┌ Warning: Instability detected. Aborting
└ @ DiffEqBase ~/.julia/packages/DiffEqBase/ytZvl/src/integrator_interface.jl:349
┌ Warning: Interrupted. Larger maxiters is needed.
└ @ DiffEqBase ~/.julia/packages/DiffEqBase/ytZvl/src/integrator_interface.jl:329
┌ Warning: dt <= dtmin. Aborting. There is either an error in your model specification or the true solution is unstable.
└ @ DiffEqBase ~/.julia/packages/DiffEqBase/ytZvl/src/integrator_interface.jl:343
Test Summary:      | Pass  Total
Layers SciML Tests |   15     15
Test Summary: | Pass  Total
Layers SDE    |    3      3
Test Summary: | Pass  Total
Layers DDE    |    3      3
4921.8252741349712317.17607567990261208.7459484255783668.1116929665505373.591041917807200.3314261180958104.4529358283813157.3138215258823534.32119428773293623.00067562422833218.12036039950615417.12597192218196818.38676708518351820.73091139694933523.31418352305031825.83594425345194528.2714424609499930.54643652796871632.53970044402460434.18718539872835.47480087834690536.3845017897876336.97114129712160537.25134235223478637.2627632524933437.0375236734635336.6005797078884635.9731756137308235.174505664654734.2262258270057533.1564351256670531.98065986407443330.7158346130359329.3810401288643227.9970894386508726.5988538685920725.17795742013946723.76978885961625222.38315600243469621.05025855873586219.78072978313227318.58395487536728317.46929326363028416.4490617863162415.51075932428809914.65835904580749213.8865830261551313.18807642074406212.55328355145845611.97154036216499411.43216286935378410.92508995804154510.4412884568655559.973053407198959.5141930211534569.060086309073828.6076180969602828.1550545807119897.7018622570740217.2485936813957846.79678428158544456.3487040397705015.9069742122563875.4744959273566785.0552931819356054.65230431179507154.2671100272408283.9016429007326623.55721847718319633.23468157075657642.9345580479832892.6568455035211892.401832105399052.1693524505581841.95806300172912361.76607235753308721.59174649731689581.43381002324094561.29095139258983771.16175831254542181.04481083303797880.93892941264289680.84284247911402380.75528214936003710.67519208736800940.60173194629611610.53426967609171950.47233112982576810.41555162110782830.363554278364917160.31609840944450740.272919696136243450.23376622741716190.198448158162193330.166820206931371670.138755340611349980.114099500151986740.092704690960577580.07435278640620720.05878273363362060.045756485807158014Test Summary:                  | Pass  Total
Size Handling in Adjoint Tests |    1      1
Test Summary: |
odenet        | No tests
Loss: 10.983616846195044
Loss: 0.1104974563629995
Loss: 1.8404894083080738
Loss: 3.8463365944590904
Loss: 5.085074559547107
Loss: 5.641142106796103
Loss: 5.657265508613336
Loss: 5.234644450665927
Loss: 4.446698715966959
Loss: 3.369216589960995
Loss: 2.122409468002599
Loss: 0.9303970483974908
Loss: 0.17828783662470876
Loss: 0.3348187730337835
Loss: 1.4199709981556328
Loss: 2.3084622740357568
Loss: 1.9225116793245183
Loss: 0.8788739566116349
Loss: 0.20576315524552055
Loss: 0.14965705804487123
Loss: 0.45156571716137583
Loss: 0.8141045950082181
Loss: 1.0574652438934171
Loss: 1.1101261559054576
Loss: 0.9735019858716638
Loss: 0.7007391827358027
Loss: 0.3875735171567738
Loss: 0.15863770637370475
Loss: 0.12402466809746608
Loss: 0.29357773292516004
Loss: 0.5139097404074865
Loss: 0.5722535376204974
Loss: 0.42277472152419765
Loss: 0.21838903124545583
Loss: 0.11434648102859839
Loss: 0.14037143771359598
Loss: 0.2335133806408928
Loss: 0.3155990300566306
Loss: 0.3382593314733722
Loss: 0.29407066689104355
Loss: 0.2106328543551854
Loss: 0.13530093264482687
Loss: 0.11062202820035127
Loss: 0.14521949875670245
Loss: 0.2014561923338484
Loss: 0.22450780421562497
Loss: 0.1947650192995242
Loss: 0.1430200070355609
Loss: 0.11214077871563172
Loss: 0.11748694271695315
Loss: 0.14370868997647035
Loss: 0.16533092425048412
Loss: 0.16622002547642442
Loss: 0.14730522509591676
Loss: 0.12304764361202801
Loss: 0.11043284115241643
Loss: 0.11630005508955416
Loss: 0.131812197931884
Loss: 0.14061376994470592
Loss: 0.13455774358698627
Loss: 0.1204185711301915
Loss: 0.11094381553158168
Loss: 0.11214788800083696
Loss: 0.11989525089318667
Loss: 0.12580634518039727
Loss: 0.12476430254009965
Loss: 0.11816197469074544
Loss: 0.11175051191739421
Loss: 0.110421521516606
Loss: 0.11407436205165165
Loss: 0.11813481161618704
Loss: 0.11831943545870763
Loss: 0.11472742916688676
Loss: 0.11103842741563942
Loss: 0.11033346565056318
Loss: 0.11236839074283374
Loss: 0.11451842795611303
Loss: 0.11459732238304755
Loss: 0.11268558488233227
Loss: 0.11066940029228554
Loss: 0.11026232222043054
Loss: 0.1114222689563192
Loss: 0.11261203050040829
Loss: 0.11251845047961795
Loss: 0.1113346958150986
Loss: 0.11031229645768446
Loss: 0.11031818568371517
Loss: 0.11105742717952626
Loss: 0.11158949677076593
Loss: 0.11135777920752406
Loss: 0.11065552375611272
Loss: 0.11019695696101481
Loss: 0.11034317840295005
Loss: 0.11078870504156532
Loss: 0.11097465521286669
Loss: 0.11071059423108076
Loss: 0.11031281960903101
Loss: 0.11018299740528537
Loss: 0.11037322441788652
Loss: 0.11059764750055308
Loss: 0.11059052647501023
Loss: 0.11059052647501023
Loss: 10.983616846195027
Loss: 0.1104974563629996
Loss: 1.8404894101499076
Loss: 3.8463366111318638
Loss: 5.085074683032128
Loss: 5.641142392988326
Loss: 5.657265994722879
Loss: 5.234645165711661
Loss: 4.44669967657572
Loss: 3.369217777686793
Loss: 2.122410773309322
Loss: 0.930398162889526
Loss: 0.1782882371953363
Loss: 0.3348179835732101
Loss: 1.4199695218490973
Loss: 2.308461719046682
Loss: 1.9225124101517403
Loss: 0.8788748258897118
Loss: 0.2057634638002814
Loss: 0.14965689575818386
Loss: 0.4515653798097273
Loss: 0.8141043102772356
Loss: 1.0574651264313493
Loss: 1.1101262350302978
Loss: 0.9735022325215642
Loss: 0.7007395216686174
Loss: 0.3875738328183265
Loss: 0.15863786395676555
Loss: 0.12402458096501147
Loss: 0.29357747202374096
Loss: 0.5139095443452492
Loss: 0.5722535751267336
Loss: 0.42277490986689714
Loss: 0.21838918780268343
Loss: 0.11434651234394706
Loss: 0.14037136765644276
Loss: 0.23351328577039732
Loss: 0.3155989779734896
Loss: 0.3382593527970312
Loss: 0.29407075107035
Loss: 0.21063295795185452
Loss: 0.13530099873610155
Loss: 0.11062201892709149
Loss: 0.14521942909833452
Loss: 0.20145612839154722
Loss: 0.22450780449245944
Loss: 0.19476507684491884
Loss: 0.14302006572671558
Loss: 0.11214079458361864
Loss: 0.11748691613788696
Loss: 0.14370865212726822
Loss: 0.16533090730085773
Loss: 0.16622004184209532
Loss: 0.14730526284456433
Loss: 0.12304767616272509
Loss: 0.11043284628150946
Loss: 0.11630003277679493
Loss: 0.13181217139691365
Loss: 0.14061376412141816
Loss: 0.13455776115709642
Loss: 0.12041859296849178
Loss: 0.11094382254684179
Loss: 0.11214787800404101
Loss: 0.11989523648115823
Loss: 0.1258063403250738
Loss: 0.12476431134717451
Loss: 0.11816198953856559
Loss: 0.11175052067251759
Loss: 0.11042151803914305
Loss: 0.11407435141016543
Loss: 0.11813480474583568
Loss: 0.11831943816399822
Loss: 0.114727437193363
Loss: 0.11103843229708026
Loss: 0.11033346361146702
Loss: 0.11236838527931968
Loss: 0.11451842522224774
Loss: 0.11459732515824389
Loss: 0.11268559051462061
Loss: 0.11066940369439988
Loss: 0.11026232080269274
Loss: 0.11142226486317693
Loss: 0.11261202804961798
Loss: 0.11251845163614692
Loss: 0.11133469862348247
Loss: 0.11031229773612306
Loss: 0.11031818447439858
Loss: 0.1110574252970685
Loss: 0.11158949649875792
Loss: 0.11135778092875448
Loss: 0.11065552578601032
Loss: 0.11019695743615127
Loss: 0.11034317715558446
Loss: 0.11078870356940063
Loss: 0.1109746549370719
Loss: 0.1107105950702043
Loss: 0.1103128203632835
Loss: 0.11018299725515929
Loss: 0.1103732237351035
Loss: 0.11059764723864729
Loss: 0.11059052703631903
Loss: 0.11059052703631903
Test Summary:        | Pass  Total
GDP Regression Tests |    2      2
┌ Warning: Assignment to `grads` in soft scope is ambiguous because a global variable by the same name exists: `grads` will be treated as a new local. Disambiguate by using `local grads` to suppress this warning or `global grads` to assign to the existing global variable.
└ @ ~/.julia/packages/DiffEqFlux/IASg5/test/neural_de.jl:71
┌ Warning: Assignment to `grads` in soft scope is ambiguous because a global variable by the same name exists: `grads` will be treated as a new local. Disambiguate by using `local grads` to suppress this warning or `global grads` to assign to the existing global variable.
└ @ ~/.julia/packages/DiffEqFlux/IASg5/test/neural_de.jl:211
┌ Warning: Assignment to `grads` in soft scope is ambiguous because a global variable by the same name exists: `grads` will be treated as a new local. Disambiguate by using `local grads` to suppress this warning or `global grads` to assign to the existing global variable.
└ @ ~/.julia/packages/DiffEqFlux/IASg5/test/neural_de.jl:227
┌ Warning: dt <= dtmin. Aborting. There is either an error in your model specification or the true solution is unstable.
└ @ DiffEqBase ~/.julia/packages/DiffEqBase/ytZvl/src/integrator_interface.jl:343
Neural DE Tests: Test Failed at /home/pkgeval/.julia/packages/DiffEqFlux/IASg5/test/neural_de.jl:236
  Expression: !(iszero(grads[sode.p]))
Stacktrace:
 [1] top-level scope at /home/pkgeval/.julia/packages/DiffEqFlux/IASg5/test/neural_de.jl:236
 [2] include(::Function, ::Module, ::String) at ./Base.jl:380
 [3] include at ./Base.jl:368 [inlined]
 [4] include(::String) at /home/pkgeval/.julia/packages/SafeTestsets/A83XK/src/SafeTestsets.jl:23
 [5] top-level scope at /home/pkgeval/.julia/packages/DiffEqFlux/IASg5/test/runtests.jl:17
 [6] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [7] top-level scope at /home/pkgeval/.julia/packages/DiffEqFlux/IASg5/test/runtests.jl:17
Neural DE Tests: Test Failed at /home/pkgeval/.julia/packages/DiffEqFlux/IASg5/test/neural_de.jl:237
  Expression: !(iszero((grads[sode.p])[end]))
Stacktrace:
 [1] top-level scope at /home/pkgeval/.julia/packages/DiffEqFlux/IASg5/test/neural_de.jl:237
 [2] include(::Function, ::Module, ::String) at ./Base.jl:380
 [3] include at ./Base.jl:368 [inlined]
 [4] include(::String) at /home/pkgeval/.julia/packages/SafeTestsets/A83XK/src/SafeTestsets.jl:23
 [5] top-level scope at /home/pkgeval/.julia/packages/DiffEqFlux/IASg5/test/runtests.jl:17
 [6] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [7] top-level scope at /home/pkgeval/.julia/packages/DiffEqFlux/IASg5/test/runtests.jl:17
┌ Warning: Assignment to `grads` in soft scope is ambiguous because a global variable by the same name exists: `grads` will be treated as a new local. Disambiguate by using `local grads` to suppress this warning or `global grads` to assign to the existing global variable.
└ @ ~/.julia/packages/DiffEqFlux/IASg5/test/neural_de.jl:239
┌ Warning: Assignment to `grads` in soft scope is ambiguous because a global variable by the same name exists: `grads` will be treated as a new local. Disambiguate by using `local grads` to suppress this warning or `global grads` to assign to the existing global variable.
└ @ ~/.julia/packages/DiffEqFlux/IASg5/test/neural_de.jl:252
┌ Warning: Assignment to `grads` in soft scope is ambiguous because a global variable by the same name exists: `grads` will be treated as a new local. Disambiguate by using `local grads` to suppress this warning or `global grads` to assign to the existing global variable.
└ @ ~/.julia/packages/DiffEqFlux/IASg5/test/neural_de.jl:264
Test Summary:   | Pass  Fail  Broken  Total
Neural DE Tests |   75     2      12     89
  adjoint mode  |   18             3     21
  Tracker mode  |   18             3     21
ERROR: LoadError: Some tests did not pass: 75 passed, 2 failed, 0 errored, 12 broken.
in expression starting at /home/pkgeval/.julia/packages/DiffEqFlux/IASg5/test/runtests.jl:7
ERROR: Package DiffEqFlux errored during testing
Stacktrace:
 [1] pkgerror(::String, ::Vararg{String,N} where N) at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/Types.jl:53
 [2] test(::Pkg.Types.Context, ::Array{Pkg.Types.PackageSpec,1}; coverage::Bool, julia_args::Cmd, test_args::Cmd, test_fn::Nothing) at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/Operations.jl:1523
 [3] test(::Pkg.Types.Context, ::Array{Pkg.Types.PackageSpec,1}; coverage::Bool, test_fn::Nothing, julia_args::Cmd, test_args::Cmd, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:316
 [4] test(::Pkg.Types.Context, ::Array{Pkg.Types.PackageSpec,1}) at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:303
 [5] #test#68 at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:297 [inlined]
 [6] test at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:297 [inlined]
 [7] #test#67 at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:296 [inlined]
 [8] test at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:296 [inlined]
 [9] test(::String; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:295
 [10] test(::String) at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:295
 [11] top-level scope at none:16
