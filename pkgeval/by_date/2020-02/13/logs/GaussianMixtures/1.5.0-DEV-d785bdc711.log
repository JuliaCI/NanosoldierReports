Julia Version 1.5.0-DEV.263
Commit d785bdc711 (2020-02-12 15:14 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-9.0.1 (ORCJIT, skylake)
Environment:
  JULIA_DEPOT_PATH = ::/usr/local/share/julia
  JULIA_NUM_THREADS = 2

  Resolving package versions...
  Installed OpenSpecFun_jll ──── v0.5.3+1
  Installed GaussianMixtures ─── v0.3.0
  Installed URIParser ────────── v0.4.0
  Installed OpenBLAS_jll ─────── v0.3.7+5
  Installed Rmath ────────────── v0.6.0
  Installed Arpack ───────────── v0.4.0
  Installed FileIO ───────────── v1.2.2
  Installed Distributions ────── v0.22.4
  Installed DataAPI ──────────── v1.1.0
  Installed SortingAlgorithms ── v0.3.1
  Installed LegacyStrings ────── v0.4.1
  Installed StatsFuns ────────── v0.9.3
  Installed SpecialFunctions ─── v0.9.0
  Installed DataStructures ───── v0.17.9
  Installed NearestNeighbors ─── v0.4.4
  Installed BinaryProvider ───── v0.5.8
  Installed Parameters ───────── v0.12.0
  Installed StaticArrays ─────── v0.12.1
  Installed PDMats ───────────── v0.9.11
  Installed HDF5 ─────────────── v0.12.5
  Installed Missings ─────────── v0.4.3
  Installed FillArrays ───────── v0.8.4
  Installed Distances ────────── v0.8.2
  Installed QuadGK ───────────── v2.3.1
  Installed BinDeps ──────────── v1.0.0
  Installed StatsBase ────────── v0.32.0
  Installed CMake ────────────── v1.2.0
  Installed ScikitLearnBase ──── v0.5.0
  Installed Arpack_jll ───────── v3.5.0+2
  Installed Compat ───────────── v2.2.0
  Installed CMakeWrapper ─────── v0.2.3
  Installed Blosc ────────────── v0.5.1
  Installed OrderedCollections ─ v1.1.0
  Installed JLD ──────────────── v0.9.2
  Installed Clustering ───────── v0.13.3
#####                                                                      7.0%######################################################################## 100.0%
                                                                           0.1%############                                                              17.4%##############################################                            64.0%######################################################################## 100.0%
#=#=#                                                                         ######################################################################## 100.0%
   Updating `~/.julia/environments/v1.5/Project.toml`
  [cc18c42c] + GaussianMixtures v0.3.0
   Updating `~/.julia/environments/v1.5/Manifest.toml`
  [7d9fca2a] + Arpack v0.4.0
  [68821587] + Arpack_jll v3.5.0+2
  [9e28174c] + BinDeps v1.0.0
  [b99e7846] + BinaryProvider v0.5.8
  [a74b3585] + Blosc v0.5.1
  [631607c0] + CMake v1.2.0
  [d5fb7624] + CMakeWrapper v0.2.3
  [aaaa29a8] + Clustering v0.13.3
  [34da2185] + Compat v2.2.0
  [9a962f9c] + DataAPI v1.1.0
  [864edb3b] + DataStructures v0.17.9
  [b4f34e82] + Distances v0.8.2
  [31c24e10] + Distributions v0.22.4
  [5789e2e9] + FileIO v1.2.2
  [1a297f60] + FillArrays v0.8.4
  [cc18c42c] + GaussianMixtures v0.3.0
  [f67ccb44] + HDF5 v0.12.5
  [4138dd39] + JLD v0.9.2
  [1b4a561d] + LegacyStrings v0.4.1
  [e1d29d7a] + Missings v0.4.3
  [b8a86587] + NearestNeighbors v0.4.4
  [4536629a] + OpenBLAS_jll v0.3.7+5
  [efe28fd5] + OpenSpecFun_jll v0.5.3+1
  [bac558e1] + OrderedCollections v1.1.0
  [90014a1f] + PDMats v0.9.11
  [d96e819e] + Parameters v0.12.0
  [1fd47b50] + QuadGK v2.3.1
  [79098fc4] + Rmath v0.6.0
  [6e75b9c4] + ScikitLearnBase v0.5.0
  [a2af1166] + SortingAlgorithms v0.3.1
  [276daf66] + SpecialFunctions v0.9.0
  [90137ffa] + StaticArrays v0.12.1
  [2913bbd2] + StatsBase v0.32.0
  [4c63d2b9] + StatsFuns v0.9.3
  [30578b45] + URIParser v0.4.0
  [2a0f44e3] + Base64 
  [ade2ca70] + Dates 
  [8bb1440f] + DelimitedFiles 
  [8ba89e20] + Distributed 
  [b77e0a4c] + InteractiveUtils 
  [76f85450] + LibGit2 
  [8f399da3] + Libdl 
  [37e2e46d] + LinearAlgebra 
  [56ddb016] + Logging 
  [d6f4376e] + Markdown 
  [a63ad114] + Mmap 
  [44cfe95a] + Pkg 
  [de0858da] + Printf 
  [3fa0cd96] + REPL 
  [9a3f8284] + Random 
  [ea8e919c] + SHA 
  [9e88b42a] + Serialization 
  [1a1011a3] + SharedArrays 
  [6462fe0b] + Sockets 
  [2f01184e] + SparseArrays 
  [10745b16] + Statistics 
  [4607b0f0] + SuiteSparse 
  [8dfed614] + Test 
  [cf7118a7] + UUIDs 
  [4ec0a83e] + Unicode 
   Building Rmath → `~/.julia/packages/Rmath/BoBag/deps/build.log`
   Building CMake → `~/.julia/packages/CMake/ULbyn/deps/build.log`
   Building Blosc → `~/.julia/packages/Blosc/lzFr0/deps/build.log`
   Building HDF5 ─→ `~/.julia/packages/HDF5/Zh9on/deps/build.log`
    Testing GaussianMixtures
Status `/tmp/jl_GzT4Y1/Manifest.toml`
  [7d9fca2a] Arpack v0.4.0
  [68821587] Arpack_jll v3.5.0+2
  [9e28174c] BinDeps v1.0.0
  [b99e7846] BinaryProvider v0.5.8
  [a74b3585] Blosc v0.5.1
  [631607c0] CMake v1.2.0
  [d5fb7624] CMakeWrapper v0.2.3
  [aaaa29a8] Clustering v0.13.3
  [34da2185] Compat v2.2.0
  [9a962f9c] DataAPI v1.1.0
  [864edb3b] DataStructures v0.17.9
  [b4f34e82] Distances v0.8.2
  [31c24e10] Distributions v0.22.4
  [5789e2e9] FileIO v1.2.2
  [1a297f60] FillArrays v0.8.4
  [cc18c42c] GaussianMixtures v0.3.0
  [f67ccb44] HDF5 v0.12.5
  [4138dd39] JLD v0.9.2
  [1b4a561d] LegacyStrings v0.4.1
  [e1d29d7a] Missings v0.4.3
  [b8a86587] NearestNeighbors v0.4.4
  [4536629a] OpenBLAS_jll v0.3.7+5
  [efe28fd5] OpenSpecFun_jll v0.5.3+1
  [bac558e1] OrderedCollections v1.1.0
  [90014a1f] PDMats v0.9.11
  [d96e819e] Parameters v0.12.0
  [1fd47b50] QuadGK v2.3.1
  [79098fc4] Rmath v0.6.0
  [6e75b9c4] ScikitLearnBase v0.5.0
  [a2af1166] SortingAlgorithms v0.3.1
  [276daf66] SpecialFunctions v0.9.0
  [90137ffa] StaticArrays v0.12.1
  [2913bbd2] StatsBase v0.32.0
  [4c63d2b9] StatsFuns v0.9.3
  [30578b45] URIParser v0.4.0
  [2a0f44e3] Base64 
  [ade2ca70] Dates 
  [8bb1440f] DelimitedFiles 
  [8ba89e20] Distributed 
  [b77e0a4c] InteractiveUtils 
  [76f85450] LibGit2 
  [8f399da3] Libdl 
  [37e2e46d] LinearAlgebra 
  [56ddb016] Logging 
  [d6f4376e] Markdown 
  [a63ad114] Mmap 
  [44cfe95a] Pkg 
  [de0858da] Printf 
  [3fa0cd96] REPL 
  [9a3f8284] Random 
  [ea8e919c] SHA 
  [9e88b42a] Serialization 
  [1a1011a3] SharedArrays 
  [6462fe0b] Sockets 
  [2f01184e] SparseArrays 
  [10745b16] Statistics 
  [4607b0f0] SuiteSparse 
  [8dfed614] Test 
  [cf7118a7] UUIDs 
  [4ec0a83e] Unicode 
[ Info: Testing Data
(100000, -1.8509000533238035e6, [16026.978637634866, 83973.02136236515], [-3871.0754784654655 -4531.293813887367 7665.073852348515; 3491.276669254061 4212.414091001991 -7714.343658099318], [[17422.994048807213 4577.479316563347 2886.8436255747592; 4577.479316563347 2620.2820882475935 -1579.6216409053952; 2886.8436255747592 -1579.6216409053952 16861.383091151343], [83022.71017102532 -4637.635951972003 -2813.168561451331; -4637.635951972003 97141.28399514366 1957.7852614379062; -2813.1685614513312 1957.7852614379067 83442.66868672114]])
┌ Warning: rmprocs: process 1 not removed
└ @ Distributed /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Distributed/src/cluster.jl:1030
[ Info: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.233355e+03
      1       8.979737e+02      -3.353812e+02 |        3
      2       8.820247e+02      -1.594902e+01 |        3
      3       8.796951e+02      -2.329606e+00 |        0
      4       8.796951e+02       0.000000e+00 |        0
K-means converged with 4 iterations (objv = 879.6951129419804)
┌ Info: K-means with 272 data points using 4 iterations
└ 11.3 data points per parameter
[ Info: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
┌ Info: EM with 272 data points 0 iterations avll -2.076250
└ 5.8 data points per parameter
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:221
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:221
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:221
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:221
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:631 [inlined]
└ @ Core ./broadcast.jl:631
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:230
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:230
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:631 [inlined]
└ @ Core ./broadcast.jl:631
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:631 [inlined]
└ @ Core ./broadcast.jl:631
[ Info: iteration 1, lowerbound -3.826503
[ Info: iteration 2, lowerbound -3.697450
[ Info: iteration 3, lowerbound -3.544456
[ Info: iteration 4, lowerbound -3.356229
[ Info: iteration 5, lowerbound -3.159854
[ Info: iteration 6, lowerbound -2.991843
[ Info: dropping number of Gaussions to 7
[ Info: iteration 7, lowerbound -2.867724
[ Info: iteration 8, lowerbound -2.805947
[ Info: dropping number of Gaussions to 5
[ Info: iteration 9, lowerbound -2.784215
[ Info: dropping number of Gaussions to 3
[ Info: iteration 10, lowerbound -2.754860
[ Info: iteration 11, lowerbound -2.725769
[ Info: iteration 12, lowerbound -2.692613
[ Info: iteration 13, lowerbound -2.645263
[ Info: iteration 14, lowerbound -2.584092
[ Info: iteration 15, lowerbound -2.515548
[ Info: iteration 16, lowerbound -2.450654
[ Info: iteration 17, lowerbound -2.397692
[ Info: iteration 18, lowerbound -2.357754
[ Info: iteration 19, lowerbound -2.328789
[ Info: iteration 20, lowerbound -2.311371
[ Info: iteration 21, lowerbound -2.307779
[ Info: dropping number of Gaussions to 2
[ Info: iteration 22, lowerbound -2.302918
[ Info: iteration 23, lowerbound -2.299260
[ Info: iteration 24, lowerbound -2.299256
[ Info: iteration 25, lowerbound -2.299254
[ Info: iteration 26, lowerbound -2.299254
[ Info: iteration 27, lowerbound -2.299253
[ Info: iteration 28, lowerbound -2.299253
[ Info: iteration 29, lowerbound -2.299253
[ Info: iteration 30, lowerbound -2.299253
[ Info: iteration 31, lowerbound -2.299253
[ Info: iteration 32, lowerbound -2.299253
[ Info: iteration 33, lowerbound -2.299253
[ Info: iteration 34, lowerbound -2.299253
[ Info: iteration 35, lowerbound -2.299253
[ Info: iteration 36, lowerbound -2.299253
[ Info: iteration 37, lowerbound -2.299253
[ Info: iteration 38, lowerbound -2.299253
[ Info: iteration 39, lowerbound -2.299253
[ Info: iteration 40, lowerbound -2.299253
[ Info: iteration 41, lowerbound -2.299253
[ Info: iteration 42, lowerbound -2.299253
[ Info: iteration 43, lowerbound -2.299253
[ Info: iteration 44, lowerbound -2.299253
[ Info: iteration 45, lowerbound -2.299253
[ Info: iteration 46, lowerbound -2.299253
[ Info: iteration 47, lowerbound -2.299253
[ Info: iteration 48, lowerbound -2.299253
[ Info: iteration 49, lowerbound -2.299253
[ Info: iteration 50, lowerbound -2.299253
[ Info: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
History[Fri Feb 14 01:21:38 2020: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
, Fri Feb 14 01:21:46 2020: K-means with 272 data points using 4 iterations
11.3 data points per parameter
, Fri Feb 14 01:21:49 2020: EM with 272 data points 0 iterations avll -2.076250
5.8 data points per parameter
, Fri Feb 14 01:21:51 2020: GMM converted to Variational GMM
, Fri Feb 14 01:21:58 2020: iteration 1, lowerbound -3.826503
, Fri Feb 14 01:21:58 2020: iteration 2, lowerbound -3.697450
, Fri Feb 14 01:21:58 2020: iteration 3, lowerbound -3.544456
, Fri Feb 14 01:21:58 2020: iteration 4, lowerbound -3.356229
, Fri Feb 14 01:21:58 2020: iteration 5, lowerbound -3.159854
, Fri Feb 14 01:21:58 2020: iteration 6, lowerbound -2.991843
, Fri Feb 14 01:21:59 2020: dropping number of Gaussions to 7
, Fri Feb 14 01:21:59 2020: iteration 7, lowerbound -2.867724
, Fri Feb 14 01:21:59 2020: iteration 8, lowerbound -2.805947
, Fri Feb 14 01:21:59 2020: dropping number of Gaussions to 5
, Fri Feb 14 01:21:59 2020: iteration 9, lowerbound -2.784215
, Fri Feb 14 01:21:59 2020: dropping number of Gaussions to 3
, Fri Feb 14 01:21:59 2020: iteration 10, lowerbound -2.754860
, Fri Feb 14 01:21:59 2020: iteration 11, lowerbound -2.725769
, Fri Feb 14 01:21:59 2020: iteration 12, lowerbound -2.692613
, Fri Feb 14 01:21:59 2020: iteration 13, lowerbound -2.645263
, Fri Feb 14 01:21:59 2020: iteration 14, lowerbound -2.584092
, Fri Feb 14 01:21:59 2020: iteration 15, lowerbound -2.515548
, Fri Feb 14 01:21:59 2020: iteration 16, lowerbound -2.450654
, Fri Feb 14 01:21:59 2020: iteration 17, lowerbound -2.397692
, Fri Feb 14 01:21:59 2020: iteration 18, lowerbound -2.357754
, Fri Feb 14 01:21:59 2020: iteration 19, lowerbound -2.328789
, Fri Feb 14 01:21:59 2020: iteration 20, lowerbound -2.311371
, Fri Feb 14 01:21:59 2020: iteration 21, lowerbound -2.307779
, Fri Feb 14 01:21:59 2020: dropping number of Gaussions to 2
, Fri Feb 14 01:21:59 2020: iteration 22, lowerbound -2.302918
, Fri Feb 14 01:21:59 2020: iteration 23, lowerbound -2.299260
, Fri Feb 14 01:21:59 2020: iteration 24, lowerbound -2.299256
, Fri Feb 14 01:21:59 2020: iteration 25, lowerbound -2.299254
, Fri Feb 14 01:21:59 2020: iteration 26, lowerbound -2.299254
, Fri Feb 14 01:21:59 2020: iteration 27, lowerbound -2.299253
, Fri Feb 14 01:21:59 2020: iteration 28, lowerbound -2.299253
, Fri Feb 14 01:21:59 2020: iteration 29, lowerbound -2.299253
, Fri Feb 14 01:21:59 2020: iteration 30, lowerbound -2.299253
, Fri Feb 14 01:21:59 2020: iteration 31, lowerbound -2.299253
, Fri Feb 14 01:21:59 2020: iteration 32, lowerbound -2.299253
, Fri Feb 14 01:21:59 2020: iteration 33, lowerbound -2.299253
, Fri Feb 14 01:21:59 2020: iteration 34, lowerbound -2.299253
, Fri Feb 14 01:21:59 2020: iteration 35, lowerbound -2.299253
, Fri Feb 14 01:21:59 2020: iteration 36, lowerbound -2.299253
, Fri Feb 14 01:21:59 2020: iteration 37, lowerbound -2.299253
, Fri Feb 14 01:21:59 2020: iteration 38, lowerbound -2.299253
, Fri Feb 14 01:21:59 2020: iteration 39, lowerbound -2.299253
, Fri Feb 14 01:21:59 2020: iteration 40, lowerbound -2.299253
, Fri Feb 14 01:21:59 2020: iteration 41, lowerbound -2.299253
, Fri Feb 14 01:21:59 2020: iteration 42, lowerbound -2.299253
, Fri Feb 14 01:21:59 2020: iteration 43, lowerbound -2.299253
, Fri Feb 14 01:21:59 2020: iteration 44, lowerbound -2.299253
, Fri Feb 14 01:21:59 2020: iteration 45, lowerbound -2.299253
, Fri Feb 14 01:21:59 2020: iteration 46, lowerbound -2.299253
, Fri Feb 14 01:21:59 2020: iteration 47, lowerbound -2.299253
, Fri Feb 14 01:21:59 2020: iteration 48, lowerbound -2.299253
, Fri Feb 14 01:21:59 2020: iteration 49, lowerbound -2.299253
, Fri Feb 14 01:21:59 2020: iteration 50, lowerbound -2.299253
, Fri Feb 14 01:21:59 2020: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [178.0450922260163, 95.95490777398368]
β = [178.0450922260163, 95.95490777398368]
m = [4.250300733269891 79.28686694436156; 2.00022925777535 53.851987172461186]
ν = [180.0450922260163, 97.95490777398368]
W = LinearAlgebra.UpperTriangular{Float64,Array{Float64,2}}[[0.1840415554748412 -0.007644049042327388; 0.0 0.008581705166333187], [0.3758763611948767 -0.008953123827346409; 0.0 0.01274866477740948]]
┌ Warning: Assignment to `x` in soft scope is ambiguous because a global variable by the same name exists: `x` will be treated as a new local. Disambiguate by using `local x` to suppress this warning or `global x` to assign to the existing global variable.
└ @ ~/.julia/packages/GaussianMixtures/RGtTJ/test/train.jl:7
┌ Warning: Assignment to `p` in soft scope is ambiguous because a global variable by the same name exists: `p` will be treated as a new local. Disambiguate by using `local p` to suppress this warning or `global p` to assign to the existing global variable.
└ @ ~/.julia/packages/GaussianMixtures/RGtTJ/test/train.jl:17
Kind: diag, size256
nx: 100000 sum(zeroth order stats): 99999.99999999996
avll from stats: -1.0001718694738326
avll from llpg:  -1.000171869473823
avll direct:     -1.000171869473823
sum posterior: 99999.99999999999
Kind: full, size16
nx: 100000 sum(zeroth order stats): 100000.00000000001
avll from stats: -1.0021979275619217
avll from llpg:  -1.0021979275619217
avll direct:     -1.0021979275619217
sum posterior: 100000.0
┌ Warning: Assignment to `x` in soft scope is ambiguous because a global variable by the same name exists: `x` will be treated as a new local. Disambiguate by using `local x` to suppress this warning or `global x` to assign to the existing global variable.
└ @ ~/.julia/packages/GaussianMixtures/RGtTJ/test/train.jl:26
32×26 Array{Float64,2}:
  0.0126542   -0.108762    0.0120216   -0.108164    -0.0761551    0.0841643   0.0344916     0.0548558   -0.0102225     0.312888     0.00562983  -0.037051     0.0430036    -0.0554685    0.188272   -0.109042     0.0517629   -0.084452    -0.105865     0.0250208     0.055405     0.00860814   0.133477   -0.00554872  -0.0714659    0.0977823
 -0.0883062   -0.0255137  -0.229742     0.0818592    0.00507833  -0.215608   -0.0254412    -0.0343522   -0.0265883     0.0155509   -0.132121     0.0218016    0.0261971    -0.168203     0.0690579  -0.0297743    0.0501425   -0.127197     0.0887489   -0.102596      0.027616    -0.0447074   -0.210643   -0.155734    -0.156174    -0.0359618
 -0.0453377    0.1232      0.0204549    0.0542011   -0.116523     0.117534    0.032967      0.0262364    0.154619      0.0550156   -0.0993705   -0.190821    -0.0363378     0.157625    -0.0513772  -0.0284105   -0.125953     0.175206    -0.133594     0.0242669    -0.0026445   -0.0199597    0.0273471  -0.160082     0.0803329   -0.0607234
  0.00600361   0.0414638   0.060686    -0.130052    -0.121522    -0.0974324   0.0812737     0.27408      0.0112836     0.179429     0.0491593    0.193357    -0.0399641    -0.00627345   0.105437   -0.208605    -0.0242569   -0.108704    -0.0399892   -0.0221871    -0.0268505    0.167354     0.0525328   0.140477     0.0385143   -0.0739552
 -0.0659156   -0.0564091  -0.119489     0.136813     0.0228449    0.0881683  -0.0187727    -0.0721278   -0.0680194    -0.136179    -0.138443    -0.0315195   -0.223186     -0.191064     0.105119   -0.150882    -0.0073511   -0.0649194   -0.0110567    0.104242     -0.00522988   0.144765    -0.022737    0.0590364   -0.017668    -0.0643157
  0.0489184   -0.071377    0.138406    -0.170805     0.0625683   -0.0639073  -0.0401508    -0.04228     -0.131252      0.096391     0.0947266    0.0669897    0.15711       0.0622806    0.0298547   0.0449433    0.048665     0.0270599   -0.087337     0.162343      0.057531    -0.0711602    0.0500324   0.00975784  -0.14917     -0.112267
 -0.0633872    0.0617994   0.041669    -0.048224     0.0163282   -0.0705222  -0.0282341     0.0202278   -0.0570804    -0.0124541    0.186436     0.0325465   -0.0735197     0.00943725   0.0387786   0.0397599    0.062272    -0.163362    -0.0825621    0.0316045     0.100974     0.0161283   -0.156841   -0.0366037   -0.0407695   -0.0333731
 -0.0812054   -0.091831   -0.108549    -0.091457    -0.0709005    0.0453076   0.126865     -0.231986    -0.143216     -0.0350691   -0.0730161    0.0929448   -0.171809      0.113584    -0.0272324   0.043514     0.0197725   -0.0807248    0.181766    -0.124584      0.126656     0.0306714   -0.0847611   0.116914    -0.0384845    0.0191841
  0.1663      -0.0895057   0.00635425   0.0736818    0.0122874    0.150403   -0.0776749    -0.0159173   -0.116241      0.0896003    0.136517     0.0803525    0.0642719     0.00786849   0.2177      0.0114841    0.0811794   -0.113975     0.0197676    0.000983511  -0.0945945   -0.0487541   -0.0628874  -0.0809083   -0.122442    -0.033903
  0.16722      0.0804298  -0.0354384    0.0297166    0.0396979   -0.136751   -0.223232      0.0418863   -0.184773     -0.0386269   -0.0364108    0.0225711    0.0410084    -0.0634946   -0.0294672   0.0153797    0.137199     0.177711    -0.0410609   -0.00979706    0.160759     0.03722     -0.222639   -0.0210933   -0.124815    -0.0275567
  0.0316399   -0.0407957   0.0105958    0.113518     0.0685916    0.107271   -0.141205      0.138822    -0.171781     -0.0544482   -0.0701198   -0.17858     -0.0737286     0.234908    -0.15225    -0.0477699    0.00476156  -0.0124376   -0.0212741    0.165321     -0.0445331    0.00719987   0.0761347   0.032547    -0.0458037   -0.211168
  0.151384    -0.0612693   0.112804    -0.0241599    0.0255778   -0.230585   -0.0514266     0.00506857  -0.0244863    -0.0842994   -0.0277867   -0.0569346    0.0876058     0.0644282    0.110408    0.0494709    0.0185725   -0.107769    -0.0474758   -0.0203295    -0.134792    -0.0885093    0.0269392  -0.0904275    0.0898718    0.0140301
  0.0671366   -0.0797182   0.08634      0.140132    -0.0350021    0.049087   -0.190118     -0.0685344   -0.000472045  -0.0679285   -0.0161981    0.0265168   -0.182827     -0.0117647    0.110024    0.044482     0.154864    -0.0463875   -0.223717    -0.025561     -0.0690214    0.2418       0.241066   -0.00964411  -0.15671      0.0433319
  0.188577    -0.165044    0.088426    -0.00823574   0.1113      -0.0859294   0.178027     -0.0184053    0.035812     -0.00857478   0.236146     0.00671993   0.00366368    0.0583303    0.0898366   0.00638445  -0.130918    -0.0670676   -0.0199148   -0.278026      0.0805917    0.0273838   -0.156735    0.0765717    0.109954     0.171936
 -0.127778     0.0962696  -0.0397675    0.11366      0.0924801    0.0636945   0.226339     -0.0471854   -0.042959      0.161155     0.0401863    0.166907    -0.156718      0.0378086   -0.0641041   0.00579988  -0.00258767  -0.106624    -0.0761785    0.0054205     0.0678776   -0.0355345   -0.0549002  -0.245897     0.190991    -0.079603
  0.278553     0.151465    0.0734088    0.11609     -0.0272408    0.0587495   0.0392216     0.053665    -0.0396643     0.1081       0.0783089    0.162142     0.135055     -0.0627108   -0.119878    0.0536825   -0.00555297  -0.118181     0.0521059   -0.130882      0.016861    -0.087914     0.0509424   0.104729     0.075226     0.0609415
 -0.0535512    0.103405    0.0338741   -0.0339902    0.11409     -0.0433489  -0.0429709    -0.019274    -0.0399077    -0.105375    -0.0555751   -0.0372988   -0.0817116     0.027937    -0.0725991   0.0118938   -0.00124332   0.0127519    0.0277994    0.0678113    -0.0233435    0.152088    -0.1036      0.00106444  -0.0324       0.11702
 -0.0191982    0.0204163  -0.0194584    0.0658791    0.0538828    0.0320608  -0.00850166   -0.0795924    0.139891      0.0863998   -0.17719      0.0225764    0.0614362     0.0698099   -0.101281   -0.060997     0.0442371   -0.158556    -0.223638     0.0115012     0.0688496    0.00528325   0.0091272   0.0680397    0.239628     0.0395645
 -0.0146389    0.0298907   0.108645    -0.108869    -0.0366574   -0.0515474  -0.062823      0.166409     0.228236     -0.204593    -0.100791    -0.0303595    0.05371       0.0226853   -0.0147685  -0.0349216   -0.133127    -0.0370279    0.0807449   -0.0268235    -0.0478162   -0.117849    -0.102687   -0.218168    -0.190508    -0.0954938
 -0.101824    -0.129197    0.358681    -0.145963     0.140438     0.201846   -0.0485942     0.100592    -0.0230575    -0.113995    -0.111421     0.00362961  -0.14613      -0.0593653    0.171006    0.0203503   -0.00612573   0.0125111   -0.2126       0.0557577    -0.0867932   -0.241362     0.0919312   0.186466     0.0473837   -0.0228301
  0.0471074    0.18436     0.0622912   -0.144421    -0.265411    -0.0250107  -0.0429488    -0.144579     0.0294597     0.0894932    0.175621     0.0311306   -0.00938516    0.0412116    0.023679    0.180621    -0.0431028    0.0165798   -0.0433466    0.0675648     0.149022    -0.00524511  -0.0450744   0.0673924    0.00695194   0.14399
 -0.0593862    0.0883031   0.0381914   -0.0957023    0.0321233   -0.0271297   0.0592578    -0.00562178  -0.0370736     0.0137256   -0.0784358    0.0232201   -0.13427       0.0915538   -0.156393    0.0738485    0.0408711    0.124048    -0.0181302   -0.0522353    -0.0816112   -0.0619734   -0.0620767   0.0422141   -0.012294     0.166809
 -0.0268557   -0.139029    0.133355     0.173675     0.174502     0.0708265  -0.127398     -0.153119     0.140715      0.0187326   -0.0813099    0.0150633    0.0595177    -0.0346379    0.0875471   0.138428    -0.104691    -0.0393354   -0.0544766    0.0712887    -0.121527    -0.122358     0.030505    0.0918303   -0.0545414    0.010574
 -0.0968784    0.0704176  -0.100822     0.0871384   -0.146916    -0.0803443  -0.00175625   -0.0104977    0.218927     -0.04068      0.112401    -0.0263152   -0.170336     -0.0193277    0.111506   -0.0584289   -0.119974     0.281058     0.0963288   -0.140882      0.0836679   -0.0281098    0.102495   -0.162731     0.107595     0.0233727
 -0.0477836   -0.154194   -0.0109527   -0.171306     0.275214     0.122401    0.0410253    -0.130164     0.00767245    0.0722594   -0.0585331    0.123249     0.0255197     0.13347     -0.142037    0.0732695    0.002716     0.00484242  -0.00929863   0.177089     -0.196046    -0.0257624   -0.012403   -0.00294556  -0.182566    -0.0776868
  0.01968     -0.0872481   0.124504    -0.0592943   -0.150678     0.208616    0.107313      0.00102296   0.00374274    0.0652655    0.00780239   0.0292291    0.173347     -0.0374468   -0.011943   -0.0732654    0.078859     0.0503186   -0.0639116    0.0367884     0.0221762   -0.056416     0.0199152  -0.10091      0.132246    -0.0370865
 -0.0157358   -0.0955501   0.0832642    0.176019     0.0102253    0.0339994   0.16733       0.01597     -0.0482809    -0.127076    -0.0146187   -0.0104586    0.0783186    -0.101453     0.142731   -0.174329    -0.0849851   -0.191354    -0.0547526    0.182634      0.00803133   0.025802    -0.051606   -0.0746666    0.160583    -0.0298499
  0.179842    -0.119359    0.0201317    0.150087    -0.0200342   -0.0534432   0.000871497   0.107888    -0.108138      0.144396    -0.134171     0.209569     0.11435      -0.00402412  -0.0831223   0.146553    -0.00432112   0.0830619    0.115954    -0.0111026     0.068066    -0.0269558    0.0461421  -0.13711      0.163177     0.184093
  0.0907187   -0.0719648   0.0438021    0.0539588    0.0788038   -0.0895408  -0.124747      0.0319945   -0.120749      0.106951     0.0636867   -0.0363508    0.127749     -0.118249     0.102907   -0.120901    -0.107819     0.0648624    0.139255    -0.0538659     0.150571     0.0164631   -0.0954905  -0.0966716   -0.131137    -0.044517
  0.104658     0.0128013  -0.0539377   -0.0904235    0.155482    -0.107608   -0.0420464     0.00202722  -0.0208616    -0.0031306   -0.0373369   -0.0432749    0.000780721   0.0539001   -0.128223    0.0589498    0.0461203    0.0915957   -0.0740309   -0.0767577    -0.118549    -0.0426926   -0.227596    0.0234524   -0.0622455    0.0563142
  0.206194    -0.117576    0.0208386   -0.0563321   -0.0435741    0.0541806  -0.0400069    -0.124798     0.00805216    0.058053    -0.0500033    0.0329011   -0.032949     -0.0540104    0.0836031   0.0838447   -0.127475    -0.0142725   -0.025159     0.00837276    0.0804368   -0.143859    -0.043961    0.231114    -0.0324005    0.0769411
  0.0174408   -0.0379317  -0.163594    -0.0298883    0.120512    -0.190866   -0.0396312    -0.0891134    0.00747046   -0.0430315   -0.0776458   -0.0164848   -0.0554877    -0.0839028   -0.109728    0.0333742   -0.0261891    0.105643     0.0841679    0.003416      0.00402989  -0.187276     0.128378    0.0549195   -0.0600141   -0.0545755kind diag, method split
┌ Info: 0: avll = 
└   tll[1] = -1.3796215009976172
[ Info: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.379684
[ Info: iteration 2, average log likelihood -1.379600
[ Info: iteration 3, average log likelihood -1.378686
[ Info: iteration 4, average log likelihood -1.370550
[ Info: iteration 5, average log likelihood -1.354558
[ Info: iteration 6, average log likelihood -1.348555
[ Info: iteration 7, average log likelihood -1.347009
[ Info: iteration 8, average log likelihood -1.346256
[ Info: iteration 9, average log likelihood -1.345818
[ Info: iteration 10, average log likelihood -1.345592
[ Info: iteration 11, average log likelihood -1.345467
[ Info: iteration 12, average log likelihood -1.345388
[ Info: iteration 13, average log likelihood -1.345329
[ Info: iteration 14, average log likelihood -1.345277
[ Info: iteration 15, average log likelihood -1.345228
[ Info: iteration 16, average log likelihood -1.345175
[ Info: iteration 17, average log likelihood -1.345118
[ Info: iteration 18, average log likelihood -1.345064
[ Info: iteration 19, average log likelihood -1.345023
[ Info: iteration 20, average log likelihood -1.344995
[ Info: iteration 21, average log likelihood -1.344977
[ Info: iteration 22, average log likelihood -1.344965
[ Info: iteration 23, average log likelihood -1.344958
[ Info: iteration 24, average log likelihood -1.344953
[ Info: iteration 25, average log likelihood -1.344950
[ Info: iteration 26, average log likelihood -1.344948
[ Info: iteration 27, average log likelihood -1.344946
[ Info: iteration 28, average log likelihood -1.344945
[ Info: iteration 29, average log likelihood -1.344944
[ Info: iteration 30, average log likelihood -1.344943
[ Info: iteration 31, average log likelihood -1.344943
[ Info: iteration 32, average log likelihood -1.344942
[ Info: iteration 33, average log likelihood -1.344942
[ Info: iteration 34, average log likelihood -1.344942
[ Info: iteration 35, average log likelihood -1.344941
[ Info: iteration 36, average log likelihood -1.344941
[ Info: iteration 37, average log likelihood -1.344941
[ Info: iteration 38, average log likelihood -1.344941
[ Info: iteration 39, average log likelihood -1.344940
[ Info: iteration 40, average log likelihood -1.344940
[ Info: iteration 41, average log likelihood -1.344940
[ Info: iteration 42, average log likelihood -1.344940
[ Info: iteration 43, average log likelihood -1.344940
[ Info: iteration 44, average log likelihood -1.344940
[ Info: iteration 45, average log likelihood -1.344940
[ Info: iteration 46, average log likelihood -1.344939
[ Info: iteration 47, average log likelihood -1.344939
[ Info: iteration 48, average log likelihood -1.344939
[ Info: iteration 49, average log likelihood -1.344939
[ Info: iteration 50, average log likelihood -1.344939
┌ Info: EM with 100000 data points 50 iterations avll -1.344939
└ 952.4 data points per parameter
┌ Info: 1
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.379684325678005
│     -1.3795996696488582
│      ⋮
└     -1.3449390485470476
[ Info: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.345037
[ Info: iteration 2, average log likelihood -1.344937
[ Info: iteration 3, average log likelihood -1.344481
[ Info: iteration 4, average log likelihood -1.340814
[ Info: iteration 5, average log likelihood -1.328797
[ Info: iteration 6, average log likelihood -1.318108
[ Info: iteration 7, average log likelihood -1.313614
[ Info: iteration 8, average log likelihood -1.310487
[ Info: iteration 9, average log likelihood -1.308069
[ Info: iteration 10, average log likelihood -1.306709
[ Info: iteration 11, average log likelihood -1.306033
[ Info: iteration 12, average log likelihood -1.305630
[ Info: iteration 13, average log likelihood -1.305340
[ Info: iteration 14, average log likelihood -1.305107
[ Info: iteration 15, average log likelihood -1.304920
[ Info: iteration 16, average log likelihood -1.304774
[ Info: iteration 17, average log likelihood -1.304660
[ Info: iteration 18, average log likelihood -1.304569
[ Info: iteration 19, average log likelihood -1.304496
[ Info: iteration 20, average log likelihood -1.304435
[ Info: iteration 21, average log likelihood -1.304382
[ Info: iteration 22, average log likelihood -1.304337
[ Info: iteration 23, average log likelihood -1.304297
[ Info: iteration 24, average log likelihood -1.304262
[ Info: iteration 25, average log likelihood -1.304233
[ Info: iteration 26, average log likelihood -1.304208
[ Info: iteration 27, average log likelihood -1.304186
[ Info: iteration 28, average log likelihood -1.304167
[ Info: iteration 29, average log likelihood -1.304151
[ Info: iteration 30, average log likelihood -1.304138
[ Info: iteration 31, average log likelihood -1.304126
[ Info: iteration 32, average log likelihood -1.304117
[ Info: iteration 33, average log likelihood -1.304110
[ Info: iteration 34, average log likelihood -1.304104
[ Info: iteration 35, average log likelihood -1.304099
[ Info: iteration 36, average log likelihood -1.304095
[ Info: iteration 37, average log likelihood -1.304092
[ Info: iteration 38, average log likelihood -1.304090
[ Info: iteration 39, average log likelihood -1.304088
[ Info: iteration 40, average log likelihood -1.304086
[ Info: iteration 41, average log likelihood -1.304085
[ Info: iteration 42, average log likelihood -1.304084
[ Info: iteration 43, average log likelihood -1.304083
[ Info: iteration 44, average log likelihood -1.304082
[ Info: iteration 45, average log likelihood -1.304081
[ Info: iteration 46, average log likelihood -1.304080
[ Info: iteration 47, average log likelihood -1.304080
[ Info: iteration 48, average log likelihood -1.304079
[ Info: iteration 49, average log likelihood -1.304078
[ Info: iteration 50, average log likelihood -1.304078
┌ Info: EM with 100000 data points 50 iterations avll -1.304078
└ 473.9 data points per parameter
┌ Info: 2
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.3450374556349378
│     -1.3449365520994907
│      ⋮
└     -1.3040779469875374
[ Info: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.304236
[ Info: iteration 2, average log likelihood -1.304069
[ Info: iteration 3, average log likelihood -1.303280
[ Info: iteration 4, average log likelihood -1.296795
[ Info: iteration 5, average log likelihood -1.282307
[ Info: iteration 6, average log likelihood -1.272185
[ Info: iteration 7, average log likelihood -1.267552
[ Info: iteration 8, average log likelihood -1.264575
[ Info: iteration 9, average log likelihood -1.262022
[ Info: iteration 10, average log likelihood -1.259896
[ Info: iteration 11, average log likelihood -1.258123
[ Info: iteration 12, average log likelihood -1.256502
[ Info: iteration 13, average log likelihood -1.254998
[ Info: iteration 14, average log likelihood -1.253723
[ Info: iteration 15, average log likelihood -1.252714
[ Info: iteration 16, average log likelihood -1.251961
[ Info: iteration 17, average log likelihood -1.251422
[ Info: iteration 18, average log likelihood -1.251056
[ Info: iteration 19, average log likelihood -1.250822
[ Info: iteration 20, average log likelihood -1.250677
[ Info: iteration 21, average log likelihood -1.250586
[ Info: iteration 22, average log likelihood -1.250529
[ Info: iteration 23, average log likelihood -1.250495
[ Info: iteration 24, average log likelihood -1.250473
[ Info: iteration 25, average log likelihood -1.250460
[ Info: iteration 26, average log likelihood -1.250451
[ Info: iteration 27, average log likelihood -1.250445
[ Info: iteration 28, average log likelihood -1.250441
[ Info: iteration 29, average log likelihood -1.250437
[ Info: iteration 30, average log likelihood -1.250433
[ Info: iteration 31, average log likelihood -1.250429
[ Info: iteration 32, average log likelihood -1.250425
[ Info: iteration 33, average log likelihood -1.250421
[ Info: iteration 34, average log likelihood -1.250416
[ Info: iteration 35, average log likelihood -1.250410
[ Info: iteration 36, average log likelihood -1.250404
[ Info: iteration 37, average log likelihood -1.250397
[ Info: iteration 38, average log likelihood -1.250390
[ Info: iteration 39, average log likelihood -1.250381
[ Info: iteration 40, average log likelihood -1.250372
[ Info: iteration 41, average log likelihood -1.250361
[ Info: iteration 42, average log likelihood -1.250351
[ Info: iteration 43, average log likelihood -1.250340
[ Info: iteration 44, average log likelihood -1.250329
[ Info: iteration 45, average log likelihood -1.250320
[ Info: iteration 46, average log likelihood -1.250311
[ Info: iteration 47, average log likelihood -1.250305
[ Info: iteration 48, average log likelihood -1.250300
[ Info: iteration 49, average log likelihood -1.250296
[ Info: iteration 50, average log likelihood -1.250294
┌ Info: EM with 100000 data points 50 iterations avll -1.250294
└ 236.4 data points per parameter
┌ Info: 3
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.3042361294706426
│     -1.304069089334175
│      ⋮
└     -1.2502939866909493
[ Info: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.250473
[ Info: iteration 2, average log likelihood -1.250258
[ Info: iteration 3, average log likelihood -1.249184
[ Info: iteration 4, average log likelihood -1.237952
[ Info: iteration 5, average log likelihood -1.204967
[ Info: iteration 6, average log likelihood -1.176322
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.162653
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.164643
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.168932
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.172089
[ Info: iteration 11, average log likelihood -1.169872
[ Info: iteration 12, average log likelihood -1.161539
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      3
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 13, average log likelihood -1.154777
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     13
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 14, average log likelihood -1.173187
[ Info: iteration 15, average log likelihood -1.172255
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 16, average log likelihood -1.161896
[ Info: iteration 17, average log likelihood -1.161008
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      3
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 18, average log likelihood -1.147532
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 19, average log likelihood -1.175333
[ Info: iteration 20, average log likelihood -1.169668
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 21, average log likelihood -1.157643
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 22, average log likelihood -1.163865
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 23, average log likelihood -1.161265
[ Info: iteration 24, average log likelihood -1.167046
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     14
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 25, average log likelihood -1.151782
[ Info: iteration 26, average log likelihood -1.172598
[ Info: iteration 27, average log likelihood -1.159858
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      3
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 28, average log likelihood -1.149818
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 29, average log likelihood -1.162218
[ Info: iteration 30, average log likelihood -1.171611
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 31, average log likelihood -1.159902
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 32, average log likelihood -1.157913
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 33, average log likelihood -1.157713
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 34, average log likelihood -1.168578
[ Info: iteration 35, average log likelihood -1.164406
[ Info: iteration 36, average log likelihood -1.152708
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      3
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 37, average log likelihood -1.139765
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 38, average log likelihood -1.170515
[ Info: iteration 39, average log likelihood -1.166726
[ Info: iteration 40, average log likelihood -1.155103
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      3
│     14
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 41, average log likelihood -1.142259
[ Info: iteration 42, average log likelihood -1.178285
[ Info: iteration 43, average log likelihood -1.163222
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 44, average log likelihood -1.153171
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      3
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 45, average log likelihood -1.148043
[ Info: iteration 46, average log likelihood -1.174749
[ Info: iteration 47, average log likelihood -1.161287
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 48, average log likelihood -1.151009
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      3
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 49, average log likelihood -1.148157
[ Info: iteration 50, average log likelihood -1.174698
┌ Info: EM with 100000 data points 50 iterations avll -1.174698
└ 118.1 data points per parameter
┌ Info: 4
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.2504731565747313
│     -1.2502579710764519
│      ⋮
└     -1.174698293553005
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 1, average log likelihood -1.161412
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 2, average log likelihood -1.151479
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      6
│     27
│     28
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 3, average log likelihood -1.141084
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      5
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.138911
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     14
│     27
│     28
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.105186
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      3
│      4
│      5
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.077945
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│     14
│     27
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.077905
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      3
│      5
│     22
│     25
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.066050
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      4
│     14
│     24
│     27
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.063831
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      3
│      5
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.080061
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      4
│     14
│     25
│     27
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 11, average log likelihood -1.056651
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      3
│      5
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 12, average log likelihood -1.075916
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      4
│     14
│     22
│     27
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 13, average log likelihood -1.054896
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      3
│      5
│     24
│     25
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 14, average log likelihood -1.071473
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      4
│     14
│     27
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 15, average log likelihood -1.071357
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      3
│      5
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 16, average log likelihood -1.073816
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      4
│     14
│     25
│     27
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 17, average log likelihood -1.052206
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      3
│      5
│     22
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 18, average log likelihood -1.071818
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      4
│     14
│     24
│     27
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 19, average log likelihood -1.062675
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      3
│      5
│     25
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 20, average log likelihood -1.076924
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      4
│     14
│     27
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 21, average log likelihood -1.065577
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      3
│      5
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 22, average log likelihood -1.069740
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      4
│     14
│     22
│     25
│     27
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 23, average log likelihood -1.048404
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      3
│      5
│     24
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 24, average log likelihood -1.078740
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      4
│     14
│     21
│     27
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 25, average log likelihood -1.065263
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      3
│     25
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 26, average log likelihood -1.080260
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      4
│      5
│     14
│     27
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 27, average log likelihood -1.055180
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      3
│     22
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 28, average log likelihood -1.072164
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      4
│      5
│     14
│     24
│      ⋮
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 29, average log likelihood -1.047568
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      3
│     21
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 30, average log likelihood -1.086592
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      4
│     14
│     27
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 31, average log likelihood -1.065883
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      3
│      5
│     25
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 32, average log likelihood -1.066958
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      4
│     14
│     22
│     27
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 33, average log likelihood -1.057025
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      3
│      5
│     24
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 34, average log likelihood -1.070824
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      4
│     14
│     21
│     25
│     27
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 35, average log likelihood -1.056594
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      3
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 36, average log likelihood -1.088577
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      4
│      5
│     14
│     27
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 37, average log likelihood -1.052239
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      3
│     22
│     25
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 38, average log likelihood -1.068928
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      4
│      5
│     14
│     24
│     27
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 39, average log likelihood -1.055743
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      3
│     21
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 40, average log likelihood -1.079806
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      4
│     14
│     25
│     27
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 41, average log likelihood -1.059877
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      3
│      5
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 42, average log likelihood -1.075396
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      4
│     14
│     22
│     27
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 43, average log likelihood -1.054245
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      3
│      5
│     24
│     25
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 44, average log likelihood -1.067985
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      4
│     14
│     21
│     27
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 45, average log likelihood -1.065341
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      3
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 46, average log likelihood -1.082726
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      4
│      5
│     14
│     25
│     27
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 47, average log likelihood -1.046028
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      3
│     22
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 48, average log likelihood -1.077546
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      4
│      5
│     14
│     24
│     27
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 49, average log likelihood -1.053236
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      3
│     21
│     25
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 50, average log likelihood -1.077133
┌ Info: EM with 100000 data points 50 iterations avll -1.077133
└ 59.0 data points per parameter
┌ Info: 5
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.1614115558708016
│     -1.1514791256094807
│      ⋮
└     -1.077133055407122
┌ Info: Total log likelihood: 
│   tll =
│    251-element Array{Float64,1}:
│     -1.3796215009976172
│     -1.379684325678005
│     -1.3795996696488582
│     -1.3786860915998895
│      ⋮
│     -1.0775456655118916
│     -1.0532362410753013
└     -1.077133055407122
32×26 Array{Float64,2}:
 -0.0731176  -0.104603     -0.0601369   -0.120368    -0.0634939    0.0994112   0.111221    -0.316598    -0.154988    -0.011513    -0.112497    0.127666    -0.282917      0.116556    -0.0423355   0.048223     0.00660592  -0.0825508    0.130187     -0.0990398    0.153011    -0.471149    -0.0966544    0.135528    -0.0176282    0.00596998
 -0.0815944  -0.0885417    -0.133778    -0.0527041   -0.0804911   -0.016397    0.167049    -0.0802917   -0.108368    -0.0810401   -0.0298956   0.0196528   -0.114515      0.105406    -0.0208211   0.0278584    0.0145179   -0.0842971    0.165412     -0.138377     0.13303      0.547278    -0.107884     0.127638    -0.0535705    0.0103959
  0.0988332   0.0269726    -0.0591218   -0.076015     0.179261    -0.106998   -0.053173     0.00175542  -0.016363     0.0105437   -0.0334133  -0.0762035    0.0333964     0.0510079   -0.13343     0.0789489    0.0403684    0.091452    -0.0746545    -0.0767924   -0.121269    -0.0371933   -0.214045     0.0168545   -0.0680002    0.0450236
 -0.109466    0.103518     -0.0345704    0.159602     0.106206     0.0917305   0.209171    -0.0383652   -0.0447534    0.166204     0.0410448   0.164419    -0.178478      0.0326088   -0.0552719   0.0633418   -0.0268095   -0.107941    -0.0532428     0.00618657   0.0638152   -0.0414477   -0.0373603   -0.242873     0.202774    -0.0787535
  0.160153    0.0714798    -0.0288571    0.0228181    0.0287975   -0.132818   -0.232769     0.0946303   -0.168783    -0.0399199   -0.0274005   0.0167311    0.0411451    -0.0502786   -0.0106351   0.0323474    0.147077     0.172791    -0.0293676    -0.0209934    0.147454     0.0390479   -0.190472     0.00492165  -0.106684    -0.0472755
 -0.0202969   0.032987     -0.01547      0.0817566    0.0554547    0.0258556   0.00158863  -0.0429561    0.0943303    0.0729872   -0.161464   -0.00856218   0.0490213     0.0791594   -0.141149   -0.0553688    0.0182653   -0.141622    -0.191617      0.0100402    0.0577338    0.0074865    0.0248808    0.014724     0.197292     0.0370511
  0.0206251   0.180522      0.0641825   -0.148659    -0.266303    -0.0200473  -0.0375652   -0.15076      0.0386696    0.102906     0.171914    0.028926    -0.0274425     0.0401275    0.0225955   0.179482    -0.0499918    0.0119561   -0.0317134     0.0626453    0.155147     0.00441586  -0.0964104    0.0729841    0.00699984   0.141525
  0.0425971   0.0443604     0.0599912   -0.123781    -0.162604    -0.095182    0.072575     0.301897     0.00950118   0.165669     0.0809765   0.19252     -0.0449391    -0.00642837   0.111509   -0.212283    -0.0254563   -0.109316    -0.0381177    -0.0386053   -0.0157132    0.161596     0.0736565    0.135374     0.0459182   -0.0807675
  0.0421738   0.0474404     0.0565485   -0.199051    -0.337525    -0.114293   -0.0365086   -0.0619335   -0.0954444    0.148379     0.0937495   0.0694532    0.12164       0.0560784   -0.0050193   0.111892     0.0618906    0.0104133   -0.139486      0.102805     0.0541036   -0.0875621    0.0200024    0.0084598   -0.213128    -0.120926
  0.140118   -0.145489      0.344477    -0.139878     0.564253     0.0137795  -0.050763    -0.0324162   -0.202581     0.0735631    0.098497    0.0928366    0.154481      0.0101509    0.0505955   0.00939254   0.0189814    0.0359247    0.0696868     0.183984     0.0785397   -0.0870807    0.0852247    0.00987164  -0.0500484   -0.0733249
 -0.0180736   0.00446669    0.106308    -0.109604    -0.0332038   -0.0805187  -0.0623422    0.16721      0.173374    -0.206694    -0.0752262  -0.031657     0.0580681     0.0142963   -0.10466    -0.0203765   -0.109692    -0.0381674    0.0853709    -0.0245592   -0.0509507   -0.12165     -0.10393     -0.208982    -0.1892      -0.0924582
  0.0263677  -0.0266897    -0.039503     0.0863585   -0.012651    -0.0405476  -0.0665652    0.00576484   0.0108814    0.0563618    0.0982259  -0.0214233   -0.00185752   -0.0771369    0.105948   -0.0876462   -0.104942     0.114611     0.109292     -0.0872762    0.0939896    0.00968909  -0.00483866  -0.119131    -0.0211499   -0.00324382
 -0.065748   -0.0395691    -0.118891     0.136924     0.0286829   -0.0443887  -0.0293783   -0.0665248   -0.0942853   -0.151976    -0.129354    0.00636663  -0.24206      -0.19218      0.151157   -0.107717     0.00869432  -0.0650581   -0.00840721    0.135024    -0.00433603   0.130883    -0.0262269    0.0415277   -0.0214297   -0.063418
 -0.100863   -0.040658      0.33659     -0.13825      0.13604      0.184032   -0.0369782    0.0958093    0.00242625  -0.122045    -0.10943    -0.0235993   -0.140218     -0.0380742    0.170629    0.030707    -0.0088294    0.00375655  -0.21102       0.0508279   -0.0638689   -0.216935     0.0916292    0.159151     0.051474    -0.0286908
 -0.0299611  -0.0121475     0.0893276    0.0572522    0.137269     0.0350537  -0.0842167   -0.0850426    0.045109    -0.0268996   -0.0791612  -0.00394634   0.00258253    0.00179934   0.0208213   0.0765788   -0.0602768   -0.0277619   -0.00511592    0.0682863   -0.0703898   -0.00760885  -0.0114361    0.04162     -0.0428001    0.0681641
  0.0173274  -0.000547677   0.0676496    0.00321433  -0.167121     0.153334    0.0650659    0.0165911    0.0759646    0.0554708   -0.0183486  -0.067763     0.0755162     0.0540561   -0.020414   -0.0485802    0.00169124   0.136267    -0.0940528     0.0267014   -0.00434511  -0.0226362    0.00928533  -0.134916     0.0952435   -0.0455332
  0.0927052   0.00948806    0.145815    -0.00757809   0.0270397   -0.231822   -0.042749     0.0208443   -0.0790504   -0.0543444   -0.209779   -0.0781082   -0.0911134     0.0542478    0.135224    0.100244     0.0114837   -0.107906    -0.0419202    -0.0116071   -0.079388    -0.0618767    0.0338827   -0.0833793    0.0190689   -0.08854
  0.184063   -0.140815      0.0885528   -0.0363937    0.0149158   -0.227987   -0.0669752   -0.0392953    0.0646266   -0.117135     0.229768   -0.0549576    0.244406      0.0743672    0.0732069   0.0206574    0.0248326   -0.111746    -0.0694068    -0.0615096   -0.276264    -0.120702     0.0175522   -0.0955926    0.245051     0.112068
  0.0652012   0.0440223     0.0707768    0.14701     -0.0382088    0.0394682  -0.25962     -0.070636    -0.0250671   -0.0651299   -0.0157165   0.0494491   -0.180778     -0.0171053    0.108563    0.0460812    0.150626    -0.0212534   -0.204262     -0.0877887   -0.838676     0.230331     0.128003    -0.0788549   -0.085457     0.100247
  0.0974118  -0.194186      0.0776807    0.12922     -0.0357541    0.0579366  -0.144351    -0.0674929    0.0125597   -0.0673944   -0.0290768   0.0498793   -0.16159      -0.0629045    0.106699    0.0447996    0.147003    -0.0591161   -0.241065      0.0099806    0.764692     0.268374     0.370138     0.110137    -0.177843    -0.013711
  0.278974    0.152048      0.0875827    0.116082    -0.0263181    0.0498874   0.0174542    0.0541914   -0.0240767    0.107591     0.0898631   0.123263     0.12897      -0.0621079   -0.120785    0.0536436   -0.01296     -0.0972921    0.0271035    -0.128494     0.0263263   -0.0869163    0.0178968    0.106511     0.0727449    0.0552052
 -0.0181577  -0.033867      0.0155724   -0.0517689   -0.00475904   0.0197415  -0.024989     0.0244111   -0.0443789    0.134384     0.10256    -0.0225876   -0.000302732  -0.00272892   0.104753   -0.0377187    0.0311758   -0.102871    -0.0872276     0.0620524    0.0672372    0.0100139   -0.0313456   -0.00979902  -0.0771803    0.0234256
  0.0937304  -0.0876305     0.044693     0.073724     0.0192631    0.0680069   0.00589657  -0.0235769   -0.0687041   -0.0464309   -0.0332736  -0.0433507   -0.00268819    0.00946872   0.0585343  -0.068787    -0.0934508   -0.100225    -0.0286852     0.117038     0.0162577   -0.0557036   -0.0229012    0.0591558    0.019829    -0.0757083
  0.150947   -0.104115      0.0274192    0.136792     0.021633    -0.0246061  -0.0197117    0.126332    -0.0814666    0.0907175   -0.135152    0.173576     0.151338      0.0144281   -0.086691    0.123371    -0.0129986    0.0664208    0.0486436     0.0404988    0.0657602   -0.0212039    0.0376227   -0.125743     0.138953     0.166036
  0.0227441   0.00046479   -0.160507    -0.0321517    0.115853    -0.192791   -0.0337221   -0.112779    -0.0132342   -0.0737712   -0.0782427  -0.0175071   -0.0366063    -0.101673    -0.110024    0.0305823   -0.0119313    0.0770651    0.0688793     0.0301944    0.00790877  -0.187455     0.131719     0.050687    -0.0555478   -0.0642529
 -0.063713   -0.157611     -0.00840503  -0.163644     0.246821     0.122335    0.0427461   -0.142112     0.0137318    0.0782018   -0.0512823   0.13583     -0.000877655   0.132144    -0.110895    0.0777743   -0.00274857   0.00490751   0.00371987    0.184744    -0.196715    -0.00766786  -0.0278454   -0.00253018  -0.175051    -0.0738294
 -0.0779912  -0.0270448    -0.214027    -0.0269918   -0.0825243   -0.206537   -0.0268084   -0.0389728   -0.0274365    0.0742221   -0.129691    0.0487028    0.048236     -0.169924    -0.378796   -0.0148484   -0.129527    -0.22607      0.0743699    -0.056102     0.243894    -0.04253     -0.19969     -0.133948    -0.150106    -0.0175422
 -0.0591873  -0.0288849    -0.211881     0.137718     0.0520529   -0.206206   -0.0267297   -0.0359702   -0.0268439   -0.0130966   -0.130038    0.0338181    0.0468666    -0.170019     0.391417   -0.0154405    0.138203    -0.105547     0.0794555    -0.11763     -0.145059    -0.0479113   -0.202784    -0.157577    -0.132061    -0.0517455
  0.223772   -0.119903      0.11747     -0.189844     0.162981    -0.0973369  -0.167323    -0.115464     0.109599    -0.00014881   0.239597    0.00690017   0.0314617     0.053734     0.0788555   0.0111416   -0.0848969   -0.00527346  -0.0251199    -0.258031     0.0831013    0.0487212   -0.180798     0.0644216    0.122553     0.15259
  0.172448   -0.215697      0.0572869    0.122422     0.0780055   -0.0788163   0.436277     0.0643604    0.0212306   -0.0134202    0.233801    0.00588384  -0.01874       0.038687     0.102881    0.0186896   -0.187168    -0.132418     0.00481548   -0.294578     0.0773543    0.0367418   -0.119494     0.0528521    0.0912053    0.193684
 -0.0590825   0.161862      0.0386298   -0.118412     0.0242453   -0.0269979   0.160373    -0.0798331   -0.0667368   -0.00648347  -0.0782134   0.0301191   -0.100906      0.0786962   -0.160153    0.048055     0.021912     0.110986     0.000604153   0.0256635   -0.0878417    0.0152054   -0.0684889    0.0417682    0.00298398  -0.355117
 -0.0579146   0.0329488     0.0354591   -0.133222     0.0387204   -0.0268133  -0.230553     0.153972     0.00947967   0.0637627   -0.0810684   0.0142796   -0.296529      0.127214    -0.147922    0.145704     0.111278     0.21515      0.00368121   -0.278673    -0.0762063   -0.232088    -0.0347645    0.0416134    0.10129      1.37026[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      4
│     14
│     27
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 1, average log likelihood -1.068760
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      3
│      4
│      5
│     14
│     27
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 2, average log likelihood -1.040828
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      4
│      5
│     14
│     22
│      ⋮
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 3, average log likelihood -1.042876
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      3
│      4
│      5
│     14
│      ⋮
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.043451
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      4
│      5
│     14
│     21
│     27
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.046434
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      3
│      4
│     14
│     22
│      ⋮
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.045165
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      4
│      5
│     14
│     27
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.054274
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      3
│      4
│      5
│     14
│      ⋮
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.036459
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      4
│      5
│     14
│     21
│      ⋮
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.043869
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      3
│      4
│     14
│     27
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.057822
┌ Info: EM with 100000 data points 10 iterations avll -1.057822
└ 59.0 data points per parameter
kind diag, method kmeans
[ Info: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.171156e+05
      1       6.353837e+05      -1.817319e+05 |       32
      2       6.082965e+05      -2.708723e+04 |       32
      3       5.941740e+05      -1.412254e+04 |       32
      4       5.866968e+05      -7.477182e+03 |       32
      5       5.817743e+05      -4.922478e+03 |       32
      6       5.787619e+05      -3.012429e+03 |       32
      7       5.768933e+05      -1.868624e+03 |       32
      8       5.758015e+05      -1.091772e+03 |       32
      9       5.750871e+05      -7.144030e+02 |       32
     10       5.744574e+05      -6.297256e+02 |       32
     11       5.738957e+05      -5.617080e+02 |       32
     12       5.734793e+05      -4.163376e+02 |       32
     13       5.731657e+05      -3.135747e+02 |       32
     14       5.728968e+05      -2.688962e+02 |       32
     15       5.726783e+05      -2.185439e+02 |       32
     16       5.724997e+05      -1.785997e+02 |       32
     17       5.723302e+05      -1.695218e+02 |       32
     18       5.721443e+05      -1.858346e+02 |       32
     19       5.719339e+05      -2.104792e+02 |       32
     20       5.716613e+05      -2.726042e+02 |       32
     21       5.712748e+05      -3.864791e+02 |       32
     22       5.708747e+05      -4.000644e+02 |       32
     23       5.704351e+05      -4.395908e+02 |       32
     24       5.700839e+05      -3.512531e+02 |       32
     25       5.696949e+05      -3.889927e+02 |       32
     26       5.692245e+05      -4.704154e+02 |       32
     27       5.686132e+05      -6.112604e+02 |       32
     28       5.679627e+05      -6.504927e+02 |       32
     29       5.673764e+05      -5.862908e+02 |       32
     30       5.668243e+05      -5.520794e+02 |       32
     31       5.665640e+05      -2.603088e+02 |       32
     32       5.664800e+05      -8.400272e+01 |       32
     33       5.664477e+05      -3.228231e+01 |       32
     34       5.664331e+05      -1.462004e+01 |       31
     35       5.664248e+05      -8.358036e+00 |       30
     36       5.664180e+05      -6.724792e+00 |       30
     37       5.664117e+05      -6.347607e+00 |       26
     38       5.664085e+05      -3.152837e+00 |       22
     39       5.664059e+05      -2.613933e+00 |       20
     40       5.664034e+05      -2.482755e+00 |       24
     41       5.664007e+05      -2.705508e+00 |       19
     42       5.663971e+05      -3.608066e+00 |       22
     43       5.663936e+05      -3.583993e+00 |       23
     44       5.663888e+05      -4.766972e+00 |       23
     45       5.663828e+05      -6.009493e+00 |       27
     46       5.663717e+05      -1.112281e+01 |       28
     47       5.663529e+05      -1.875103e+01 |       27
     48       5.663293e+05      -2.364201e+01 |       29
     49       5.662891e+05      -4.017295e+01 |       31
     50       5.662332e+05      -5.587908e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 566233.2078633504)
┌ Info: K-means with 32000 data points using 50 iterations
└ 37.0 data points per parameter
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.299369
[ Info: iteration 2, average log likelihood -1.268307
[ Info: iteration 3, average log likelihood -1.236600
[ Info: iteration 4, average log likelihood -1.201269
[ Info: iteration 5, average log likelihood -1.160606
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      9
│     21
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.102061
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      6
│     12
│     17
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.065905
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      2
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.081066
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     22
│     27
│     28
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.065106
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      9
│     21
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.081717
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      6
│     11
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 11, average log likelihood -1.072105
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     12
│     17
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 12, average log likelihood -1.049378
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     2
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 13, average log likelihood -1.052674
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      9
│     21
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 14, average log likelihood -1.031231
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     22
│     25
│     29
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 15, average log likelihood -1.045377
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      6
│     12
│     17
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 16, average log likelihood -1.052358
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     21
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 17, average log likelihood -1.070981
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      2
│      9
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 18, average log likelihood -1.035775
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      6
│     12
│     17
│     22
│     25
│     28
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 19, average log likelihood -1.027243
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     21
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 20, average log likelihood -1.102690
[ Info: iteration 21, average log likelihood -1.070067
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      6
│      9
│     12
│     17
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 22, average log likelihood -1.007088
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      2
│     21
│     22
│     28
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 23, average log likelihood -1.054942
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 24, average log likelihood -1.091774
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     6
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 25, average log likelihood -1.047482
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      9
│     12
│     17
│     21
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 26, average log likelihood -1.001781
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      2
│     22
│     28
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 27, average log likelihood -1.068109
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     6
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 28, average log likelihood -1.070361
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     21
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 29, average log likelihood -1.034221
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      9
│     12
│     17
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 30, average log likelihood -1.018150
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      2
│      6
│     22
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 31, average log likelihood -1.051890
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     21
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 32, average log likelihood -1.066735
[ Info: iteration 33, average log likelihood -1.047191
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      6
│      9
│     12
│     17
│     25
│     27
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 34, average log likelihood -0.981489
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      2
│     21
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 35, average log likelihood -1.084268
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 36, average log likelihood -1.077953
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     6
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 37, average log likelihood -1.037796
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      2
│      9
│     12
│     17
│      ⋮
│     25
│     27
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 38, average log likelihood -0.993378
[ Info: iteration 39, average log likelihood -1.124905
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      6
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 40, average log likelihood -1.051850
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      9
│     12
│     21
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 41, average log likelihood -1.028862
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      2
│     17
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 42, average log likelihood -1.054532
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      6
│     22
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 43, average log likelihood -1.043669
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     12
│     21
│     28
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 44, average log likelihood -1.043243
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      9
│     17
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 45, average log likelihood -1.071336
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     2
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 46, average log likelihood -1.051460
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      6
│     12
│     21
│     22
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 47, average log likelihood -1.002710
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      9
│     17
│     27
│     28
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 48, average log likelihood -1.057235
[ Info: iteration 49, average log likelihood -1.093648
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      2
│      6
│     21
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 50, average log likelihood -1.034962
┌ Info: EM with 100000 data points 50 iterations avll -1.034962
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
  0.027962     0.0491905    0.0866003   -0.111596   -0.113302    -0.0968832   0.00412044   0.342845    -0.0427695    0.0312019    0.0989128    0.182356    -0.0724817   -0.011278    -0.0818217  -0.23444     -0.040082     -0.0695638   -0.033645     -0.0384567    -0.075998     0.128388     -0.533353     0.0360681     0.0369003   -0.175372
 -0.0595451    0.124781     0.0345031   -0.124928    0.0287573   -0.0270911   0.0373946   -0.00404182  -0.0433836    0.0151571   -0.0800279    0.0249948   -0.162858     0.0969608   -0.158097    0.0801396    0.0504399     0.147059     0.000720138  -0.0724181    -0.0851379   -0.0615687    -0.0591243    0.0420389     0.0329553    0.187147
  0.157148    -0.0814778    0.00279914   0.171926    0.0253379    0.147091   -0.0722721   -0.0169727   -0.114562     0.0958613    0.141341     0.0829752    0.0846917   -0.0142641    0.204516    0.00946745   0.0713193    -0.107082     0.0151155    -0.0118904    -0.0689954   -0.0254908    -0.0655214   -0.0830864    -0.121402    -0.034304
 -0.066909    -0.158227    -0.0109567   -0.159767    0.249138     0.121583    0.0463264   -0.145116     0.014273     0.0840733   -0.0492187    0.13748      0.00210793   0.136316    -0.115575    0.0781525   -0.00254571    0.00455648   0.00412429    0.186215     -0.194082    -0.00993262   -0.0277533   -0.00347183   -0.17616     -0.0780672
  0.0218291    0.180507     0.0639733   -0.147301   -0.2666      -0.0206341  -0.036143    -0.151129     0.0393581    0.0999349    0.175943     0.0296166   -0.0270477    0.0401083    0.02362     0.179792    -0.0485216     0.0113805   -0.0311819     0.0624472     0.158303     0.00334506   -0.0942448    0.0751795     0.00727636   0.142401
 -0.0804936   -0.0195488   -0.227171     0.0705918   0.00262788  -0.215257   -0.0231918   -0.0371005   -0.0269568    0.00035592  -0.128569     0.0346377    0.0339188   -0.167152     0.0668944  -0.018171     0.0170846    -0.154827     0.0812792    -0.0915636     0.0145395   -0.0444398    -0.210433    -0.152325     -0.141028    -0.0484661
  0.198669    -0.165891     0.0880918   -0.0372251   0.120648    -0.0882588   0.12969     -0.0270651    0.0659552   -0.00674409   0.236726     0.0062323    0.00689807   0.046751     0.0902554   0.0146639   -0.135515     -0.0675427   -0.0104606    -0.276099      0.0802697    0.042285     -0.151746     0.0587073     0.107653     0.172874
  0.108807    -0.0873299    0.0208586    0.0474897   0.0981654   -0.0802102  -0.123108     0.0225826   -0.134038     0.13537      0.0623819   -0.0354105    0.133223    -0.106438     0.0704161  -0.157646    -0.113527      0.0459034    0.139335     -0.0622509     0.148146     0.025006     -0.0888165   -0.098181     -0.127778    -0.0456282
  0.0208059   -0.00230857  -0.15927     -0.0303797   0.110774    -0.189114   -0.0327625   -0.111167    -0.0131366   -0.0729578   -0.0797154   -0.018322    -0.0328293   -0.102667    -0.107704    0.0303064   -0.012151      0.0733201    0.0751058     0.0295898     0.0057075   -0.184734      0.129917     0.0463808    -0.0522283   -0.0535515
 -0.0541439    0.12804      0.00944045   0.0495077  -0.141582     0.117356    0.0299682    0.0319517    0.170099     0.0520555   -0.107177    -0.191685    -0.0107862    0.154003    -0.0705177  -0.0340871   -0.132991      0.188671    -0.129676      0.0194979    -0.0100221   -0.00337744    0.0196936   -0.155897      0.0759885   -0.0742066
 -0.00974571   0.0128775   -0.0201756    0.0842767   0.0582402    0.0164989  -0.0147751   -0.0605221    0.101738     0.0846795   -0.159277     0.0130199    0.052498     0.0620756   -0.12864    -0.0559865    0.0347653    -0.142176    -0.21528       0.000451848   0.0690776    0.000448567   0.0227244    0.0293871     0.229771     0.0333032
 -0.0916348   -0.0590965    0.331396    -0.143112    0.132077     0.170973   -0.0286055    0.0988642   -0.00503153  -0.107169    -0.104965    -0.0312649   -0.123428    -0.041189     0.169594    0.0260798    0.000928411  -0.00407592  -0.201867      0.0570431    -0.0651133   -0.214439      0.0879057    0.150652      0.0474939   -0.0156182
  0.0975419    0.0236801    0.00402      0.0133834  -0.0490103    0.0445031   0.0757838   -0.0753444   -0.0808219    0.0231253    0.00472266   0.104272    -0.0361554    0.0280772   -0.0716004   0.0452171   -0.00244894   -0.0927762    0.0958487    -0.126564      0.0873869   -0.0166976    -0.0475829    0.126968      0.0155159    0.0304382
 -0.0546773    0.104745     0.0295391   -0.0898803   0.0773275   -0.0127467  -0.0445459   -0.00133418  -0.0422409   -0.100854    -0.0686133   -0.0550262   -0.0725755    0.0311335   -0.0764287   0.011835     0.00848483   -0.00126209   0.0400339     0.0667875    -0.01744      0.146573     -0.112596     0.000829021  -0.0270508    0.119238
  0.0575312    0.0416207    0.0404775   -0.128158   -0.191712    -0.0980292   0.114702     0.268167     0.045196     0.253949     0.0553392    0.198483    -0.0198437   -0.0038456    0.25633    -0.197975    -0.0113124    -0.121269    -0.0470753    -0.0346495     0.0314803    0.180518      0.509321     0.208092      0.0485283   -0.00651994
 -0.114142     0.0673562   -0.113303     0.0356442  -0.141378    -0.0846439  -0.00182985  -0.00469135   0.218717    -0.0492586    0.105089    -0.0255613   -0.165694    -0.0661633    0.114213   -0.0526782   -0.116116      0.28292      0.0840605    -0.135172      0.0997166   -0.00842877    0.102729    -0.153504      0.112231     0.0699773
 -0.113091     0.0892798   -0.0431619    0.166869    0.103926     0.0976111   0.230754    -0.0437645   -0.0424679    0.179459     0.0316321    0.16213     -0.224527     0.0354098   -0.05419     0.0714878   -0.0172208    -0.101063    -0.0411385     0.0024791     0.0661425   -0.0339234    -0.0307748   -0.233386      0.210603    -0.0749219
  0.203327    -0.117285     0.0284512   -0.055324   -0.0444725    0.0265325  -0.0370695   -0.123796     0.0249103    0.049113    -0.0505713    0.0383233   -0.0676321   -0.0493579    0.0703282   0.0740714   -0.110449     -0.00815238  -0.0178126     0.0281578     0.102533    -0.162869     -0.0359455    0.235435     -0.0414563    0.0842444
  0.00108647  -0.0803605    0.0926652    0.174885    0.0414627    0.0500962   0.149008     0.00868742  -0.0561647   -0.127426    -0.0329314   -0.0312658    0.105449    -0.109858     0.124035   -0.16369     -0.0918542    -0.192275    -0.06601       0.18099       0.0130816    0.00878918   -0.0569143   -0.0801295     0.157716    -0.0656287
 -0.0668101   -0.0658812   -0.124708     0.117027    0.0305072   -0.186476   -0.0246832   -0.0789902   -0.0647892   -0.149736    -0.103913     0.0299197   -0.23756     -0.192912     0.172571   -0.12488      0.00212379   -0.0622555   -0.00433626    0.119912     -0.00332076   0.130619     -0.0283375    0.0318457    -0.0152466   -0.0634212
  0.0973381    0.029267    -0.058859    -0.0750927   0.178088    -0.10756    -0.0471911    0.00176182  -0.0178084    0.0102725   -0.0322257   -0.0699361    0.0303253    0.051031    -0.133034    0.0829914    0.0414115     0.0889171   -0.0744775    -0.0755581    -0.114318    -0.0359823    -0.215373     0.0121098    -0.0671157    0.040101
 -0.00754461  -0.101168     0.124354     0.173503    0.180897     0.0657014  -0.121275    -0.144652     0.107145     0.059867    -0.0796257    0.0167975    0.0669491   -0.0280345    0.0759223   0.123523    -0.117198     -0.0779351   -0.0572664     0.0804121    -0.101035    -0.117689      0.0720199    0.0707736    -0.0501595    0.0445854
  0.0899407   -0.0456522    0.200077    -0.171761    0.0935379   -0.0608686  -0.0434719   -0.0477933   -0.151133     0.112365     0.0962313    0.0793369    0.138622     0.03858      0.0161338   0.0624179    0.0384684     0.0234182   -0.0363046     0.147038      0.0653146   -0.0832669     0.0530864    0.00940928   -0.135302    -0.0984396
  0.140049    -0.0669676    0.117752    -0.0228008   0.0207977   -0.23023    -0.0541815   -0.00993088  -0.00760068  -0.0866357    0.0125086   -0.0662681    0.0778268    0.0648852    0.101614    0.0608587    0.0181864    -0.110079    -0.0561519    -0.0380286    -0.179509    -0.0928713     0.0252158   -0.0898918     0.133948     0.0131342
  0.0473138   -0.0905097    0.118736    -0.0332816  -0.180502     0.183617    0.113203     0.00647641   0.00264553   0.0675983    0.0311329    0.0325344    0.164261    -0.0228641   -0.0138229  -0.0739304    0.0650442     0.0577458   -0.065917      0.0322788     0.0221175   -0.0518555     0.0200198   -0.105106      0.14025     -0.0379381
 -0.0249274    0.00995061   0.107296    -0.114101   -0.0333256   -0.0852826  -0.062348     0.174609     0.178967    -0.208779    -0.080132    -0.0329385    0.0575808    0.0150595   -0.110613   -0.0215754   -0.113744     -0.0421633    0.0858047    -0.0250923    -0.0525829   -0.1219       -0.101673    -0.219315     -0.19006     -0.096865
  0.00961259  -0.149509    -0.075586    -0.14891    -0.0701772    0.124148    0.035568     0.0669195   -0.0168469    0.370159     0.00769487  -0.0327445    0.0644171   -0.0442754    0.176881   -0.129364     0.0175932    -0.0462981   -0.105059      0.0107523     0.0551864    0.00168367    0.113794    -0.00207595   -0.0822922    0.103625
  0.151632    -0.0957445    0.0209295    0.135727    0.00850913  -0.0470851  -0.0535153    0.175923    -0.0823601    0.102137    -0.118907     0.204685     0.176746    -0.00482732  -0.0805774   0.134668    -0.0152665     0.0846473    0.0757502     0.00742777    0.0959116   -0.0171046     0.0374683   -0.12824       0.14495      0.175945
  0.106652     0.0202016   -0.0117969    0.0587781   0.0507269   -0.033907   -0.191598     0.100649    -0.179667    -0.0468384   -0.0484201   -0.067547    -0.00293285   0.0806973   -0.0706526  -0.0126042    0.05478       0.0988691   -0.0334118     0.0604468     0.0586701    0.0237127    -0.0719007    0.00135513   -0.0823027   -0.119301
  0.0840674   -0.0704415    0.0749909    0.137805   -0.0371354    0.0476862  -0.208986    -0.0692847   -0.00715512  -0.0689647   -0.0235099    0.0507145   -0.172357    -0.0388177    0.107479    0.0455622    0.15036      -0.0445635   -0.224864     -0.0397935    -0.054477     0.248526      0.246849     0.017594     -0.132613     0.0421457
 -0.0540331    0.0687397    0.0984527   -0.0517915   0.0256062   -0.0731638  -0.0020487   -0.0610288   -0.0613374   -0.0161259    0.221528    -0.00773327  -0.0754018    0.0290467    0.0494589   0.0247711    0.0636542    -0.235435    -0.086728      0.112754      0.101264     0.024723     -0.184344    -0.00702905   -0.0927432   -0.0386972
 -0.0630018    0.108284    -0.0933922    0.240799    0.0220133    0.761041   -0.0567237    0.0110629   -0.250851    -0.18326     -0.289145    -0.113425    -0.270234    -0.187145     0.0344      0.00614281   0.0627404    -0.0863687   -0.0388079     0.219564     -0.0142614    0.111211     -0.00792297   0.0979147    -0.0479378   -0.0761592[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 1, average log likelihood -1.034977
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      9
│     12
│     17
│     22
│     25
│     27
│     28
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 2, average log likelihood -0.986807
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      2
│      6
│      9
│     12
│      ⋮
│     22
│     25
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 3, average log likelihood -0.982927
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     25
│     28
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.029133
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      9
│     12
│     17
│     22
│     25
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -0.990822
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      2
│      6
│      9
│     12
│      ⋮
│     27
│     28
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -0.976571
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.034844
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      9
│     12
│     17
│     22
│     25
│     27
│     28
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -0.986036
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      2
│      6
│      9
│     12
│      ⋮
│     22
│     25
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -0.981683
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     25
│     28
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.029103
┌ Info: EM with 100000 data points 10 iterations avll -1.029103
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
  0.00244304  -0.187459      0.0876149   -0.136239     0.0136431  -0.00558175   0.0810261   -0.0776768   -0.0167712     0.144885     0.145538    -0.0165405   -0.0604238    0.0777235    0.167192   -0.127658    -0.206289    -0.0609809    0.0575125    -0.0395574   -0.0990378    -0.0735034  -0.0164414    0.0573955   -0.11824     -0.139691
  0.146122     0.0438984     0.179776    -0.160105     0.157668   -0.014204    -0.0531381   -0.0099539   -0.0754912     0.076667     0.12166     -0.033931     0.115621     0.108803     0.0265308   0.0351826    0.0841682    0.0601486   -0.126389      0.0237557    0.100671     -0.0807985   0.00175048  -0.141669    -0.192148     0.0350464
  0.118339    -0.107704      0.239388    -0.0945943   -0.0170133  -0.167008    -0.0602165    0.0108556    0.174642     -0.111485     0.103682     0.117147     0.0662794   -0.0123289    0.0393137  -0.0135524    0.0345959    0.111601    -0.000444133   0.136722    -0.110932     -0.1333      0.0471636   -0.0446992    0.197312    -0.0820925
  0.082106     0.0788045    -0.109464    -0.149028     0.0442592  -0.134655    -0.0880389   -0.183141     0.0585671    -0.0584655    0.0647305   -0.0112217   -0.0283384   -0.0232552   -0.0478235  -0.123753     0.189525    -0.0627366    0.119472     -0.133551     0.0992559    -0.0213582  -0.00799231  -0.0286377   -0.0277495   -0.214771
  0.276457     0.12746       0.0977991    0.0187492    0.0406335   0.12183     -0.0250342    0.0330211    0.000221811   0.102887    -0.0477994    0.0715269   -0.0517643   -0.149559    -0.117752    0.201626     0.0971824   -0.043454    -0.147186     -0.157269    -0.00620354    0.0514224  -0.0577082    0.0590098    0.194832    -0.138553
 -0.0650603   -0.0894149     0.086799    -0.159923    -0.0140244   0.14558     -0.0340753    0.168298    -0.0149066     0.0838382    0.110676     0.00583126   0.168881    -0.00765796  -0.048968    0.0932073    0.0261993   -0.0042449   -0.0134674     0.136743     0.123499      0.149772    0.0745706    0.132372    -0.0880489    0.0408068
 -0.0880302    0.000208449   0.0950039   -0.0750963   -0.110756    0.139442     0.0556249    0.0112344   -0.233334      0.0151329   -0.125954    -0.0343171    0.0927309   -0.116289     0.0940857   0.0761451    0.0745747    0.0373407   -0.0095347     0.038466    -0.0398683    -0.0347205   0.185315    -0.119433     0.0747759    0.0553193
  0.0741923    0.00323686   -0.056509    -0.0715688    0.0518357  -0.135189     0.0380108    0.0374499   -0.027196     -0.160184     0.160293     0.122775     0.0745482    0.0139112    0.111118    0.045905    -0.0761791   -0.123062    -0.0875569     0.13327      0.000863936  -0.0593827   0.0748551   -0.128882     0.066198     0.114898
  0.165371    -0.0411743     0.0267148   -0.0597697    0.188726    0.166776     0.0330165   -0.0580787    0.0124423     0.00190091  -0.0301764    0.0772957   -0.138948    -0.119699    -0.097765    0.0794627   -0.031961     0.0145799   -0.0152415     0.25678      0.015779      0.137156   -0.0550469   -0.0325536   -0.140961     0.174311
 -0.0450269    0.183014     -0.22714     -0.0496408   -0.201553   -0.0332183   -0.0597279    0.0307564    0.120141      0.0134348    0.156463     0.156356     0.0514689   -0.0712147   -0.0608405  -0.0393901   -0.00547597   0.00200627   0.106479      0.0403571   -0.15246      -0.127386    0.0469253    0.194458    -0.0458511   -0.014782
 -0.108842     0.192161      0.0786106   -0.138811    -0.0969908   0.111558     0.0294354   -0.0686299   -0.0534706    -0.115652     0.0165922   -0.0495373   -0.050974    -0.138407     0.0976555  -0.119965    -0.138066     0.0861834   -0.0408587     0.0617274   -0.0373219     0.0282828   0.00765414  -0.137876     0.195472     0.182006
 -0.0174748    0.00202451    0.0254263   -0.0221474    0.0410955  -0.0753182   -0.00796916   0.110948     0.000237328   0.0234104    0.11087      0.0499049   -0.0893662   -0.2322      -0.0438939   0.0867428    0.051485     0.050973     0.0891611     0.0998224    0.0640658     0.0282597  -0.142073     0.103677     0.0781867   -0.0615308
  0.0278396   -0.0526507    -0.101943     0.0810151   -0.122163   -0.0200856   -0.125738     0.0239029    0.22939      -0.117571    -0.193317    -0.116919    -0.00309673  -0.0632232   -0.0179484  -0.0623779   -0.23733     -0.0391927   -0.0703644    -0.0281395    0.181655     -0.157109   -0.0381399   -0.104776    -0.0648486   -0.0105882
 -0.0550938    0.00359095   -0.121047     0.139758    -0.0910165  -0.107776     0.0817307   -0.0983358    0.0319862     0.111725     0.108501    -0.0537617   -0.209587    -0.0703227    0.0732973  -0.107742     0.0688382   -0.0412755   -0.123056      0.103444     0.0184197     0.0512209   0.0380961   -0.0949372   -0.0434503    0.0272811
  0.0362536   -0.0305517    -0.0550906    0.108488     0.0494572  -0.0626684    0.105751    -0.151221    -0.0996793    -0.108505    -0.0528828   -0.146469    -0.0547493    0.0552058   -0.0916992   0.107767    -0.0355475   -0.117574    -0.231041     -0.147295    -0.112365     -0.103737    0.162594     0.194886     0.00113517   0.00247318
  0.0133043   -0.0293126    -0.11783     -0.0728468   -0.105678    0.104809     0.0174456    0.0525002   -0.113091     -0.0394838   -0.0337931   -0.0879834   -0.161709    -0.0698298    0.0661138   0.0316915   -0.00580605   0.0735233    0.0723321     0.109575     0.261895     -0.0170085  -0.00800117  -0.028482    -0.0575607   -0.181215
 -0.0357068    0.112506      0.132467     0.0218047   -0.0131917   0.12916     -0.144372    -0.0431373   -0.0826275     0.126262    -0.0658443    0.0279272   -0.0775334   -0.00772989   0.0446874  -0.0782676   -0.121948    -0.0322662   -0.0456697     0.196117    -0.185483     -0.229151   -0.102691     0.00719363  -0.0546201    0.0942031
  0.0364409   -0.125017     -0.16952     -0.17663      0.0542395   0.10898      0.117054    -0.027816    -0.0419162     0.245875    -0.164538    -0.0967542    0.0424756   -0.0127481   -0.0136328   0.072918     0.0626048   -0.0724181   -0.054652     -0.129016    -0.0748208     0.212888   -0.190407    -0.0618992    0.045222    -0.160212
  0.0307712   -0.245996      0.0620426    0.00475725  -0.0177162   0.109874    -0.0292007   -0.141306     0.143654      0.173413     0.0669128   -0.0012331   -0.136815    -0.0407661    0.189529   -0.0995668   -0.0710055    0.156466     0.0361427     0.00133818   0.0928171     0.0623648  -0.026769     0.224        0.0358407    0.0297834
  0.0477165    0.127308     -0.0234492    0.167433    -0.0757051  -0.0545213   -0.0618842   -0.0236586   -0.0619522    -0.0211887   -0.0790224    0.12384      0.112032    -0.0538847   -0.0256477  -0.0192642    0.126689    -0.0845114   -0.0901315     0.0998839   -0.0294603    -0.0705424  -0.128074     0.223809    -0.0570091   -0.115476
 -0.0171112   -0.19477      -0.0522213    0.0364984   -0.134172   -0.0921366   -0.265266     0.104316     0.151041      0.253521    -0.205449    -0.108436     0.100009    -0.136638     0.104541    0.0207674    0.0281824    0.13376      0.040395     -0.117431     0.00162467    0.142804   -0.0846617   -0.0866263    0.143075     0.18964
 -0.0499863   -0.0376088     0.115009     0.107738    -0.15306    -0.0208259    0.0210319    0.139874    -0.0101569    -0.21575      0.181357    -0.244012    -0.206611     0.0356216   -0.0769262   0.16469      0.100508     0.131514    -0.0879058     0.100938    -0.0018142     0.011691   -0.0280425    0.108626    -0.150996    -0.0183682
  0.0605112    0.0282015     0.00270672   0.0343297    0.152891   -0.0754641    0.128796    -0.0262235    0.186077      0.0100988   -0.158258    -0.144966    -0.0931288    0.0197586   -0.0694156   0.110187     0.0230752   -0.0110914   -0.124172      0.025535     0.0955938     0.147877   -0.104338     0.0722184    0.190908     0.0479471
  0.102041     0.191999     -0.172665     0.067703     0.0352254  -0.00626728   0.167981     0.211966    -0.00774587   -0.00705188   0.00387814   0.0810543    0.0804199    0.154376    -0.0759246   0.107311    -0.149094     0.00751359  -0.0997778     0.151929    -0.0585024     0.0261586  -0.0620402   -0.0951266    0.107584     0.0212133
 -0.225791     0.103816      0.116033     0.103755     0.113056    0.0405671   -0.0949419    0.00157841  -0.043976     -0.0781315    0.0452392   -0.174471     0.00455507  -0.0359011   -0.0975356  -0.101058     0.0171193   -0.0476144    0.0427073    -0.140335     0.00189251    0.174334    0.0708783   -0.0018726   -0.102599    -0.0622554
 -0.208059    -0.215968      0.0917432   -0.0507672   -0.0920773   0.0183959    0.0776093    0.0979102    0.205433     -0.0528643    0.0441015   -0.0471061   -0.0114862   -0.211822     0.0310085   0.00580407  -0.0384809   -0.0283092   -0.12091       0.0409985    0.0807896     0.0983291  -0.0329657   -0.0147107   -0.0517929   -0.105518
  0.00838623   0.130041      0.223084     0.0782321    0.211634    0.0344348   -0.0023655    0.0277104    0.0751502    -0.0473965    0.254843     0.163947    -0.0144682   -0.0141081   -0.133616    0.0771247    0.14302     -0.0951961   -0.134043      0.0137419   -0.133249      0.135933    0.107684     0.0485431    0.112984     0.0128457
 -0.00554426   0.069111      0.170915     0.122006     0.10409     0.0621339    0.185577     0.0581492   -0.0397825     0.0484618    0.00755613   0.146039     0.0227343   -0.139237     0.10083     0.131279    -0.0487489    0.068792    -0.287503      0.115545    -0.0365546    -0.0918655  -0.0955486    0.186445    -0.143706     0.0251991
  0.0144125    0.0802378     0.00203038   0.0797143   -0.06353    -0.0479578    0.116663     0.20144     -0.085123      0.0637552   -0.107213    -0.278814    -0.0150638   -0.04832      0.195794   -0.122871    -0.0853159    0.246528     0.023614     -0.13047      0.0006327     0.0325793   0.0526427    0.062906     0.282052     0.144458
  0.0774199   -0.0199887    -0.0501956    0.0647897   -0.193407   -0.0820478    0.234331     0.0591647    0.103487      0.135086     0.287        0.0401198   -0.174488     0.148604    -0.0391376  -0.0342939    0.0746397    0.0715478    0.0250613    -0.108205    -0.0264021    -0.0445142   0.106237     0.146462     0.108214     0.133872
 -0.0241089   -0.0580645    -0.0413783   -0.0339325    0.0549958   0.10732      0.0103411    0.00757073  -0.0138048     0.0655948    0.0755507    0.255271    -0.0464555   -0.213419     0.179376   -0.119053     0.00635036   0.0675301    0.00621035    0.0552944    0.027168     -0.146617    0.0485121    0.0920499    0.0341382    0.125117
 -0.101541     0.156523     -0.0770587    0.0414298   -0.123538    0.0488436    0.0275252    0.120914     0.0285923     0.0116307   -0.00487727  -0.0164092    0.0399503   -0.0188594   -0.122924   -0.0148338   -0.0631036   -0.100066     0.0173597    -0.0513335   -0.0255593     0.117199   -0.0793845    0.06616      0.0762777   -0.0538765kind full, method split
┌ Info: 0: avll = 
└   tll[1] = -1.4206201293330887
[ Info: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.420639
[ Info: iteration 2, average log likelihood -1.420568
[ Info: iteration 3, average log likelihood -1.420508
[ Info: iteration 4, average log likelihood -1.420429
[ Info: iteration 5, average log likelihood -1.420316
[ Info: iteration 6, average log likelihood -1.420142
[ Info: iteration 7, average log likelihood -1.419850
[ Info: iteration 8, average log likelihood -1.419336
[ Info: iteration 9, average log likelihood -1.418501
[ Info: iteration 10, average log likelihood -1.417422
[ Info: iteration 11, average log likelihood -1.416439
[ Info: iteration 12, average log likelihood -1.415819
[ Info: iteration 13, average log likelihood -1.415518
[ Info: iteration 14, average log likelihood -1.415388
[ Info: iteration 15, average log likelihood -1.415332
[ Info: iteration 16, average log likelihood -1.415308
[ Info: iteration 17, average log likelihood -1.415298
[ Info: iteration 18, average log likelihood -1.415294
[ Info: iteration 19, average log likelihood -1.415292
[ Info: iteration 20, average log likelihood -1.415291
[ Info: iteration 21, average log likelihood -1.415290
[ Info: iteration 22, average log likelihood -1.415290
[ Info: iteration 23, average log likelihood -1.415290
[ Info: iteration 24, average log likelihood -1.415290
[ Info: iteration 25, average log likelihood -1.415290
[ Info: iteration 26, average log likelihood -1.415290
[ Info: iteration 27, average log likelihood -1.415290
[ Info: iteration 28, average log likelihood -1.415290
[ Info: iteration 29, average log likelihood -1.415289
[ Info: iteration 30, average log likelihood -1.415289
[ Info: iteration 31, average log likelihood -1.415289
[ Info: iteration 32, average log likelihood -1.415289
[ Info: iteration 33, average log likelihood -1.415289
[ Info: iteration 34, average log likelihood -1.415289
[ Info: iteration 35, average log likelihood -1.415289
[ Info: iteration 36, average log likelihood -1.415289
[ Info: iteration 37, average log likelihood -1.415289
[ Info: iteration 38, average log likelihood -1.415289
[ Info: iteration 39, average log likelihood -1.415289
[ Info: iteration 40, average log likelihood -1.415289
[ Info: iteration 41, average log likelihood -1.415289
[ Info: iteration 42, average log likelihood -1.415289
[ Info: iteration 43, average log likelihood -1.415289
[ Info: iteration 44, average log likelihood -1.415289
[ Info: iteration 45, average log likelihood -1.415289
[ Info: iteration 46, average log likelihood -1.415289
[ Info: iteration 47, average log likelihood -1.415289
[ Info: iteration 48, average log likelihood -1.415289
[ Info: iteration 49, average log likelihood -1.415289
[ Info: iteration 50, average log likelihood -1.415289
┌ Info: EM with 100000 data points 50 iterations avll -1.415289
└ 952.4 data points per parameter
┌ Info: 1
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4206392414658675
│     -1.4205683903583064
│      ⋮
└     -1.4152890872293733
[ Info: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.415304
[ Info: iteration 2, average log likelihood -1.415243
[ Info: iteration 3, average log likelihood -1.415187
[ Info: iteration 4, average log likelihood -1.415118
[ Info: iteration 5, average log likelihood -1.415030
[ Info: iteration 6, average log likelihood -1.414924
[ Info: iteration 7, average log likelihood -1.414809
[ Info: iteration 8, average log likelihood -1.414700
[ Info: iteration 9, average log likelihood -1.414607
[ Info: iteration 10, average log likelihood -1.414532
[ Info: iteration 11, average log likelihood -1.414473
[ Info: iteration 12, average log likelihood -1.414428
[ Info: iteration 13, average log likelihood -1.414392
[ Info: iteration 14, average log likelihood -1.414365
[ Info: iteration 15, average log likelihood -1.414343
[ Info: iteration 16, average log likelihood -1.414326
[ Info: iteration 17, average log likelihood -1.414311
[ Info: iteration 18, average log likelihood -1.414299
[ Info: iteration 19, average log likelihood -1.414289
[ Info: iteration 20, average log likelihood -1.414279
[ Info: iteration 21, average log likelihood -1.414270
[ Info: iteration 22, average log likelihood -1.414261
[ Info: iteration 23, average log likelihood -1.414252
[ Info: iteration 24, average log likelihood -1.414244
[ Info: iteration 25, average log likelihood -1.414236
[ Info: iteration 26, average log likelihood -1.414228
[ Info: iteration 27, average log likelihood -1.414220
[ Info: iteration 28, average log likelihood -1.414212
[ Info: iteration 29, average log likelihood -1.414204
[ Info: iteration 30, average log likelihood -1.414197
[ Info: iteration 31, average log likelihood -1.414189
[ Info: iteration 32, average log likelihood -1.414182
[ Info: iteration 33, average log likelihood -1.414175
[ Info: iteration 34, average log likelihood -1.414168
[ Info: iteration 35, average log likelihood -1.414161
[ Info: iteration 36, average log likelihood -1.414155
[ Info: iteration 37, average log likelihood -1.414149
[ Info: iteration 38, average log likelihood -1.414143
[ Info: iteration 39, average log likelihood -1.414138
[ Info: iteration 40, average log likelihood -1.414133
[ Info: iteration 41, average log likelihood -1.414128
[ Info: iteration 42, average log likelihood -1.414124
[ Info: iteration 43, average log likelihood -1.414119
[ Info: iteration 44, average log likelihood -1.414116
[ Info: iteration 45, average log likelihood -1.414112
[ Info: iteration 46, average log likelihood -1.414109
[ Info: iteration 47, average log likelihood -1.414105
[ Info: iteration 48, average log likelihood -1.414102
[ Info: iteration 49, average log likelihood -1.414100
[ Info: iteration 50, average log likelihood -1.414097
┌ Info: EM with 100000 data points 50 iterations avll -1.414097
└ 473.9 data points per parameter
┌ Info: 2
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4153044975508735
│     -1.4152425989094577
│      ⋮
└     -1.4140969937520331
[ Info: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.414105
[ Info: iteration 2, average log likelihood -1.414044
[ Info: iteration 3, average log likelihood -1.413994
[ Info: iteration 4, average log likelihood -1.413940
[ Info: iteration 5, average log likelihood -1.413876
[ Info: iteration 6, average log likelihood -1.413800
[ Info: iteration 7, average log likelihood -1.413712
[ Info: iteration 8, average log likelihood -1.413616
[ Info: iteration 9, average log likelihood -1.413517
[ Info: iteration 10, average log likelihood -1.413422
[ Info: iteration 11, average log likelihood -1.413335
[ Info: iteration 12, average log likelihood -1.413258
[ Info: iteration 13, average log likelihood -1.413193
[ Info: iteration 14, average log likelihood -1.413138
[ Info: iteration 15, average log likelihood -1.413093
[ Info: iteration 16, average log likelihood -1.413056
[ Info: iteration 17, average log likelihood -1.413025
[ Info: iteration 18, average log likelihood -1.413000
[ Info: iteration 19, average log likelihood -1.412978
[ Info: iteration 20, average log likelihood -1.412960
[ Info: iteration 21, average log likelihood -1.412944
[ Info: iteration 22, average log likelihood -1.412929
[ Info: iteration 23, average log likelihood -1.412916
[ Info: iteration 24, average log likelihood -1.412904
[ Info: iteration 25, average log likelihood -1.412893
[ Info: iteration 26, average log likelihood -1.412882
[ Info: iteration 27, average log likelihood -1.412872
[ Info: iteration 28, average log likelihood -1.412862
[ Info: iteration 29, average log likelihood -1.412853
[ Info: iteration 30, average log likelihood -1.412844
[ Info: iteration 31, average log likelihood -1.412834
[ Info: iteration 32, average log likelihood -1.412825
[ Info: iteration 33, average log likelihood -1.412817
[ Info: iteration 34, average log likelihood -1.412808
[ Info: iteration 35, average log likelihood -1.412799
[ Info: iteration 36, average log likelihood -1.412790
[ Info: iteration 37, average log likelihood -1.412782
[ Info: iteration 38, average log likelihood -1.412773
[ Info: iteration 39, average log likelihood -1.412765
[ Info: iteration 40, average log likelihood -1.412756
[ Info: iteration 41, average log likelihood -1.412748
[ Info: iteration 42, average log likelihood -1.412740
[ Info: iteration 43, average log likelihood -1.412732
[ Info: iteration 44, average log likelihood -1.412724
[ Info: iteration 45, average log likelihood -1.412716
[ Info: iteration 46, average log likelihood -1.412709
[ Info: iteration 47, average log likelihood -1.412701
[ Info: iteration 48, average log likelihood -1.412694
[ Info: iteration 49, average log likelihood -1.412687
[ Info: iteration 50, average log likelihood -1.412680
┌ Info: EM with 100000 data points 50 iterations avll -1.412680
└ 236.4 data points per parameter
┌ Info: 3
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4141051935446183
│     -1.4140439773007778
│      ⋮
└     -1.4126803976211255
[ Info: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.412683
[ Info: iteration 2, average log likelihood -1.412626
[ Info: iteration 3, average log likelihood -1.412576
[ Info: iteration 4, average log likelihood -1.412522
[ Info: iteration 5, average log likelihood -1.412460
[ Info: iteration 6, average log likelihood -1.412388
[ Info: iteration 7, average log likelihood -1.412305
[ Info: iteration 8, average log likelihood -1.412214
[ Info: iteration 9, average log likelihood -1.412117
[ Info: iteration 10, average log likelihood -1.412018
[ Info: iteration 11, average log likelihood -1.411920
[ Info: iteration 12, average log likelihood -1.411826
[ Info: iteration 13, average log likelihood -1.411736
[ Info: iteration 14, average log likelihood -1.411654
[ Info: iteration 15, average log likelihood -1.411578
[ Info: iteration 16, average log likelihood -1.411511
[ Info: iteration 17, average log likelihood -1.411451
[ Info: iteration 18, average log likelihood -1.411398
[ Info: iteration 19, average log likelihood -1.411351
[ Info: iteration 20, average log likelihood -1.411309
[ Info: iteration 21, average log likelihood -1.411272
[ Info: iteration 22, average log likelihood -1.411238
[ Info: iteration 23, average log likelihood -1.411206
[ Info: iteration 24, average log likelihood -1.411177
[ Info: iteration 25, average log likelihood -1.411150
[ Info: iteration 26, average log likelihood -1.411124
[ Info: iteration 27, average log likelihood -1.411100
[ Info: iteration 28, average log likelihood -1.411077
[ Info: iteration 29, average log likelihood -1.411055
[ Info: iteration 30, average log likelihood -1.411035
[ Info: iteration 31, average log likelihood -1.411015
[ Info: iteration 32, average log likelihood -1.410997
[ Info: iteration 33, average log likelihood -1.410980
[ Info: iteration 34, average log likelihood -1.410963
[ Info: iteration 35, average log likelihood -1.410947
[ Info: iteration 36, average log likelihood -1.410932
[ Info: iteration 37, average log likelihood -1.410918
[ Info: iteration 38, average log likelihood -1.410904
[ Info: iteration 39, average log likelihood -1.410890
[ Info: iteration 40, average log likelihood -1.410877
[ Info: iteration 41, average log likelihood -1.410865
[ Info: iteration 42, average log likelihood -1.410852
[ Info: iteration 43, average log likelihood -1.410840
[ Info: iteration 44, average log likelihood -1.410829
[ Info: iteration 45, average log likelihood -1.410817
[ Info: iteration 46, average log likelihood -1.410806
[ Info: iteration 47, average log likelihood -1.410795
[ Info: iteration 48, average log likelihood -1.410785
[ Info: iteration 49, average log likelihood -1.410775
[ Info: iteration 50, average log likelihood -1.410765
┌ Info: EM with 100000 data points 50 iterations avll -1.410765
└ 118.1 data points per parameter
┌ Info: 4
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4126834735120237
│     -1.4126256213861952
│      ⋮
└     -1.4107646696623344
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.410763
[ Info: iteration 2, average log likelihood -1.410703
[ Info: iteration 3, average log likelihood -1.410647
[ Info: iteration 4, average log likelihood -1.410584
[ Info: iteration 5, average log likelihood -1.410507
[ Info: iteration 6, average log likelihood -1.410414
[ Info: iteration 7, average log likelihood -1.410300
[ Info: iteration 8, average log likelihood -1.410169
[ Info: iteration 9, average log likelihood -1.410025
[ Info: iteration 10, average log likelihood -1.409873
[ Info: iteration 11, average log likelihood -1.409720
[ Info: iteration 12, average log likelihood -1.409572
[ Info: iteration 13, average log likelihood -1.409434
[ Info: iteration 14, average log likelihood -1.409308
[ Info: iteration 15, average log likelihood -1.409194
[ Info: iteration 16, average log likelihood -1.409094
[ Info: iteration 17, average log likelihood -1.409006
[ Info: iteration 18, average log likelihood -1.408930
[ Info: iteration 19, average log likelihood -1.408862
[ Info: iteration 20, average log likelihood -1.408803
[ Info: iteration 21, average log likelihood -1.408750
[ Info: iteration 22, average log likelihood -1.408703
[ Info: iteration 23, average log likelihood -1.408660
[ Info: iteration 24, average log likelihood -1.408621
[ Info: iteration 25, average log likelihood -1.408586
[ Info: iteration 26, average log likelihood -1.408553
[ Info: iteration 27, average log likelihood -1.408523
[ Info: iteration 28, average log likelihood -1.408495
[ Info: iteration 29, average log likelihood -1.408468
[ Info: iteration 30, average log likelihood -1.408444
[ Info: iteration 31, average log likelihood -1.408420
[ Info: iteration 32, average log likelihood -1.408398
[ Info: iteration 33, average log likelihood -1.408378
[ Info: iteration 34, average log likelihood -1.408358
[ Info: iteration 35, average log likelihood -1.408339
[ Info: iteration 36, average log likelihood -1.408321
[ Info: iteration 37, average log likelihood -1.408304
[ Info: iteration 38, average log likelihood -1.408287
[ Info: iteration 39, average log likelihood -1.408271
[ Info: iteration 40, average log likelihood -1.408256
[ Info: iteration 41, average log likelihood -1.408241
[ Info: iteration 42, average log likelihood -1.408227
[ Info: iteration 43, average log likelihood -1.408213
[ Info: iteration 44, average log likelihood -1.408199
[ Info: iteration 45, average log likelihood -1.408186
[ Info: iteration 46, average log likelihood -1.408173
[ Info: iteration 47, average log likelihood -1.408160
[ Info: iteration 48, average log likelihood -1.408147
[ Info: iteration 49, average log likelihood -1.408135
[ Info: iteration 50, average log likelihood -1.408122
┌ Info: EM with 100000 data points 50 iterations avll -1.408122
└ 59.0 data points per parameter
┌ Info: 5
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.410762557765862
│     -1.410702627748252
│      ⋮
└     -1.4081221223066616
┌ Info: Total log likelihood: 
│   tll =
│    251-element Array{Float64,1}:
│     -1.4206201293330887
│     -1.4206392414658675
│     -1.4205683903583064
│     -1.4205078969331648
│      ⋮
│     -1.4081472870687972
│     -1.4081346631004243
└     -1.4081221223066616
32×26 Array{Float64,2}:
  0.0205289    0.217449   -0.300824     0.154509     0.175473    0.675194    0.631021     0.165663    -0.0686811   -0.467836     0.243969    -0.362028   -0.324301   -0.452168      0.509818     0.00132426  -0.503217   -0.498197    0.0139858    -0.322251   -0.316012    -0.142975   -0.0485203   0.714729    -0.12828     -0.307146
 -0.305155     0.589993   -0.335753    -0.0598305   -0.427653    0.404127    0.333958    -0.0892229   -0.756891    -0.719111     0.846307    -0.0361136   0.160406   -0.47789      -0.0964982   -0.145964    -0.265625    0.242509    0.122167      1.14454     0.125872    -0.127617   -0.0609963   0.373262     0.0298134    0.0508016
  0.0825855   -0.0831612  -0.312925     0.0258776   -0.114917    0.0986163   0.0833902    0.111838    -0.0861024   -0.140437     0.106954    -0.0998927   0.126888    0.0866638    -0.116835     0.0457399    0.111      -0.111775   -0.0368558     0.123095   -0.0796564   -0.0111803  -0.0513032   0.0873729   -0.167126     0.0957811
  0.0401784   -0.0601793   0.853063     0.0800745    0.0358857   0.0174085  -0.313037    -0.223233     0.0296142    0.0532309   -0.0972984    0.0143027  -0.629065   -0.580603      0.197609    -0.382228    -0.552211    0.151048    0.0290578    -0.128143    0.237122    -0.399094    0.357254    0.257433     0.143456    -0.0913551
  0.107952    -0.291833   -1.26417     -0.0859779    0.205987   -0.287164   -0.0911217    0.292539     0.0716181    0.489646    -0.639729     0.46483     0.502189    0.261424      0.354371    -0.276142     0.0634302  -0.0448676   0.211622      0.391342   -0.0349873   -0.275974   -0.746806    0.605004     0.0353378   -0.153523
  0.356254    -0.27845     0.218633    -0.171922     0.416805   -0.0815083  -0.310273     0.53364      0.53994      0.867893    -0.43814      0.44976    -0.130665    0.838301      0.274746     0.325425     0.303118   -0.0665849  -0.10129      -0.216236    0.0262504    0.0308039   0.266249   -0.287192    -0.0662884   -0.107438
 -0.269861     0.504739   -0.00964851   0.537722     0.133832   -0.433185    0.152843    -0.490485     0.38354     -0.0196632    0.288958     0.104581    0.402122   -0.225089      0.0468918    0.336301     0.38133     0.400712   -0.0130601    -0.48163    -0.04631      0.224891   -0.305329   -0.428861     0.39227     -0.550471
 -0.336299    -0.354147    0.236659     0.0892484   -0.287808   -0.727105   -0.58509      0.294209    -0.120546    -0.134596    -0.0323701    0.218053    0.834916    0.405732     -0.73006      0.346895     0.329163    0.171175    0.000309896   0.224424    0.529196     0.351874   -0.312909   -0.444523     0.548741     0.436953
 -0.787667     0.328314    0.00763064   0.464765     0.0861438  -0.324818    0.467212     0.211437     0.27529      0.502622     0.100178    -0.0709017  -0.819771    0.634507     -0.216374     0.0345218   -0.12209     0.138892   -0.109704     -0.87638     0.266217     0.376603   -0.0202867   0.0560875    0.415099     0.334683
  0.201704    -0.0766174   0.233814     0.0392052   -0.302387    0.0415459   0.122403     0.0742798    0.706835     0.554715    -0.125651    -0.0954763  -0.633796    0.15814       0.00305133   0.285095    -0.380784   -0.0453371  -0.947517      0.0756853  -0.947052     0.578834   -0.149385    0.425369     0.599312    -0.0241679
 -0.0913367   -0.138795   -0.446187    -0.143695    -0.379075   -0.536038    0.514065     0.373723     0.430963     0.165133    -0.772116    -0.341381    0.0341335   0.164345      0.2329       0.417849     0.406758    0.474102   -0.201162     -0.505437   -0.441266     0.285874   -0.119458   -0.124018    -0.237397    -0.243257
  0.0823918    0.0983513   0.0537897   -0.32542     -0.20529    -0.16489     0.151132    -0.0642894    0.224307     0.0133296   -0.441654     0.237915    0.401184    0.355469      0.258044     0.670653    -0.223491    0.236062    0.987928     -0.0175026  -0.700043    -0.142427   -0.0482182   0.0697464   -0.110231    -0.0214235
  0.217342    -0.217456   -0.0981253   -0.174711    -0.224747    0.281086   -0.197849    -0.66684      0.753568     0.374217    -0.397787    -0.544665   -0.0915658  -0.258049      0.141016    -0.411669     0.0583546   0.748468    0.270753     -0.145629   -0.346563    -0.0650694  -0.334967    0.00656253   0.534829    -0.0474843
  0.762374    -0.496764    0.157581    -0.159812    -0.107087   -0.0671372  -0.203048    -0.251574     0.29103      0.00519837  -0.282607     0.544397    0.220258   -0.657124      0.677356    -0.130851     0.270421   -0.093742   -0.0137296    -0.116568    0.0696511   -0.0649543  -0.217968    0.403317     0.388676    -0.141363
  0.189321     0.274418    0.389084     0.170898    -0.326569    0.173003   -0.412704    -0.325809     0.197567    -0.523729    -0.178184    -0.719107   -0.21023    -0.0528732    -0.0555139    0.610231     0.396489   -0.264335    0.296327     -0.564961   -0.0351404   -0.238759    0.212522   -0.00175012  -0.381758    -0.0783942
  0.33968      0.0761558   0.407013     0.320677    -0.0787713   0.777303   -0.747253    -0.823119     0.288354     0.469238     0.134682     0.217976   -0.0408521  -0.477072     -0.0887896    0.0876259    0.101672   -0.254691    0.371861     -0.232715    0.167515     0.232526    0.561693    0.194362     0.772377     0.154028
 -0.0604214    0.20218     0.421059     0.0678558   -0.0631353   0.195722    0.105904    -0.167629     0.241517     0.121579     0.203183     0.0996397  -0.539552   -0.471913      0.355113    -0.181471    -0.210721   -0.0152491  -0.315792     -0.136321   -0.065354    -0.103613    0.211534    0.334547     0.120113    -0.209541
  0.10492     -0.191781    0.357145    -0.146011    -0.107455    0.17835    -0.192457     0.171151    -0.067151     0.144389    -0.233665    -0.151934   -0.758737    0.529027     -0.00655283  -0.0950427   -0.215404   -0.200456   -0.0353579     0.135222   -0.154661    -0.291934    0.0556254   0.235245    -0.0737162   -0.00136678
  0.0880802   -0.635859   -0.345029     0.167757    -0.399732   -0.0425282  -0.256894     0.469084    -0.326458    -0.0588763    0.389794    -0.333347   -0.364753    0.29087      -0.0552181   -0.851951     0.28487     0.448562   -0.309818      0.413793    0.66919      0.332575   -0.331729   -0.123119     0.0681159   -0.263133
  0.246721    -0.506575    0.203043    -0.00036315   0.482758    0.287761   -0.426531     0.20636     -0.557699    -0.0312175   -0.368971    -0.12974     0.266385   -0.0919164    -0.304663    -0.241381    -0.366452   -0.202034    0.315971      0.226647    0.73498     -0.293488   -0.178625   -0.405023    -0.108646     0.0378134
 -0.481075     0.0184949   0.23324     -0.419218     0.0739324  -0.165393    0.00556532   0.00460757  -0.415461     0.221581     0.536717     0.475129   -0.299216   -0.000578329   0.108933    -0.779233    -0.351486    0.338283   -0.406521      0.625099   -0.194798     0.0870473   0.238113   -0.580582     0.0915335    0.109997
 -0.524632     0.304138    0.464162     0.1564       0.257433    0.0920054   0.126388    -0.237481    -0.0772234   -0.0828649    0.425762    -0.285841   -0.219877    0.036819     -0.309174    -0.0250669   -0.302114    0.525746    0.161866      0.0871104  -0.179344    -0.163609    0.762298   -0.594283    -0.00826515  -0.0153543
 -0.232387     0.658255   -0.0196681   -0.246977     0.556741   -0.159376    0.19557      0.354108    -0.0606003   -0.440871     0.232782     0.441987    0.081471   -0.121164      0.514022    -0.0102375    0.62522     0.136794    0.578198      0.0745561   0.595309    -0.41145     0.0037629  -0.231658    -0.165499    -0.304409
 -0.229695     0.739264   -0.222839     0.327848     0.41689     0.0675103   0.0091448   -0.117377    -0.327527     0.258419     0.146586    -0.21256     0.0883464   0.075672      0.109052     0.369955     0.0878566  -0.190467   -0.332211      0.452017   -0.118319    -0.565999   -0.144829   -0.233953    -0.19472      0.0234593
  0.00458367  -0.0919577   0.14209     -0.169964     0.0529069  -0.147744   -0.0982073    0.203255    -0.0669057    0.228136    -0.0706472    0.179067    0.157116    0.00648812   -0.0332627    0.0986099    0.0579189   0.050008   -0.33572       0.184192    0.0668196    0.152304    0.088751   -0.353695    -0.0481698    0.113883
  0.0171987    0.101709   -0.18257      0.0469211   -0.0123493   0.0106092   0.088663    -0.0213233   -0.00817843  -0.0653881    0.00458783  -0.0907155   0.085329    0.00738353   -0.00691143  -0.0229062    0.0405649   0.0881288   0.185727      0.0115559   0.00288216  -0.0936897  -0.122506    0.0542539   -0.00940868  -0.0299548
 -0.265036     0.103317   -0.619559    -0.290315    -0.375856   -0.0574037   0.0182206   -0.304471    -0.0586402   -0.126356     0.204428    -0.110499    0.250565   -0.597133     -0.445042    -0.0244216   -0.159038   -0.199027   -0.190268     -0.390958   -0.100399     0.94154    -0.0348777   0.11369     -0.00840833  -0.441051
 -0.29593     -0.437927    0.270831     0.381667    -0.316877   -0.236015    0.44731      0.0748572    0.015729    -0.63873      0.0931733   -0.288994    0.0869468  -0.230372     -0.213318     0.191646    -0.0203374  -0.0135368  -0.0432585    -0.704252    0.196646     0.201919   -0.248628    0.291172    -0.074448    -0.159682
  0.402573     0.093255   -0.267708    -0.126047    -0.116812    0.113359    0.103625    -0.0695725   -0.0494026   -0.0146707    0.340749     0.232615    0.338885    0.122821     -0.369728     0.160874    -0.133986   -0.178102    0.240408      0.289739   -0.349208     0.0588634  -0.227021    0.610283     0.048403     0.640608
  0.118977    -0.553952   -0.359802    -0.726199    -0.198356   -0.0489316   0.393302     0.495866    -0.243512    -0.33459     -0.0992553   -0.0739973   0.0354162   0.257545     -0.394541    -0.570714    -0.292919    0.164299    0.0194698     0.263731    0.184509     0.0915734   0.532099    0.36841     -0.61463      1.03869
  0.28338      0.199988   -0.179574    -0.0412203    0.179178   -0.253367    0.55291      0.155627    -0.435834    -0.100718     0.261888     0.299074    0.927999   -0.520338     -0.133426     0.103164     0.528254   -0.201925   -0.806846      0.228179    0.0915013    0.635103    0.265258   -0.427525    -0.317835     0.0917185
  0.445082    -0.251324   -0.207076    -0.162869    -0.0135456   0.104243    0.0411827    0.531116     0.378683    -0.0765704    0.0114543    0.126269    0.342364   -0.0686652    -0.291537     0.543218     0.562234   -0.94623    -0.476012      0.187115    0.342097     0.361114   -0.348473    0.243207    -0.481282     0.294699[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.408110
[ Info: iteration 2, average log likelihood -1.408097
[ Info: iteration 3, average log likelihood -1.408085
[ Info: iteration 4, average log likelihood -1.408073
[ Info: iteration 5, average log likelihood -1.408060
[ Info: iteration 6, average log likelihood -1.408048
[ Info: iteration 7, average log likelihood -1.408036
[ Info: iteration 8, average log likelihood -1.408024
[ Info: iteration 9, average log likelihood -1.408013
[ Info: iteration 10, average log likelihood -1.408002
┌ Info: EM with 100000 data points 10 iterations avll -1.408002
└ 59.0 data points per parameter
kind full, method kmeans
[ Info: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.973397e+05
      1       6.987797e+05      -2.985600e+05 |       31
      2       6.883970e+05      -1.038267e+04 |       31
      3       6.842510e+05      -4.146009e+03 |       31
      4       6.818014e+05      -2.449573e+03 |       31
      5       6.801421e+05      -1.659351e+03 |       31
      6       6.789350e+05      -1.207070e+03 |       31
      7       6.780952e+05      -8.398221e+02 |       31
      8       6.774397e+05      -6.554659e+02 |       31
      9       6.769106e+05      -5.290632e+02 |       31
     10       6.764610e+05      -4.496480e+02 |       31
     11       6.760854e+05      -3.755373e+02 |       31
     12       6.757689e+05      -3.165698e+02 |       31
     13       6.755013e+05      -2.675357e+02 |       31
     14       6.752449e+05      -2.564535e+02 |       31
     15       6.749896e+05      -2.553073e+02 |       31
     16       6.747547e+05      -2.348375e+02 |       31
     17       6.745564e+05      -1.983864e+02 |       31
     18       6.743877e+05      -1.686296e+02 |       31
     19       6.742323e+05      -1.554164e+02 |       31
     20       6.740953e+05      -1.369751e+02 |       31
     21       6.739747e+05      -1.206845e+02 |       31
     22       6.738545e+05      -1.201180e+02 |       31
     23       6.737471e+05      -1.074243e+02 |       31
     24       6.736540e+05      -9.315301e+01 |       31
     25       6.735769e+05      -7.707367e+01 |       31
     26       6.734988e+05      -7.813056e+01 |       31
     27       6.734165e+05      -8.221046e+01 |       31
     28       6.733327e+05      -8.382616e+01 |       31
     29       6.732515e+05      -8.118767e+01 |       31
     30       6.731756e+05      -7.592871e+01 |       31
     31       6.731041e+05      -7.154043e+01 |       31
     32       6.730330e+05      -7.103063e+01 |       31
     33       6.729626e+05      -7.042522e+01 |       31
     34       6.729018e+05      -6.078712e+01 |       31
     35       6.728293e+05      -7.247009e+01 |       31
     36       6.727600e+05      -6.936888e+01 |       31
     37       6.727013e+05      -5.868645e+01 |       31
     38       6.726474e+05      -5.386114e+01 |       31
     39       6.725947e+05      -5.272497e+01 |       31
     40       6.725404e+05      -5.433082e+01 |       31
     41       6.724919e+05      -4.844647e+01 |       31
     42       6.724492e+05      -4.270799e+01 |       31
     43       6.724122e+05      -3.706246e+01 |       31
     44       6.723798e+05      -3.235789e+01 |       31
     45       6.723472e+05      -3.263635e+01 |       31
     46       6.723194e+05      -2.774915e+01 |       31
     47       6.722922e+05      -2.720973e+01 |       31
     48       6.722600e+05      -3.223139e+01 |       31
     49       6.722239e+05      -3.605997e+01 |       31
     50       6.721936e+05      -3.034634e+01 |       31
K-means terminated without convergence after 50 iterations (objv = 672193.5656495993)
┌ Info: K-means with 32000 data points using 50 iterations
└ 37.0 data points per parameter
ERROR: LoadError: LoadError: UndefVarError: ind2sub not defined
Stacktrace:
 [1] sanitycheck!(::GMM{Float64,Array{Float64,2}}) at /home/pkgeval/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:54
 [2] GMMk(::Int64, ::Array{Float64,2}; kind::Symbol, nInit::Int64, nIter::Int64, sparse::Int64) at /home/pkgeval/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:140
 [3] #GMM#7 at /home/pkgeval/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:36 [inlined]
 [4] top-level scope at /home/pkgeval/.julia/packages/GaussianMixtures/RGtTJ/test/train.jl:29
 [5] include(::String) at ./client.jl:441
 [6] top-level scope at /home/pkgeval/.julia/packages/GaussianMixtures/RGtTJ/test/runtests.jl:7
 [7] include(::String) at ./client.jl:441
 [8] top-level scope at none:6
in expression starting at /home/pkgeval/.julia/packages/GaussianMixtures/RGtTJ/test/train.jl:22
in expression starting at /home/pkgeval/.julia/packages/GaussianMixtures/RGtTJ/test/runtests.jl:7
ERROR: Package GaussianMixtures errored during testing
Stacktrace:
 [1] pkgerror(::String, ::Vararg{String,N} where N) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Pkg/src/Types.jl:53
 [2] test(::Pkg.Types.Context, ::Array{Pkg.Types.PackageSpec,1}; coverage::Bool, julia_args::Cmd, test_args::Cmd, test_fn::Nothing) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Pkg/src/Operations.jl:1503
 [3] test(::Pkg.Types.Context, ::Array{Pkg.Types.PackageSpec,1}; coverage::Bool, test_fn::Nothing, julia_args::Cmd, test_args::Cmd, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:316
 [4] test(::Pkg.Types.Context, ::Array{Pkg.Types.PackageSpec,1}) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:303
 [5] #test#68 at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:297 [inlined]
 [6] test at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:297 [inlined]
 [7] #test#67 at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:296 [inlined]
 [8] test at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:296 [inlined]
 [9] test(::String; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:295
 [10] test(::String) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:295
 [11] top-level scope at none:13
