Julia Version 1.5.0-DEV.252
Commit b52be3dc7e (2020-02-08 18:26 UTC)
Platform Info:
  OS: Linux (x86_64-linux-gnu)
  CPU: Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-9.0.1 (ORCJIT, skylake)
Environment:
  JULIA_DEPOT_PATH = ::/usr/local/share/julia
  JULIA_NUM_THREADS = 2

  Resolving package versions...
  Installed GaussianMixtures ─── v0.3.0
  Installed PDMats ───────────── v0.9.11
  Installed SortingAlgorithms ── v0.3.1
  Installed JLD ──────────────── v0.9.2
  Installed Rmath ────────────── v0.6.0
  Installed NearestNeighbors ─── v0.4.4
  Installed OrderedCollections ─ v1.1.0
  Installed QuadGK ───────────── v2.3.1
  Installed BinDeps ──────────── v1.0.0
  Installed Distances ────────── v0.8.2
  Installed Parameters ───────── v0.12.0
  Installed BinaryProvider ───── v0.5.8
  Installed StatsFuns ────────── v0.9.3
  Installed FillArrays ───────── v0.8.4
  Installed DataStructures ───── v0.17.9
  Installed Arpack ───────────── v0.4.0
  Installed OpenBLAS_jll ─────── v0.3.7+5
  Installed Missings ─────────── v0.4.3
  Installed StaticArrays ─────── v0.12.1
  Installed CMakeWrapper ─────── v0.2.3
  Installed ScikitLearnBase ──── v0.5.0
  Installed Compat ───────────── v2.2.0
  Installed SpecialFunctions ─── v0.9.0
  Installed LegacyStrings ────── v0.4.1
  Installed OpenSpecFun_jll ──── v0.5.3+1
  Installed DataAPI ──────────── v1.1.0
  Installed CMake ────────────── v1.1.2
  Installed Blosc ────────────── v0.5.1
  Installed FileIO ───────────── v1.2.1
  Installed HDF5 ─────────────── v0.12.5
  Installed Clustering ───────── v0.13.3
  Installed Distributions ────── v0.22.4
  Installed Arpack_jll ───────── v3.5.0+2
  Installed URIParser ────────── v0.4.0
  Installed StatsBase ────────── v0.32.0
#=#=#                                                                         #                                                                          2.6%#####                                                                      8.2%###########                                                               15.9%####################                                                      28.0%################################                                          45.0%###################################################                       70.9%#####################################################################     96.5%######################################################################## 100.0%
#=#=#                                                                         ##O#- #                                                                       ##O=#  #                                                                      #=#=-#  #                                                                     -#O#- #   #                                                                   -=#=#   #   #                                                                 ######################################################################## 100.0%
#=#=#                                                                         ######################################################################## 100.0%
   Updating `~/.julia/environments/v1.5/Project.toml`
  [cc18c42c] + GaussianMixtures v0.3.0
   Updating `~/.julia/environments/v1.5/Manifest.toml`
  [7d9fca2a] + Arpack v0.4.0
  [68821587] + Arpack_jll v3.5.0+2
  [9e28174c] + BinDeps v1.0.0
  [b99e7846] + BinaryProvider v0.5.8
  [a74b3585] + Blosc v0.5.1
  [631607c0] + CMake v1.1.2
  [d5fb7624] + CMakeWrapper v0.2.3
  [aaaa29a8] + Clustering v0.13.3
  [34da2185] + Compat v2.2.0
  [9a962f9c] + DataAPI v1.1.0
  [864edb3b] + DataStructures v0.17.9
  [b4f34e82] + Distances v0.8.2
  [31c24e10] + Distributions v0.22.4
  [5789e2e9] + FileIO v1.2.1
  [1a297f60] + FillArrays v0.8.4
  [cc18c42c] + GaussianMixtures v0.3.0
  [f67ccb44] + HDF5 v0.12.5
  [4138dd39] + JLD v0.9.2
  [1b4a561d] + LegacyStrings v0.4.1
  [e1d29d7a] + Missings v0.4.3
  [b8a86587] + NearestNeighbors v0.4.4
  [4536629a] + OpenBLAS_jll v0.3.7+5
  [efe28fd5] + OpenSpecFun_jll v0.5.3+1
  [bac558e1] + OrderedCollections v1.1.0
  [90014a1f] + PDMats v0.9.11
  [d96e819e] + Parameters v0.12.0
  [1fd47b50] + QuadGK v2.3.1
  [79098fc4] + Rmath v0.6.0
  [6e75b9c4] + ScikitLearnBase v0.5.0
  [a2af1166] + SortingAlgorithms v0.3.1
  [276daf66] + SpecialFunctions v0.9.0
  [90137ffa] + StaticArrays v0.12.1
  [2913bbd2] + StatsBase v0.32.0
  [4c63d2b9] + StatsFuns v0.9.3
  [30578b45] + URIParser v0.4.0
  [2a0f44e3] + Base64 
  [ade2ca70] + Dates 
  [8bb1440f] + DelimitedFiles 
  [8ba89e20] + Distributed 
  [b77e0a4c] + InteractiveUtils 
  [76f85450] + LibGit2 
  [8f399da3] + Libdl 
  [37e2e46d] + LinearAlgebra 
  [56ddb016] + Logging 
  [d6f4376e] + Markdown 
  [a63ad114] + Mmap 
  [44cfe95a] + Pkg 
  [de0858da] + Printf 
  [3fa0cd96] + REPL 
  [9a3f8284] + Random 
  [ea8e919c] + SHA 
  [9e88b42a] + Serialization 
  [1a1011a3] + SharedArrays 
  [6462fe0b] + Sockets 
  [2f01184e] + SparseArrays 
  [10745b16] + Statistics 
  [4607b0f0] + SuiteSparse 
  [8dfed614] + Test 
  [cf7118a7] + UUIDs 
  [4ec0a83e] + Unicode 
   Building Rmath → `~/.julia/packages/Rmath/BoBag/deps/build.log`
   Building CMake → `~/.julia/packages/CMake/nSK2r/deps/build.log`
   Building Blosc → `~/.julia/packages/Blosc/lzFr0/deps/build.log`
   Building HDF5 ─→ `~/.julia/packages/HDF5/Zh9on/deps/build.log`
    Testing GaussianMixtures
Status `/tmp/jl_VTxWLh/Manifest.toml`
  [7d9fca2a] Arpack v0.4.0
  [68821587] Arpack_jll v3.5.0+2
  [9e28174c] BinDeps v1.0.0
  [b99e7846] BinaryProvider v0.5.8
  [a74b3585] Blosc v0.5.1
  [631607c0] CMake v1.1.2
  [d5fb7624] CMakeWrapper v0.2.3
  [aaaa29a8] Clustering v0.13.3
  [34da2185] Compat v2.2.0
  [9a962f9c] DataAPI v1.1.0
  [864edb3b] DataStructures v0.17.9
  [b4f34e82] Distances v0.8.2
  [31c24e10] Distributions v0.22.4
  [5789e2e9] FileIO v1.2.1
  [1a297f60] FillArrays v0.8.4
  [cc18c42c] GaussianMixtures v0.3.0
  [f67ccb44] HDF5 v0.12.5
  [4138dd39] JLD v0.9.2
  [1b4a561d] LegacyStrings v0.4.1
  [e1d29d7a] Missings v0.4.3
  [b8a86587] NearestNeighbors v0.4.4
  [4536629a] OpenBLAS_jll v0.3.7+5
  [efe28fd5] OpenSpecFun_jll v0.5.3+1
  [bac558e1] OrderedCollections v1.1.0
  [90014a1f] PDMats v0.9.11
  [d96e819e] Parameters v0.12.0
  [1fd47b50] QuadGK v2.3.1
  [79098fc4] Rmath v0.6.0
  [6e75b9c4] ScikitLearnBase v0.5.0
  [a2af1166] SortingAlgorithms v0.3.1
  [276daf66] SpecialFunctions v0.9.0
  [90137ffa] StaticArrays v0.12.1
  [2913bbd2] StatsBase v0.32.0
  [4c63d2b9] StatsFuns v0.9.3
  [30578b45] URIParser v0.4.0
  [2a0f44e3] Base64 
  [ade2ca70] Dates 
  [8bb1440f] DelimitedFiles 
  [8ba89e20] Distributed 
  [b77e0a4c] InteractiveUtils 
  [76f85450] LibGit2 
  [8f399da3] Libdl 
  [37e2e46d] LinearAlgebra 
  [56ddb016] Logging 
  [d6f4376e] Markdown 
  [a63ad114] Mmap 
  [44cfe95a] Pkg 
  [de0858da] Printf 
  [3fa0cd96] REPL 
  [9a3f8284] Random 
  [ea8e919c] SHA 
  [9e88b42a] Serialization 
  [1a1011a3] SharedArrays 
  [6462fe0b] Sockets 
  [2f01184e] SparseArrays 
  [10745b16] Statistics 
  [4607b0f0] SuiteSparse 
  [8dfed614] Test 
  [cf7118a7] UUIDs 
  [4ec0a83e] Unicode 
[ Info: Testing Data
(100000, -2.0663715705341098e6, [8364.687561633416, 91635.31243836657], [3040.236046639534 -391.5602456830729 1289.6164273223653; -3240.822739891776 932.5867976171759 -916.2658587997776], [[1387.2150420117255 1321.614515635675 1214.7241282745956; 1321.614515635675 11731.971406402958 1773.0671399775442; 1214.7241282745956 1773.0671399775442 7669.3064705568695], [98688.99472528206 -914.1194283761326 -1498.7568431563182; -914.1194283761326 88479.76877043686 -2163.46679551337; -1498.7568431563184 -2163.4667955133696 91855.80686680764]])
┌ Warning: rmprocs: process 1 not removed
└ @ Distributed /workspace/srcdir/usr/share/julia/stdlib/v1.5/Distributed/src/cluster.jl:1030
[ Info: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       2.457087e+03
      1       1.508661e+03      -9.484259e+02 |        6
      2       1.341968e+03      -1.666934e+02 |        6
      3       1.284338e+03      -5.762953e+01 |        0
      4       1.284338e+03       0.000000e+00 |        0
K-means converged with 4 iterations (objv = 1284.3381037966387)
┌ Info: K-means with 272 data points using 4 iterations
└ 11.3 data points per parameter
[ Info: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
┌ Info: EM with 272 data points 0 iterations avll -2.059872
└ 5.8 data points per parameter
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:221
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:221
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:221
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:221
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:631 [inlined]
└ @ Core ./broadcast.jl:631
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:230
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:230
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:631 [inlined]
└ @ Core ./broadcast.jl:631
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:631 [inlined]
└ @ Core ./broadcast.jl:631
[ Info: iteration 1, lowerbound -3.719499
[ Info: iteration 2, lowerbound -3.627700
[ Info: iteration 3, lowerbound -3.525665
[ Info: iteration 4, lowerbound -3.398606
[ Info: iteration 5, lowerbound -3.251386
[ Info: iteration 6, lowerbound -3.096245
[ Info: iteration 7, lowerbound -2.955112
[ Info: dropping number of Gaussions to 7
[ Info: iteration 8, lowerbound -2.838211
[ Info: dropping number of Gaussions to 5
[ Info: iteration 9, lowerbound -2.742398
[ Info: iteration 10, lowerbound -2.658367
[ Info: dropping number of Gaussions to 4
[ Info: iteration 11, lowerbound -2.577397
[ Info: iteration 12, lowerbound -2.492771
[ Info: iteration 13, lowerbound -2.418212
[ Info: iteration 14, lowerbound -2.359852
[ Info: iteration 15, lowerbound -2.324811
[ Info: dropping number of Gaussions to 3
[ Info: iteration 16, lowerbound -2.311364
[ Info: dropping number of Gaussions to 2
[ Info: iteration 17, lowerbound -2.302971
[ Info: iteration 18, lowerbound -2.299266
[ Info: iteration 19, lowerbound -2.299259
[ Info: iteration 20, lowerbound -2.299256
[ Info: iteration 21, lowerbound -2.299254
[ Info: iteration 22, lowerbound -2.299254
[ Info: iteration 23, lowerbound -2.299253
[ Info: iteration 24, lowerbound -2.299253
[ Info: iteration 25, lowerbound -2.299253
[ Info: iteration 26, lowerbound -2.299253
[ Info: iteration 27, lowerbound -2.299253
[ Info: iteration 28, lowerbound -2.299253
[ Info: iteration 29, lowerbound -2.299253
[ Info: iteration 30, lowerbound -2.299253
[ Info: iteration 31, lowerbound -2.299253
[ Info: iteration 32, lowerbound -2.299253
[ Info: iteration 33, lowerbound -2.299253
[ Info: iteration 34, lowerbound -2.299253
[ Info: iteration 35, lowerbound -2.299253
[ Info: iteration 36, lowerbound -2.299253
[ Info: iteration 37, lowerbound -2.299253
[ Info: iteration 38, lowerbound -2.299253
[ Info: iteration 39, lowerbound -2.299253
[ Info: iteration 40, lowerbound -2.299253
[ Info: iteration 41, lowerbound -2.299253
[ Info: iteration 42, lowerbound -2.299253
[ Info: iteration 43, lowerbound -2.299253
[ Info: iteration 44, lowerbound -2.299253
[ Info: iteration 45, lowerbound -2.299253
[ Info: iteration 46, lowerbound -2.299253
[ Info: iteration 47, lowerbound -2.299253
[ Info: iteration 48, lowerbound -2.299253
[ Info: iteration 49, lowerbound -2.299253
[ Info: iteration 50, lowerbound -2.299253
[ Info: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
History[Sun Feb  9 04:40:56 2020: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
, Sun Feb  9 04:41:04 2020: K-means with 272 data points using 4 iterations
11.3 data points per parameter
, Sun Feb  9 04:41:06 2020: EM with 272 data points 0 iterations avll -2.059872
5.8 data points per parameter
, Sun Feb  9 04:41:08 2020: GMM converted to Variational GMM
, Sun Feb  9 04:41:17 2020: iteration 1, lowerbound -3.719499
, Sun Feb  9 04:41:17 2020: iteration 2, lowerbound -3.627700
, Sun Feb  9 04:41:17 2020: iteration 3, lowerbound -3.525665
, Sun Feb  9 04:41:17 2020: iteration 4, lowerbound -3.398606
, Sun Feb  9 04:41:17 2020: iteration 5, lowerbound -3.251386
, Sun Feb  9 04:41:17 2020: iteration 6, lowerbound -3.096245
, Sun Feb  9 04:41:17 2020: iteration 7, lowerbound -2.955112
, Sun Feb  9 04:41:17 2020: dropping number of Gaussions to 7
, Sun Feb  9 04:41:17 2020: iteration 8, lowerbound -2.838211
, Sun Feb  9 04:41:17 2020: dropping number of Gaussions to 5
, Sun Feb  9 04:41:17 2020: iteration 9, lowerbound -2.742398
, Sun Feb  9 04:41:17 2020: iteration 10, lowerbound -2.658367
, Sun Feb  9 04:41:17 2020: dropping number of Gaussions to 4
, Sun Feb  9 04:41:17 2020: iteration 11, lowerbound -2.577397
, Sun Feb  9 04:41:17 2020: iteration 12, lowerbound -2.492771
, Sun Feb  9 04:41:17 2020: iteration 13, lowerbound -2.418212
, Sun Feb  9 04:41:17 2020: iteration 14, lowerbound -2.359852
, Sun Feb  9 04:41:17 2020: iteration 15, lowerbound -2.324811
, Sun Feb  9 04:41:17 2020: dropping number of Gaussions to 3
, Sun Feb  9 04:41:17 2020: iteration 16, lowerbound -2.311364
, Sun Feb  9 04:41:17 2020: dropping number of Gaussions to 2
, Sun Feb  9 04:41:17 2020: iteration 17, lowerbound -2.302971
, Sun Feb  9 04:41:17 2020: iteration 18, lowerbound -2.299266
, Sun Feb  9 04:41:17 2020: iteration 19, lowerbound -2.299259
, Sun Feb  9 04:41:17 2020: iteration 20, lowerbound -2.299256
, Sun Feb  9 04:41:17 2020: iteration 21, lowerbound -2.299254
, Sun Feb  9 04:41:17 2020: iteration 22, lowerbound -2.299254
, Sun Feb  9 04:41:17 2020: iteration 23, lowerbound -2.299253
, Sun Feb  9 04:41:17 2020: iteration 24, lowerbound -2.299253
, Sun Feb  9 04:41:17 2020: iteration 25, lowerbound -2.299253
, Sun Feb  9 04:41:17 2020: iteration 26, lowerbound -2.299253
, Sun Feb  9 04:41:17 2020: iteration 27, lowerbound -2.299253
, Sun Feb  9 04:41:17 2020: iteration 28, lowerbound -2.299253
, Sun Feb  9 04:41:17 2020: iteration 29, lowerbound -2.299253
, Sun Feb  9 04:41:17 2020: iteration 30, lowerbound -2.299253
, Sun Feb  9 04:41:17 2020: iteration 31, lowerbound -2.299253
, Sun Feb  9 04:41:17 2020: iteration 32, lowerbound -2.299253
, Sun Feb  9 04:41:17 2020: iteration 33, lowerbound -2.299253
, Sun Feb  9 04:41:17 2020: iteration 34, lowerbound -2.299253
, Sun Feb  9 04:41:17 2020: iteration 35, lowerbound -2.299253
, Sun Feb  9 04:41:17 2020: iteration 36, lowerbound -2.299253
, Sun Feb  9 04:41:17 2020: iteration 37, lowerbound -2.299253
, Sun Feb  9 04:41:17 2020: iteration 38, lowerbound -2.299253
, Sun Feb  9 04:41:17 2020: iteration 39, lowerbound -2.299253
, Sun Feb  9 04:41:17 2020: iteration 40, lowerbound -2.299253
, Sun Feb  9 04:41:17 2020: iteration 41, lowerbound -2.299253
, Sun Feb  9 04:41:17 2020: iteration 42, lowerbound -2.299253
, Sun Feb  9 04:41:17 2020: iteration 43, lowerbound -2.299253
, Sun Feb  9 04:41:17 2020: iteration 44, lowerbound -2.299253
, Sun Feb  9 04:41:18 2020: iteration 45, lowerbound -2.299253
, Sun Feb  9 04:41:18 2020: iteration 46, lowerbound -2.299253
, Sun Feb  9 04:41:18 2020: iteration 47, lowerbound -2.299253
, Sun Feb  9 04:41:18 2020: iteration 48, lowerbound -2.299253
, Sun Feb  9 04:41:18 2020: iteration 49, lowerbound -2.299253
, Sun Feb  9 04:41:18 2020: iteration 50, lowerbound -2.299253
, Sun Feb  9 04:41:18 2020: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [95.95490777398598, 178.04509222601402]
β = [95.95490777398598, 178.04509222601402]
m = [2.000229257775369 53.85198717246128; 4.250300733269908 79.28686694436182]
ν = [97.95490777398598, 180.04509222601402]
W = LinearAlgebra.UpperTriangular{Float64,Array{Float64,2}}[[0.37587636119484347 -0.008953123827346131; 0.0 0.012748664777409349], [0.18404155547484516 -0.007644049042327527; 0.0 0.00858170516633356]]
┌ Warning: Assignment to `x` in soft scope is ambiguous because a global variable by the same name exists: `x` will be treated as a new local. Disambiguate by using `local x` to suppress this warning or `global x` to assign to the existing global variable.
└ @ ~/.julia/packages/GaussianMixtures/RGtTJ/test/train.jl:7
┌ Warning: Assignment to `p` in soft scope is ambiguous because a global variable by the same name exists: `p` will be treated as a new local. Disambiguate by using `local p` to suppress this warning or `global p` to assign to the existing global variable.
└ @ ~/.julia/packages/GaussianMixtures/RGtTJ/test/train.jl:17
Kind: diag, size256
nx: 100000 sum(zeroth order stats): 100000.0
avll from stats: -1.0032695609510944
avll from llpg:  -1.0032695609510838
avll direct:     -1.0032695609510838
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 100000.0
avll from stats: -1.0142924126267272
avll from llpg:  -1.0142924126267272
avll direct:     -1.0142924126267272
sum posterior: 100000.0
┌ Warning: Assignment to `x` in soft scope is ambiguous because a global variable by the same name exists: `x` will be treated as a new local. Disambiguate by using `local x` to suppress this warning or `global x` to assign to the existing global variable.
└ @ ~/.julia/packages/GaussianMixtures/RGtTJ/test/train.jl:26
32×26 Array{Float64,2}:
 -0.0338904   -0.0591169   -0.0181498    0.0394238     0.135371    -0.0440513    0.0146791    0.161269     0.0245172    0.0629827   -0.140337    -0.0314131   -0.0689173   -0.183609    -0.00406419  -0.123496      0.0491095    0.108611     0.0816708   -0.0779916   -0.268948     -0.0725541    0.0602518     0.0260733   -0.119553    -0.0117667
  0.0122182    0.0897501    0.0126207    0.159147     -0.0213388   -0.0206721    0.00496389   0.0768614    0.033011     0.0365264    0.0670593    0.00933516   0.136269     0.0131136   -0.14139      0.00253593    0.0756712    0.072841     0.151979    -0.04868     -0.0150249    -0.0573896    0.0544162    -0.0110982   -0.0144899    0.0989805
  0.0486839    0.0690956    0.138719     0.186546      0.145938    -0.0996303   -0.0125265    0.0848488   -0.0704336    0.142947    -0.0626418   -0.0354147    0.134914    -0.0713502    0.00993359   0.00193454    0.0904991   -0.0742984   -0.121739     0.0215386    0.0436124    -0.0416876   -0.00746355   -0.117019    -0.0834073    0.042132
 -0.00697019   0.0175056    0.190604     0.184008      0.0141004    0.017244     0.138975    -0.125936    -0.0194973    0.0188424    0.0758395    0.05952      0.0937161   -0.113523    -0.136971    -0.0512489     0.0184888   -0.0727084   -0.0135754    0.0407452   -0.0445311    -0.00330821  -0.111632     -0.138394     0.126396     0.167962
 -0.00378184   0.204238     0.00335362  -0.119649     -0.0298441    0.172399     0.0662606   -0.011412    -0.0522574    0.0662253   -0.00590549   0.118242     0.0949384    0.00226936   0.0871535   -0.190554      0.0401068   -0.0662107   -0.20574     -0.00638641  -0.0269881    -0.0942803    0.0296092    -0.0596973    0.0770427   -0.128445
 -0.113402     0.0401887   -0.0113889   -0.0744436     0.18543     -0.0201871    0.0794577   -0.0141072   -0.00554711   0.0182973    0.0230574    0.00927724   0.0302896   -0.182011    -0.0249772   -0.142335      0.0613943    0.0769517    0.0498865   -0.0543797   -0.0953436     0.0517791   -0.0354364     0.052166    -0.121783     0.0545541
 -0.00442809  -0.0488231    0.057512    -0.0408316    -0.114896     0.122836    -0.0987311   -0.0289465   -0.0242181   -0.0417619    0.158227     0.17806     -0.0886223   -0.0997374   -0.0503206   -0.0645786    -0.0572755    0.183302     0.0295169    0.0262438   -0.0654234     0.0698607    0.0813876    -0.0159338    0.155629    -0.218579
  0.148861    -0.16716     -0.00552624   0.109811     -0.0683222    0.146431    -0.100158     0.0817494    0.104903    -0.0403788    0.14971      0.0366764   -0.0267753   -0.0262876   -0.0265357    0.127525      0.0421179    0.081216    -0.0212731    0.0114254    0.324409      0.026909     0.120903      0.225858    -0.136662     0.144012
  0.0290563   -0.131885     0.0439193    0.023006     -0.0443206    0.0513401    0.0862459   -0.238724     0.0838114   -0.153931     0.151456    -0.0953644   -0.0107977    0.0793491   -0.134763     0.078726     -0.00914152   0.131684     0.0876605   -0.220436     0.0654246     0.0100698   -0.0947869     0.0669964   -0.00436924   0.0581235
  0.0650905   -0.144657     0.0665702   -0.130341      0.0465655   -0.19139     -0.017806    -0.0495222    0.0114716    0.136553     0.228967    -0.09077     -0.0202352    0.0653832   -0.0449407   -0.0524136     0.137526    -0.0173892   -0.00532883  -0.0859025    0.129141     -0.00784409  -0.0391785     0.0427118   -0.00248249   0.139285
 -0.0572999    0.0141338    0.0674241    0.000125044   0.176636    -0.0684035    0.167735    -0.0759473   -0.119381    -0.0162081   -0.0494238   -0.0615752   -0.0162791   -0.0373551    0.223483     0.0626453     0.061596     0.0349062    0.0591038   -0.135269    -0.0307057    -0.0547573   -0.0238521    -0.0945211    0.00259647   0.103243
 -0.00480797  -0.0558521   -0.0653554   -0.199001     -0.032357    -0.22472     -0.203928    -0.0280854   -0.0109946    0.019164    -0.044961     0.037591     0.0289798    0.298674    -0.0582423    0.074519      0.0603226    0.174096    -0.110889    -0.0715908   -0.000663929   0.105377    -0.0434345     0.0338553    0.0534538   -0.00531877
  0.307398     0.108455     0.0244574    0.0022167    -0.0340321   -0.0877057    0.0788992    0.0888053    0.0509555    0.00866632   0.106773     0.173325     0.119096    -0.16271      0.0400423    0.00733948    0.030825     0.0173492   -0.0907648   -0.00313224  -0.106806      0.0986081   -0.148994      0.00588195   0.231501     0.0791802
 -0.0467497    0.0729553   -0.13189     -0.0979642    -0.18948     -0.0473168    0.045597    -0.142189    -0.00234924   0.113374     0.0739239   -0.250047     0.173843     0.0774496    0.0453422   -0.285973      0.109354     0.0861637   -0.0264622   -0.0991425    0.161657     -0.155546     0.0130028    -0.139304     0.102783     0.0277314
 -0.124363     0.0621184    0.12547     -0.00177039   -0.158034    -0.0227827   -0.0702546    0.0325891    0.0527595    0.0161217   -0.00578322   0.0178549   -0.161546    -0.0177426   -0.148582     0.0143008    -0.0182241    0.177002    -0.00538342   0.22091      0.0914753     0.10896      0.0566866     0.0175035    0.0537626    0.0847851
 -0.00473132  -0.157678    -0.133496     0.0950318     0.0857179   -0.0537113   -0.197736     0.0911017   -0.123431     0.110725     0.0256794    0.111662     0.0831499   -0.172275     0.0160345   -0.136648      0.202546     0.0962915    0.0149452    0.0514637   -0.0500152     0.048024     0.18618       0.0564018    0.0521535    0.128516
  0.108178    -0.0594892   -0.111426     0.137037      0.106655    -0.0370097   -0.0357975    0.0689606    0.0543215   -0.103719    -0.0930202   -0.0312219   -0.053332     0.0315346    0.154714     0.048795     -0.0581361    0.115504    -0.0548109    0.133771     0.0855632    -0.0478014    0.0180597    -0.123805    -0.129278     0.0967832
 -0.0603162    0.196342     0.113662    -0.0362614     0.0483442    0.125677    -0.130933    -0.0115857    0.0755717    0.0818659    0.202702    -0.129477     0.042557     0.0894107   -0.0195008    0.000712598   0.109405    -0.123454     0.0301351    0.0405239    0.110344      0.0137634    0.142173     -0.0624362   -0.0251658   -0.166382
 -0.061834     0.105292    -0.00253046   0.0875313     0.0501535    0.117479    -0.0269327   -0.0811383    0.133365     0.178619     0.0279639   -0.0241712    0.125858     0.0359636   -0.208279     0.18006      -0.14965      0.146271    -0.00669587  -0.0321853   -0.0889083    -0.00718176   0.145427      0.0702805   -0.0703247   -0.149201
 -0.00437564   0.00953151   0.0289324   -0.0979595     0.07041     -0.0636046   -0.0342962   -0.026894    -0.106595     0.0144088   -0.101356    -0.0562583   -2.45022e-5  -0.112669    -0.160371    -0.0251111     0.206636     0.114145     0.0165583    0.00846655  -0.0891715     0.0598367   -0.00482816    0.0752413   -0.0538377    0.0538854
 -0.191285    -0.0182691    0.0647963   -0.299804      0.0734875   -0.134872    -0.0592136    0.028256     0.0290203   -0.245429    -0.0248523   -0.00817152  -0.044255    -0.0196066    0.10399     -0.0362583    -0.186388     0.0948963    0.186027     0.0173034   -0.0693676    -0.0381172    0.0877987     0.0739803   -0.136635     0.13478
 -0.0695337    0.157462     0.0518166    0.0190062     0.0721202    0.0998485   -0.0874308    0.104138     0.0692592    0.120301    -0.0922513    0.0389069   -0.132969    -0.0140868    0.0138688    0.146859     -0.0301462   -0.043358     0.0355198   -0.194433     0.00252343    0.113945     0.180638      0.023406     0.0593056   -0.0135154
  0.08343     -0.146691     0.0289259    0.0775673     0.161763    -0.00407549  -0.0626157    0.042034     0.0527088    0.133789     0.0545933    0.065456     0.246643     0.0748748    0.0767927    0.0490464    -0.141111    -0.0695823   -0.0892618    0.0986966    0.25947       0.0346366    0.0318437    -0.0620884   -0.0813419    0.0503448
 -0.00720801  -0.0375488    0.0636864   -0.123342     -0.125971    -0.045958     0.105469    -0.168288     0.0438147    0.186356    -0.0114349    0.00283272   0.0211049    0.254472    -0.0192728    0.00162141    0.0114171   -0.0382404    0.0852748   -0.0389441   -0.18147      -0.0684013    0.0247513    -0.0924495   -0.0378504   -0.018384
 -0.0484463    0.0987004    0.0670339   -0.101631     -0.0972176   -0.0981566    0.165409    -0.0903653   -0.0372633    0.16228     -0.159065     0.0435447    0.105584     0.103221    -0.00747716  -0.0129472     0.0425067   -0.0624247    0.00984405  -0.00860643  -0.0902279    -0.0936967    0.0129211     0.0114504    0.0733717    0.00226191
  0.262877     0.0335421    0.097341     0.0869748    -0.114367    -0.0242398    0.119123    -0.0268335    0.236787    -0.0797869   -0.0294533   -0.161533    -0.199549    -0.0383331    0.205614    -0.0516655     0.123355     0.133233    -0.146703     0.0318454   -0.08035      -0.230874     0.0745671     0.0236308    0.0544766    0.114746
 -0.0565946    0.0344605   -0.0520425    0.0410685    -0.00613042   0.0183041    0.0834385    0.0594996   -0.0143618   -0.0344306    0.042031     0.0334838   -0.168425    -0.0978889    0.0144551   -0.0273624    -0.1503       0.00792321   0.0699015    0.0735408    0.182154      0.0748965   -0.0363903     0.0342646   -0.0703588   -0.0773674
 -0.0423818   -0.175886     0.119567    -0.0222078     0.0439273    0.0737433   -0.00880249   0.110847     0.00536986   0.173804     0.00575959   0.12394      0.0774182   -0.0654396   -0.0737915   -0.0691648     0.188454     0.1878       0.330265     0.0259437   -0.00649663   -0.032617    -0.0705202    -0.11519      0.0460933    0.0233432
  0.0208352   -0.0506829   -0.211306    -0.0271469    -0.252758    -0.0622371    0.0165464   -0.0577064    0.100928     0.0612233   -0.0599219    0.0374868   -0.0173598    0.115046     0.0707871   -0.0409376     0.140139    -0.0353632   -0.0385873    0.0147371   -0.182903      0.0609714    0.0307852    -0.013393    -0.234033     0.0370092
 -0.0278016   -0.100037    -0.0748416    0.0642566     0.103934    -0.143765    -0.0173206    0.127347    -0.134104     0.08834      0.0825052    0.0456234    0.219958     0.0341295   -0.086679     0.00788764   -0.0869321    0.0108084   -0.0280767   -0.0756995    0.0479425     0.113231     0.0893338     0.114184     0.00677367   0.095221
 -0.045583    -0.0260031    0.0645035   -0.0436053    -0.0260009    0.0419041   -0.104105    -0.0563426   -0.0254488   -0.173626     0.0436308   -0.0424545    0.0953247    0.107175     0.0620376   -0.0850515    -0.0751655   -0.00227504  -0.0124374    0.137226    -0.0485041     0.240706    -0.00181851   -0.185976    -0.103073    -0.0989098
  0.0264262   -0.0580705    0.0197796    0.140521      0.00765617  -0.0231947    0.0245121    0.00556716   0.0318517   -0.189123    -0.00896427  -0.0271188    0.126307    -0.0214447    0.0797596    0.0341098    -0.0527625   -0.0324574   -0.00686705   0.0284725    0.025903     -0.100264    -0.000102709   0.101667     0.154153    -0.162289kind diag, method split
┌ Info: 0: avll = 
└   tll[1] = -1.4050478998868918
[ Info: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.405123
[ Info: iteration 2, average log likelihood -1.405055
[ Info: iteration 3, average log likelihood -1.404684
[ Info: iteration 4, average log likelihood -1.400214
[ Info: iteration 5, average log likelihood -1.385230
[ Info: iteration 6, average log likelihood -1.376232
[ Info: iteration 7, average log likelihood -1.373923
[ Info: iteration 8, average log likelihood -1.372879
[ Info: iteration 9, average log likelihood -1.372246
[ Info: iteration 10, average log likelihood -1.371816
[ Info: iteration 11, average log likelihood -1.371483
[ Info: iteration 12, average log likelihood -1.371198
[ Info: iteration 13, average log likelihood -1.370916
[ Info: iteration 14, average log likelihood -1.370631
[ Info: iteration 15, average log likelihood -1.370378
[ Info: iteration 16, average log likelihood -1.370169
[ Info: iteration 17, average log likelihood -1.370013
[ Info: iteration 18, average log likelihood -1.369905
[ Info: iteration 19, average log likelihood -1.369832
[ Info: iteration 20, average log likelihood -1.369781
[ Info: iteration 21, average log likelihood -1.369740
[ Info: iteration 22, average log likelihood -1.369699
[ Info: iteration 23, average log likelihood -1.369653
[ Info: iteration 24, average log likelihood -1.369591
[ Info: iteration 25, average log likelihood -1.369506
[ Info: iteration 26, average log likelihood -1.369397
[ Info: iteration 27, average log likelihood -1.369274
[ Info: iteration 28, average log likelihood -1.369153
[ Info: iteration 29, average log likelihood -1.369050
[ Info: iteration 30, average log likelihood -1.368968
[ Info: iteration 31, average log likelihood -1.368904
[ Info: iteration 32, average log likelihood -1.368853
[ Info: iteration 33, average log likelihood -1.368812
[ Info: iteration 34, average log likelihood -1.368779
[ Info: iteration 35, average log likelihood -1.368751
[ Info: iteration 36, average log likelihood -1.368728
[ Info: iteration 37, average log likelihood -1.368708
[ Info: iteration 38, average log likelihood -1.368689
[ Info: iteration 39, average log likelihood -1.368671
[ Info: iteration 40, average log likelihood -1.368653
[ Info: iteration 41, average log likelihood -1.368634
[ Info: iteration 42, average log likelihood -1.368611
[ Info: iteration 43, average log likelihood -1.368583
[ Info: iteration 44, average log likelihood -1.368547
[ Info: iteration 45, average log likelihood -1.368504
[ Info: iteration 46, average log likelihood -1.368461
[ Info: iteration 47, average log likelihood -1.368428
[ Info: iteration 48, average log likelihood -1.368408
[ Info: iteration 49, average log likelihood -1.368398
[ Info: iteration 50, average log likelihood -1.368392
┌ Info: EM with 100000 data points 50 iterations avll -1.368392
└ 952.4 data points per parameter
┌ Info: 1
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4051227296724729
│     -1.4050546857039314
│      ⋮
└     -1.368392053832907
[ Info: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.368521
[ Info: iteration 2, average log likelihood -1.368405
[ Info: iteration 3, average log likelihood -1.368035
[ Info: iteration 4, average log likelihood -1.364274
[ Info: iteration 5, average log likelihood -1.350912
[ Info: iteration 6, average log likelihood -1.337718
[ Info: iteration 7, average log likelihood -1.331265
[ Info: iteration 8, average log likelihood -1.328453
[ Info: iteration 9, average log likelihood -1.327350
[ Info: iteration 10, average log likelihood -1.326842
[ Info: iteration 11, average log likelihood -1.326526
[ Info: iteration 12, average log likelihood -1.326288
[ Info: iteration 13, average log likelihood -1.326078
[ Info: iteration 14, average log likelihood -1.325880
[ Info: iteration 15, average log likelihood -1.325689
[ Info: iteration 16, average log likelihood -1.325512
[ Info: iteration 17, average log likelihood -1.325359
[ Info: iteration 18, average log likelihood -1.325236
[ Info: iteration 19, average log likelihood -1.325144
[ Info: iteration 20, average log likelihood -1.325080
[ Info: iteration 21, average log likelihood -1.325037
[ Info: iteration 22, average log likelihood -1.325009
[ Info: iteration 23, average log likelihood -1.324992
[ Info: iteration 24, average log likelihood -1.324981
[ Info: iteration 25, average log likelihood -1.324974
[ Info: iteration 26, average log likelihood -1.324970
[ Info: iteration 27, average log likelihood -1.324967
[ Info: iteration 28, average log likelihood -1.324964
[ Info: iteration 29, average log likelihood -1.324962
[ Info: iteration 30, average log likelihood -1.324961
[ Info: iteration 31, average log likelihood -1.324960
[ Info: iteration 32, average log likelihood -1.324959
[ Info: iteration 33, average log likelihood -1.324958
[ Info: iteration 34, average log likelihood -1.324957
[ Info: iteration 35, average log likelihood -1.324956
[ Info: iteration 36, average log likelihood -1.324955
[ Info: iteration 37, average log likelihood -1.324955
[ Info: iteration 38, average log likelihood -1.324954
[ Info: iteration 39, average log likelihood -1.324953
[ Info: iteration 40, average log likelihood -1.324953
[ Info: iteration 41, average log likelihood -1.324952
[ Info: iteration 42, average log likelihood -1.324951
[ Info: iteration 43, average log likelihood -1.324950
[ Info: iteration 44, average log likelihood -1.324949
[ Info: iteration 45, average log likelihood -1.324948
[ Info: iteration 46, average log likelihood -1.324947
[ Info: iteration 47, average log likelihood -1.324946
[ Info: iteration 48, average log likelihood -1.324945
[ Info: iteration 49, average log likelihood -1.324943
[ Info: iteration 50, average log likelihood -1.324941
┌ Info: EM with 100000 data points 50 iterations avll -1.324941
└ 473.9 data points per parameter
┌ Info: 2
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.3685212702151217
│     -1.3684047609662278
│      ⋮
└     -1.3249406027328956
[ Info: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.325084
[ Info: iteration 2, average log likelihood -1.324937
[ Info: iteration 3, average log likelihood -1.324453
[ Info: iteration 4, average log likelihood -1.319686
[ Info: iteration 5, average log likelihood -1.305514
[ Info: iteration 6, average log likelihood -1.291638
[ Info: iteration 7, average log likelihood -1.284628
[ Info: iteration 8, average log likelihood -1.282017
[ Info: iteration 9, average log likelihood -1.280849
[ Info: iteration 10, average log likelihood -1.280149
[ Info: iteration 11, average log likelihood -1.279605
[ Info: iteration 12, average log likelihood -1.279106
[ Info: iteration 13, average log likelihood -1.278554
[ Info: iteration 14, average log likelihood -1.277791
[ Info: iteration 15, average log likelihood -1.276516
[ Info: iteration 16, average log likelihood -1.274564
[ Info: iteration 17, average log likelihood -1.272390
[ Info: iteration 18, average log likelihood -1.270612
[ Info: iteration 19, average log likelihood -1.269475
[ Info: iteration 20, average log likelihood -1.268809
[ Info: iteration 21, average log likelihood -1.268327
[ Info: iteration 22, average log likelihood -1.267850
[ Info: iteration 23, average log likelihood -1.267258
[ Info: iteration 24, average log likelihood -1.266417
[ Info: iteration 25, average log likelihood -1.265200
[ Info: iteration 26, average log likelihood -1.263718
[ Info: iteration 27, average log likelihood -1.262485
[ Info: iteration 28, average log likelihood -1.261792
[ Info: iteration 29, average log likelihood -1.261422
[ Info: iteration 30, average log likelihood -1.261249
[ Info: iteration 31, average log likelihood -1.261146
[ Info: iteration 32, average log likelihood -1.261055
[ Info: iteration 33, average log likelihood -1.260953
[ Info: iteration 34, average log likelihood -1.260828
[ Info: iteration 35, average log likelihood -1.260670
[ Info: iteration 36, average log likelihood -1.260456
[ Info: iteration 37, average log likelihood -1.260171
[ Info: iteration 38, average log likelihood -1.259799
[ Info: iteration 39, average log likelihood -1.259319
[ Info: iteration 40, average log likelihood -1.258793
[ Info: iteration 41, average log likelihood -1.258303
[ Info: iteration 42, average log likelihood -1.257953
[ Info: iteration 43, average log likelihood -1.257767
[ Info: iteration 44, average log likelihood -1.257673
[ Info: iteration 45, average log likelihood -1.257617
[ Info: iteration 46, average log likelihood -1.257576
[ Info: iteration 47, average log likelihood -1.257544
[ Info: iteration 48, average log likelihood -1.257517
[ Info: iteration 49, average log likelihood -1.257494
[ Info: iteration 50, average log likelihood -1.257476
┌ Info: EM with 100000 data points 50 iterations avll -1.257476
└ 236.4 data points per parameter
┌ Info: 3
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.32508390063034
│     -1.3249365984341954
│      ⋮
└     -1.2574762743037238
[ Info: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.257671
[ Info: iteration 2, average log likelihood -1.257408
[ Info: iteration 3, average log likelihood -1.255934
[ Info: iteration 4, average log likelihood -1.242691
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.217511
[ Info: iteration 6, average log likelihood -1.208092
[ Info: iteration 7, average log likelihood -1.191788
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.183623
[ Info: iteration 9, average log likelihood -1.187241
[ Info: iteration 10, average log likelihood -1.177866
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 11, average log likelihood -1.173383
[ Info: iteration 12, average log likelihood -1.180156
[ Info: iteration 13, average log likelihood -1.173895
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 14, average log likelihood -1.172002
[ Info: iteration 15, average log likelihood -1.179779
[ Info: iteration 16, average log likelihood -1.173719
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 17, average log likelihood -1.171870
[ Info: iteration 18, average log likelihood -1.179655
[ Info: iteration 19, average log likelihood -1.173594
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 20, average log likelihood -1.171720
[ Info: iteration 21, average log likelihood -1.179497
[ Info: iteration 22, average log likelihood -1.173454
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 23, average log likelihood -1.171573
[ Info: iteration 24, average log likelihood -1.179366
[ Info: iteration 25, average log likelihood -1.173364
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 26, average log likelihood -1.171495
[ Info: iteration 27, average log likelihood -1.179292
[ Info: iteration 28, average log likelihood -1.173324
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 29, average log likelihood -1.171463
[ Info: iteration 30, average log likelihood -1.179243
[ Info: iteration 31, average log likelihood -1.173297
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 32, average log likelihood -1.171443
[ Info: iteration 33, average log likelihood -1.179195
[ Info: iteration 34, average log likelihood -1.173265
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 35, average log likelihood -1.171415
[ Info: iteration 36, average log likelihood -1.179125
[ Info: iteration 37, average log likelihood -1.173199
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 38, average log likelihood -1.171342
[ Info: iteration 39, average log likelihood -1.178997
[ Info: iteration 40, average log likelihood -1.173069
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 41, average log likelihood -1.171221
[ Info: iteration 42, average log likelihood -1.178861
[ Info: iteration 43, average log likelihood -1.172965
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 44, average log likelihood -1.171158
[ Info: iteration 45, average log likelihood -1.178814
[ Info: iteration 46, average log likelihood -1.172936
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 47, average log likelihood -1.171145
[ Info: iteration 48, average log likelihood -1.178807
[ Info: iteration 49, average log likelihood -1.172931
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 50, average log likelihood -1.171143
┌ Info: EM with 100000 data points 50 iterations avll -1.171143
└ 118.1 data points per parameter
┌ Info: 4
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.2576706509765112
│     -1.2574076245942376
│      ⋮
└     -1.1711432343147319
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.179155
[ Info: iteration 2, average log likelihood -1.172895
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     15
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 3, average log likelihood -1.169655
[ Info: iteration 4, average log likelihood -1.160414
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     16
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.110961
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     15
│     19
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.074346
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      3
│     19
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.071613
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     15
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.080993
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     19
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.062579
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      3
│     15
│     19
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.044221
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 11, average log likelihood -1.070718
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     15
│     19
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 12, average log likelihood -1.048649
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      3
│     19
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 13, average log likelihood -1.049494
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     15
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 14, average log likelihood -1.064426
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 15, average log likelihood -1.056347
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      3
│     15
│     19
│     26
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 16, average log likelihood -1.038010
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 17, average log likelihood -1.073775
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     15
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 18, average log likelihood -1.050732
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      3
│     19
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 19, average log likelihood -1.045884
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     15
│     19
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 20, average log likelihood -1.062217
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 21, average log likelihood -1.059999
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      3
│     15
│     19
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 22, average log likelihood -1.040293
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 23, average log likelihood -1.070211
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     15
│     19
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 24, average log likelihood -1.048657
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      3
│     19
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 25, average log likelihood -1.049649
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     15
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 26, average log likelihood -1.064499
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 27, average log likelihood -1.056545
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      3
│     15
│     19
│     26
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 28, average log likelihood -1.038254
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 29, average log likelihood -1.073734
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     15
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 30, average log likelihood -1.050769
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      3
│     19
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 31, average log likelihood -1.045969
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     15
│     19
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 32, average log likelihood -1.062308
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 33, average log likelihood -1.059989
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      3
│     15
│     19
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 34, average log likelihood -1.040319
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 35, average log likelihood -1.070246
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     15
│     19
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 36, average log likelihood -1.048710
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      3
│     19
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 37, average log likelihood -1.049647
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     15
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 38, average log likelihood -1.064505
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 39, average log likelihood -1.056564
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      3
│     15
│     19
│     26
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 40, average log likelihood -1.038280
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 41, average log likelihood -1.073726
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     15
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 42, average log likelihood -1.050771
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      3
│     19
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 43, average log likelihood -1.045978
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     15
│     19
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 44, average log likelihood -1.062318
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 45, average log likelihood -1.059986
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      3
│     15
│     19
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 46, average log likelihood -1.040320
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 47, average log likelihood -1.070249
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     15
│     19
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 48, average log likelihood -1.048716
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      3
│     19
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 49, average log likelihood -1.049646
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     15
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 50, average log likelihood -1.064503
┌ Info: EM with 100000 data points 50 iterations avll -1.064503
└ 59.0 data points per parameter
┌ Info: 5
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.1791547770962048
│     -1.1728945154974462
│      ⋮
└     -1.06450344802272
┌ Info: Total log likelihood: 
│   tll =
│    251-element Array{Float64,1}:
│     -1.4050478998868918
│     -1.4051227296724729
│     -1.4050546857039314
│     -1.4046842011832765
│      ⋮
│     -1.0487164657535388
│     -1.0496455035799699
└     -1.06450344802272
32×26 Array{Float64,2}:
 -0.0133837    0.126289   -0.0900316   -0.113074    -0.102448      0.0663254    0.056068    -0.033208    -0.0356942    0.0834825    0.0313778   -0.0618918    0.133662     0.0333942   0.0613253   -0.217127     0.0533237   0.00352097  -0.12827     -0.0507708    0.0676215   -0.118338     0.0395463   -0.0888019     0.0910528   -0.0505911
  0.0650231   -0.0870924   0.0619283   -0.129546     0.0543558    -0.192805    -0.0176933   -0.0548199    0.01509      0.105167     0.226176    -0.0717482   -0.0137051    0.0682917  -0.0406906   -0.053812     0.145614   -0.0338809    0.0187447   -0.0709632    0.121284    -0.00562531  -0.0460988   -0.000699095   0.00569203   0.144172
 -0.018063     0.0112041   0.164849     0.211112     0.0320774     0.00990311   0.0984479   -0.11545     -0.00543509   0.00258282   0.0382921    0.0714801    0.0711686   -0.108931   -0.125631    -0.0456739    0.0231019  -0.046754    -0.0195776    0.0408408   -0.0373048    0.00933168  -0.107602    -0.140679      0.140469     0.161245
  0.0470732   -0.072519   -0.0244058    0.0874712   -0.0495651     0.0893151   -0.011807     0.0566689    0.0514526   -0.0423774    0.0969061    0.0214433   -0.104644    -0.0599227   0.00133865   0.0554752   -0.0488761   0.0624333   -0.00163951   0.0324667    0.277035     0.0437406    0.0499247    0.136193     -0.114334     0.0357449
  0.0220755   -0.0568893   0.0475902    0.00395683  -0.0383256     0.129212    -0.0827909    0.0924878   -0.0295309    0.0317804    0.165163     0.172279    -0.0502338   -0.101331   -0.0387205   -0.0682528   -0.176008    0.372847    -0.0341701    0.0360596   -0.0887631    0.0270137    0.0348581    0.0098754     0.188091    -0.915503
 -0.0294507   -0.0459385   0.0882503   -0.108085    -0.144072      0.107676    -0.189033    -0.150864    -0.0119716   -0.0768549    0.13596      0.182723    -0.0375324   -0.0896993  -0.0581055   -0.0905821    0.0716618   0.0796539    0.122949     0.0165564   -0.0371589    0.101741     0.108736    -0.0555918     0.12552      0.498272
  0.00683693  -0.0142316   0.0506833   -0.0974244    0.0713949    -0.0644995   -0.0249718   -0.07241     -0.105267     0.017625    -0.10307     -0.0544087    0.00105515  -0.11644    -0.160373    -0.0298931    0.228915    0.0924551    0.0147264    0.00922002  -0.102958     0.0520412   -0.00681045   0.0796448    -0.0417086    0.0630151
  0.303419     0.0897928   0.0687452   -0.00526722   0.0470957    -0.0886935    0.0754299    0.0718622    0.050246    -0.00866442   0.0970992    0.169215     0.120425    -0.145074    0.0411164    0.0185592    0.0340932   0.0125337   -0.0826355   -0.00395604  -0.0768375    0.0967767   -0.169917    -0.0170549     0.228425     0.0629902
 -0.158776     0.101127    0.065863     0.0827612   -0.0292941    -0.102758     0.146953    -0.0461049   -0.0338207    0.184708    -0.16467     -0.0487547    0.0754189    0.10461    -0.0924672   -0.00253116   0.0134125  -0.172876     0.00533179  -0.0596525   -0.160404    -0.124092    -0.137586     0.0168665     0.0745261   -0.761011
  0.205706     0.0973703   0.0661801   -0.299826    -0.0671035    -0.0264136    0.198242    -0.110472    -0.014029     0.147781    -0.119193     0.137785     0.127158     0.0979248   0.00313585   0.0650531    0.0697726   0.0502311    0.014699     0.0161223   -0.03406     -0.111261     0.0988967   -0.0524195     0.0693183    0.87048
  0.0258503   -0.0814006   0.0175937    0.00833291   0.00797564    0.00683032   0.015516     0.00665779   0.0464362    0.0691753   -0.0411274    0.00987427   0.00358586   0.0769766   0.0437554    0.00393604   0.0516857   0.0929823    0.114358     0.0627843   -0.0272611   -0.0439398   -0.0104671   -0.108669     -0.0618037    0.0430347
 -0.100812     0.0324104  -0.00977973  -0.111443     0.194577     -0.0356879    0.105645    -0.0210866   -0.0176634    0.0131464    0.0173423    0.0135866    0.028879    -0.180458   -0.0123375   -0.145064     0.0560002   0.0775288    0.0383467   -0.0875572   -0.0853774    0.0439637   -0.0565762    0.0452491    -0.12576      0.0533195
 -0.0487748   -0.192626   -0.111523     0.0788209    0.0807608    -0.0579363   -0.195659     0.0979946    0.025702    -0.588932     0.0194435    0.109251     0.091767    -0.149038    0.00361938  -0.18426      0.195679    0.132042    -0.01225      0.0501856   -0.0763109   -0.00709222   0.122444     0.0731183     0.0693797    0.138223
 -0.0243551   -0.118286   -0.168878     0.113243     0.0833859    -0.0442645   -0.199508     0.0985263   -0.291079     0.752122     0.0378006    0.115993     0.0741893   -0.221201    0.0180047   -0.0548845    0.146313    0.0496579    0.0579605    0.0493193   -0.0611282    0.158422     0.280518     0.0570066     0.0399859    0.119385
  0.0471936    0.067242    0.140887     0.142436     0.130967     -0.111682    -0.00670352   0.0836921   -0.0655486    0.148051    -0.0659234   -0.0985659    0.134617    -0.0842205   0.00531177   0.0243292    0.126887   -0.0596126   -0.121549     0.0207804    0.0733727   -0.0204993   -0.0576263   -0.135464     -0.0868525    0.0515365
  0.0120701   -0.0698625   0.0215806    0.145601     0.00253935   -0.0394426    0.0291493    0.00543246   0.0367477   -0.206137    -0.00903901  -0.0386619    0.127888    -0.0219554   0.0723569    0.0292276   -0.0737009  -0.0368315   -0.00927321   0.029373     0.0289204   -0.101149     0.00729935   0.0982235     0.162253    -0.121675
 -0.0154616    0.0130983   0.0668909    0.0232014    0.146826     -0.110698     0.142798    -0.0600218   -0.114338    -0.0411962   -0.050051    -0.0634965   -0.0198035   -0.0665726   0.224117     0.0640683    0.0619198   0.0502613    0.0433604   -0.13297     -0.0291523   -0.0554764   -0.0351602   -0.0581659     0.0165954    0.0953263
  0.0796535   -0.145023    0.011293     0.104969     0.159753     -0.0155098   -0.0706912    0.0424803    0.0455155    0.164862     0.0473708    0.0734097    0.245746     0.0456158   0.058235     0.0464451   -0.13809    -0.0671359   -0.0994483    0.101185     0.260653     0.0347047    0.0282654   -0.0634436    -0.0788111    0.0516542
  0.0220541    0.064692    0.00581304   0.139213    -0.0308881    -0.00990449   0.0151492    0.0824789    0.0328503    0.030451     0.060348    -0.00432711   0.113384     0.0159928  -0.137663    -0.0447007    0.076837    0.0717901    0.145434    -0.0439479   -0.00579705  -0.0519067    0.0580698   -0.0415783    -0.0206465    0.0809938
  0.0087886   -0.0468447  -0.214175    -0.0396497   -0.257601     -0.0489791    0.00191746  -0.0710384    0.0959566    0.0699828   -0.0705309    0.0374689   -0.0242285    0.157908    0.0915997   -0.0167728    0.142764   -0.052603    -0.0402869    0.0165694   -0.235701     0.0620103    0.0454749   -0.0120324    -0.248371     0.05563
  0.0137513   -0.0505413  -0.0582201    0.20104     -0.000460223   0.0573182   -0.0579478    0.0934509   -0.133345     0.0606736    0.0802072    0.149716     0.190649    -0.0401566  -0.0608116    0.257658    -0.0873343   0.00887896  -0.0343528   -0.0715484    0.155947     0.126562     0.109774     0.0382848    -1.3625       0.166996
 -0.200506    -0.115569   -0.0167854    0.220706     0.224541     -0.3314      -0.0441331    0.0909168   -0.130793     0.109676     0.0851125    0.0545555    0.159901     0.0259828  -0.0824814    0.0391574   -0.0969912  -0.0554126   -0.0518054   -0.0783163    0.0696146    0.132864     0.124037     0.0578822     0.822931     0.169022
 -0.0408823   -0.170335   -0.00683493  -0.312286     0.124587     -0.238286     0.0967122    0.187069    -0.133009     0.0772179    0.0901897    0.0393044    0.312255    -0.0688089  -0.0308309   -0.271845    -0.0792546   0.0142802   -0.0748407   -0.0797652   -0.224412     0.119875    -0.00580271   0.219444     -0.874491     0.134557
  0.244191    -0.184117   -0.166708    -0.0403016    0.0234572    -0.0879866    0.0322048    0.177394    -0.133218     0.10479      0.07815     -0.0939926    0.233383     0.21512    -0.204339    -0.0894002   -0.069797    0.0846516    0.0273891   -0.0659229    0.209631     0.0803412    0.0021962    0.201865      1.43508      0.0961076
 -0.0631094    0.121119    0.00332207   0.0913642    0.0589343     0.171881    -0.0330314   -0.0720371    0.1618       0.184779     0.0139087   -0.0179504    0.129325     0.0252001  -0.186316     0.161058    -0.0684729   0.138198    -0.0192748   -0.0186389   -0.0980983    0.0159245    0.137695     0.0309596    -0.0691579   -0.199448
  0.0141956   -0.122837    0.0312134    0.0392917   -0.0447683     0.059138     0.0811706   -0.218658     0.0789543   -0.149391     0.139823    -0.0919587   -0.00400963   0.0718793  -0.121366     0.0810817   -0.039672    0.127107     0.0869592   -0.210976     0.0371983   -0.00488265  -0.0974225    0.0632553    -0.00779626   0.0596158
  0.26586      0.0745853   0.0993482    0.0712768   -0.116507     -0.0389823    0.118547    -0.0284988    0.224358    -0.0717604   -0.0343374   -0.17363     -0.187297    -0.0589061   0.198883    -0.046098     0.110761    0.145366    -0.164993     0.0164163   -0.0756964   -0.234407     0.104228    -0.0127213     0.0684355    0.116053
 -0.0589552    0.166443    0.146952    -0.0258036    0.0491693     0.13404     -0.121343    -0.0115246    0.0862959    0.0709213    0.199399    -0.118701     0.0830774    0.0896252  -0.0316911   -0.0152126    0.109899   -0.121212     0.0333398    0.0362666    0.11544      0.00437818   0.144694    -0.060596     -0.0257755   -0.163983
 -0.103004    -0.0266059   0.0346215   -0.11251      0.0983256    -0.0920826   -0.0472793    0.100967     0.0148263   -0.0915726   -0.0804714   -0.0278809   -0.0401683   -0.0794422   0.0480707   -0.0987388   -0.0642723   0.101351     0.107348    -0.0151634   -0.153205    -0.0299131    0.0651431    0.0265465    -0.102609     0.0454471
 -0.048324     0.0665482   0.0606078    0.00517852   0.0464642     0.0889133   -0.0942987    0.0283712    0.0228319    0.0225843   -0.0389368    0.0124737   -0.0423633    0.0312707   0.055659     0.0597712   -0.0531221  -0.0375889    0.0241792   -0.0681829   -0.0220822    0.18119      0.11674     -0.0705064    -0.0314397   -0.0599935
 -0.132907     0.069807    0.102605    -0.0136978   -0.163092     -0.0249455   -0.0354339    0.0160165    0.0572001    0.00860914  -0.00505663   0.0118437   -0.146911    -0.0144057  -0.1521       0.0144778   -0.0139399   0.187158     0.0136018    0.2386       0.0776002    0.107886     0.0422199   -0.00759941    0.0586672    0.0907535
 -0.0129715   -0.0553954  -0.0402971   -0.201552    -0.0300615    -0.228992    -0.203801    -0.0254614   -0.0287873    0.0232875   -0.0491659    0.0438022    0.0472593    0.318619   -0.0618124    0.0557265    0.056309    0.174867    -0.11707     -0.125302     0.0217997    0.105008    -0.0606965    0.0185831     0.0474639   -0.00348065[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 1, average log likelihood -1.056566
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      3
│     15
│     19
│     26
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 2, average log likelihood -1.038283
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 3, average log likelihood -1.056387
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      3
│     15
│     19
│     26
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.037569
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     19
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.056391
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      3
│     15
│     19
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.038956
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     19
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.054946
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      3
│     15
│     19
│     26
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.039008
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.056383
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      3
│     15
│     19
│     26
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.037516
┌ Info: EM with 100000 data points 10 iterations avll -1.037516
└ 59.0 data points per parameter
kind diag, method kmeans
[ Info: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.743239e+05
      1       6.737915e+05      -2.005325e+05 |       32
      2       6.473044e+05      -2.648710e+04 |       32
      3       6.335902e+05      -1.371411e+04 |       32
      4       6.242043e+05      -9.385925e+03 |       32
      5       6.176968e+05      -6.507484e+03 |       32
      6       6.126504e+05      -5.046423e+03 |       32
      7       6.086503e+05      -4.000082e+03 |       32
      8       6.051130e+05      -3.537287e+03 |       32
      9       6.028455e+05      -2.267579e+03 |       32
     10       6.016334e+05      -1.212056e+03 |       32
     11       6.006827e+05      -9.506549e+02 |       32
     12       5.997831e+05      -8.996596e+02 |       32
     13       5.990631e+05      -7.199452e+02 |       32
     14       5.985119e+05      -5.512887e+02 |       32
     15       5.981584e+05      -3.534384e+02 |       32
     16       5.979324e+05      -2.260621e+02 |       32
     17       5.977683e+05      -1.640790e+02 |       32
     18       5.976649e+05      -1.034109e+02 |       32
     19       5.975934e+05      -7.149842e+01 |       32
     20       5.975175e+05      -7.587895e+01 |       32
     21       5.974242e+05      -9.332138e+01 |       32
     22       5.972735e+05      -1.507163e+02 |       32
     23       5.970867e+05      -1.867999e+02 |       32
     24       5.968439e+05      -2.427803e+02 |       32
     25       5.965297e+05      -3.141529e+02 |       32
     26       5.960905e+05      -4.391937e+02 |       32
     27       5.956016e+05      -4.889471e+02 |       32
     28       5.952077e+05      -3.938506e+02 |       32
     29       5.949782e+05      -2.294857e+02 |       32
     30       5.948177e+05      -1.605759e+02 |       32
     31       5.946928e+05      -1.248485e+02 |       32
     32       5.945281e+05      -1.647330e+02 |       32
     33       5.943189e+05      -2.092048e+02 |       32
     34       5.940927e+05      -2.261551e+02 |       32
     35       5.938796e+05      -2.131367e+02 |       32
     36       5.937187e+05      -1.609354e+02 |       32
     37       5.936021e+05      -1.165298e+02 |       32
     38       5.935245e+05      -7.757584e+01 |       31
     39       5.934733e+05      -5.127189e+01 |       32
     40       5.934425e+05      -3.081295e+01 |       32
     41       5.934239e+05      -1.856479e+01 |       29
     42       5.934076e+05      -1.634186e+01 |       31
     43       5.933916e+05      -1.597193e+01 |       30
     44       5.933786e+05      -1.299957e+01 |       28
     45       5.933674e+05      -1.122624e+01 |       27
     46       5.933552e+05      -1.214952e+01 |       31
     47       5.933407e+05      -1.450926e+01 |       28
     48       5.933176e+05      -2.311079e+01 |       31
     49       5.932806e+05      -3.701149e+01 |       30
     50       5.932210e+05      -5.955550e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 593221.0191119736)
┌ Info: K-means with 32000 data points using 50 iterations
└ 37.0 data points per parameter
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.317964
[ Info: iteration 2, average log likelihood -1.285429
[ Info: iteration 3, average log likelihood -1.254515
[ Info: iteration 4, average log likelihood -1.227871
[ Info: iteration 5, average log likelihood -1.194766
[ Info: iteration 6, average log likelihood -1.151212
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      1
│      4
│      6
│     10
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.106254
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     12
│     20
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.108340
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     16
│     21
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.091819
[ Info: iteration 10, average log likelihood -1.091792
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      1
│      4
│      6
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 11, average log likelihood -1.050243
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     10
│     12
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 12, average log likelihood -1.063832
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     14
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 13, average log likelihood -1.064747
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     20
│     21
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 14, average log likelihood -1.052537
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      4
│      6
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 15, average log likelihood -1.055661
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      1
│     12
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 16, average log likelihood -1.059658
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     10
│     14
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 17, average log likelihood -1.059863
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      4
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 18, average log likelihood -1.064207
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      6
│     20
│     21
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 19, average log likelihood -1.047046
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     12
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 20, average log likelihood -1.069484
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      1
│      4
│     14
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 21, average log likelihood -1.052214
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     10
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 22, average log likelihood -1.067647
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      6
│     21
│     22
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 23, average log likelihood -1.042645
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     12
│     20
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 24, average log likelihood -1.062506
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      4
│     14
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 25, average log likelihood -1.062840
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      1
│     10
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 26, average log likelihood -1.064252
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      6
│     22
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 27, average log likelihood -1.051733
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     12
│     21
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 28, average log likelihood -1.052223
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      1
│      4
│     14
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 29, average log likelihood -1.059671
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     10
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 30, average log likelihood -1.074255
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      6
│     20
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 31, average log likelihood -1.047250
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     12
│     21
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 32, average log likelihood -1.046936
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      1
│      4
│     14
│     16
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 33, average log likelihood -1.053322
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     10
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 34, average log likelihood -1.082278
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      6
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 35, average log likelihood -1.059334
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     12
│     21
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 36, average log likelihood -1.038831
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      1
│      4
│     14
│     16
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 37, average log likelihood -1.039705
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      6
│     10
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 38, average log likelihood -1.067162
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 39, average log likelihood -1.061814
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      4
│     12
│     21
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 40, average log likelihood -1.022364
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      1
│      6
│     10
│     14
│     16
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 41, average log likelihood -1.041143
[ Info: iteration 42, average log likelihood -1.091122
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 43, average log likelihood -1.041907
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      4
│      6
│     12
│     21
│     26
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 44, average log likelihood -1.013767
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      1
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 45, average log likelihood -1.068159
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     10
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 46, average log likelihood -1.064757
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 47, average log likelihood -1.043977
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      1
│      4
│      6
│     12
│     21
│     26
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 48, average log likelihood -1.010508
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 49, average log likelihood -1.079619
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     10
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 50, average log likelihood -1.061094
┌ Info: EM with 100000 data points 50 iterations avll -1.061094
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
  0.000379282   0.095227     0.0279004    0.133341    -0.0178828    0.00608497    0.00264262   0.0842543    0.0303504    0.0485417    0.0561984    0.00459424   0.0976346    0.00474687  -0.141997    -0.0354658    0.0692774    0.0544323    0.156671    -0.0691249   -0.0021233  -0.043543     0.0774582   -0.0354888    -0.0076587    0.0627897
 -0.0126748     0.0147638    0.0650058    0.0281543    0.137244    -0.111175      0.144596    -0.054313    -0.108432    -0.0413342   -0.0464525   -0.061993    -0.0189682   -0.0700836    0.21638      0.0626058    0.0602836    0.0615228    0.0470028   -0.132814    -0.0286293  -0.0564465   -0.0335451   -0.0554549     0.0153154    0.0986689
  0.0777552    -0.142879     0.0136817    0.108112     0.162387    -0.0185706    -0.069528     0.0427914    0.0438991    0.163081     0.0449867    0.0737942    0.242736     0.0431074    0.0571491    0.0484055   -0.139441    -0.0687217   -0.102212     0.0994989    0.262288    0.0343195    0.0275219   -0.0654037    -0.0795212    0.0550118
 -0.193566      0.014115     0.0776017   -0.296093     0.094711    -0.109229     -0.0647447    0.038641     0.0152502   -0.164443    -0.050197    -0.00216846  -0.0481471   -0.0225672    0.129967    -0.0429051   -0.203648     0.0676723    0.163302     0.00283916  -0.058792   -0.0167382    0.0847115    0.0688938    -0.091712     0.13402
 -0.0381164    -0.154951    -0.139266     0.0956856    0.0825265   -0.0509824    -0.197497     0.0986728   -0.130994     0.0734558    0.0281175    0.112603     0.0830654   -0.184878     0.0106115   -0.119845     0.172175     0.0916323    0.0217454    0.0498361   -0.0700184   0.0749479    0.199223     0.0648765     0.0546979    0.128952
  0.0287335     0.175163    -0.0253587   -0.128311    -0.0042792    0.159992      0.0557887    0.00384759  -0.0591494    0.0875844   -0.0078617    0.110539     0.0856852   -0.0210423    0.0916437   -0.173626    -0.00667466  -0.0570195   -0.187544     0.0141184   -0.0265579  -0.0904107    0.0239298   -0.0245695     0.0693548   -0.102405
 -0.0264079    -0.00247086   0.0204051    0.0676878    0.00405037   0.112199      0.0346833   -0.145886     0.11923      0.00940263   0.0740209   -0.0581044    0.0539468    0.0480994   -0.148992     0.113699    -0.0595146    0.127142     0.0360861   -0.123456    -0.0348695   0.0017033    0.0171518    0.0492488    -0.0369529   -0.0518043
  0.0100637    -0.0453514   -0.205144    -0.0207915   -0.237799    -0.0498387     0.00286754  -0.0476407    0.0901198    0.0674518   -0.0588577    0.0285143   -0.0141054    0.144444     0.0737267   -0.0192007    0.136307    -0.0416259   -0.0394685    0.00915286  -0.20374     0.0504376    0.0439994   -0.00507271   -0.228088     0.061027
  0.0095845     0.0990769    0.0675966   -0.076914    -0.0324241   -0.0634326     0.160563    -0.0650045   -0.0299509    0.165391    -0.140219     0.0282317    0.1011       0.0989688   -0.0426122    0.02973      0.0370003   -0.0709248    0.00133369  -0.0210836   -0.10608    -0.113601    -0.0323014   -0.0219316     0.0640878   -0.00743031
 -0.051847      0.0639943   -0.170426    -0.10124     -0.189127    -0.0369907     0.0466755   -0.0734409   -0.00634331   0.0886281    0.0692938   -0.236365     0.18823      0.0620979    0.0460171   -0.248247     0.102947     0.064251    -0.0615361   -0.112863     0.160451   -0.136699     0.044366    -0.14449       0.102723     0.0245408
  0.309126      0.0914528    0.0676966   -0.00730902   0.0469973   -0.0883299     0.0773176    0.0729365    0.052308    -0.00372816   0.0967658    0.171645     0.120802    -0.149161     0.0413645    0.0178273    0.0353639    0.0121182   -0.0854031   -0.00589565  -0.0753626   0.0966472   -0.176152    -0.0158194     0.230418     0.0662309
 -0.0350227     0.0167727    0.18282      0.249186     0.0355638    0.00561846    0.155255    -0.12655     -0.00846387  -0.00034641   0.0209971    0.0720902    0.061143    -0.118017    -0.189312    -0.0514048    0.00656362  -0.0766588   -0.0140362    0.0412774   -0.0419732   0.0164731   -0.127104    -0.224938      0.160868     0.135063
  0.265362      0.0747098    0.0993244    0.0743969   -0.116569    -0.0404278     0.11881     -0.0281859    0.223214    -0.0722163   -0.0348875   -0.173822    -0.187019    -0.0597263    0.198295    -0.0454816    0.111684     0.145876    -0.164126     0.0163763   -0.075309   -0.23501      0.104372    -0.0129411     0.0687993    0.116704
 -0.0189828    -0.0491242    0.0694668   -0.116889    -0.136317    -0.0500529     0.108549    -0.153927     0.0338071    0.162183    -0.0122676    0.00201167   0.0138137    0.269834     0.00318365   0.00359651   0.00192258  -0.0400046    0.0777427   -0.0343739   -0.167146   -0.0631318    0.0212107   -0.0900891    -0.0669014   -0.0184098
 -0.107636      0.0546146   -0.016424    -0.706991     0.166554    -0.00302006   -0.00202252   0.0183352    0.0434099    0.0058948   -0.0638175    0.0776086    0.0188048   -0.235047    -0.0548928   -0.201783     0.0568567    0.0881083   -0.058603    -0.057011    -0.0642568   0.248153     0.00612835   0.0626157    -0.150389     0.0545252
  0.0683563     0.0778596    0.133666     0.168815     0.133325    -0.133352     -0.016339     0.0745703   -0.0521189    0.154533    -0.0593906   -0.107118     0.124418    -0.10697     -0.00633601   0.0283581    0.164359    -0.058354    -0.119345     0.0216562    0.113803   -0.0110085   -0.0571669   -0.147518     -0.0905282    0.0447974
 -0.035852     -0.0174033    0.0528547   -0.0242392    0.0162497    0.0458492    -0.0960743   -0.0431999   -0.0159126   -0.159328     0.0345132   -0.0337612    0.0911027    0.0953353    0.060713    -0.0786795   -0.0801336    0.00423242   0.00144769   0.131835    -0.0758815   0.223241    -0.0047543   -0.160087     -0.0942733   -0.112543
 -0.131785      0.0704402    0.102741    -0.015402    -0.163487    -0.0253562    -0.0354756    0.0149861    0.0570803    0.00889447  -0.00498622   0.0113148   -0.146425    -0.014611    -0.151253     0.0142567   -0.0134552    0.187517     0.0141675    0.239522     0.0791665   0.107706     0.0409566   -0.00485675    0.0589925    0.0902775
 -0.0597592     0.166298     0.145353    -0.0252722    0.0492771    0.132639     -0.12164     -0.0112003    0.0860507    0.0708665    0.199682    -0.118876     0.0823055    0.0895301   -0.0326247   -0.0141338    0.109948    -0.121151     0.0333754    0.0362766    0.11565     0.0029352    0.14512     -0.0608096    -0.0257768   -0.164917
 -0.0118113    -0.0542936   -0.0358781   -0.211031    -0.030148    -0.232791     -0.200665    -0.0276768   -0.028781     0.0201594   -0.0475036    0.0443185    0.0491941    0.320943    -0.06008      0.0651498    0.0578217    0.173514    -0.121236    -0.124576     0.0280765   0.105423    -0.0596492    0.0207469     0.0480559   -0.00235664
  0.142919     -0.138192     0.0167773    0.13405     -0.0705199    0.151088     -0.0998684    0.0697541    0.094546    -0.0509588    0.134439     0.0291947   -0.0555641   -0.0409335   -0.0112484    0.090009     0.0577014    0.101691    -0.0620752    0.013439     0.336149    0.0250038    0.0912589    0.229652     -0.132409     0.107857
 -0.0457607    -0.0449111    0.00249803   0.0284467    0.128361    -0.0529887    -0.020855     0.16046      0.0170906    0.0490713   -0.131781    -0.0361885   -0.0630809   -0.169624    -0.0031792   -0.123759     0.0517309    0.104896     0.0676566   -0.0756242   -0.215577   -0.0618585    0.061958     0.0232552    -0.0925866    0.0035559
  0.0119512    -0.0698696    0.0223193    0.146096     0.00274055  -0.0386246     0.0277661    0.00518578   0.0354972   -0.203702    -0.00915127  -0.035308     0.128467    -0.0220316    0.0721512    0.0278297   -0.0790746   -0.0358631   -0.00973214   0.0288805    0.0299177  -0.101163     0.00681728   0.0997984     0.158989    -0.119344
 -0.00221391   -0.0514721    0.0678372   -0.0488959   -0.0879403    0.118971     -0.132559    -0.022858    -0.0214504   -0.0192824    0.150547     0.177131    -0.0417766   -0.0964518   -0.0477977   -0.0805354   -0.0556588    0.233402     0.0398472    0.0271311   -0.0646211   0.0613131    0.0696724   -0.023049      0.157901    -0.239527
 -0.0372388    -0.151324     0.11747     -0.0229209    0.0547241    0.106637     -0.0139368    0.0682989    0.0103833    0.171227    -0.0148668    0.123074     0.0753403   -0.062304    -0.0660494   -0.0525406    0.231176     0.187972     0.325674     0.0330104   -0.0131128  -0.0228313   -0.0704246   -0.111202      0.0491433    0.0598829
 -0.100884      0.0411988   -0.0614273    0.0737058   -0.019162     0.000589692   0.0751398    0.0218046   -0.00302031  -0.0424493    0.0443566    0.0324806   -0.209252    -0.0761954    0.00510776   0.00222062  -0.203825    -0.00271997   0.0909767    0.0647693    0.157917    0.081241    -0.00432741   0.0609256    -0.0508644   -0.00990584
  0.0652674    -0.0865994    0.0626977   -0.129558     0.055334    -0.192909     -0.0175216   -0.0553566    0.0156187    0.101934     0.226327    -0.0728722   -0.0151744    0.0695077   -0.0400863   -0.0533133    0.144547    -0.0341293    0.020195    -0.0726959    0.121535   -0.00505235  -0.046578     0.000262543   0.0047863    0.145983
  0.11208      -0.0389644   -0.105346     0.145354     0.101331    -0.0341984    -0.0364286    0.103929     0.0764593   -0.113717    -0.0883464   -0.0837065   -0.0626596    0.0330826    0.164666     0.0631629   -0.0657049    0.12827     -0.0514711    0.154442     0.100992   -0.0501045    0.0288234   -0.123221     -0.145661     0.0684097
  0.000760527  -0.121553    -0.0577964    0.0380336    0.0945087   -0.139272      0.0011453    0.131993    -0.132594     0.0887103    0.0831771    0.036613     0.216055     0.0304905   -0.090519     0.00389645  -0.084709     0.010439    -0.0347971   -0.0724236    0.0634483   0.113561     0.0623074    0.117825     -0.0311936    0.138651
 -0.0335539     0.129183     0.0418887    0.0248101    0.0459075    0.10189      -0.0822197    0.107635     0.0474667    0.110756    -0.0807       0.0443774   -0.211062    -0.0241176    0.0801139    0.246326    -0.0368797   -0.0379392    0.0317576   -0.262045     0.0229931   0.168859     0.234668     0.0134944     0.00923711  -0.0537511
  0.00783071   -0.0135546    0.0509161   -0.0975769    0.0718995   -0.0643637    -0.0243501   -0.0719968   -0.105177     0.018005    -0.103029    -0.0544272    0.00100379  -0.116497    -0.160301    -0.0301338    0.226098     0.0921808    0.0145476    0.0091003   -0.100398    0.0518901   -0.00659622   0.07977      -0.0416477    0.0598391
 -0.094098     -0.00523871  -0.0025925    0.612219     0.239573    -0.0840583     0.229485    -0.0676147   -0.0892615    0.0220128    0.113134    -0.0667205    0.0366629   -0.147338     0.0371973   -0.0760538    0.0562558    0.0660238    0.147861    -0.121192    -0.107005   -0.191936    -0.14609      0.0291802    -0.101496     0.0554066[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 1, average log likelihood -1.041314
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      1
│      4
│      6
│     12
│      ⋮
│     22
│     26
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 2, average log likelihood -0.996839
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     14
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 3, average log likelihood -1.018997
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      1
│      4
│      6
│     10
│      ⋮
│     22
│     26
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -0.992937
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.033513
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      1
│      4
│      6
│     12
│      ⋮
│     22
│     26
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -0.986311
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     10
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.028318
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      1
│      4
│      6
│     12
│      ⋮
│     22
│     26
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.001797
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     14
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.020026
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      1
│      4
│      6
│     10
│      ⋮
│     22
│     26
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -0.993134
┌ Info: EM with 100000 data points 10 iterations avll -0.993134
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
  0.17591      0.159468    -0.246376     0.182268    -0.021155     0.00862859  -0.162278    -0.0990061     0.0869517   -0.00897442  -0.117232      0.100495     0.00914984  -0.121066    -0.125222    -0.129982     0.136053    0.0641727   0.00803559   0.0312976   -0.0405484   -0.122916     0.134409    -0.0708688   -0.0931476   0.153248
  0.0642531    0.0428735    0.00676665   0.00383253  -0.105403     0.125127    -0.158517    -0.16256      -0.131126    -0.112556     0.040654      0.105205    -0.0908797    0.140247     0.0457835   -0.0783089   -0.0552299  -0.0880165  -0.0382046   -0.0359139   -0.153949    -0.150983    -0.150128    -0.0385892   -0.0727333  -0.0497934
 -0.0265432    0.069725     0.0795375    0.135293     0.216662    -0.0967527    0.077041     0.0288799    -0.183087     0.053561    -0.17171       0.104257    -0.141893    -0.0474588    0.189006     0.0139007   -0.10311     0.103192    0.172101    -0.0303571    0.0515052   -0.073475    -0.0868685    0.118471    -0.0266163   0.000487163
 -0.0331541   -0.0362214    0.0547742   -0.0557935    0.122206     0.0507807   -0.133043    -0.0468752    -0.159151     0.0905511   -0.0467579    -0.0890095    0.0196073    0.0270516    0.0334464   -0.0708577    0.0170116   0.0293325  -0.110273    -0.031155     0.0984314   -0.0994936   -0.195683     0.0772547   -0.0198623   0.0200489
  0.0406663    0.064933     0.0960043   -0.0464789    0.0751358    0.0555829    0.128557    -0.0141013    -0.00831761  -0.156848     0.113539      0.0141224    0.0504278   -0.0107672    0.028433    -0.00089552  -0.116312   -0.0745184  -0.0984152    0.04482     -0.0395124    0.0540861   -0.00151317   0.0352294    0.0798786  -0.21886
 -0.0595908   -0.304661    -0.0101791   -0.0114627   -0.0235981   -0.0794859    0.065333    -0.0163966    -0.0894148    0.0520768   -0.0201036     0.0382913    0.00761419   0.00513235   0.157079    -0.0343662    0.254377    0.0248957   0.00747825  -0.0435335   -0.152028    -0.092677    -0.152284     0.0515813   -0.0793326   0.11337
 -0.0267569    0.129369     0.00951002   0.0470568   -0.029308     0.0176941    0.154269     0.0955703    -0.00304417  -0.171139    -0.18256       0.0342526   -0.0734233    0.057708     0.0501537   -0.142437     0.0172441   0.149797   -0.0197648   -0.0225823    0.106951    -0.149082    -0.148364    -0.0652451   -0.0264704  -0.17944
  0.0745914   -0.0334326    0.17354      0.0208046   -0.0941302   -0.123093    -0.114284    -0.153582      0.0131972   -0.0805299    0.0417637    -0.0739217   -0.00328226   0.111845     0.0880127   -0.16155      0.0107709   0.0755991   0.0847408   -0.0793313   -0.0454777   -0.0351706    0.0939777   -0.0944169   -0.0934888  -0.00268028
 -0.115644     0.0588258   -0.129849     0.0236208   -0.00338061  -0.0239993    0.271399    -0.000186726  -0.168589     0.0136604   -0.0922759     0.151536    -0.125263    -0.00578527  -0.0144877    0.0891915   -0.0138859  -0.132098   -0.209051     0.118306    -0.218188    -0.010853     0.0753033   -0.13557     -0.0617807   0.2155
 -0.139389    -0.162538    -0.00205737   0.0805393    0.137284    -0.0398179    0.0651062   -0.0752149     0.080859    -0.0110704   -0.0540898    -0.066825    -0.0048545    0.078118    -0.023342     0.0367568   -0.14174    -0.160291    0.0241204    0.0347699   -0.0732906   -0.141367    -0.074537     0.0427772   -0.0112491  -0.0599614
 -0.139316     0.0969538    0.0504377    0.0234487   -0.198212     0.127894    -0.157222     0.071576     -0.122513    -0.0174412    0.0611189    -0.0405987   -0.176092     0.234877     0.0903964    0.176829     0.115783    0.051604   -0.0400961   -0.135281     0.119876     0.177104    -0.136606    -0.0600257    0.156075    0.0488364
 -0.0298531    0.0301852   -0.0145039   -0.132469    -0.0802458   -0.083381     0.0790931    0.0322436    -0.037226     0.069691     0.11041      -0.206224     0.0375967   -0.0263239    0.00573141   0.120095    -0.0589843   0.137984   -0.178574     0.1605       0.0297113    0.0881132    0.00216282  -0.14265     -0.0132223   0.189038
 -0.145992     0.0170695   -0.00307392   0.196452    -0.0711398    0.197619    -0.0155699   -0.131451     -0.0428697    0.199199     0.118171      0.0683459    0.0617679   -0.123716    -0.10625      0.0219424   -0.0732607   0.0470163   0.148603    -0.0643492   -0.003599    -0.0245554   -0.116629     0.034362    -0.0252525   0.0259461
  0.048304    -0.0885223    0.0967384    0.217152    -0.0782278   -0.0562832    0.0540415    0.0254635    -0.0151669   -0.0983484   -0.0123948    -0.0575433   -0.199208     0.0410218   -0.00744743  -0.0922764   -0.169807   -0.0445059   0.0570725   -0.129737    -0.0214146   -0.0395169   -0.178443     0.179922    -0.142216   -0.14562
 -0.0868968   -0.02561      0.0312427    0.18367      0.161381     0.0992975   -0.0648635   -0.0310588     0.0494075   -0.0313044   -0.068669     -0.0348227   -0.0763547    0.133155    -0.0474915   -0.00395597  -0.071701   -0.113594   -0.0205259    0.0459942    0.132714    -0.0178038    0.100794    -0.041565    -0.0613837  -0.00313188
 -0.10524     -0.0709485   -0.00355049   0.11043     -0.111482    -0.0283808    0.125469    -0.0879986     0.0988142    0.104081    -0.137885     -0.0872317    0.054536    -0.0120357    0.11486     -0.137004     0.0601842  -0.0762363   0.0979632   -0.0675581    0.123996    -0.00963525  -0.102068    -0.0368596   -0.170933    0.0637305
 -0.115117    -0.0361792    0.0658165   -0.00577576  -0.109034    -0.0339654    0.0467717   -0.0870458    -0.201328     0.0311116   -0.108526      0.13822     -0.0129494    0.110502    -0.0831297    0.0730425   -0.0654066   0.0490403   0.17371      0.0230693   -0.106797     0.038558     0.0819134   -0.0890004    0.0241702  -0.225141
  0.0799029    0.281028     0.00453303   0.135318    -0.0790296    0.0338446    0.0383304    0.0376716     0.18065     -0.117309     0.126218     -0.00706681   0.0831861   -0.070669    -0.080216    -0.193826     0.180868    0.0470222  -0.010574     0.00163678  -0.105943     0.0164865   -0.0398807   -0.113374    -0.0248542   0.153019
 -0.0599318   -0.104371     0.0608752    0.00936129  -0.151722     0.0834573    0.121757    -0.104721      0.172627     0.0591628    0.135105     -0.0123948    0.0691409   -0.00342774   0.156051    -0.194256     0.0354779  -0.071962   -0.0259724   -0.109309     0.00882785   0.0754441    0.112473    -0.113643     0.0846335   0.070787
  0.183236    -0.0219001    0.0634866   -0.164909    -0.130642    -0.0159975    0.016829     0.0320954     0.0248812    0.0433563   -0.180032      0.0154913   -0.2141      -0.105663    -0.00723901  -0.171412    -0.166377   -0.125535   -0.0105132   -0.0074477    0.132337    -0.119134    -0.03367      0.0759325    0.0386512  -0.12548
  0.00699264  -0.0602872   -0.136041     0.0301542   -0.120933     0.0989878    0.0196251   -0.0981666    -0.0120264   -0.0207414   -0.0316642    -0.124623    -0.169617    -0.0501343    0.0515125   -0.0533379   -0.0171869   0.0971515   0.0964488    0.0777094   -0.0913479    0.098254     0.0609923   -0.0679824   -0.045795   -0.0539288
  0.0406763   -0.0764916   -0.120545    -0.108242    -0.12838      0.00102162  -0.00507314  -0.123371     -0.183321    -0.163638     0.0414768     0.105797    -0.0538522    0.0719077   -0.0148683   -0.104597    -0.148365   -0.073345    0.00598993   0.138134    -0.267764    -0.172805     0.0353086    0.088009     0.101194    0.0686973
 -0.147308    -0.198077    -0.0656166    0.105042     0.151596     0.0349533   -0.093731     0.0289317    -0.144701     0.157525    -0.0919218    -0.143491    -0.00227967   0.00430374   0.119556    -0.165066     0.0911818   0.089415    0.28306      0.229024    -0.127955     0.136171    -0.0342227   -0.102387    -0.048334   -0.325229
 -0.110534    -0.0224293    0.105273     0.101455     0.172863    -0.0523023    0.0282119   -0.157813      0.0345125    0.0439536   -0.121544     -0.0151135   -0.0684461    0.142112    -0.0898726    0.0412942    0.161889    0.0946636  -0.0149655    0.018712    -0.16333     -0.0670727   -0.0563965    0.00309159   0.120395   -0.0754216
 -0.0872215    0.0989821   -0.0755081    0.152611    -0.121033    -0.0495309    0.0431199    0.125377      0.148302    -0.0185146    0.0283015    -0.0419987    0.131116    -0.121523     0.219217     0.0515092    0.206687    0.0349347  -0.0199939    0.0689714   -0.0293495    0.0451196   -0.00706027   0.196273    -0.0912466   0.0537596
 -0.0235137    0.113058    -0.0318183    0.16104      0.0250951   -0.156914     0.173966    -0.0752256     0.0256822    0.141381    -0.000220985   0.0394724    0.0746043    0.124263     0.170589    -0.164935    -0.0818297   0.0630308   0.131413    -0.0521726   -0.0158848    0.126404    -0.0310606   -0.024955    -0.0194662  -0.0199275
 -0.10673      0.263731     0.0193445   -0.231171     0.134328    -0.12334      0.0588487    0.0270739     0.0526229    0.0237743    0.0267378    -0.0301307    0.108057     0.0887874   -0.0876474    0.0440435   -0.0342659  -0.0270647   0.107696    -0.0498041   -0.0925298   -0.029059    -0.0408262    0.161398     0.0561949   0.0966054
 -0.00723378   0.009461     0.0907542   -0.143946    -0.0863197    0.189999     0.0876814   -0.0248154     0.00882354   0.156595     0.0580112     0.0439574   -0.131085     0.0474098    0.0437769    0.164511    -0.227485   -0.066094    0.0629266   -0.00314669  -0.169381     0.1181      -0.0763786   -0.106843     0.0759176   0.170978
  0.0985353    0.0693084   -0.137539     0.0654904    0.0289182    0.134281     0.0483079    0.330868     -0.260866     0.0537266   -0.0172876    -0.0562125   -0.0576567   -0.0870875   -0.0581721    0.0315587   -0.225459    0.0435172  -0.0113231    0.0415799    0.0957545   -0.101809     0.0223814    0.0460147   -0.102317   -0.0142641
 -0.0252624    0.0411524    0.00676517   0.0762859    0.0308745    0.0578982    0.0683161    0.00526835   -0.083849     0.140644    -0.115191      0.17205     -0.109144     0.132074    -0.0820707    0.0902167    0.0563353  -0.192926    0.057783     0.0429427    0.0331708   -0.0268496    0.116786    -0.0402466   -0.0975965  -0.0460763
  0.122566    -0.00315022  -0.162376    -0.0543792   -0.0935937   -0.10553     -0.0820111   -0.052349      0.00290808   0.0773854    0.0176853    -0.160315    -0.143938    -0.0300442    0.0135883    0.0486638   -0.180228   -0.0796257   0.0640098   -0.100123     0.0494443    0.0197657   -0.0622862   -0.170769    -0.019782    0.125646
 -0.0424749   -0.0683088   -0.0769741   -0.0464881    0.0713211   -0.0194392    0.0311834    0.0798073     0.135314     0.0721294   -0.0698449    -0.108113    -0.0242476    0.12352     -0.0406784    0.196879     0.159602   -0.081776    0.0951858    0.0410193   -0.0325909    0.087958     0.0043567   -0.018284    -0.074467    0.0950568kind full, method split
┌ Info: 0: avll = 
└   tll[1] = -1.4264800286544033
[ Info: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.426501
[ Info: iteration 2, average log likelihood -1.426406
[ Info: iteration 3, average log likelihood -1.426319
[ Info: iteration 4, average log likelihood -1.426206
[ Info: iteration 5, average log likelihood -1.426053
[ Info: iteration 6, average log likelihood -1.425849
[ Info: iteration 7, average log likelihood -1.425567
[ Info: iteration 8, average log likelihood -1.425155
[ Info: iteration 9, average log likelihood -1.424540
[ Info: iteration 10, average log likelihood -1.423697
[ Info: iteration 11, average log likelihood -1.422759
[ Info: iteration 12, average log likelihood -1.421979
[ Info: iteration 13, average log likelihood -1.421493
[ Info: iteration 14, average log likelihood -1.421247
[ Info: iteration 15, average log likelihood -1.421135
[ Info: iteration 16, average log likelihood -1.421086
[ Info: iteration 17, average log likelihood -1.421064
[ Info: iteration 18, average log likelihood -1.421055
[ Info: iteration 19, average log likelihood -1.421050
[ Info: iteration 20, average log likelihood -1.421048
[ Info: iteration 21, average log likelihood -1.421046
[ Info: iteration 22, average log likelihood -1.421046
[ Info: iteration 23, average log likelihood -1.421045
[ Info: iteration 24, average log likelihood -1.421044
[ Info: iteration 25, average log likelihood -1.421044
[ Info: iteration 26, average log likelihood -1.421044
[ Info: iteration 27, average log likelihood -1.421043
[ Info: iteration 28, average log likelihood -1.421043
[ Info: iteration 29, average log likelihood -1.421043
[ Info: iteration 30, average log likelihood -1.421043
[ Info: iteration 31, average log likelihood -1.421043
[ Info: iteration 32, average log likelihood -1.421042
[ Info: iteration 33, average log likelihood -1.421042
[ Info: iteration 34, average log likelihood -1.421042
[ Info: iteration 35, average log likelihood -1.421042
[ Info: iteration 36, average log likelihood -1.421042
[ Info: iteration 37, average log likelihood -1.421042
[ Info: iteration 38, average log likelihood -1.421042
[ Info: iteration 39, average log likelihood -1.421042
[ Info: iteration 40, average log likelihood -1.421042
[ Info: iteration 41, average log likelihood -1.421041
[ Info: iteration 42, average log likelihood -1.421041
[ Info: iteration 43, average log likelihood -1.421041
[ Info: iteration 44, average log likelihood -1.421041
[ Info: iteration 45, average log likelihood -1.421041
[ Info: iteration 46, average log likelihood -1.421041
[ Info: iteration 47, average log likelihood -1.421041
[ Info: iteration 48, average log likelihood -1.421041
[ Info: iteration 49, average log likelihood -1.421041
[ Info: iteration 50, average log likelihood -1.421041
┌ Info: EM with 100000 data points 50 iterations avll -1.421041
└ 952.4 data points per parameter
┌ Info: 1
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4265013547479228
│     -1.4264057994039139
│      ⋮
└     -1.4210411135846248
[ Info: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.421062
[ Info: iteration 2, average log likelihood -1.420963
[ Info: iteration 3, average log likelihood -1.420872
[ Info: iteration 4, average log likelihood -1.420758
[ Info: iteration 5, average log likelihood -1.420619
[ Info: iteration 6, average log likelihood -1.420470
[ Info: iteration 7, average log likelihood -1.420336
[ Info: iteration 8, average log likelihood -1.420232
[ Info: iteration 9, average log likelihood -1.420161
[ Info: iteration 10, average log likelihood -1.420113
[ Info: iteration 11, average log likelihood -1.420079
[ Info: iteration 12, average log likelihood -1.420052
[ Info: iteration 13, average log likelihood -1.420029
[ Info: iteration 14, average log likelihood -1.420008
[ Info: iteration 15, average log likelihood -1.419989
[ Info: iteration 16, average log likelihood -1.419969
[ Info: iteration 17, average log likelihood -1.419948
[ Info: iteration 18, average log likelihood -1.419926
[ Info: iteration 19, average log likelihood -1.419902
[ Info: iteration 20, average log likelihood -1.419876
[ Info: iteration 21, average log likelihood -1.419847
[ Info: iteration 22, average log likelihood -1.419817
[ Info: iteration 23, average log likelihood -1.419785
[ Info: iteration 24, average log likelihood -1.419752
[ Info: iteration 25, average log likelihood -1.419720
[ Info: iteration 26, average log likelihood -1.419690
[ Info: iteration 27, average log likelihood -1.419662
[ Info: iteration 28, average log likelihood -1.419637
[ Info: iteration 29, average log likelihood -1.419615
[ Info: iteration 30, average log likelihood -1.419597
[ Info: iteration 31, average log likelihood -1.419582
[ Info: iteration 32, average log likelihood -1.419569
[ Info: iteration 33, average log likelihood -1.419558
[ Info: iteration 34, average log likelihood -1.419549
[ Info: iteration 35, average log likelihood -1.419542
[ Info: iteration 36, average log likelihood -1.419537
[ Info: iteration 37, average log likelihood -1.419532
[ Info: iteration 38, average log likelihood -1.419528
[ Info: iteration 39, average log likelihood -1.419525
[ Info: iteration 40, average log likelihood -1.419522
[ Info: iteration 41, average log likelihood -1.419519
[ Info: iteration 42, average log likelihood -1.419518
[ Info: iteration 43, average log likelihood -1.419516
[ Info: iteration 44, average log likelihood -1.419514
[ Info: iteration 45, average log likelihood -1.419513
[ Info: iteration 46, average log likelihood -1.419512
[ Info: iteration 47, average log likelihood -1.419511
[ Info: iteration 48, average log likelihood -1.419510
[ Info: iteration 49, average log likelihood -1.419509
[ Info: iteration 50, average log likelihood -1.419509
┌ Info: EM with 100000 data points 50 iterations avll -1.419509
└ 473.9 data points per parameter
┌ Info: 2
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4210621096177247
│     -1.4209627949823735
│      ⋮
└     -1.4195085456855774
[ Info: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.419521
[ Info: iteration 2, average log likelihood -1.419460
[ Info: iteration 3, average log likelihood -1.419409
[ Info: iteration 4, average log likelihood -1.419349
[ Info: iteration 5, average log likelihood -1.419277
[ Info: iteration 6, average log likelihood -1.419193
[ Info: iteration 7, average log likelihood -1.419100
[ Info: iteration 8, average log likelihood -1.419005
[ Info: iteration 9, average log likelihood -1.418914
[ Info: iteration 10, average log likelihood -1.418830
[ Info: iteration 11, average log likelihood -1.418755
[ Info: iteration 12, average log likelihood -1.418686
[ Info: iteration 13, average log likelihood -1.418623
[ Info: iteration 14, average log likelihood -1.418564
[ Info: iteration 15, average log likelihood -1.418509
[ Info: iteration 16, average log likelihood -1.418458
[ Info: iteration 17, average log likelihood -1.418410
[ Info: iteration 18, average log likelihood -1.418366
[ Info: iteration 19, average log likelihood -1.418326
[ Info: iteration 20, average log likelihood -1.418289
[ Info: iteration 21, average log likelihood -1.418257
[ Info: iteration 22, average log likelihood -1.418228
[ Info: iteration 23, average log likelihood -1.418203
[ Info: iteration 24, average log likelihood -1.418181
[ Info: iteration 25, average log likelihood -1.418162
[ Info: iteration 26, average log likelihood -1.418145
[ Info: iteration 27, average log likelihood -1.418130
[ Info: iteration 28, average log likelihood -1.418117
[ Info: iteration 29, average log likelihood -1.418106
[ Info: iteration 30, average log likelihood -1.418096
[ Info: iteration 31, average log likelihood -1.418087
[ Info: iteration 32, average log likelihood -1.418079
[ Info: iteration 33, average log likelihood -1.418071
[ Info: iteration 34, average log likelihood -1.418065
[ Info: iteration 35, average log likelihood -1.418058
[ Info: iteration 36, average log likelihood -1.418052
[ Info: iteration 37, average log likelihood -1.418047
[ Info: iteration 38, average log likelihood -1.418042
[ Info: iteration 39, average log likelihood -1.418037
[ Info: iteration 40, average log likelihood -1.418032
[ Info: iteration 41, average log likelihood -1.418027
[ Info: iteration 42, average log likelihood -1.418023
[ Info: iteration 43, average log likelihood -1.418018
[ Info: iteration 44, average log likelihood -1.418014
[ Info: iteration 45, average log likelihood -1.418010
[ Info: iteration 46, average log likelihood -1.418006
[ Info: iteration 47, average log likelihood -1.418002
[ Info: iteration 48, average log likelihood -1.417998
[ Info: iteration 49, average log likelihood -1.417994
[ Info: iteration 50, average log likelihood -1.417990
┌ Info: EM with 100000 data points 50 iterations avll -1.417990
└ 236.4 data points per parameter
┌ Info: 3
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4195214466239314
│     -1.4194599793953033
│      ⋮
└     -1.4179903049002327
[ Info: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.417995
[ Info: iteration 2, average log likelihood -1.417935
[ Info: iteration 3, average log likelihood -1.417879
[ Info: iteration 4, average log likelihood -1.417813
[ Info: iteration 5, average log likelihood -1.417732
[ Info: iteration 6, average log likelihood -1.417633
[ Info: iteration 7, average log likelihood -1.417522
[ Info: iteration 8, average log likelihood -1.417407
[ Info: iteration 9, average log likelihood -1.417295
[ Info: iteration 10, average log likelihood -1.417193
[ Info: iteration 11, average log likelihood -1.417101
[ Info: iteration 12, average log likelihood -1.417019
[ Info: iteration 13, average log likelihood -1.416946
[ Info: iteration 14, average log likelihood -1.416880
[ Info: iteration 15, average log likelihood -1.416820
[ Info: iteration 16, average log likelihood -1.416765
[ Info: iteration 17, average log likelihood -1.416714
[ Info: iteration 18, average log likelihood -1.416669
[ Info: iteration 19, average log likelihood -1.416626
[ Info: iteration 20, average log likelihood -1.416588
[ Info: iteration 21, average log likelihood -1.416552
[ Info: iteration 22, average log likelihood -1.416518
[ Info: iteration 23, average log likelihood -1.416487
[ Info: iteration 24, average log likelihood -1.416457
[ Info: iteration 25, average log likelihood -1.416429
[ Info: iteration 26, average log likelihood -1.416402
[ Info: iteration 27, average log likelihood -1.416376
[ Info: iteration 28, average log likelihood -1.416351
[ Info: iteration 29, average log likelihood -1.416327
[ Info: iteration 30, average log likelihood -1.416304
[ Info: iteration 31, average log likelihood -1.416282
[ Info: iteration 32, average log likelihood -1.416260
[ Info: iteration 33, average log likelihood -1.416239
[ Info: iteration 34, average log likelihood -1.416219
[ Info: iteration 35, average log likelihood -1.416200
[ Info: iteration 36, average log likelihood -1.416181
[ Info: iteration 37, average log likelihood -1.416162
[ Info: iteration 38, average log likelihood -1.416144
[ Info: iteration 39, average log likelihood -1.416127
[ Info: iteration 40, average log likelihood -1.416110
[ Info: iteration 41, average log likelihood -1.416093
[ Info: iteration 42, average log likelihood -1.416077
[ Info: iteration 43, average log likelihood -1.416061
[ Info: iteration 44, average log likelihood -1.416045
[ Info: iteration 45, average log likelihood -1.416029
[ Info: iteration 46, average log likelihood -1.416014
[ Info: iteration 47, average log likelihood -1.415999
[ Info: iteration 48, average log likelihood -1.415984
[ Info: iteration 49, average log likelihood -1.415969
[ Info: iteration 50, average log likelihood -1.415954
┌ Info: EM with 100000 data points 50 iterations avll -1.415954
└ 118.1 data points per parameter
┌ Info: 4
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4179954405324577
│     -1.4179349890241726
│      ⋮
└     -1.415953791750735
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.415948
[ Info: iteration 2, average log likelihood -1.415869
[ Info: iteration 3, average log likelihood -1.415791
[ Info: iteration 4, average log likelihood -1.415696
[ Info: iteration 5, average log likelihood -1.415577
[ Info: iteration 6, average log likelihood -1.415428
[ Info: iteration 7, average log likelihood -1.415253
[ Info: iteration 8, average log likelihood -1.415064
[ Info: iteration 9, average log likelihood -1.414874
[ Info: iteration 10, average log likelihood -1.414693
[ Info: iteration 11, average log likelihood -1.414530
[ Info: iteration 12, average log likelihood -1.414386
[ Info: iteration 13, average log likelihood -1.414262
[ Info: iteration 14, average log likelihood -1.414155
[ Info: iteration 15, average log likelihood -1.414065
[ Info: iteration 16, average log likelihood -1.413987
[ Info: iteration 17, average log likelihood -1.413918
[ Info: iteration 18, average log likelihood -1.413858
[ Info: iteration 19, average log likelihood -1.413805
[ Info: iteration 20, average log likelihood -1.413757
[ Info: iteration 21, average log likelihood -1.413713
[ Info: iteration 22, average log likelihood -1.413672
[ Info: iteration 23, average log likelihood -1.413635
[ Info: iteration 24, average log likelihood -1.413599
[ Info: iteration 25, average log likelihood -1.413566
[ Info: iteration 26, average log likelihood -1.413535
[ Info: iteration 27, average log likelihood -1.413504
[ Info: iteration 28, average log likelihood -1.413475
[ Info: iteration 29, average log likelihood -1.413447
[ Info: iteration 30, average log likelihood -1.413420
[ Info: iteration 31, average log likelihood -1.413394
[ Info: iteration 32, average log likelihood -1.413368
[ Info: iteration 33, average log likelihood -1.413342
[ Info: iteration 34, average log likelihood -1.413318
[ Info: iteration 35, average log likelihood -1.413294
[ Info: iteration 36, average log likelihood -1.413270
[ Info: iteration 37, average log likelihood -1.413247
[ Info: iteration 38, average log likelihood -1.413225
[ Info: iteration 39, average log likelihood -1.413203
[ Info: iteration 40, average log likelihood -1.413182
[ Info: iteration 41, average log likelihood -1.413162
[ Info: iteration 42, average log likelihood -1.413142
[ Info: iteration 43, average log likelihood -1.413123
[ Info: iteration 44, average log likelihood -1.413105
[ Info: iteration 45, average log likelihood -1.413088
[ Info: iteration 46, average log likelihood -1.413071
[ Info: iteration 47, average log likelihood -1.413055
[ Info: iteration 48, average log likelihood -1.413040
[ Info: iteration 49, average log likelihood -1.413025
[ Info: iteration 50, average log likelihood -1.413011
┌ Info: EM with 100000 data points 50 iterations avll -1.413011
└ 59.0 data points per parameter
┌ Info: 5
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.415947752576216
│     -1.4158688032054076
│      ⋮
└     -1.4130108655429114
┌ Info: Total log likelihood: 
│   tll =
│    251-element Array{Float64,1}:
│     -1.4264800286544033
│     -1.4265013547479228
│     -1.4264057994039139
│     -1.4263189897822015
│      ⋮
│     -1.4130395688279909
│     -1.4130248913813503
└     -1.4130108655429114
32×26 Array{Float64,2}:
 -0.0319716    0.559797    -0.770644     -0.606992     0.604791    0.0656823  -0.679087    0.643487    -0.0699687    0.957972   -0.441253     -0.258396    -0.243537    -0.160486   -0.539005    0.51885      0.257956     0.154369    0.846557    -0.350874     0.218526   -0.148011   -0.0860005   -0.406962     0.439545    0.191952
  0.0145845    0.582065     0.166025     -0.187396    -0.279029   -0.758806    0.049575   -0.0617961    0.603265     0.729098   -0.267615      0.465608    -0.387299    -0.120911   -0.693683    0.158961    -0.0496115   -0.0523754   0.338689     0.0935765   -0.526169    0.774419    0.413692    -0.247416    -0.0693717   0.876356
  0.250989     0.073658     0.128107      0.165455     0.0470904   0.368051   -0.0537631   0.249237     0.363701     0.248504   -0.46289       0.0163239   -0.108634     0.154558   -0.527869    1.07545     -0.156614    -0.47356     0.107886    -0.0555388   -0.822113    0.379422   -0.438868     0.500475    -0.0347275  -0.149195
  0.0685195    0.0315338   -0.0487337     0.348002     0.235714    0.0615907  -0.64623    -0.095621    -0.371401    -0.146812   -0.678775     -0.310464     0.0239816    0.77244    -0.136577    0.797573    -0.226725     0.11029    -0.0734074   -0.0539337   -0.0994714   0.713481    0.667084    -0.385076    -0.245363   -0.273477
 -0.0136108    0.179088    -0.213868      0.108661    -0.166899    0.96902    -0.165871   -1.09819     -0.342592    -0.765956   -0.0539653    -0.155893     0.216761     0.293249    0.473784    0.2557      -0.455268    -0.461969   -0.368998    -0.0914005   -0.726167   -0.593671    0.175863     0.435909    -0.376634   -0.123101
 -0.0558142   -0.185173     1.06637       0.56905     -0.406635   -0.0286403   0.478858   -1.05207      0.0414092   -0.205671    0.0327238     0.179983     0.00201743   0.226838    0.0582513  -0.26129     -0.221336    -0.441523   -0.330109    -0.00628838  -0.765043    0.324885   -0.170921     0.427764     0.0475155  -0.324963
 -0.489423    -0.29186      0.412442      0.118066     0.0753785   0.467676   -0.218832   -0.12052      0.0960662    0.531682    0.000150457   1.2257       0.551085    -0.0415643   0.053996    0.00450868  -0.186621    -0.508933    0.319099    -0.356962     0.546131    0.385088    0.0816763    0.0261768    0.204389    0.223444
  0.0554365   -0.0527038    0.0596679     0.276604     0.180639   -0.0827984   0.107751   -0.613816     0.416557     0.138516    0.210179      0.00559846   0.661397    -0.0392029   0.804543    0.167834     0.00170968  -0.242614    0.444414    -0.0946528   -0.591076   -0.134093   -0.375881     0.348707     0.131625    0.86047
  0.233727    -0.141539    -0.282873     -0.275473    -0.351273   -0.109063    0.103621    0.235142    -0.320954    -0.464531   -0.321377     -1.28258     -0.772666     0.476704   -0.0816639  -0.348503    -0.043846     0.779095   -0.0838343    0.136573    -0.0820182   0.149806    0.17331     -0.518339    -0.642314   -0.475977
 -0.59062     -0.351702    -0.233588     -0.127811     0.0230514   0.0471955  -0.07645    -0.0372156   -0.288636    -0.180949   -0.0650114    -1.12892     -0.4213      -0.014929   -0.197543    0.0166803   -0.353813     0.121807    0.248977     0.166261    -0.189802    0.274299    0.174132     0.167717    -0.362898    0.675807
  0.10122     -0.0949316   -0.149754     -0.703161    -0.0970599   0.22672     0.144884    0.899903     0.214182    -0.0539948   0.0794466     0.440757    -0.108677    -0.0850525  -0.288155   -0.208674    -0.203028    -0.248812   -0.301096     0.0885722    0.45893     0.36075    -0.451805    -0.214941    -0.412809   -0.196213
 -0.215318    -0.0511195    0.15322      -0.891886    -0.378997    0.624338   -0.115397   -0.280447    -0.34981      0.27623    -0.31395       0.307133    -0.281234     0.341968   -0.38301    -0.878697    -0.5032      -0.440986   -0.0111773    0.861359     0.188322    0.119642    0.623684    -0.148822    -0.316929   -0.578929
  0.524926    -1.05653     -0.00986621    0.37698     -0.095016   -0.051316   -0.268437   -0.00893347   0.261973    -0.332658    0.391693     -0.560945     0.917043    -0.0669718   0.102584   -0.411834    -0.0970589    0.199637   -0.753052     0.559066     0.200593   -0.761342   -0.0819911   -0.443118    -0.326456   -0.311075
 -0.034358     0.132403    -0.0147247    -0.688909    -0.168612   -0.213813    0.0697589   0.673705    -1.02925     -0.391637    0.39219      -0.728654     0.416255     0.0897552   0.510776    0.250941     0.022453     0.0782658   0.00789858   0.127283     0.27485    -0.289959   -0.0130667   -0.106035    -0.380137   -0.285029
 -0.078013     0.240223    -0.0378314     0.165192    -0.108215   -0.13963     0.163195   -0.0292648   -0.00490955   0.228996   -0.209654      0.0821151   -0.372308     0.139251   -0.607154    0.215092    -0.00634581  -0.306059    0.0757717   -0.109368    -0.193912   -0.0434607  -0.166985     0.142421     0.1376      0.00751082
  0.00217623  -0.107279     0.219444      0.0614127    0.0192239   0.0822847  -0.176654   -0.159532     0.0162697   -0.151467    0.280125      0.276901     0.25753     -0.126693    1.05721    -0.587973     0.13405      0.487874    0.238492    -0.034924     0.336176   -0.142385    0.0284304   -0.0840628   -0.0491372   0.0473264
 -0.0601646   -0.0293715    0.0681889     0.0196735   -0.0116848  -0.0663409   0.0652515   0.114218    -0.00478492   0.0509173  -0.0270301     0.0328934   -0.202065    -0.120009   -0.340781    0.0146223   -0.108346    -0.0312976  -0.0400219   -0.0189302    0.097302    0.093991   -0.072834     0.00374426  -0.0591338  -0.0726087
  0.0209386    0.137528     0.0119729     0.133648     0.119623   -0.0116843   0.0649709  -0.0811995   -0.00234416   0.100925    0.0537679     0.151338     0.161208     0.031961    0.456302   -0.0395113    0.177045     0.0251485  -0.0250221   -0.0585044   -0.0135329  -0.141722   -0.0163578   -0.017425     0.213063   -0.0698503
  0.183827     0.201615    -0.330622      0.00554786  -0.0918501  -0.280175    0.222957    0.00216936   0.0182584   -0.329495   -0.310566     -0.632696    -0.54389      0.414339   -0.53795     0.102725     0.0662029    0.0903446  -0.104931     0.372509    -0.438789   -0.219603   -0.0673919    0.0845624   -0.227578   -0.507665
 -0.177498     0.0730883   -0.102063     -0.0539001   -0.0508003   0.207275   -0.0644176  -0.212053    -0.0194769    0.0446113  -0.398801     -0.351883    -0.110732     0.295288   -0.231072    0.439933    -0.0812981   -0.320561   -0.0447686    0.0972326   -0.349401    0.253463    0.13861      0.183186    -0.228041    0.361308
 -0.193956    -0.211598    -0.361181     -0.131049     0.0593049  -0.383764   -0.872267    0.249209    -0.0144218   -0.154171    0.226909     -0.0233771   -0.091946    -0.0489938   0.237451   -0.36942     -0.04126      0.182662    0.325892     0.23639      0.340624   -0.156336    0.528198    -0.325416    -0.140131    0.551592
  0.213989    -0.200742    -0.000641927  -0.302708    -0.146906    0.301812    0.0112122  -0.0640152   -0.198708    -0.0185449   0.261561     -0.280458    -0.0433483    0.0773814   0.305613   -0.493127    -0.387749     0.34586     0.253439     0.0546446    0.0648676   0.208582   -0.135107    -0.305134    -0.323864    0.163428
 -0.202576    -0.266769     0.326869      0.079211    -0.227689    0.0837886   0.0703678  -0.0901102    0.650821    -0.0910751  -0.670739      0.239916    -0.1017       0.256204    0.333094   -0.512297     0.207308     0.228271    0.554487    -0.12127     -0.25583     0.0877921  -0.00576257   0.10855     -0.519088   -0.130493
 -0.636821    -0.044194     0.807549     -0.060643    -0.554094   -0.129704    0.0925951   0.277375    -0.129477    -0.302575    0.464607     -0.0227429    0.00239998   0.146042   -0.114418    0.145789     0.0328114    0.0174282   0.00208227  -0.187519     0.363189    0.120369   -0.013689     0.439005     0.0523061   0.139999
  0.33848      0.390598    -0.634962      0.177587     0.864904    0.401993    0.0795781  -0.247271    -0.213377     0.573001   -0.138619      0.153006    -0.454957    -0.178921    0.100589   -0.273533    -0.213252    -0.0191853   0.11892      0.0382095   -0.231514    0.162456   -0.0331971   -0.260928     0.280222   -0.357091
  0.368583     0.400988    -0.371147      0.292517     0.25816    -0.230695   -0.0847825  -0.669113    -0.133249    -0.155874   -0.287109      0.488603     0.350707     0.0568382   0.414053   -0.520327     0.167739     0.162459   -0.686553     0.244872     0.458999    0.290039    0.261677    -0.428468    -0.222588   -0.115545
 -0.189804     0.00626607  -0.0838095     0.180215    -0.136103   -0.276665    0.271736   -0.104225    -0.348692    -0.0711745   0.238938      0.345853     0.111984    -0.682416    0.368286   -0.824044     0.0838403    0.51802     0.195908    -0.0956173    0.741991   -0.820525   -0.115816    -0.429853     0.380586   -0.234028
 -0.00648573   0.357912     0.0842538     0.53581     -0.145441   -0.176449    0.0589773  -0.925816    -0.331839     0.335119   -0.306764      0.112869     0.0297903    0.18508     0.0727499  -0.323408     0.35153      0.182042    0.404256     0.0189156   -0.274801   -0.658009    0.372462    -0.0306144    0.290691   -0.0802736
  0.274401     0.400402     0.0383521     0.524413    -0.202463   -0.0279629   0.267497    0.233266    -0.0109743    0.0871838  -0.232089      0.323502     0.110814    -0.0428896  -0.214737    0.428292     0.371201    -0.150016   -0.554568     0.00494649   0.0962773  -0.403399   -0.468448     0.171771     0.419009   -0.921888
 -0.0467994    0.136465    -0.303238      0.282803    -0.0635068  -0.51457     0.0933289   0.219967    -0.25823      0.135375    0.22356      -0.0146827    0.319336    -0.152716   -0.314692    0.651967     0.346072    -0.539841   -0.345877    -0.0327003   -0.143671   -0.325816   -0.29962     -0.128417     0.417103    0.641679
 -0.0317425   -0.0760294   -0.0408658    -0.0275411    0.189237   -0.163447    0.422866    0.274987     0.490997    -0.12679     0.723395      0.0515278   -0.44936     -0.797583    0.139556   -0.025437    -0.00131883   0.246607    0.0883878   -0.430999    -0.0780371  -0.267692   -0.294696     0.494173     0.263801    0.400683
  0.0670624   -0.525941    -0.286047      0.349432     0.707263    0.311352    0.265891    0.487339     0.194966    -0.0857112   0.450479     -0.0450853   -0.15547     -0.0404983   0.535704    0.309083     0.17913      0.261117    0.0089721   -0.480702     0.0510841  -0.0787254  -0.345488    -0.046572     0.376231   -0.381493[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.412997
[ Info: iteration 2, average log likelihood -1.412985
[ Info: iteration 3, average log likelihood -1.412972
[ Info: iteration 4, average log likelihood -1.412961
[ Info: iteration 5, average log likelihood -1.412950
[ Info: iteration 6, average log likelihood -1.412939
[ Info: iteration 7, average log likelihood -1.412929
[ Info: iteration 8, average log likelihood -1.412920
[ Info: iteration 9, average log likelihood -1.412910
[ Info: iteration 10, average log likelihood -1.412902
┌ Info: EM with 100000 data points 10 iterations avll -1.412902
└ 59.0 data points per parameter
kind full, method kmeans
[ Info: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.562911e+05
      1       7.097922e+05      -2.464990e+05 |       32
      2       6.949561e+05      -1.483604e+04 |       32
      3       6.890572e+05      -5.898975e+03 |       32
      4       6.860706e+05      -2.986577e+03 |       32
      5       6.842309e+05      -1.839693e+03 |       32
      6       6.828594e+05      -1.371514e+03 |       32
      7       6.818762e+05      -9.831457e+02 |       32
      8       6.811807e+05      -6.955561e+02 |       32
      9       6.806535e+05      -5.271323e+02 |       32
     10       6.802222e+05      -4.313320e+02 |       32
     11       6.798456e+05      -3.765628e+02 |       32
     12       6.795276e+05      -3.180841e+02 |       32
     13       6.792546e+05      -2.729931e+02 |       32
     14       6.790384e+05      -2.161284e+02 |       32
     15       6.788607e+05      -1.777409e+02 |       32
     16       6.787047e+05      -1.560037e+02 |       32
     17       6.785577e+05      -1.469883e+02 |       32
     18       6.784268e+05      -1.308631e+02 |       32
     19       6.783157e+05      -1.111541e+02 |       32
     20       6.782061e+05      -1.095381e+02 |       32
     21       6.780950e+05      -1.111547e+02 |       32
     22       6.779866e+05      -1.083910e+02 |       32
     23       6.778732e+05      -1.134327e+02 |       32
     24       6.777807e+05      -9.242989e+01 |       32
     25       6.776906e+05      -9.013134e+01 |       32
     26       6.776066e+05      -8.399675e+01 |       32
     27       6.775264e+05      -8.017367e+01 |       32
     28       6.774548e+05      -7.161730e+01 |       32
     29       6.773885e+05      -6.634280e+01 |       32
     30       6.773294e+05      -5.906493e+01 |       32
     31       6.772738e+05      -5.559514e+01 |       32
     32       6.772231e+05      -5.068809e+01 |       32
     33       6.771755e+05      -4.761626e+01 |       32
     34       6.771303e+05      -4.524638e+01 |       32
     35       6.770871e+05      -4.311879e+01 |       32
     36       6.770433e+05      -4.389653e+01 |       32
     37       6.769993e+05      -4.395753e+01 |       32
     38       6.769540e+05      -4.530438e+01 |       32
     39       6.769088e+05      -4.522448e+01 |       32
     40       6.768708e+05      -3.791944e+01 |       32
     41       6.768389e+05      -3.191370e+01 |       32
     42       6.768058e+05      -3.310888e+01 |       32
     43       6.767721e+05      -3.370597e+01 |       32
     44       6.767430e+05      -2.914135e+01 |       32
     45       6.767123e+05      -3.071901e+01 |       32
     46       6.766820e+05      -3.030206e+01 |       32
     47       6.766548e+05      -2.711170e+01 |       32
     48       6.766294e+05      -2.544229e+01 |       32
     49       6.766039e+05      -2.546343e+01 |       32
     50       6.765747e+05      -2.920915e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 676574.7267230959)
┌ Info: K-means with 32000 data points using 50 iterations
└ 37.0 data points per parameter
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.424665
[ Info: iteration 2, average log likelihood -1.419769
[ Info: iteration 3, average log likelihood -1.418448
[ Info: iteration 4, average log likelihood -1.417447
[ Info: iteration 5, average log likelihood -1.416388
[ Info: iteration 6, average log likelihood -1.415426
[ Info: iteration 7, average log likelihood -1.414786
[ Info: iteration 8, average log likelihood -1.414441
[ Info: iteration 9, average log likelihood -1.414254
[ Info: iteration 10, average log likelihood -1.414136
[ Info: iteration 11, average log likelihood -1.414050
[ Info: iteration 12, average log likelihood -1.413981
[ Info: iteration 13, average log likelihood -1.413923
[ Info: iteration 14, average log likelihood -1.413873
[ Info: iteration 15, average log likelihood -1.413827
[ Info: iteration 16, average log likelihood -1.413786
[ Info: iteration 17, average log likelihood -1.413748
[ Info: iteration 18, average log likelihood -1.413713
[ Info: iteration 19, average log likelihood -1.413679
[ Info: iteration 20, average log likelihood -1.413648
[ Info: iteration 21, average log likelihood -1.413618
[ Info: iteration 22, average log likelihood -1.413589
[ Info: iteration 23, average log likelihood -1.413562
[ Info: iteration 24, average log likelihood -1.413536
[ Info: iteration 25, average log likelihood -1.413511
[ Info: iteration 26, average log likelihood -1.413487
[ Info: iteration 27, average log likelihood -1.413464
[ Info: iteration 28, average log likelihood -1.413442
[ Info: iteration 29, average log likelihood -1.413421
[ Info: iteration 30, average log likelihood -1.413400
[ Info: iteration 31, average log likelihood -1.413379
[ Info: iteration 32, average log likelihood -1.413359
[ Info: iteration 33, average log likelihood -1.413339
[ Info: iteration 34, average log likelihood -1.413320
[ Info: iteration 35, average log likelihood -1.413300
[ Info: iteration 36, average log likelihood -1.413281
[ Info: iteration 37, average log likelihood -1.413262
[ Info: iteration 38, average log likelihood -1.413243
[ Info: iteration 39, average log likelihood -1.413224
[ Info: iteration 40, average log likelihood -1.413205
[ Info: iteration 41, average log likelihood -1.413186
[ Info: iteration 42, average log likelihood -1.413168
[ Info: iteration 43, average log likelihood -1.413150
[ Info: iteration 44, average log likelihood -1.413132
[ Info: iteration 45, average log likelihood -1.413114
[ Info: iteration 46, average log likelihood -1.413097
[ Info: iteration 47, average log likelihood -1.413081
[ Info: iteration 48, average log likelihood -1.413064
[ Info: iteration 49, average log likelihood -1.413049
[ Info: iteration 50, average log likelihood -1.413034
┌ Info: EM with 100000 data points 50 iterations avll -1.413034
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
  0.366366    0.568353    -0.022299     0.566621   -0.014488     0.131339      0.214202    -0.908687   -0.350473    0.106485   -0.332477    0.170085     0.258462    -0.0430862   0.0986569   0.260104    0.0523543   -0.222091    -0.406063    0.17917      -0.356787    -0.596168     0.0108723    0.146833    0.358345   -0.488414
  0.136739   -0.236282     0.0157263    0.213135    0.70752     -0.120671      0.340577     0.365932    1.22322    -0.386917    0.242871    0.423025    -0.43854     -0.0823078   0.656268    0.179775    0.45057      0.142302     0.0857223  -0.531594     -0.342388     0.111316     0.116025     0.396653    0.0669546   0.0476976
  0.340855    0.260997     0.00431672  -0.430956   -0.379013    -0.322696      0.38359      0.571062   -0.442785   -0.407435    0.158881   -0.569298    -0.0772235   -0.0419472   0.149397    0.152297    0.305452     0.103592    -0.331097    0.172848      0.135356    -0.3958      -0.343953     0.0483329  -0.344384   -0.56111
 -0.32769    -0.0835509    0.43376      0.201623   -0.142099     0.0842304     0.0041086   -0.0188683  -0.0995973   0.264493    0.248643    1.03384      0.573567    -0.374519    0.629466   -0.241338    0.144107     0.0304968    0.289143   -0.428185      0.572062    -0.150415     0.0351766   -0.0584084   0.337913   -0.104415
  0.372067    0.106522    -0.00667321   0.125963    0.0674919    0.329592     -0.339022     0.0859523   0.281418    0.068708   -0.545751   -0.0961749   -0.0074605    0.58336    -0.42283     0.827325   -0.185033    -0.366739    -0.194912    0.372697     -0.650252     0.520476     0.0407201    0.187501   -0.215861   -0.241852
  0.366658    0.540779    -0.328256     0.362492    0.25006     -0.316079     -0.0456347   -0.796815   -0.214514   -0.0477393  -0.482245    0.635884     0.175752     0.0463687   0.333551   -0.614183    0.193874     0.235921    -0.367775    0.131281      0.444302     0.139151     0.322902    -0.423575   -0.179963   -0.149781
 -0.48042    -0.227181     0.227377     0.421644    0.242352     0.0787516    -0.031544    -0.869574    0.275067    0.549821   -0.166808    0.390686     0.419047     0.263728    0.0990117   0.118821    0.0514448   -0.581165     0.426536   -0.0368261    -0.261564     0.084907    -0.058716     0.209882    0.300598    0.721742
 -0.453189   -0.0855989    0.981696     0.529991   -0.783394    -0.000712627   0.475348    -0.58769     0.134657   -0.640697    0.340516   -0.0236302   -0.0404744    0.360284    0.0924694  -0.109821   -0.0624153   -0.444057    -0.434977   -0.0371001    -0.568506     0.318407    -0.0606688    0.649852   -0.082404   -0.0571973
 -0.135648    0.0754871   -0.056071     0.150028    0.146577    -0.210079      0.0330991   -0.169501   -0.043043   -0.108762    0.365258   -0.106057    -0.0713382   -0.316933    0.588614   -0.460815    0.136194     0.571883     0.168182    0.000479541   0.169071    -0.301855     0.0703519   -0.07591     0.0919448   0.129667
  0.0121623   0.122864    -0.145079     0.659316    0.0494762   -0.273832     -0.168988    -0.481417   -0.234873   -0.0855471   0.0804835  -0.458915     0.00661504  -0.0467731   0.243732    0.201698    0.339104     0.0912349    0.272941   -0.324301     -0.703803    -0.540705     0.00451297   0.172275    0.108934    0.247841
  0.150698    0.466191    -0.946232    -0.320691    0.95929      0.253742     -0.597766     0.327941    0.038565    0.990644   -0.363238   -0.157326    -0.330603     0.0521417  -0.058362    0.625923   -0.124995     0.447122     0.972475   -0.517749     -0.0145579    0.0964505   -0.0681922   -0.258736    0.244295   -0.327547
  0.255004   -0.237431     0.0462977    0.450109   -0.0320257    0.220492      0.874338     0.151549    0.169239    0.182265    0.318213   -0.179926    -0.299443    -0.506719   -0.219119    0.312337   -0.262835     0.558478     0.272783   -0.666843     -0.380093    -0.190584    -1.01293      0.237867    0.598381    0.213408
 -0.0470134   0.548123     0.127043    -0.195744   -0.343343    -0.742871      0.0186325   -0.0127198   0.567225    0.752241   -0.312277    0.391919    -0.3862      -0.181687   -0.695548    0.176796   -0.00410014  -0.06059      0.385924    0.0700742    -0.532434     0.688075     0.408293    -0.228827   -0.0582243   0.878251
  0.184613   -0.126798     0.122302    -0.239336   -0.0478371    0.195585     -0.0539046   -0.0575727  -0.0580026  -0.10629     0.13129     0.0391486    0.0527708    0.0229075   0.162653   -0.210313   -0.37247     -0.00143755   0.0698574  -0.180091      0.00586335   0.272672    -0.221407    -0.349153   -0.240697    0.167889
  0.0562347   0.219837    -0.251626     0.257826    0.364374    -0.504036      0.253323     0.593111   -0.237768    0.0928479   0.483164    0.345907     0.104708    -0.193696   -0.358071    0.530597    0.631227    -0.497582    -0.64179    -0.172465      0.0294812   -0.173537    -0.25198     -0.543962    0.630397   -0.0574737
  0.166706   -0.615669     0.221309     0.016787   -0.392357     0.286339     -0.273257    -0.391246    0.59498    -0.170257   -0.456537   -0.120521     0.107038     0.0343754   0.685591   -0.656614   -0.140735     0.588609     0.63803     0.05128      -0.287358    -0.0351064   -0.0581622    0.257518   -0.672907    0.0127692
 -0.324662   -0.203836     0.0986483   -0.765974   -0.36129      0.489214      0.0425984    0.22286    -0.0754974   0.146252   -0.261655    0.560392    -0.239284     0.0605406  -0.476356   -0.771857   -0.406746    -0.306331    -0.196472    0.753341      0.712494     0.301215     0.432542    -0.233548   -0.191236   -0.643022
  0.0721648   0.129401     0.0611169    0.151318   -0.134445    -0.12631       0.0837942   -0.340175   -0.0405506   0.181507   -0.126941    0.0850371   -0.0866345    0.134823   -0.0322198  -0.239256    0.0593805    0.102679     0.0233485   0.205433     -0.0776122   -0.125299     0.216278     0.0491128   0.0327815  -0.116163
  0.120994    0.299914     0.140481    -0.453546   -0.161603     0.50439      -0.136261    -1.07065    -0.455347    0.167854   -0.498277   -0.04575     -0.376747     0.498334   -0.305012   -0.49664    -0.474644    -0.348499     0.284045    0.150379     -0.707181     0.0092433    0.378369     0.0809465  -0.286805   -0.307123
  0.412425   -0.920602    -0.160356     0.284339   -0.184303    -0.359054     -0.443164     0.156357    0.286619   -0.241935    0.385543   -0.492103     0.993477    -0.0796278   0.0717248  -0.277849   -0.0535023    0.174651    -0.636092    0.624236      0.175018    -0.801282    -0.0674884   -0.521747   -0.31288    -0.0382103
  0.390779    0.0558652   -0.267717    -0.321515    0.364604     0.367046      0.109379     0.376334    0.0608703   0.296304    0.217784    0.595911     0.121478    -0.896048    0.0967854   0.0539948  -0.224065    -1.03034     -0.287164   -0.221451      0.0553903    0.100563    -0.609756     0.236558   -0.0720855   0.312196
 -0.32633    -0.0839926   -0.168386    -0.0512001  -0.00353833  -0.389064     -0.209882     0.0978442  -0.0309246  -0.374497    0.320961    0.00551864  -0.233492    -0.432376    0.218828   -0.65459     0.141096     0.482048     0.189839    0.0246425     0.529806    -0.484592     0.0872017   -0.21033     0.232745    0.328806
 -0.428607   -0.020401     0.126169    -0.777909   -0.176703    -0.0648146    -0.550838     0.444942   -0.590433   -0.245196    0.302242   -0.10195      0.390966     0.270754    0.300018    0.107009   -0.299472    -0.162415     0.535621   -0.00920033    0.232295     0.283189     0.31571     -0.122455   -0.426137    0.54569
  0.0215465  -0.122633    -0.20357      0.109243   -0.217794    -0.305375     -0.134358     0.191049   -0.438214   -0.509012   -0.526611   -0.866114    -0.609709     0.646456   -0.105659   -0.07473    -0.0349526    0.736295    -0.0528987  -0.0333652     0.158906     0.186176     0.371095    -0.676953   -0.576832   -0.425316
  0.254993    0.0552464   -0.758982     0.0230544   0.589826     0.092962     -0.0678314    0.163383   -0.309907    0.419525    0.138475   -0.314225    -0.013851    -0.043189    0.456438   -0.172623   -0.155055     0.293043     0.0520228   0.301783      0.0317202    0.0913105    0.101852    -0.518211    0.16837    -0.017673
 -0.237698    0.0858629   -0.460004    -0.104199   -0.0623827   -0.0828527     0.146002     0.399804   -0.130366   -0.0686738  -0.236771   -0.791129    -0.502041     0.0735472  -0.966165    0.606272   -0.136942    -0.400095    -0.0598818   0.148711     -0.404317     0.0436205   -0.0576618    0.145095   -0.03141     0.247301
  0.142375    0.296626     0.336348     0.412432   -0.425704     0.0405943    -0.00738707   0.335105    0.119142    0.314886   -0.484607    0.689503     0.057519     0.323213   -0.460514    0.47302     0.188763    -0.313732    -0.109013   -0.162708      0.163333    -0.0473969   -0.661774     0.171312    0.150787   -0.492804
 -0.143876    0.0659727   -0.00142564   0.0564029   0.0725262    0.2299        0.273898     0.218789    0.393434    0.0445626  -0.381279    0.283237    -0.133656     0.261205    0.0334643  -0.11604     0.305168    -0.105951     0.15783    -0.102163     -0.117972     0.0572182   -0.382103    -0.0331591  -0.0155349  -0.370207
 -0.309096    0.00726229   0.149056     0.0717782  -0.0491418   -0.143863      0.0177138    0.262632   -0.084854   -0.0345789   0.158948   -0.154382     0.0151713   -0.100441   -0.185695    0.514707    0.0147145   -0.116269    -0.147125   -0.141949      0.152229    -0.00942165  -0.0252779    0.301918    0.150332    0.123547
  0.214351   -0.255939     0.233882    -0.207875   -0.23681     -0.0251897     0.120352     0.597624    0.417508    0.398784    0.973577    0.0203483   -0.500855    -0.092062   -0.253692   -0.582992   -0.402787     0.782705    -0.188597    0.111227      0.547854     0.307047     0.147802    -0.03103    -0.201825    0.0180889
 -0.419286   -0.287959    -0.110458    -0.328662   -0.0488121    0.27548      -0.00256069  -0.0390919  -0.265632   -0.208901   -0.176266   -0.720749    -0.431956     0.165829   -0.139226   -0.0951214  -0.320314    -0.0354148    0.211031    0.199776     -0.0779198    0.25252      0.102875     0.15626    -0.546962    0.309408
 -0.149679   -0.562076    -0.0970711    0.147901    0.347774     1.01921       0.252456    -0.11273    -0.547354   -0.60335     0.315581   -0.366221     0.488264     0.296748    0.852547   -0.286281   -0.0767669    0.0332112   -0.36163    -0.0490544     0.459304    -0.430502    -0.297512    -0.0264431   0.162715   -0.695628[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.413019
[ Info: iteration 2, average log likelihood -1.413006
[ Info: iteration 3, average log likelihood -1.412993
[ Info: iteration 4, average log likelihood -1.412980
[ Info: iteration 5, average log likelihood -1.412969
[ Info: iteration 6, average log likelihood -1.412958
[ Info: iteration 7, average log likelihood -1.412947
[ Info: iteration 8, average log likelihood -1.412937
[ Info: iteration 9, average log likelihood -1.412928
[ Info: iteration 10, average log likelihood -1.412919
┌ Info: EM with 100000 data points 10 iterations avll -1.412919
└ 59.0 data points per parameter
[ Info: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.678561e+05
      1       2.230230e+04      -1.455538e+05 |        2
      2       7.823675e+03      -1.447862e+04 |        0
      3       7.823675e+03       0.000000e+00 |        0
K-means converged with 3 iterations (objv = 7823.67549422947)
┌ Info: K-means with 900 data points using 3 iterations
└ 150.0 data points per parameter
[ Info: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
[ Info: iteration 1, average log likelihood -2.043155
[ Info: iteration 2, average log likelihood -2.043154
[ Info: iteration 3, average log likelihood -2.043154
[ Info: iteration 4, average log likelihood -2.043154
[ Info: iteration 5, average log likelihood -2.043154
[ Info: iteration 6, average log likelihood -2.043154
[ Info: iteration 7, average log likelihood -2.043154
[ Info: iteration 8, average log likelihood -2.043154
[ Info: iteration 9, average log likelihood -2.043154
[ Info: iteration 10, average log likelihood -2.043154
┌ Info: EM with 900 data points 10 iterations avll -2.043154
└ 81.8 data points per parameter
    Testing GaussianMixtures tests passed 
