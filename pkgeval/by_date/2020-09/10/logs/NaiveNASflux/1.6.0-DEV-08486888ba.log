Julia Version 1.6.0-DEV.878
Commit 08486888ba (2020-09-09 16:27 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-9.0.1 (ORCJIT, skylake-avx512)
Environment:
  JULIA_DEPOT_PATH = ::/usr/local/share/julia
  JULIA_NUM_THREADS = 2

  Resolving package versions...
[ Info: LEGAL NOTICE: package operations send anonymous data about your system to https://pkg.julialang.org (your current package server), including the operating system and Julia versions you are using, and a random client UUID. Running `Pkg.telemetryinfo()` will show exactly what data is sent. See https://julialang.org/legal/data/ for more details about what this data is used for, how long it is retained, and how to opt out of sending it.
  Installed IniFile ────────────────────── v0.5.0
  Installed AbstractFFTs ───────────────── v0.5.0
  Installed SIMDPirates ────────────────── v0.8.25
  Installed JSONSchema ─────────────────── v0.3.2
  Installed NaiveNASflux ───────────────── v1.5.0
  Installed OffsetArrays ───────────────── v1.1.3
  Installed NNlib ──────────────────────── v0.7.4
  Installed ChainRulesCore ─────────────── v0.9.8
  Installed IRTools ────────────────────── v0.4.1
  Installed BenchmarkTools ─────────────── v0.5.0
  Installed Calculus ───────────────────── v0.5.1
  Installed Cgl_jll ────────────────────── v0.60.2+5
  Installed NaiveNASlib ────────────────── v1.3.0
  Installed VectorizationBase ──────────── v0.12.33
  Installed Zygote ─────────────────────── v0.5.6
  Installed OpenBLAS32_jll ─────────────── v0.3.9+4
  Installed CoinUtils_jll ──────────────── v2.11.3+3
  Installed Setfield ───────────────────── v0.7.0
  Installed Colors ─────────────────────── v0.12.4
  Installed Zlib_jll ───────────────────── v1.2.11+16
  Installed MetaGraphs ─────────────────── v0.6.5
  Installed TranscodingStreams ─────────── v0.9.5
  Installed ExprTools ──────────────────── v0.1.2
  Installed ArnoldiMethod ──────────────── v0.0.4
  Installed MacroTools ─────────────────── v0.5.5
  Installed DiffRules ──────────────────── v1.0.1
  Installed HTTP ───────────────────────── v0.8.17
  Installed Osi_jll ────────────────────── v0.108.5+3
  Installed SimpleTraits ───────────────── v0.9.3
  Installed ZygoteRules ────────────────── v0.2.0
  Installed OrderedCollections ─────────── v1.3.0
  Installed Inflate ────────────────────── v0.1.2
  Installed DataStructures ─────────────── v0.17.20
  Installed CompilerSupportLibraries_jll ─ v0.3.3+0
  Installed SpecialFunctions ───────────── v0.10.3
  Installed Functors ───────────────────── v0.1.0
  Installed ConstructionBase ───────────── v1.0.0
  Installed CEnum ──────────────────────── v0.3.0
  Installed MbedTLS_jll ────────────────── v2.16.8+0
  Installed StaticArrays ───────────────── v0.12.4
  Installed JLD2 ───────────────────────── v0.1.14
  Installed ChainRules ─────────────────── v0.7.18
  Installed LLVM ───────────────────────── v2.0.0
  Installed OpenSpecFun_jll ────────────── v0.5.3+3
  Installed Cbc_jll ────────────────────── v2.10.3+4
  Installed Requires ───────────────────── v1.0.2
  Installed MuladdMacro ────────────────── v0.2.2
  Installed AbstractTrees ──────────────── v0.3.3
  Installed GPUCompiler ────────────────── v0.6.1
  Installed Clp_jll ────────────────────── v1.17.6+6
  Installed Reexport ───────────────────── v0.2.0
  Installed DiffResults ────────────────── v1.0.2
  Installed Juno ───────────────────────── v0.8.3
  Installed SLEEFPirates ───────────────── v0.5.5
  Installed LoopVectorization ──────────── v0.8.26
  Installed CUDA ───────────────────────── v1.3.3
  Installed LightGraphs ────────────────── v1.3.3
  Installed JSON ───────────────────────── v0.21.1
  Installed DataAPI ────────────────────── v1.3.0
  Installed CodecZlib ──────────────────── v0.7.0
  Installed MbedTLS ────────────────────── v1.0.2
  Installed MutableArithmetics ─────────── v0.2.10
  Installed CommonSubexpressions ───────── v0.3.0
  Installed Adapt ──────────────────────── v2.0.2
  Installed MathOptInterface ───────────── v0.9.14
  Installed CodecBzip2 ─────────────────── v0.7.2
  Installed Parsers ────────────────────── v1.0.10
  Installed FileIO ─────────────────────── v1.4.3
  Installed FillArrays ─────────────────── v0.9.6
  Installed ArrayLayouts ───────────────── v0.4.7
  Installed BinaryProvider ─────────────── v0.5.10
  Installed ColorTypes ─────────────────── v0.10.9
  Installed TimerOutputs ───────────────── v0.5.6
  Installed Missings ───────────────────── v0.4.4
  Installed UnPack ─────────────────────── v1.0.2
  Installed ForwardDiff ────────────────── v0.10.12
  Installed NaNMath ────────────────────── v0.3.4
  Installed SortingAlgorithms ──────────── v0.3.1
  Installed Bzip2_jll ──────────────────── v1.0.6+4
  Installed ZipFile ────────────────────── v0.9.2
  Installed CpuId ──────────────────────── v0.2.2
  Installed FixedPointNumbers ──────────── v0.8.4
  Installed DocStringExtensions ────────── v0.8.3
  Installed Cbc ────────────────────────── v0.7.0
  Installed Flux ───────────────────────── v0.11.1
  Installed JuMP ───────────────────────── v0.21.3
  Installed Media ──────────────────────── v0.5.0
  Installed GPUArrays ──────────────────── v5.1.0
  Installed StatsBase ──────────────────── v0.33.1
Updating `~/.julia/environments/v1.6/Project.toml`
  [85610aed] + NaiveNASflux v1.5.0
Updating `~/.julia/environments/v1.6/Manifest.toml`
  [621f4979] + AbstractFFTs v0.5.0
  [1520ce14] + AbstractTrees v0.3.3
  [79e6a3ab] + Adapt v2.0.2
  [ec485272] + ArnoldiMethod v0.0.4
  [4c555306] + ArrayLayouts v0.4.7
  [6e4b80f9] + BenchmarkTools v0.5.0
  [b99e7846] + BinaryProvider v0.5.10
  [6e34b625] + Bzip2_jll v1.0.6+4
  [fa961155] + CEnum v0.3.0
  [052768ef] + CUDA v1.3.3
  [49dc2e85] + Calculus v0.5.1
  [9961bab8] + Cbc v0.7.0
  [38041ee0] + Cbc_jll v2.10.3+4
  [3830e938] + Cgl_jll v0.60.2+5
  [082447d4] + ChainRules v0.7.18
  [d360d2e6] + ChainRulesCore v0.9.8
  [06985876] + Clp_jll v1.17.6+6
  [523fee87] + CodecBzip2 v0.7.2
  [944b1d66] + CodecZlib v0.7.0
  [be027038] + CoinUtils_jll v2.11.3+3
  [3da002f7] + ColorTypes v0.10.9
  [5ae59095] + Colors v0.12.4
  [bbf7d656] + CommonSubexpressions v0.3.0
  [e66e0078] + CompilerSupportLibraries_jll v0.3.3+0
  [187b0558] + ConstructionBase v1.0.0
  [adafc99b] + CpuId v0.2.2
  [9a962f9c] + DataAPI v1.3.0
  [864edb3b] + DataStructures v0.17.20
  [163ba53b] + DiffResults v1.0.2
  [b552c78f] + DiffRules v1.0.1
  [ffbed154] + DocStringExtensions v0.8.3
  [e2ba6199] + ExprTools v0.1.2
  [5789e2e9] + FileIO v1.4.3
  [1a297f60] + FillArrays v0.9.6
  [53c48c17] + FixedPointNumbers v0.8.4
  [587475ba] + Flux v0.11.1
  [f6369f11] + ForwardDiff v0.10.12
  [d9f16b24] + Functors v0.1.0
  [0c68f7d7] + GPUArrays v5.1.0
  [61eb1bfa] + GPUCompiler v0.6.1
  [cd3eb016] + HTTP v0.8.17
  [7869d1d1] + IRTools v0.4.1
  [d25df0c9] + Inflate v0.1.2
  [83e8ac13] + IniFile v0.5.0
  [033835bb] + JLD2 v0.1.14
  [682c06a0] + JSON v0.21.1
  [7d188eb4] + JSONSchema v0.3.2
  [4076af6c] + JuMP v0.21.3
  [e5e0dc1b] + Juno v0.8.3
  [929cbde3] + LLVM v2.0.0
  [093fc24a] + LightGraphs v1.3.3
  [bdcacae8] + LoopVectorization v0.8.26
  [1914dd2f] + MacroTools v0.5.5
  [b8f27783] + MathOptInterface v0.9.14
  [739be429] + MbedTLS v1.0.2
  [c8ffd9c3] + MbedTLS_jll v2.16.8+0
  [e89f7d12] + Media v0.5.0
  [626554b9] + MetaGraphs v0.6.5
  [e1d29d7a] + Missings v0.4.4
  [46d2c3a1] + MuladdMacro v0.2.2
  [d8a4904e] + MutableArithmetics v0.2.10
  [872c559c] + NNlib v0.7.4
  [77ba4419] + NaNMath v0.3.4
  [85610aed] + NaiveNASflux v1.5.0
  [bd45eb3e] + NaiveNASlib v1.3.0
  [6fe1bfb0] + OffsetArrays v1.1.3
  [656ef2d0] + OpenBLAS32_jll v0.3.9+4
  [efe28fd5] + OpenSpecFun_jll v0.5.3+3
  [bac558e1] + OrderedCollections v1.3.0
  [7da25872] + Osi_jll v0.108.5+3
  [69de0a69] + Parsers v1.0.10
  [189a3867] + Reexport v0.2.0
  [ae029012] + Requires v1.0.2
  [21efa798] + SIMDPirates v0.8.25
  [476501e8] + SLEEFPirates v0.5.5
  [efcf1570] + Setfield v0.7.0
  [699a6c99] + SimpleTraits v0.9.3
  [a2af1166] + SortingAlgorithms v0.3.1
  [276daf66] + SpecialFunctions v0.10.3
  [90137ffa] + StaticArrays v0.12.4
  [2913bbd2] + StatsBase v0.33.1
  [a759f4b9] + TimerOutputs v0.5.6
  [3bb67fe8] + TranscodingStreams v0.9.5
  [3a884ed6] + UnPack v1.0.2
  [3d5dd08c] + VectorizationBase v0.12.33
  [a5390f91] + ZipFile v0.9.2
  [83775a58] + Zlib_jll v1.2.11+16
  [e88e6eb3] + Zygote v0.5.6
  [700de1a5] + ZygoteRules v0.2.0
  [2a0f44e3] + Base64
  [ade2ca70] + Dates
  [8bb1440f] + DelimitedFiles
  [8ba89e20] + Distributed
  [9fa8497b] + Future
  [b77e0a4c] + InteractiveUtils
  [76f85450] + LibGit2
  [8f399da3] + Libdl
  [37e2e46d] + LinearAlgebra
  [56ddb016] + Logging
  [d6f4376e] + Markdown
  [a63ad114] + Mmap
  [44cfe95a] + Pkg
  [de0858da] + Printf
  [9abbd945] + Profile
  [3fa0cd96] + REPL
  [9a3f8284] + Random
  [ea8e919c] + SHA
  [9e88b42a] + Serialization
  [1a1011a3] + SharedArrays
  [6462fe0b] + Sockets
  [2f01184e] + SparseArrays
  [10745b16] + Statistics
  [fa267f1f] + TOML
  [8dfed614] + Test
  [cf7118a7] + UUIDs
  [4ec0a83e] + Unicode
   Building SLEEFPirates → `~/.julia/packages/SLEEFPirates/jGsib/deps/build.log`
   Building Cbc ─────────→ `~/.julia/packages/Cbc/f5sSt/deps/build.log`
    Testing NaiveNASflux
Status `/tmp/jl_0EhfFS/Project.toml`
  [587475ba] Flux v0.11.1
  [4076af6c] JuMP v0.21.3
  [85610aed] NaiveNASflux v1.5.0
  [bd45eb3e] NaiveNASlib v1.3.0
  [189a3867] Reexport v0.2.0
  [efcf1570] Setfield v0.7.0
  [b77e0a4c] InteractiveUtils
  [37e2e46d] LinearAlgebra
  [9a3f8284] Random
  [10745b16] Statistics
  [8dfed614] Test
Status `/tmp/jl_0EhfFS/Manifest.toml`
  [621f4979] AbstractFFTs v0.5.0
  [1520ce14] AbstractTrees v0.3.3
  [79e6a3ab] Adapt v2.0.2
  [ec485272] ArnoldiMethod v0.0.4
  [4c555306] ArrayLayouts v0.4.7
  [6e4b80f9] BenchmarkTools v0.5.0
  [b99e7846] BinaryProvider v0.5.10
  [6e34b625] Bzip2_jll v1.0.6+4
  [fa961155] CEnum v0.3.0
  [052768ef] CUDA v1.3.3
  [49dc2e85] Calculus v0.5.1
  [9961bab8] Cbc v0.7.0
  [38041ee0] Cbc_jll v2.10.3+4
  [3830e938] Cgl_jll v0.60.2+5
  [082447d4] ChainRules v0.7.18
  [d360d2e6] ChainRulesCore v0.9.8
  [06985876] Clp_jll v1.17.6+6
  [523fee87] CodecBzip2 v0.7.2
  [944b1d66] CodecZlib v0.7.0
  [be027038] CoinUtils_jll v2.11.3+3
  [3da002f7] ColorTypes v0.10.9
  [5ae59095] Colors v0.12.4
  [bbf7d656] CommonSubexpressions v0.3.0
  [e66e0078] CompilerSupportLibraries_jll v0.3.3+0
  [187b0558] ConstructionBase v1.0.0
  [adafc99b] CpuId v0.2.2
  [9a962f9c] DataAPI v1.3.0
  [864edb3b] DataStructures v0.17.20
  [163ba53b] DiffResults v1.0.2
  [b552c78f] DiffRules v1.0.1
  [ffbed154] DocStringExtensions v0.8.3
  [e2ba6199] ExprTools v0.1.2
  [5789e2e9] FileIO v1.4.3
  [1a297f60] FillArrays v0.9.6
  [53c48c17] FixedPointNumbers v0.8.4
  [587475ba] Flux v0.11.1
  [f6369f11] ForwardDiff v0.10.12
  [d9f16b24] Functors v0.1.0
  [0c68f7d7] GPUArrays v5.1.0
  [61eb1bfa] GPUCompiler v0.6.1
  [cd3eb016] HTTP v0.8.17
  [7869d1d1] IRTools v0.4.1
  [d25df0c9] Inflate v0.1.2
  [83e8ac13] IniFile v0.5.0
  [033835bb] JLD2 v0.1.14
  [682c06a0] JSON v0.21.1
  [7d188eb4] JSONSchema v0.3.2
  [4076af6c] JuMP v0.21.3
  [e5e0dc1b] Juno v0.8.3
  [929cbde3] LLVM v2.0.0
  [093fc24a] LightGraphs v1.3.3
  [bdcacae8] LoopVectorization v0.8.26
  [1914dd2f] MacroTools v0.5.5
  [b8f27783] MathOptInterface v0.9.14
  [739be429] MbedTLS v1.0.2
  [c8ffd9c3] MbedTLS_jll v2.16.8+0
  [e89f7d12] Media v0.5.0
  [626554b9] MetaGraphs v0.6.5
  [e1d29d7a] Missings v0.4.4
  [46d2c3a1] MuladdMacro v0.2.2
  [d8a4904e] MutableArithmetics v0.2.10
  [872c559c] NNlib v0.7.4
  [77ba4419] NaNMath v0.3.4
  [85610aed] NaiveNASflux v1.5.0
  [bd45eb3e] NaiveNASlib v1.3.0
  [6fe1bfb0] OffsetArrays v1.1.3
  [656ef2d0] OpenBLAS32_jll v0.3.9+4
  [efe28fd5] OpenSpecFun_jll v0.5.3+3
  [bac558e1] OrderedCollections v1.3.0
  [7da25872] Osi_jll v0.108.5+3
  [69de0a69] Parsers v1.0.10
  [189a3867] Reexport v0.2.0
  [ae029012] Requires v1.0.2
  [21efa798] SIMDPirates v0.8.25
  [476501e8] SLEEFPirates v0.5.5
  [efcf1570] Setfield v0.7.0
  [699a6c99] SimpleTraits v0.9.3
  [a2af1166] SortingAlgorithms v0.3.1
  [276daf66] SpecialFunctions v0.10.3
  [90137ffa] StaticArrays v0.12.4
  [2913bbd2] StatsBase v0.33.1
  [a759f4b9] TimerOutputs v0.5.6
  [3bb67fe8] TranscodingStreams v0.9.5
  [3a884ed6] UnPack v1.0.2
  [3d5dd08c] VectorizationBase v0.12.33
  [a5390f91] ZipFile v0.9.2
  [83775a58] Zlib_jll v1.2.11+16
  [e88e6eb3] Zygote v0.5.6
  [700de1a5] ZygoteRules v0.2.0
  [2a0f44e3] Base64
  [ade2ca70] Dates
  [8bb1440f] DelimitedFiles
  [8ba89e20] Distributed
  [9fa8497b] Future
  [b77e0a4c] InteractiveUtils
  [76f85450] LibGit2
  [8f399da3] Libdl
  [37e2e46d] LinearAlgebra
  [56ddb016] Logging
  [d6f4376e] Markdown
  [a63ad114] Mmap
  [44cfe95a] Pkg
  [de0858da] Printf
  [9abbd945] Profile
  [3fa0cd96] REPL
  [9a3f8284] Random
  [ea8e919c] SHA
  [9e88b42a] Serialization
  [1a1011a3] SharedArrays
  [6462fe0b] Sockets
  [2f01184e] SparseArrays
  [10745b16] Statistics
  [fa267f1f] TOML
  [8dfed614] Test
  [cf7118a7] UUIDs
  [4ec0a83e] Unicode
    Testing Running tests...
[ Info: Testing util
[ Info: Testing select
[ Info: Testing mutable
[ Info: Testing vertex
Dense-Dense-Dense: Error During Test at /home/pkgeval/.julia/packages/NaiveNASflux/0lGnm/test/vertex.jl:355
  Got exception outside of a @test
  MethodError: no method matching Core.Compiler.IRCode(::Vector{Any}, ::Vector{Any}, ::Vector{Int32}, ::Vector{UInt8}, ::Core.Compiler.CFG, ::Vector{Core.LineInfoNode}, ::Vector{Any}, ::Vector{Any}, ::Vector{Any})
  Stacktrace:
    [1] Core.Compiler.IRCode(ir::IRTools.Inner.IR)
      @ IRTools.Inner.Wrap ~/.julia/packages/IRTools/GVPoj/src/ir/wrap.jl:55
    [2] update!(ci::Core.CodeInfo, ir::IRTools.Inner.IR)
      @ IRTools.Inner ~/.julia/packages/IRTools/GVPoj/src/reflection/utils.jl:143
    [3] #s2778#1234
      @ ~/.julia/packages/Zygote/Xgcgs/src/compiler/interface2.jl:34 [inlined]
    [4] #s2778#1234(::Any, ctx::Any, f::Any, args::Any)
      @ Zygote ./none:0
    [5] (::Core.GeneratedFunctionStub)(::Any, ::Vararg{Any, N} where N)
      @ Core ./boot.jl:556
    [6] pullback(f::Function, ps::Zygote.Params)
      @ Zygote ~/.julia/packages/Zygote/Xgcgs/src/compiler/interface.jl:172
    [7] gradient(f::Function, args::Zygote.Params)
      @ Zygote ~/.julia/packages/Zygote/Xgcgs/src/compiler/interface.jl:53
    [8] macro expansion
      @ ~/.julia/packages/Flux/05b38/src/optimise/train.jl:82 [inlined]
    [9] macro expansion
      @ ~/.julia/packages/Juno/hEPx8/src/progress.jl:134 [inlined]
   [10] train!(loss::Function, ps::Zygote.Params, data::Vector{Tuple{Matrix{Float64}, Matrix{Float64}}}, opt::Descent; cb::Flux.Optimise.var"#16#22")
      @ Flux.Optimise ~/.julia/packages/Flux/05b38/src/optimise/train.jl:80
   [11] train!(loss::Function, ps::Zygote.Params, data::Vector{Tuple{Matrix{Float64}, Matrix{Float64}}}, opt::Descent)
      @ Flux.Optimise ~/.julia/packages/Flux/05b38/src/optimise/train.jl:78
   [12] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/vertex.jl:375
   [13] top-level scope
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1113
   [14] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/vertex.jl:356
   [15] top-level scope
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1113
   [16] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/vertex.jl:352
   [17] include(fname::String)
      @ Base.MainInclude ./client.jl:444
   [18] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/runtests.jl:23
   [19] top-level scope
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1113
   [20] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/runtests.jl:13
   [21] include(fname::String)
      @ Base.MainInclude ./client.jl:444
   [22] top-level scope
      @ none:6
   [23] eval(m::Module, e::Any)
      @ Core ./boot.jl:345
   [24] exec_options(opts::Base.JLOptions)
      @ Base ./client.jl:261
   [25] _start()
      @ Base ./client.jl:485
Conv-Bn-Conv: Error During Test at /home/pkgeval/.julia/packages/NaiveNASflux/0lGnm/test/vertex.jl:382
  Got exception outside of a @test
  MethodError: no method matching Core.Compiler.IRCode(::Vector{Any}, ::Vector{Any}, ::Vector{Int32}, ::Vector{UInt8}, ::Core.Compiler.CFG, ::Vector{Core.LineInfoNode}, ::Vector{Any}, ::Vector{Any}, ::Vector{Any})
  Stacktrace:
    [1] Core.Compiler.IRCode(ir::IRTools.Inner.IR)
      @ IRTools.Inner.Wrap ~/.julia/packages/IRTools/GVPoj/src/ir/wrap.jl:55
    [2] update!(ci::Core.CodeInfo, ir::IRTools.Inner.IR)
      @ IRTools.Inner ~/.julia/packages/IRTools/GVPoj/src/reflection/utils.jl:143
    [3] #s2778#1234
      @ ~/.julia/packages/Zygote/Xgcgs/src/compiler/interface2.jl:34 [inlined]
    [4] #s2778#1234(::Any, ctx::Any, f::Any, args::Any)
      @ Zygote ./none:0
    [5] (::Core.GeneratedFunctionStub)(::Any, ::Vararg{Any, N} where N)
      @ Core ./boot.jl:556
    [6] pullback(f::Function, ps::Zygote.Params)
      @ Zygote ~/.julia/packages/Zygote/Xgcgs/src/compiler/interface.jl:172
    [7] gradient(f::Function, args::Zygote.Params)
      @ Zygote ~/.julia/packages/Zygote/Xgcgs/src/compiler/interface.jl:53
    [8] macro expansion
      @ ~/.julia/packages/Flux/05b38/src/optimise/train.jl:82 [inlined]
    [9] macro expansion
      @ ~/.julia/packages/Juno/hEPx8/src/progress.jl:134 [inlined]
   [10] train!(loss::Function, ps::Zygote.Params, data::Vector{Tuple{Array{Float32, 4}, Array{Float32, 4}}}, opt::Descent; cb::Flux.Optimise.var"#16#22")
      @ Flux.Optimise ~/.julia/packages/Flux/05b38/src/optimise/train.jl:80
   [11] train!(loss::Function, ps::Zygote.Params, data::Vector{Tuple{Array{Float32, 4}, Array{Float32, 4}}}, opt::Descent)
      @ Flux.Optimise ~/.julia/packages/Flux/05b38/src/optimise/train.jl:78
   [12] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/vertex.jl:401
   [13] top-level scope
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1113
   [14] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/vertex.jl:383
   [15] top-level scope
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1113
   [16] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/vertex.jl:352
   [17] include(fname::String)
      @ Base.MainInclude ./client.jl:444
   [18] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/runtests.jl:23
   [19] top-level scope
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1113
   [20] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/runtests.jl:13
   [21] include(fname::String)
      @ Base.MainInclude ./client.jl:444
   [22] top-level scope
      @ none:6
   [23] eval(m::Module, e::Any)
      @ Core ./boot.jl:345
   [24] exec_options(opts::Base.JLOptions)
      @ Base ./client.jl:261
   [25] _start()
      @ Base ./client.jl:485
WARNING: both Flux and NaiveNASlib export "flatten"; uses of it in module NaiveNASflux must be qualified
Conv-Conv-Conv: Error During Test at /home/pkgeval/.julia/packages/NaiveNASflux/0lGnm/test/vertex.jl:407
  Got exception outside of a @test
  MethodError: no method matching Core.Compiler.IRCode(::Vector{Any}, ::Vector{Any}, ::Vector{Int32}, ::Vector{UInt8}, ::Core.Compiler.CFG, ::Vector{Core.LineInfoNode}, ::Vector{Any}, ::Vector{Any}, ::Vector{Any})
  Stacktrace:
    [1] Core.Compiler.IRCode(ir::IRTools.Inner.IR)
      @ IRTools.Inner.Wrap ~/.julia/packages/IRTools/GVPoj/src/ir/wrap.jl:55
    [2] update!(ci::Core.CodeInfo, ir::IRTools.Inner.IR)
      @ IRTools.Inner ~/.julia/packages/IRTools/GVPoj/src/reflection/utils.jl:143
    [3] #s2778#1234
      @ ~/.julia/packages/Zygote/Xgcgs/src/compiler/interface2.jl:34 [inlined]
    [4] #s2778#1234(::Any, ctx::Any, f::Any, args::Any)
      @ Zygote ./none:0
    [5] (::Core.GeneratedFunctionStub)(::Any, ::Vararg{Any, N} where N)
      @ Core ./boot.jl:556
    [6] pullback(f::Function, ps::Zygote.Params)
      @ Zygote ~/.julia/packages/Zygote/Xgcgs/src/compiler/interface.jl:172
    [7] gradient(f::Function, args::Zygote.Params)
      @ Zygote ~/.julia/packages/Zygote/Xgcgs/src/compiler/interface.jl:53
    [8] macro expansion
      @ ~/.julia/packages/Flux/05b38/src/optimise/train.jl:82 [inlined]
    [9] macro expansion
      @ ~/.julia/packages/Juno/hEPx8/src/progress.jl:134 [inlined]
   [10] train!(loss::Function, ps::Zygote.Params, data::Vector{Tuple{Array{Float32, 4}, Array{Float32, 4}}}, opt::Descent; cb::Flux.Optimise.var"#16#22")
      @ Flux.Optimise ~/.julia/packages/Flux/05b38/src/optimise/train.jl:80
   [11] train!(loss::Function, ps::Zygote.Params, data::Vector{Tuple{Array{Float32, 4}, Array{Float32, 4}}}, opt::Descent)
      @ Flux.Optimise ~/.julia/packages/Flux/05b38/src/optimise/train.jl:78
   [12] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/vertex.jl:428
   [13] top-level scope
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1113
   [14] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/vertex.jl:408
   [15] top-level scope
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1113
   [16] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/vertex.jl:352
   [17] include(fname::String)
      @ Base.MainInclude ./client.jl:444
   [18] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/runtests.jl:23
   [19] top-level scope
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1113
   [20] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/runtests.jl:13
   [21] include(fname::String)
      @ Base.MainInclude ./client.jl:444
   [22] top-level scope
      @ none:6
   [23] eval(m::Module, e::Any)
      @ Core ./boot.jl:345
   [24] exec_options(opts::Base.JLOptions)
      @ Base ./client.jl:261
   [25] _start()
      @ Base ./client.jl:485
[ Info: Testing pruning
ActivationContribution no grad: Error During Test at /home/pkgeval/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:67
  Got exception outside of a @test
  MethodError: no method matching Core.Compiler.IRCode(::Vector{Any}, ::Vector{Any}, ::Vector{Int32}, ::Vector{UInt8}, ::Core.Compiler.CFG, ::Vector{Core.LineInfoNode}, ::Vector{Any}, ::Vector{Any}, ::Vector{Any})
  Stacktrace:
    [1] Core.Compiler.IRCode(ir::IRTools.Inner.IR)
      @ IRTools.Inner.Wrap ~/.julia/packages/IRTools/GVPoj/src/ir/wrap.jl:55
    [2] update!(ci::Core.CodeInfo, ir::IRTools.Inner.IR)
      @ IRTools.Inner ~/.julia/packages/IRTools/GVPoj/src/reflection/utils.jl:143
    [3] #s2778#1234
      @ ~/.julia/packages/Zygote/Xgcgs/src/compiler/interface2.jl:34 [inlined]
    [4] #s2778#1234(::Any, ctx::Any, f::Any, args::Any)
      @ Zygote ./none:0
    [5] (::Core.GeneratedFunctionStub)(::Any, ::Vararg{Any, N} where N)
      @ Core ./boot.jl:556
    [6] pullback(f::Function, ps::Zygote.Params)
      @ Zygote ~/.julia/packages/Zygote/Xgcgs/src/compiler/interface.jl:172
    [7] gradient(f::Function, args::Zygote.Params)
      @ Zygote ~/.julia/packages/Zygote/Xgcgs/src/compiler/interface.jl:53
    [8] macro expansion
      @ ~/.julia/packages/Flux/05b38/src/optimise/train.jl:82 [inlined]
    [9] macro expansion
      @ ~/.julia/packages/Juno/hEPx8/src/progress.jl:134 [inlined]
   [10] train!(loss::Function, ps::Zygote.Params, data::Vector{Tuple{Matrix{Float32}, Matrix{Float32}}}, opt::Descent; cb::Flux.Optimise.var"#16#22")
      @ Flux.Optimise ~/.julia/packages/Flux/05b38/src/optimise/train.jl:80
   [11] train!(loss::Function, ps::Zygote.Params, data::Vector{Tuple{Matrix{Float32}, Matrix{Float32}}}, opt::Descent)
      @ Flux.Optimise ~/.julia/packages/Flux/05b38/src/optimise/train.jl:78
   [12] (::var"#tr#107"{var"#tr#95#108"})(l::MutationVertex, data::Matrix{Float32}; loss::Base.ComposedFunction{var"#f#112", typeof(Flux.Losses.mse)})
      @ Main ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:10
   [13] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:72
   [14] top-level scope
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1113
   [15] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:68
   [16] top-level scope
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1113
   [17] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:4
   [18] include(fname::String)
      @ Base.MainInclude ./client.jl:444
   [19] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/runtests.jl:26
   [20] top-level scope
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1113
   [21] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/runtests.jl:13
   [22] include(fname::String)
      @ Base.MainInclude ./client.jl:444
   [23] top-level scope
      @ none:6
   [24] eval(m::Module, e::Any)
      @ Core ./boot.jl:345
   [25] exec_options(opts::Base.JLOptions)
      @ Base ./client.jl:261
   [26] _start()
      @ Base ./client.jl:485
Neuron value Dense act contrib: Error During Test at /home/pkgeval/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:77
  Got exception outside of a @test
  MethodError: no method matching Core.Compiler.IRCode(::Vector{Any}, ::Vector{Any}, ::Vector{Int32}, ::Vector{UInt8}, ::Core.Compiler.CFG, ::Vector{Core.LineInfoNode}, ::Vector{Any}, ::Vector{Any}, ::Vector{Any})
  Stacktrace:
    [1] Core.Compiler.IRCode(ir::IRTools.Inner.IR)
      @ IRTools.Inner.Wrap ~/.julia/packages/IRTools/GVPoj/src/ir/wrap.jl:55
    [2] update!(ci::Core.CodeInfo, ir::IRTools.Inner.IR)
      @ IRTools.Inner ~/.julia/packages/IRTools/GVPoj/src/reflection/utils.jl:143
    [3] #s2778#1234
      @ ~/.julia/packages/Zygote/Xgcgs/src/compiler/interface2.jl:34 [inlined]
    [4] #s2778#1234(::Any, ctx::Any, f::Any, args::Any)
      @ Zygote ./none:0
    [5] (::Core.GeneratedFunctionStub)(::Any, ::Vararg{Any, N} where N)
      @ Core ./boot.jl:556
    [6] pullback(f::Function, ps::Zygote.Params)
      @ Zygote ~/.julia/packages/Zygote/Xgcgs/src/compiler/interface.jl:172
    [7] gradient(f::Function, args::Zygote.Params)
      @ Zygote ~/.julia/packages/Zygote/Xgcgs/src/compiler/interface.jl:53
    [8] macro expansion
      @ ~/.julia/packages/Flux/05b38/src/optimise/train.jl:82 [inlined]
    [9] macro expansion
      @ ~/.julia/packages/Juno/hEPx8/src/progress.jl:134 [inlined]
   [10] train!(loss::Function, ps::Zygote.Params, data::Vector{Tuple{Matrix{Float32}, Matrix{Float32}}}, opt::Descent; cb::Flux.Optimise.var"#16#22")
      @ Flux.Optimise ~/.julia/packages/Flux/05b38/src/optimise/train.jl:80
   [11] train!(loss::Function, ps::Zygote.Params, data::Vector{Tuple{Matrix{Float32}, Matrix{Float32}}}, opt::Descent)
      @ Flux.Optimise ~/.julia/packages/Flux/05b38/src/optimise/train.jl:78
   [12] (::var"#tr#107"{var"#tr#95#108"})(l::MutationVertex, data::Matrix{Float32}; loss::typeof(Flux.Losses.mse))
      @ Main ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:10
   [13] (::var"#tr#107"{var"#tr#95#108"})(l::MutationVertex, data::Matrix{Float32})
      @ Main ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:7
   [14] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:80
   [15] top-level scope
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1113
   [16] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:78
   [17] top-level scope
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1113
   [18] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:4
   [19] include(fname::String)
      @ Base.MainInclude ./client.jl:444
   [20] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/runtests.jl:26
   [21] top-level scope
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1113
   [22] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/runtests.jl:13
   [23] include(fname::String)
      @ Base.MainInclude ./client.jl:444
   [24] top-level scope
      @ none:6
   [25] eval(m::Module, e::Any)
      @ Core ./boot.jl:345
   [26] exec_options(opts::Base.JLOptions)
      @ Base ./client.jl:261
   [27] _start()
      @ Base ./client.jl:485
Neuron value Dense act contrib every 4: Error During Test at /home/pkgeval/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:85
  Got exception outside of a @test
  MethodError: no method matching Core.Compiler.IRCode(::Vector{Any}, ::Vector{Any}, ::Vector{Int32}, ::Vector{UInt8}, ::Core.Compiler.CFG, ::Vector{Core.LineInfoNode}, ::Vector{Any}, ::Vector{Any}, ::Vector{Any})
  Stacktrace:
    [1] Core.Compiler.IRCode(ir::IRTools.Inner.IR)
      @ IRTools.Inner.Wrap ~/.julia/packages/IRTools/GVPoj/src/ir/wrap.jl:55
    [2] update!(ci::Core.CodeInfo, ir::IRTools.Inner.IR)
      @ IRTools.Inner ~/.julia/packages/IRTools/GVPoj/src/reflection/utils.jl:143
    [3] #s2778#1234
      @ ~/.julia/packages/Zygote/Xgcgs/src/compiler/interface2.jl:34 [inlined]
    [4] #s2778#1234(::Any, ctx::Any, f::Any, args::Any)
      @ Zygote ./none:0
    [5] (::Core.GeneratedFunctionStub)(::Any, ::Vararg{Any, N} where N)
      @ Core ./boot.jl:556
    [6] pullback(f::Function, ps::Zygote.Params)
      @ Zygote ~/.julia/packages/Zygote/Xgcgs/src/compiler/interface.jl:172
    [7] gradient(f::Function, args::Zygote.Params)
      @ Zygote ~/.julia/packages/Zygote/Xgcgs/src/compiler/interface.jl:53
    [8] macro expansion
      @ ~/.julia/packages/Flux/05b38/src/optimise/train.jl:82 [inlined]
    [9] macro expansion
      @ ~/.julia/packages/Juno/hEPx8/src/progress.jl:134 [inlined]
   [10] train!(loss::Function, ps::Zygote.Params, data::Vector{Tuple{Matrix{Float32}, Matrix{Float32}}}, opt::Descent; cb::Flux.Optimise.var"#16#22")
      @ Flux.Optimise ~/.julia/packages/Flux/05b38/src/optimise/train.jl:80
   [11] train!(loss::Function, ps::Zygote.Params, data::Vector{Tuple{Matrix{Float32}, Matrix{Float32}}}, opt::Descent)
      @ Flux.Optimise ~/.julia/packages/Flux/05b38/src/optimise/train.jl:78
   [12] (::var"#tr#107"{var"#tr#95#108"})(l::MutationVertex, data::Matrix{Float32}; loss::typeof(Flux.Losses.mse))
      @ Main ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:10
   [13] (::var"#tr#107"{var"#tr#95#108"})(l::MutationVertex, data::Matrix{Float32})
      @ Main ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:7
   [14] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:89
   [15] top-level scope
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1113
   [16] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:86
   [17] top-level scope
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1113
   [18] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:4
   [19] include(fname::String)
      @ Base.MainInclude ./client.jl:444
   [20] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/runtests.jl:26
   [21] top-level scope
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1113
   [22] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/runtests.jl:13
   [23] include(fname::String)
      @ Base.MainInclude ./client.jl:444
   [24] top-level scope
      @ none:6
   [25] eval(m::Module, e::Any)
      @ Core ./boot.jl:345
   [26] exec_options(opts::Base.JLOptions)
      @ Base ./client.jl:261
   [27] _start()
      @ Base ./client.jl:485
Neuron value RNN act contrib: Error During Test at /home/pkgeval/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:102
  Got exception outside of a @test
  MethodError: no method matching Core.Compiler.IRCode(::Vector{Any}, ::Vector{Any}, ::Vector{Int32}, ::Vector{UInt8}, ::Core.Compiler.CFG, ::Vector{Core.LineInfoNode}, ::Vector{Any}, ::Vector{Any}, ::Vector{Any})
  Stacktrace:
    [1] Core.Compiler.IRCode(ir::IRTools.Inner.IR)
      @ IRTools.Inner.Wrap ~/.julia/packages/IRTools/GVPoj/src/ir/wrap.jl:55
    [2] update!(ci::Core.CodeInfo, ir::IRTools.Inner.IR)
      @ IRTools.Inner ~/.julia/packages/IRTools/GVPoj/src/reflection/utils.jl:143
    [3] #s2778#1234
      @ ~/.julia/packages/Zygote/Xgcgs/src/compiler/interface2.jl:34 [inlined]
    [4] #s2778#1234(::Any, ctx::Any, f::Any, args::Any)
      @ Zygote ./none:0
    [5] (::Core.GeneratedFunctionStub)(::Any, ::Vararg{Any, N} where N)
      @ Core ./boot.jl:556
    [6] pullback(f::Function, ps::Zygote.Params)
      @ Zygote ~/.julia/packages/Zygote/Xgcgs/src/compiler/interface.jl:172
    [7] gradient(f::Function, args::Zygote.Params)
      @ Zygote ~/.julia/packages/Zygote/Xgcgs/src/compiler/interface.jl:53
    [8] macro expansion
      @ ~/.julia/packages/Flux/05b38/src/optimise/train.jl:82 [inlined]
    [9] macro expansion
      @ ~/.julia/packages/Juno/hEPx8/src/progress.jl:134 [inlined]
   [10] train!(loss::Function, ps::Zygote.Params, data::Vector{Tuple{Matrix{Float32}, Matrix{Float32}}}, opt::Descent; cb::Flux.Optimise.var"#16#22")
      @ Flux.Optimise ~/.julia/packages/Flux/05b38/src/optimise/train.jl:80
   [11] train!(loss::Function, ps::Zygote.Params, data::Vector{Tuple{Matrix{Float32}, Matrix{Float32}}}, opt::Descent)
      @ Flux.Optimise ~/.julia/packages/Flux/05b38/src/optimise/train.jl:78
   [12] (::var"#tr#107"{var"#tr#95#108"})(l::MutationVertex, data::Matrix{Float32}; loss::typeof(Flux.Losses.mse))
      @ Main ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:10
   [13] (::var"#tr#107"{var"#tr#95#108"})(l::MutationVertex, data::Matrix{Float32})
      @ Main ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:7
   [14] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:105
   [15] top-level scope
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1113
   [16] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:103
   [17] top-level scope
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1113
   [18] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:4
   [19] include(fname::String)
      @ Base.MainInclude ./client.jl:444
   [20] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/runtests.jl:26
   [21] top-level scope
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1113
   [22] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/runtests.jl:13
   [23] include(fname::String)
      @ Base.MainInclude ./client.jl:444
   [24] top-level scope
      @ none:6
   [25] eval(m::Module, e::Any)
      @ Core ./boot.jl:345
   [26] exec_options(opts::Base.JLOptions)
      @ Base ./client.jl:261
   [27] _start()
      @ Base ./client.jl:485
Neuron value Conv act contrib: Error During Test at /home/pkgeval/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:110
  Got exception outside of a @test
  MethodError: no method matching Core.Compiler.IRCode(::Vector{Any}, ::Vector{Any}, ::Vector{Int32}, ::Vector{UInt8}, ::Core.Compiler.CFG, ::Vector{Core.LineInfoNode}, ::Vector{Any}, ::Vector{Any}, ::Vector{Any})
  Stacktrace:
    [1] Core.Compiler.IRCode(ir::IRTools.Inner.IR)
      @ IRTools.Inner.Wrap ~/.julia/packages/IRTools/GVPoj/src/ir/wrap.jl:55
    [2] update!(ci::Core.CodeInfo, ir::IRTools.Inner.IR)
      @ IRTools.Inner ~/.julia/packages/IRTools/GVPoj/src/reflection/utils.jl:143
    [3] #s2778#1234
      @ ~/.julia/packages/Zygote/Xgcgs/src/compiler/interface2.jl:34 [inlined]
    [4] #s2778#1234(::Any, ctx::Any, f::Any, args::Any)
      @ Zygote ./none:0
    [5] (::Core.GeneratedFunctionStub)(::Any, ::Vararg{Any, N} where N)
      @ Core ./boot.jl:556
    [6] pullback(f::Function, ps::Zygote.Params)
      @ Zygote ~/.julia/packages/Zygote/Xgcgs/src/compiler/interface.jl:172
    [7] gradient(f::Function, args::Zygote.Params)
      @ Zygote ~/.julia/packages/Zygote/Xgcgs/src/compiler/interface.jl:53
    [8] macro expansion
      @ ~/.julia/packages/Flux/05b38/src/optimise/train.jl:82 [inlined]
    [9] macro expansion
      @ ~/.julia/packages/Juno/hEPx8/src/progress.jl:134 [inlined]
   [10] train!(loss::Function, ps::Zygote.Params, data::Vector{Tuple{Array{Float32, 4}, Array{Float32, 4}}}, opt::Descent; cb::Flux.Optimise.var"#16#22")
      @ Flux.Optimise ~/.julia/packages/Flux/05b38/src/optimise/train.jl:80
   [11] train!(loss::Function, ps::Zygote.Params, data::Vector{Tuple{Array{Float32, 4}, Array{Float32, 4}}}, opt::Descent)
      @ Flux.Optimise ~/.julia/packages/Flux/05b38/src/optimise/train.jl:78
   [12] (::var"#tr#107"{var"#tr#95#108"})(l::MutationVertex, data::Array{Float32, 4}; loss::typeof(Flux.Losses.mse))
      @ Main ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:10
   [13] (::var"#tr#107"{var"#tr#95#108"})(l::MutationVertex, data::Array{Float32, 4})
      @ Main ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:7
   [14] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:113
   [15] top-level scope
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1113
   [16] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:111
   [17] top-level scope
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1113
   [18] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:4
   [19] include(fname::String)
      @ Base.MainInclude ./client.jl:444
   [20] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/runtests.jl:26
   [21] top-level scope
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1113
   [22] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/runtests.jl:13
   [23] include(fname::String)
      @ Base.MainInclude ./client.jl:444
   [24] top-level scope
      @ none:6
   [25] eval(m::Module, e::Any)
      @ Core ./boot.jl:345
   [26] exec_options(opts::Base.JLOptions)
      @ Base ./client.jl:261
   [27] _start()
      @ Base ./client.jl:485
Neuron value MaxPool act contrib: Error During Test at /home/pkgeval/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:118
  Got exception outside of a @test
  MethodError: no method matching Core.Compiler.IRCode(::Vector{Any}, ::Vector{Any}, ::Vector{Int32}, ::Vector{UInt8}, ::Core.Compiler.CFG, ::Vector{Core.LineInfoNode}, ::Vector{Any}, ::Vector{Any}, ::Vector{Any})
  Stacktrace:
    [1] Core.Compiler.IRCode(ir::IRTools.Inner.IR)
      @ IRTools.Inner.Wrap ~/.julia/packages/IRTools/GVPoj/src/ir/wrap.jl:55
    [2] update!(ci::Core.CodeInfo, ir::IRTools.Inner.IR)
      @ IRTools.Inner ~/.julia/packages/IRTools/GVPoj/src/reflection/utils.jl:143
    [3] #s2778#1234
      @ ~/.julia/packages/Zygote/Xgcgs/src/compiler/interface2.jl:34 [inlined]
    [4] #s2778#1234(::Any, ctx::Any, f::Any, args::Any)
      @ Zygote ./none:0
    [5] (::Core.GeneratedFunctionStub)(::Any, ::Vararg{Any, N} where N)
      @ Core ./boot.jl:556
    [6] pullback(f::Function, ps::Zygote.Params)
      @ Zygote ~/.julia/packages/Zygote/Xgcgs/src/compiler/interface.jl:172
    [7] gradient(f::Function, args::Zygote.Params)
      @ Zygote ~/.julia/packages/Zygote/Xgcgs/src/compiler/interface.jl:53
    [8] macro expansion
      @ ~/.julia/packages/Flux/05b38/src/optimise/train.jl:82 [inlined]
    [9] macro expansion
      @ ~/.julia/packages/Juno/hEPx8/src/progress.jl:134 [inlined]
   [10] train!(loss::Function, ps::Zygote.Params, data::Vector{Tuple{Array{Float32, 4}, Array{Float32, 4}}}, opt::Descent; cb::Flux.Optimise.var"#16#22")
      @ Flux.Optimise ~/.julia/packages/Flux/05b38/src/optimise/train.jl:80
   [11] train!(loss::Function, ps::Zygote.Params, data::Vector{Tuple{Array{Float32, 4}, Array{Float32, 4}}}, opt::Descent)
      @ Flux.Optimise ~/.julia/packages/Flux/05b38/src/optimise/train.jl:78
   [12] (::var"#tr#107"{var"#tr#95#108"})(l::MutationVertex, data::Array{Float32, 4}; loss::typeof(Flux.Losses.mse))
      @ Main ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:10
   [13] (::var"#tr#107"{var"#tr#95#108"})(l::MutationVertex, data::Array{Float32, 4})
      @ Main ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:7
   [14] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:121
   [15] top-level scope
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1113
   [16] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:119
   [17] top-level scope
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1113
   [18] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:4
   [19] include(fname::String)
      @ Base.MainInclude ./client.jl:444
   [20] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/runtests.jl:26
   [21] top-level scope
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1113
   [22] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/runtests.jl:13
   [23] include(fname::String)
      @ Base.MainInclude ./client.jl:444
   [24] top-level scope
      @ none:6
   [25] eval(m::Module, e::Any)
      @ Core ./boot.jl:345
   [26] exec_options(opts::Base.JLOptions)
      @ Base ./client.jl:261
   [27] _start()
      @ Base ./client.jl:485
Neuron value GlobalMeanPool act contrib: Error During Test at /home/pkgeval/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:128
  Got exception outside of a @test
  MethodError: no method matching Core.Compiler.IRCode(::Vector{Any}, ::Vector{Any}, ::Vector{Int32}, ::Vector{UInt8}, ::Core.Compiler.CFG, ::Vector{Core.LineInfoNode}, ::Vector{Any}, ::Vector{Any}, ::Vector{Any})
  Stacktrace:
    [1] Core.Compiler.IRCode(ir::IRTools.Inner.IR)
      @ IRTools.Inner.Wrap ~/.julia/packages/IRTools/GVPoj/src/ir/wrap.jl:55
    [2] update!(ci::Core.CodeInfo, ir::IRTools.Inner.IR)
      @ IRTools.Inner ~/.julia/packages/IRTools/GVPoj/src/reflection/utils.jl:143
    [3] #s2778#1234
      @ ~/.julia/packages/Zygote/Xgcgs/src/compiler/interface2.jl:34 [inlined]
    [4] #s2778#1234(::Any, ctx::Any, f::Any, args::Any)
      @ Zygote ./none:0
    [5] (::Core.GeneratedFunctionStub)(::Any, ::Vararg{Any, N} where N)
      @ Core ./boot.jl:556
    [6] pullback(f::Function, ps::Zygote.Params)
      @ Zygote ~/.julia/packages/Zygote/Xgcgs/src/compiler/interface.jl:172
    [7] gradient(f::Function, args::Zygote.Params)
      @ Zygote ~/.julia/packages/Zygote/Xgcgs/src/compiler/interface.jl:53
    [8] macro expansion
      @ ~/.julia/packages/Flux/05b38/src/optimise/train.jl:82 [inlined]
    [9] macro expansion
      @ ~/.julia/packages/Juno/hEPx8/src/progress.jl:134 [inlined]
   [10] train!(loss::Function, ps::Zygote.Params, data::Vector{Tuple{Array{Float32, 4}, Array{Float32, 4}}}, opt::Descent; cb::Flux.Optimise.var"#16#22")
      @ Flux.Optimise ~/.julia/packages/Flux/05b38/src/optimise/train.jl:80
   [11] train!(loss::Function, ps::Zygote.Params, data::Vector{Tuple{Array{Float32, 4}, Array{Float32, 4}}}, opt::Descent)
      @ Flux.Optimise ~/.julia/packages/Flux/05b38/src/optimise/train.jl:78
   [12] (::var"#tr#107"{var"#tr#95#108"})(l::MutationVertex, data::Array{Float32, 4}; loss::typeof(Flux.Losses.mse))
      @ Main ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:10
   [13] (::var"#tr#107"{var"#tr#95#108"})(l::MutationVertex, data::Array{Float32, 4})
      @ Main ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:7
   [14] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:131
   [15] top-level scope
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1113
   [16] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:129
   [17] top-level scope
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1113
   [18] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:4
   [19] include(fname::String)
      @ Base.MainInclude ./client.jl:444
   [20] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/runtests.jl:26
   [21] top-level scope
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1113
   [22] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/runtests.jl:13
   [23] include(fname::String)
      @ Base.MainInclude ./client.jl:444
   [24] top-level scope
      @ none:6
   [25] eval(m::Module, e::Any)
      @ Core ./boot.jl:345
   [26] exec_options(opts::Base.JLOptions)
      @ Base ./client.jl:261
   [27] _start()
      @ Base ./client.jl:485
Elem add ActivationContribution: Error During Test at /home/pkgeval/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:138
  Got exception outside of a @test
  MethodError: no method matching Core.Compiler.IRCode(::Vector{Any}, ::Vector{Any}, ::Vector{Int32}, ::Vector{UInt8}, ::Core.Compiler.CFG, ::Vector{Core.LineInfoNode}, ::Vector{Any}, ::Vector{Any}, ::Vector{Any})
  Stacktrace:
    [1] Core.Compiler.IRCode(ir::IRTools.Inner.IR)
      @ IRTools.Inner.Wrap ~/.julia/packages/IRTools/GVPoj/src/ir/wrap.jl:55
    [2] update!(ci::Core.CodeInfo, ir::IRTools.Inner.IR)
      @ IRTools.Inner ~/.julia/packages/IRTools/GVPoj/src/reflection/utils.jl:143
    [3] #s2778#1234
      @ ~/.julia/packages/Zygote/Xgcgs/src/compiler/interface2.jl:34 [inlined]
    [4] #s2778#1234(::Any, ctx::Any, f::Any, args::Any)
      @ Zygote ./none:0
    [5] (::Core.GeneratedFunctionStub)(::Any, ::Vararg{Any, N} where N)
      @ Core ./boot.jl:556
    [6] pullback(f::Function, ps::Zygote.Params)
      @ Zygote ~/.julia/packages/Zygote/Xgcgs/src/compiler/interface.jl:172
    [7] gradient(f::Function, args::Zygote.Params)
      @ Zygote ~/.julia/packages/Zygote/Xgcgs/src/compiler/interface.jl:53
    [8] macro expansion
      @ ~/.julia/packages/Flux/05b38/src/optimise/train.jl:82 [inlined]
    [9] macro expansion
      @ ~/.julia/packages/Juno/hEPx8/src/progress.jl:134 [inlined]
   [10] train!(loss::Function, ps::Zygote.Params, data::Vector{Tuple{Tuple{LinearAlgebra.Adjoint{Int64, Matrix{Int64}}, LinearAlgebra.Adjoint{Int64, Matrix{Int64}}}, LinearAlgebra.Adjoint{Int64, Matrix{Int64}}}}, opt::Descent; cb::Flux.Optimise.var"#16#22")
      @ Flux.Optimise ~/.julia/packages/Flux/05b38/src/optimise/train.jl:80
   [11] train!(loss::Function, ps::Zygote.Params, data::Vector{Tuple{Tuple{LinearAlgebra.Adjoint{Int64, Matrix{Int64}}, LinearAlgebra.Adjoint{Int64, Matrix{Int64}}}, LinearAlgebra.Adjoint{Int64, Matrix{Int64}}}}, opt::Descent)
      @ Flux.Optimise ~/.julia/packages/Flux/05b38/src/optimise/train.jl:78
   [12] (::var"#tr#107"{var"#tr#95#108"})(::MutationVertex, ::LinearAlgebra.Adjoint{Int64, Matrix{Int64}}, ::LinearAlgebra.Adjoint{Int64, Matrix{Int64}}, ::Vararg{LinearAlgebra.Adjoint{Int64, Matrix{Int64}}, N} where N)
      @ Main ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:15
   [13] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:141
   [14] top-level scope
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1113
   [15] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:139
   [16] top-level scope
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1113
   [17] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:4
   [18] include(fname::String)
      @ Base.MainInclude ./client.jl:444
   [19] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/runtests.jl:26
   [20] top-level scope
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1113
   [21] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/runtests.jl:13
   [22] include(fname::String)
      @ Base.MainInclude ./client.jl:444
   [23] top-level scope
      @ none:6
   [24] eval(m::Module, e::Any)
      @ Core ./boot.jl:345
   [25] exec_options(opts::Base.JLOptions)
      @ Base ./client.jl:261
   [26] _start()
      @ Base ./client.jl:485
Concat ActivationContribution: Error During Test at /home/pkgeval/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:151
  Got exception outside of a @test
  MethodError: no method matching Core.Compiler.IRCode(::Vector{Any}, ::Vector{Any}, ::Vector{Int32}, ::Vector{UInt8}, ::Core.Compiler.CFG, ::Vector{Core.LineInfoNode}, ::Vector{Any}, ::Vector{Any}, ::Vector{Any})
  Stacktrace:
    [1] Core.Compiler.IRCode(ir::IRTools.Inner.IR)
      @ IRTools.Inner.Wrap ~/.julia/packages/IRTools/GVPoj/src/ir/wrap.jl:55
    [2] update!(ci::Core.CodeInfo, ir::IRTools.Inner.IR)
      @ IRTools.Inner ~/.julia/packages/IRTools/GVPoj/src/reflection/utils.jl:143
    [3] #s2778#1234
      @ ~/.julia/packages/Zygote/Xgcgs/src/compiler/interface2.jl:34 [inlined]
    [4] #s2778#1234(::Any, ctx::Any, f::Any, args::Any)
      @ Zygote ./none:0
    [5] (::Core.GeneratedFunctionStub)(::Any, ::Vararg{Any, N} where N)
      @ Core ./boot.jl:556
    [6] pullback(f::Function, ps::Zygote.Params)
      @ Zygote ~/.julia/packages/Zygote/Xgcgs/src/compiler/interface.jl:172
    [7] gradient(f::Function, args::Zygote.Params)
      @ Zygote ~/.julia/packages/Zygote/Xgcgs/src/compiler/interface.jl:53
    [8] macro expansion
      @ ~/.julia/packages/Flux/05b38/src/optimise/train.jl:82 [inlined]
    [9] macro expansion
      @ ~/.julia/packages/Juno/hEPx8/src/progress.jl:134 [inlined]
   [10] train!(loss::Function, ps::Zygote.Params, data::Vector{Tuple{Tuple{LinearAlgebra.Adjoint{Int64, Matrix{Int64}}, LinearAlgebra.Adjoint{Int64, Matrix{Int64}}}, Matrix{Float64}}}, opt::Descent; cb::Flux.Optimise.var"#16#22")
      @ Flux.Optimise ~/.julia/packages/Flux/05b38/src/optimise/train.jl:80
   [11] train!(loss::Function, ps::Zygote.Params, data::Vector{Tuple{Tuple{LinearAlgebra.Adjoint{Int64, Matrix{Int64}}, LinearAlgebra.Adjoint{Int64, Matrix{Int64}}}, Matrix{Float64}}}, opt::Descent)
      @ Flux.Optimise ~/.julia/packages/Flux/05b38/src/optimise/train.jl:78
   [12] (::var"#tr#107"{var"#tr#95#108"})(::MutationVertex, ::Matrix{Float64}, ::LinearAlgebra.Adjoint{Int64, Matrix{Int64}}, ::Vararg{LinearAlgebra.Adjoint{Int64, Matrix{Int64}}, N} where N)
      @ Main ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:15
   [13] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:153
   [14] top-level scope
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1113
   [15] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:152
   [16] top-level scope
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1113
   [17] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:4
   [18] include(fname::String)
      @ Base.MainInclude ./client.jl:444
   [19] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/runtests.jl:26
   [20] top-level scope
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1113
   [21] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/runtests.jl:13
   [22] include(fname::String)
      @ Base.MainInclude ./client.jl:444
   [23] top-level scope
      @ none:6
   [24] eval(m::Module, e::Any)
      @ Core ./boot.jl:345
   [25] exec_options(opts::Base.JLOptions)
      @ Base ./client.jl:261
   [26] _start()
      @ Base ./client.jl:485
Function ActivationContribution: Error During Test at /home/pkgeval/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:163
  Got exception outside of a @test
  MethodError: no method matching Core.Compiler.IRCode(::Vector{Any}, ::Vector{Any}, ::Vector{Int32}, ::Vector{UInt8}, ::Core.Compiler.CFG, ::Vector{Core.LineInfoNode}, ::Vector{Any}, ::Vector{Any}, ::Vector{Any})
  Stacktrace:
    [1] Core.Compiler.IRCode(ir::IRTools.Inner.IR)
      @ IRTools.Inner.Wrap ~/.julia/packages/IRTools/GVPoj/src/ir/wrap.jl:55
    [2] update!(ci::Core.CodeInfo, ir::IRTools.Inner.IR)
      @ IRTools.Inner ~/.julia/packages/IRTools/GVPoj/src/reflection/utils.jl:143
    [3] #s2778#1234
      @ ~/.julia/packages/Zygote/Xgcgs/src/compiler/interface2.jl:34 [inlined]
    [4] #s2778#1234(::Any, ctx::Any, f::Any, args::Any)
      @ Zygote ./none:0
    [5] (::Core.GeneratedFunctionStub)(::Any, ::Vararg{Any, N} where N)
      @ Core ./boot.jl:556
    [6] pullback(f::Function, ps::Zygote.Params)
      @ Zygote ~/.julia/packages/Zygote/Xgcgs/src/compiler/interface.jl:172
    [7] gradient(f::Function, args::Zygote.Params)
      @ Zygote ~/.julia/packages/Zygote/Xgcgs/src/compiler/interface.jl:53
    [8] macro expansion
      @ ~/.julia/packages/Flux/05b38/src/optimise/train.jl:82 [inlined]
    [9] macro expansion
      @ ~/.julia/packages/Juno/hEPx8/src/progress.jl:134 [inlined]
   [10] train!(loss::Function, ps::Zygote.Params, data::Vector{Tuple{Matrix{Float64}, Matrix{Float32}}}, opt::Descent; cb::Flux.Optimise.var"#16#22")
      @ Flux.Optimise ~/.julia/packages/Flux/05b38/src/optimise/train.jl:80
   [11] train!(loss::Function, ps::Zygote.Params, data::Vector{Tuple{Matrix{Float64}, Matrix{Float32}}}, opt::Descent)
      @ Flux.Optimise ~/.julia/packages/Flux/05b38/src/optimise/train.jl:78
   [12] (::var"#tr#107"{var"#tr#95#108"})(l::MutationVertex, data::Matrix{Float64}; loss::typeof(Flux.Losses.mse))
      @ Main ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:10
   [13] (::var"#tr#107"{var"#tr#95#108"})(l::MutationVertex, data::Matrix{Float64})
      @ Main ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:7
   [14] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:166
   [15] top-level scope
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1113
   [16] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:165
   [17] top-level scope
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1113
   [18] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:4
   [19] include(fname::String)
      @ Base.MainInclude ./client.jl:444
   [20] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/runtests.jl:26
   [21] top-level scope
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1113
   [22] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/runtests.jl:13
   [23] include(fname::String)
      @ Base.MainInclude ./client.jl:444
   [24] top-level scope
      @ none:6
   [25] eval(m::Module, e::Any)
      @ Core ./boot.jl:345
   [26] exec_options(opts::Base.JLOptions)
      @ Base ./client.jl:261
   [27] _start()
      @ Base ./client.jl:485
Mutate ActivationContribution MaxPool: Error During Test at /home/pkgeval/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:185
  Got exception outside of a @test
  MethodError: no method matching Core.Compiler.IRCode(::Vector{Any}, ::Vector{Any}, ::Vector{Int32}, ::Vector{UInt8}, ::Core.Compiler.CFG, ::Vector{Core.LineInfoNode}, ::Vector{Any}, ::Vector{Any}, ::Vector{Any})
  Stacktrace:
    [1] Core.Compiler.IRCode(ir::IRTools.Inner.IR)
      @ IRTools.Inner.Wrap ~/.julia/packages/IRTools/GVPoj/src/ir/wrap.jl:55
    [2] update!(ci::Core.CodeInfo, ir::IRTools.Inner.IR)
      @ IRTools.Inner ~/.julia/packages/IRTools/GVPoj/src/reflection/utils.jl:143
    [3] #s2778#1234
      @ ~/.julia/packages/Zygote/Xgcgs/src/compiler/interface2.jl:34 [inlined]
    [4] #s2778#1234(::Any, ctx::Any, f::Any, args::Any)
      @ Zygote ./none:0
    [5] (::Core.GeneratedFunctionStub)(::Any, ::Vararg{Any, N} where N)
      @ Core ./boot.jl:556
    [6] pullback(f::Function, ps::Zygote.Params)
      @ Zygote ~/.julia/packages/Zygote/Xgcgs/src/compiler/interface.jl:172
    [7] gradient(f::Function, args::Zygote.Params)
      @ Zygote ~/.julia/packages/Zygote/Xgcgs/src/compiler/interface.jl:53
    [8] macro expansion
      @ ~/.julia/packages/Flux/05b38/src/optimise/train.jl:82 [inlined]
    [9] macro expansion
      @ ~/.julia/packages/Juno/hEPx8/src/progress.jl:134 [inlined]
   [10] train!(loss::Function, ps::Zygote.Params, data::Vector{Tuple{Tuple{Array{Float32, 4}}, Array{Float32, 4}}}, opt::Descent; cb::Flux.Optimise.var"#16#22")
      @ Flux.Optimise ~/.julia/packages/Flux/05b38/src/optimise/train.jl:80
   [11] train!
      @ ~/.julia/packages/Flux/05b38/src/optimise/train.jl:78 [inlined]
   [12] (::var"#tr#107"{var"#tr#95#108"})(l::CompGraph, output::Array{Float32, 4}, inputs::Array{Float32, 4})
      @ Main ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:15
   [13] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:203
   [14] top-level scope
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1113
   [15] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:186
   [16] top-level scope
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1113
   [17] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/pruning.jl:4
   [18] include(fname::String)
      @ Base.MainInclude ./client.jl:444
   [19] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/runtests.jl:26
   [20] top-level scope
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1113
   [21] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/runtests.jl:13
   [22] include(fname::String)
      @ Base.MainInclude ./client.jl:444
   [23] top-level scope
      @ none:6
   [24] eval(m::Module, e::Any)
      @ Core ./boot.jl:345
   [25] exec_options(opts::Base.JLOptions)
      @ Base ./client.jl:261
   [26] _start()
      @ Base ./client.jl:485
[ Info: Testing examples
Pruning xor example: Error During Test at /home/pkgeval/.julia/packages/NaiveNASflux/0lGnm/test/examples.jl:86
  Got exception outside of a @test
  MethodError: no method matching Core.Compiler.IRCode(::Vector{Any}, ::Vector{Any}, ::Vector{Int32}, ::Vector{UInt8}, ::Core.Compiler.CFG, ::Vector{Core.LineInfoNode}, ::Vector{Any}, ::Vector{Any}, ::Vector{Any})
  Stacktrace:
    [1] Core.Compiler.IRCode(ir::IRTools.Inner.IR)
      @ IRTools.Inner.Wrap ~/.julia/packages/IRTools/GVPoj/src/ir/wrap.jl:55
    [2] update!(ci::Core.CodeInfo, ir::IRTools.Inner.IR)
      @ IRTools.Inner ~/.julia/packages/IRTools/GVPoj/src/reflection/utils.jl:143
    [3] #s2778#1234
      @ ~/.julia/packages/Zygote/Xgcgs/src/compiler/interface2.jl:34 [inlined]
    [4] #s2778#1234(::Any, ctx::Any, f::Any, args::Any)
      @ Zygote ./none:0
    [5] (::Core.GeneratedFunctionStub)(::Any, ::Vararg{Any, N} where N)
      @ Core ./boot.jl:556
    [6] pullback(f::Function, ps::Zygote.Params)
      @ Zygote ~/.julia/packages/Zygote/Xgcgs/src/compiler/interface.jl:172
    [7] gradient(f::Function, args::Zygote.Params)
      @ Zygote ~/.julia/packages/Zygote/Xgcgs/src/compiler/interface.jl:53
    [8] macro expansion
      @ ~/.julia/packages/Flux/05b38/src/optimise/train.jl:82 [inlined]
    [9] macro expansion
      @ ~/.julia/packages/Juno/hEPx8/src/progress.jl:134 [inlined]
   [10] train!(loss::Function, ps::Zygote.Params, data::Vector{Tuple{Matrix{Float32}, Matrix{Float32}}}, opt::ADAM; cb::Flux.Optimise.var"#16#22")
      @ Flux.Optimise ~/.julia/packages/Flux/05b38/src/optimise/train.jl:80
   [11] train!(loss::Function, ps::Zygote.Params, data::Vector{Tuple{Matrix{Float32}, Matrix{Float32}}}, opt::ADAM)
      @ Flux.Optimise ~/.julia/packages/Flux/05b38/src/optimise/train.jl:78
   [12] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/examples.jl:113
   [13] top-level scope
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1113
   [14] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/examples.jl:87
   [15] top-level scope
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1113
   [16] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/examples.jl:4
   [17] include(fname::String)
      @ Base.MainInclude ./client.jl:444
   [18] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/runtests.jl:29
   [19] top-level scope
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1113
   [20] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/runtests.jl:13
   [21] include(fname::String)
      @ Base.MainInclude ./client.jl:444
   [22] top-level scope
      @ none:6
   [23] eval(m::Module, e::Any)
      @ Core ./boot.jl:345
   [24] exec_options(opts::Base.JLOptions)
      @ Base ./client.jl:261
   [25] _start()
      @ Base ./client.jl:485
Conv 2D xor example: Error During Test at /home/pkgeval/.julia/packages/NaiveNASflux/0lGnm/test/examples.jl:145
  Got exception outside of a @test
  MethodError: no method matching Core.Compiler.IRCode(::Vector{Any}, ::Vector{Any}, ::Vector{Int32}, ::Vector{UInt8}, ::Core.Compiler.CFG, ::Vector{Core.LineInfoNode}, ::Vector{Any}, ::Vector{Any}, ::Vector{Any})
  Stacktrace:
    [1] Core.Compiler.IRCode(ir::IRTools.Inner.IR)
      @ IRTools.Inner.Wrap ~/.julia/packages/IRTools/GVPoj/src/ir/wrap.jl:55
    [2] update!(ci::Core.CodeInfo, ir::IRTools.Inner.IR)
      @ IRTools.Inner ~/.julia/packages/IRTools/GVPoj/src/reflection/utils.jl:143
    [3] #s2778#1234
      @ ~/.julia/packages/Zygote/Xgcgs/src/compiler/interface2.jl:34 [inlined]
    [4] #s2778#1234(::Any, ctx::Any, f::Any, args::Any)
      @ Zygote ./none:0
    [5] (::Core.GeneratedFunctionStub)(::Any, ::Vararg{Any, N} where N)
      @ Core ./boot.jl:556
    [6] pullback(f::Function, ps::Zygote.Params)
      @ Zygote ~/.julia/packages/Zygote/Xgcgs/src/compiler/interface.jl:172
    [7] gradient(f::Function, args::Zygote.Params)
      @ Zygote ~/.julia/packages/Zygote/Xgcgs/src/compiler/interface.jl:53
    [8] macro expansion
      @ ~/.julia/packages/Flux/05b38/src/optimise/train.jl:82 [inlined]
    [9] macro expansion
      @ ~/.julia/packages/Juno/hEPx8/src/progress.jl:134 [inlined]
   [10] train!(loss::Function, ps::Zygote.Params, data::Vector{Tuple{Array{Float32, 4}, Matrix{Float32}}}, opt::ADAM; cb::Flux.Optimise.var"#16#22")
      @ Flux.Optimise ~/.julia/packages/Flux/05b38/src/optimise/train.jl:80
   [11] train!(loss::Function, ps::Zygote.Params, data::Vector{Tuple{Array{Float32, 4}, Matrix{Float32}}}, opt::ADAM)
      @ Flux.Optimise ~/.julia/packages/Flux/05b38/src/optimise/train.jl:78
   [12] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/examples.jl:197
   [13] top-level scope
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1113
   [14] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/examples.jl:146
   [15] top-level scope
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1113
   [16] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/examples.jl:4
   [17] include(fname::String)
      @ Base.MainInclude ./client.jl:444
   [18] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/runtests.jl:29
   [19] top-level scope
      @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1113
   [20] top-level scope
      @ ~/.julia/packages/NaiveNASflux/0lGnm/test/runtests.jl:13
   [21] include(fname::String)
      @ Base.MainInclude ./client.jl:444
   [22] top-level scope
      @ none:6
   [23] eval(m::Module, e::Any)
      @ Core ./boot.jl:345
   [24] exec_options(opts::Base.JLOptions)
      @ Base ./client.jl:261
   [25] _start()
      @ Base ./client.jl:485
Test Summary:                               | Pass  Error  Total
NaiveNASflux.jl                             |  483     16    499
  Utils                                     |   51            51
  Select                                    |   31            31
  Mutable computation                       |  256           256
  InputShapeVertex                          |    6             6
  Size mutations                            |   81            81
  Trait functions                           |    3             3
  Flux functor                              |    6             6
  Trainable insert values                   |    3      3      6
    Dense-Dense-Dense                       |    1      1      2
    Conv-Bn-Conv                            |    1      1      2
    Conv-Conv-Conv                          |    1      1      2
  Neuron value tests                        |   28     11     39
    Utils                                   |   11            11
    Neuron value Dense default              |    1             1
    Neuron value Dense default no bias      |    1             1
    Neuron value RNN default                |    1             1
    Neuron value Conv default               |    1             1
    Neuron value unkown default             |    1             1
    ActivationContribution no grad          |    1      1      2
    Neuron value Dense act contrib          |    1      1      2
    Neuron value Dense act contrib every 4  |    1      1      2
    Neuron value RNN act contrib            |    1      1      2
    Neuron value Conv act contrib           |    1      1      2
    Neuron value MaxPool act contrib        |    1      1      2
    Neuron value GlobalMeanPool act contrib |    1      1      2
    Elem add ActivationContribution         |           1      1
    Concat ActivationContribution           |           1      1
    Function ActivationContribution         |           1      1
    Mutate ActivationContribution           |    2             2
    Mutate ActivationContribution MaxPool   |    3      1      4
  Examples                                  |   18      2     20
    Quick reference example                 |   18            18
    Pruning xor example                     |           1      1
    Conv 2D xor example                     |           1      1
ERROR: LoadError: Some tests did not pass: 483 passed, 0 failed, 16 errored, 0 broken.
in expression starting at /home/pkgeval/.julia/packages/NaiveNASflux/0lGnm/test/runtests.jl:11
ERROR: Package NaiveNASflux errored during testing
Stacktrace:
  [1] pkgerror(::String, ::Vararg{String, N} where N)
    @ Pkg.Types /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Pkg/src/Types.jl:52
  [2] test(ctx::Pkg.Types.Context, pkgs::Vector{Pkg.Types.PackageSpec}; coverage::Bool, julia_args::Cmd, test_args::Cmd, test_fn::Nothing)
    @ Pkg.Operations /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Pkg/src/Operations.jl:1580
  [3] test(ctx::Pkg.Types.Context, pkgs::Vector{Pkg.Types.PackageSpec}; coverage::Bool, test_fn::Nothing, julia_args::Cmd, test_args::Cmd, kwargs::Base.Iterators.Pairs{Union{}, Union{}, Tuple{}, NamedTuple{(), Tuple{}}})
    @ Pkg.API /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Pkg/src/API.jl:328
  [4] test(ctx::Pkg.Types.Context, pkgs::Vector{Pkg.Types.PackageSpec})
    @ Pkg.API /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Pkg/src/API.jl:315
  [5] #test#62
    @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Pkg/src/API.jl:67 [inlined]
  [6] test
    @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Pkg/src/API.jl:67 [inlined]
  [7] #test#61
    @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Pkg/src/API.jl:66 [inlined]
  [8] test
    @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Pkg/src/API.jl:66 [inlined]
  [9] test(pkg::String; kwargs::Base.Iterators.Pairs{Union{}, Union{}, Tuple{}, NamedTuple{(), Tuple{}}})
    @ Pkg.API /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Pkg/src/API.jl:65
 [10] test(pkg::String)
    @ Pkg.API /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Pkg/src/API.jl:65
 [11] top-level scope
    @ none:19
