Julia Version 1.4.0-DEV.670
Commit 032dbe33e3 (2019-12-29 22:39 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-8.0.1 (ORCJIT, skylake)
Environment:
  JULIA_DEPOT_PATH = ::/usr/local/share/julia

 Resolving package versions...
 Installed GaussianMixtures ─── v0.3.0
 Installed LegacyStrings ────── v0.4.1
 Installed JLD ──────────────── v0.9.1
 Installed FillArrays ───────── v0.8.2
 Installed FileIO ───────────── v1.2.1
 Installed StatsFuns ────────── v0.9.3
 Installed StatsBase ────────── v0.32.0
 Installed Distributions ────── v0.21.11
 Installed CMake ────────────── v1.1.2
 Installed Parameters ───────── v0.12.0
 Installed Arpack ───────────── v0.4.0
 Installed SpecialFunctions ─── v0.9.0
 Installed ScikitLearnBase ──── v0.5.0
 Installed BinaryProvider ───── v0.5.8
 Installed Distances ────────── v0.8.2
 Installed Clustering ───────── v0.13.3
 Installed OpenBLAS_jll ─────── v0.3.7+1
 Installed DataAPI ──────────── v1.1.0
 Installed BinDeps ──────────── v1.0.0
 Installed QuadGK ───────────── v2.3.1
 Installed Rmath ────────────── v0.6.0
 Installed URIParser ────────── v0.4.0
 Installed StaticArrays ─────── v0.12.1
 Installed OrderedCollections ─ v1.1.0
 Installed Missings ─────────── v0.4.3
 Installed Arpack_jll ───────── v3.5.0+2
 Installed HDF5 ─────────────── v0.12.5
 Installed SortingAlgorithms ── v0.3.1
 Installed DataStructures ───── v0.17.6
 Installed Blosc ────────────── v0.5.1
 Installed OpenSpecFun_jll ──── v0.5.3+1
 Installed NearestNeighbors ─── v0.4.4
 Installed PDMats ───────────── v0.9.10
 Installed Compat ───────────── v2.2.0
 Installed CMakeWrapper ─────── v0.2.3
  Updating `~/.julia/environments/v1.4/Project.toml`
  [cc18c42c] + GaussianMixtures v0.3.0
  Updating `~/.julia/environments/v1.4/Manifest.toml`
  [7d9fca2a] + Arpack v0.4.0
  [68821587] + Arpack_jll v3.5.0+2
  [9e28174c] + BinDeps v1.0.0
  [b99e7846] + BinaryProvider v0.5.8
  [a74b3585] + Blosc v0.5.1
  [631607c0] + CMake v1.1.2
  [d5fb7624] + CMakeWrapper v0.2.3
  [aaaa29a8] + Clustering v0.13.3
  [34da2185] + Compat v2.2.0
  [9a962f9c] + DataAPI v1.1.0
  [864edb3b] + DataStructures v0.17.6
  [b4f34e82] + Distances v0.8.2
  [31c24e10] + Distributions v0.21.11
  [5789e2e9] + FileIO v1.2.1
  [1a297f60] + FillArrays v0.8.2
  [cc18c42c] + GaussianMixtures v0.3.0
  [f67ccb44] + HDF5 v0.12.5
  [4138dd39] + JLD v0.9.1
  [1b4a561d] + LegacyStrings v0.4.1
  [e1d29d7a] + Missings v0.4.3
  [b8a86587] + NearestNeighbors v0.4.4
  [4536629a] + OpenBLAS_jll v0.3.7+1
  [efe28fd5] + OpenSpecFun_jll v0.5.3+1
  [bac558e1] + OrderedCollections v1.1.0
  [90014a1f] + PDMats v0.9.10
  [d96e819e] + Parameters v0.12.0
  [1fd47b50] + QuadGK v2.3.1
  [79098fc4] + Rmath v0.6.0
  [6e75b9c4] + ScikitLearnBase v0.5.0
  [a2af1166] + SortingAlgorithms v0.3.1
  [276daf66] + SpecialFunctions v0.9.0
  [90137ffa] + StaticArrays v0.12.1
  [2913bbd2] + StatsBase v0.32.0
  [4c63d2b9] + StatsFuns v0.9.3
  [30578b45] + URIParser v0.4.0
  [2a0f44e3] + Base64 
  [ade2ca70] + Dates 
  [8bb1440f] + DelimitedFiles 
  [8ba89e20] + Distributed 
  [b77e0a4c] + InteractiveUtils 
  [76f85450] + LibGit2 
  [8f399da3] + Libdl 
  [37e2e46d] + LinearAlgebra 
  [56ddb016] + Logging 
  [d6f4376e] + Markdown 
  [a63ad114] + Mmap 
  [44cfe95a] + Pkg 
  [de0858da] + Printf 
  [9abbd945] + Profile 
  [3fa0cd96] + REPL 
  [9a3f8284] + Random 
  [ea8e919c] + SHA 
  [9e88b42a] + Serialization 
  [1a1011a3] + SharedArrays 
  [6462fe0b] + Sockets 
  [2f01184e] + SparseArrays 
  [10745b16] + Statistics 
  [4607b0f0] + SuiteSparse 
  [8dfed614] + Test 
  [cf7118a7] + UUIDs 
  [4ec0a83e] + Unicode 
  Building CMake → `~/.julia/packages/CMake/nSK2r/deps/build.log`
  Updating `/tmp/jl_hEmBvE/Project.toml`
 [no changes]
  Updating `/tmp/jl_hEmBvE/Manifest.toml`
 [no changes]
  Building Rmath → `~/.julia/packages/Rmath/BoBag/deps/build.log`
  Updating `/tmp/jl_mg5fxS/Project.toml`
 [no changes]
  Updating `/tmp/jl_mg5fxS/Manifest.toml`
 [no changes]
  Building Blosc → `~/.julia/packages/Blosc/lzFr0/deps/build.log`
  Updating `/tmp/jl_VBn2HG/Project.toml`
 [no changes]
  Updating `/tmp/jl_VBn2HG/Manifest.toml`
 [no changes]
  Building HDF5 ─→ `~/.julia/packages/HDF5/Zh9on/deps/build.log`
  Updating `/tmp/jl_cxeNnf/Project.toml`
 [no changes]
  Updating `/tmp/jl_cxeNnf/Manifest.toml`
 [no changes]
   Testing GaussianMixtures
  Updating `/tmp/jl_ldlUzh/Project.toml`
 [no changes]
  Updating `/tmp/jl_ldlUzh/Manifest.toml`
 [no changes]
Running sandbox
Status `/tmp/jl_ldlUzh/Project.toml`
  [aaaa29a8] Clustering v0.13.3
  [34da2185] Compat v2.2.0
  [31c24e10] Distributions v0.21.11
  [5789e2e9] FileIO v1.2.1
  [cc18c42c] GaussianMixtures v0.3.0
  [4138dd39] JLD v0.9.1
  [90014a1f] PDMats v0.9.10
  [6e75b9c4] ScikitLearnBase v0.5.0
  [276daf66] SpecialFunctions v0.9.0
  [2913bbd2] StatsBase v0.32.0
  [8bb1440f] DelimitedFiles 
  [8ba89e20] Distributed 
  [37e2e46d] LinearAlgebra 
  [56ddb016] Logging 
  [de0858da] Printf 
  [9a3f8284] Random 
  [10745b16] Statistics 
[ Info: Testing Data
(100000, -787236.2664537872, [8005.210833696234, 91994.78916630377], [1534.3038189073873 -11943.634962696457 7556.333151395381; -1528.1694816559984 12084.965883240515 -7430.784947278895], [[4772.348857012144 -2366.819660736911 12.47877803353667; -2366.8196607369105 22348.385304663047 -7890.725605121512; 12.478778033536695 -7890.725605121512 13822.18419458336], [95547.7204796461 1934.4869338160804 -171.38571713632513; 1934.4869338160806 77354.16982229974 7330.364216827216; -171.38571713632507 7330.364216827216 85474.25615928948]])
┌ Warning: rmprocs: process 1 not removed
└ @ Distributed /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.4/Distributed/src/cluster.jl:1015
[ Info: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.324471e+03
      1       9.558783e+02      -3.685931e+02 |        7
      2       8.933976e+02      -6.248071e+01 |        4
      3       8.413979e+02      -5.199969e+01 |        2
      4       8.303698e+02      -1.102805e+01 |        2
      5       8.234973e+02      -6.872492e+00 |        3
      6       8.150919e+02      -8.405457e+00 |        0
      7       8.150919e+02       0.000000e+00 |        0
K-means converged with 7 iterations (objv = 815.091882939274)
┌ Info: K-means with 272 data points using 7 iterations
└ 11.3 data points per parameter
[ Info: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
┌ Info: EM with 272 data points 0 iterations avll -2.052558
└ 5.8 data points per parameter
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:221
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:221
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:221
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:221
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:630 [inlined]
└ @ Core ./broadcast.jl:630
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:230
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:230
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:630 [inlined]
└ @ Core ./broadcast.jl:630
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:630 [inlined]
└ @ Core ./broadcast.jl:630
[ Info: iteration 1, lowerbound -3.802675
[ Info: iteration 2, lowerbound -3.701170
[ Info: iteration 3, lowerbound -3.594590
[ Info: iteration 4, lowerbound -3.476748
[ Info: iteration 5, lowerbound -3.362877
[ Info: iteration 6, lowerbound -3.264492
[ Info: dropping number of Gaussions to 7
[ Info: iteration 7, lowerbound -3.174850
[ Info: iteration 8, lowerbound -3.088097
[ Info: dropping number of Gaussions to 5
[ Info: iteration 9, lowerbound -2.990736
[ Info: iteration 10, lowerbound -2.886758
[ Info: iteration 11, lowerbound -2.801724
[ Info: dropping number of Gaussions to 4
[ Info: iteration 12, lowerbound -2.730996
[ Info: iteration 13, lowerbound -2.666618
[ Info: dropping number of Gaussions to 3
[ Info: iteration 14, lowerbound -2.603180
[ Info: iteration 15, lowerbound -2.531556
[ Info: iteration 16, lowerbound -2.464862
[ Info: iteration 17, lowerbound -2.408817
[ Info: iteration 18, lowerbound -2.366050
[ Info: iteration 19, lowerbound -2.334630
[ Info: iteration 20, lowerbound -2.314265
[ Info: iteration 21, lowerbound -2.307396
[ Info: dropping number of Gaussions to 2
[ Info: iteration 22, lowerbound -2.302940
[ Info: iteration 23, lowerbound -2.299261
[ Info: iteration 24, lowerbound -2.299257
[ Info: iteration 25, lowerbound -2.299255
[ Info: iteration 26, lowerbound -2.299254
[ Info: iteration 27, lowerbound -2.299253
[ Info: iteration 28, lowerbound -2.299253
[ Info: iteration 29, lowerbound -2.299253
[ Info: iteration 30, lowerbound -2.299253
[ Info: iteration 31, lowerbound -2.299253
[ Info: iteration 32, lowerbound -2.299253
[ Info: iteration 33, lowerbound -2.299253
[ Info: iteration 34, lowerbound -2.299253
[ Info: iteration 35, lowerbound -2.299253
[ Info: iteration 36, lowerbound -2.299253
[ Info: iteration 37, lowerbound -2.299253
[ Info: iteration 38, lowerbound -2.299253
[ Info: iteration 39, lowerbound -2.299253
[ Info: iteration 40, lowerbound -2.299253
[ Info: iteration 41, lowerbound -2.299253
[ Info: iteration 42, lowerbound -2.299253
[ Info: iteration 43, lowerbound -2.299253
[ Info: iteration 44, lowerbound -2.299253
[ Info: iteration 45, lowerbound -2.299253
[ Info: iteration 46, lowerbound -2.299253
[ Info: iteration 47, lowerbound -2.299253
[ Info: iteration 48, lowerbound -2.299253
[ Info: iteration 49, lowerbound -2.299253
[ Info: iteration 50, lowerbound -2.299253
[ Info: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
History[Tue Dec 31 12:29:05 2019: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
, Tue Dec 31 12:29:13 2019: K-means with 272 data points using 7 iterations
11.3 data points per parameter
, Tue Dec 31 12:29:16 2019: EM with 272 data points 0 iterations avll -2.052558
5.8 data points per parameter
, Tue Dec 31 12:29:17 2019: GMM converted to Variational GMM
, Tue Dec 31 12:29:25 2019: iteration 1, lowerbound -3.802675
, Tue Dec 31 12:29:25 2019: iteration 2, lowerbound -3.701170
, Tue Dec 31 12:29:25 2019: iteration 3, lowerbound -3.594590
, Tue Dec 31 12:29:25 2019: iteration 4, lowerbound -3.476748
, Tue Dec 31 12:29:25 2019: iteration 5, lowerbound -3.362877
, Tue Dec 31 12:29:25 2019: iteration 6, lowerbound -3.264492
, Tue Dec 31 12:29:26 2019: dropping number of Gaussions to 7
, Tue Dec 31 12:29:26 2019: iteration 7, lowerbound -3.174850
, Tue Dec 31 12:29:26 2019: iteration 8, lowerbound -3.088097
, Tue Dec 31 12:29:26 2019: dropping number of Gaussions to 5
, Tue Dec 31 12:29:26 2019: iteration 9, lowerbound -2.990736
, Tue Dec 31 12:29:26 2019: iteration 10, lowerbound -2.886758
, Tue Dec 31 12:29:26 2019: iteration 11, lowerbound -2.801724
, Tue Dec 31 12:29:26 2019: dropping number of Gaussions to 4
, Tue Dec 31 12:29:26 2019: iteration 12, lowerbound -2.730996
, Tue Dec 31 12:29:26 2019: iteration 13, lowerbound -2.666618
, Tue Dec 31 12:29:26 2019: dropping number of Gaussions to 3
, Tue Dec 31 12:29:26 2019: iteration 14, lowerbound -2.603180
, Tue Dec 31 12:29:26 2019: iteration 15, lowerbound -2.531556
, Tue Dec 31 12:29:26 2019: iteration 16, lowerbound -2.464862
, Tue Dec 31 12:29:26 2019: iteration 17, lowerbound -2.408817
, Tue Dec 31 12:29:26 2019: iteration 18, lowerbound -2.366050
, Tue Dec 31 12:29:26 2019: iteration 19, lowerbound -2.334630
, Tue Dec 31 12:29:26 2019: iteration 20, lowerbound -2.314265
, Tue Dec 31 12:29:26 2019: iteration 21, lowerbound -2.307396
, Tue Dec 31 12:29:26 2019: dropping number of Gaussions to 2
, Tue Dec 31 12:29:26 2019: iteration 22, lowerbound -2.302940
, Tue Dec 31 12:29:26 2019: iteration 23, lowerbound -2.299261
, Tue Dec 31 12:29:26 2019: iteration 24, lowerbound -2.299257
, Tue Dec 31 12:29:26 2019: iteration 25, lowerbound -2.299255
, Tue Dec 31 12:29:26 2019: iteration 26, lowerbound -2.299254
, Tue Dec 31 12:29:26 2019: iteration 27, lowerbound -2.299253
, Tue Dec 31 12:29:26 2019: iteration 28, lowerbound -2.299253
, Tue Dec 31 12:29:26 2019: iteration 29, lowerbound -2.299253
, Tue Dec 31 12:29:26 2019: iteration 30, lowerbound -2.299253
, Tue Dec 31 12:29:26 2019: iteration 31, lowerbound -2.299253
, Tue Dec 31 12:29:26 2019: iteration 32, lowerbound -2.299253
, Tue Dec 31 12:29:26 2019: iteration 33, lowerbound -2.299253
, Tue Dec 31 12:29:26 2019: iteration 34, lowerbound -2.299253
, Tue Dec 31 12:29:26 2019: iteration 35, lowerbound -2.299253
, Tue Dec 31 12:29:26 2019: iteration 36, lowerbound -2.299253
, Tue Dec 31 12:29:26 2019: iteration 37, lowerbound -2.299253
, Tue Dec 31 12:29:26 2019: iteration 38, lowerbound -2.299253
, Tue Dec 31 12:29:26 2019: iteration 39, lowerbound -2.299253
, Tue Dec 31 12:29:26 2019: iteration 40, lowerbound -2.299253
, Tue Dec 31 12:29:26 2019: iteration 41, lowerbound -2.299253
, Tue Dec 31 12:29:26 2019: iteration 42, lowerbound -2.299253
, Tue Dec 31 12:29:26 2019: iteration 43, lowerbound -2.299253
, Tue Dec 31 12:29:26 2019: iteration 44, lowerbound -2.299253
, Tue Dec 31 12:29:26 2019: iteration 45, lowerbound -2.299253
, Tue Dec 31 12:29:26 2019: iteration 46, lowerbound -2.299253
, Tue Dec 31 12:29:26 2019: iteration 47, lowerbound -2.299253
, Tue Dec 31 12:29:26 2019: iteration 48, lowerbound -2.299253
, Tue Dec 31 12:29:26 2019: iteration 49, lowerbound -2.299253
, Tue Dec 31 12:29:26 2019: iteration 50, lowerbound -2.299253
, Tue Dec 31 12:29:26 2019: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [178.0450922260168, 95.95490777398318]
β = [178.0450922260168, 95.95490777398318]
m = [4.250300733269887 79.28686694436149; 2.0002292577753455 53.851987172461165]
ν = [180.0450922260168, 97.95490777398318]
W = LinearAlgebra.UpperTriangular{Float64,Array{Float64,2}}[[0.18404155547484274 -0.007644049042327494; 0.0 0.008581705166333133], [0.37587636119488443 -0.008953123827346737; 0.0 0.012748664777409545]]
Kind: diag, size256
nx: 100000 sum(zeroth order stats): 100000.00000000001
avll from stats: -0.9919433028529552
avll from llpg:  -0.9919433028529556
avll direct:     -0.9919433028529555
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 99999.99999999999
avll from stats: -0.9735441428360536
avll from llpg:  -0.9735441428360536
avll direct:     -0.9735441428360537
sum posterior: 100000.0
32×26 Array{Float64,2}:
  0.180015    -0.10483     -0.0804911   -0.00331248  -0.144782     0.0157437   -0.0749078    -0.0482222    -0.0154066    0.18553      0.15698     -0.177997     0.0722802    0.0165084    -0.110332    -0.025609    -0.223844      0.0407831    0.0322666   0.133634     -0.0637849    0.0242193   -0.0822879    0.147825     -0.120489     -0.132827
 -0.0350505   -0.159073    -0.109011     0.0521639   -0.00439433   0.116131    -0.0362085     0.0509332     0.0194545    0.0520539    0.121487    -0.0962373    0.0500288   -0.115855      0.106576     0.0468458    0.00296964    0.0707939    0.112056    0.0102619    -0.00646756   0.205697    -0.0911692   -0.0934021     0.0128465    -0.102266
  0.186173     0.0961618    0.00716075   0.159819    -0.0356148    0.0309507   -0.0966342     0.308078     -0.0225621   -0.105719     0.0854598   -0.0254207    0.00645153  -0.180419     -0.201468     0.0210935    0.145404      0.0208118   -0.114526   -0.131196      0.114326    -0.0536312    0.198977    -0.036576     -0.0188994     0.102704
 -0.0556927   -0.0706104    0.0883267   -0.0330511   -0.0742774    0.113139     0.000265748   0.00123978   -0.00525131   0.032849    -0.0308751    0.116866    -0.0750466   -0.155071     -0.0807009    0.0332004   -0.000361928  -0.13577      0.041944   -0.00780115    0.0693386   -0.112523     0.0200875   -0.0211474    -0.15055      -0.0940672
 -0.0637717   -0.0115376   -0.11273      0.00477784  -0.0387939    0.0216049    0.106621     -0.0848594     0.058056    -0.0970451    0.10122      0.16901     -0.121692    -0.00832548   -0.0151608    0.0819759    0.192401     -0.0636926   -0.0335573   0.0487329     0.136896     0.138782    -0.179904     0.0206887     0.0847417    -0.228981
 -0.0361465    0.061249     0.0328728    0.113771     0.107966    -0.0429752   -0.124806      0.11927      -0.143212    -0.171726     0.0348461    0.0467412    0.0225727    0.108537      0.107213     0.0687543    0.169842      0.138301     0.217241    0.0167453     0.11         0.0130884   -0.0550708    0.137522     -0.0514731    -0.000719905
  0.159691    -0.0402958   -0.0589163    0.0931936    0.0963127   -0.214286     0.121608     -0.0848162    -0.095691     0.128835    -0.0204693    0.0633972   -0.189375     0.0567076    -0.0585352    0.137927     0.207033      0.0869176    0.072773   -0.0211218    -0.0303556   -0.124789     0.240579    -0.201296     -0.0529819    -0.00532454
 -2.34499e-5  -0.0558908    0.159037     0.00567046   0.0473454    0.035208     0.0205119    -0.0115503    -0.128564    -0.118107     0.0937706   -0.0116489   -0.0529268   -0.0633248    -0.0536673   -0.0404513    0.0905517    -0.127065     0.0213534   0.129414      0.0213017    0.126546     0.0772354    0.172402      0.0468183    -0.064046
 -0.0620549   -0.0484962    0.0611598   -0.164008     0.0444361   -0.0297712   -0.201116      0.151913     -0.263477    -0.0617301    0.137481    -0.168838    -0.0715958   -0.00909167    0.0405301   -0.026967     0.00815834   -0.147366    -0.169537    0.0211103     0.111337    -0.140466    -0.0351389   -0.0407374     0.028303     -0.164978
 -0.0293518    0.0798293   -0.0211988   -0.245026     0.0415855    0.0291195    0.0757753    -0.0731222    -0.0123005   -0.0283532    0.166724    -0.00957351  -0.0141191    0.185688      0.130689    -0.0373114    0.14447       0.149963    -0.0382602  -0.0281745     0.0796104    0.0210241    0.0530077   -0.114412     -0.0812455    -0.241268
 -0.141252    -0.0297901   -0.0689401    0.0790702    0.147856     0.0189644    0.04374      -0.198147      0.117336    -0.089803    -0.00645524  -0.0671958    0.116858    -0.208545     -0.0885246    0.0309479    0.0267468    -0.0598158    0.150801   -0.153571     -0.0778303    0.0348798   -0.0814634    0.165943      0.0790169     0.0219166
  0.0822091    0.0436991   -0.0256128    0.0333181   -0.107687    -0.056206    -0.0668444     0.0188063    -0.0686114    0.155958    -0.116263    -0.0982137    0.0920304    0.0353296    -0.0168157   -0.0436995    0.100911     -0.0567761    0.282313   -0.0681225    -0.0492284    0.0713151   -0.145939     0.24673       0.00263384   -0.00697671
 -0.0995381    0.06264      0.0116475   -0.10919     -0.00422829   0.117458    -0.0649294     0.0511937     0.00018342  -0.0185521   -0.0293368   -0.0827265   -0.26523      0.112424     -0.0718678   -0.0762831   -0.147024     -0.0369723   -0.175307   -0.0643353     0.0243843   -0.0522312   -0.0261085    0.026802      0.137738     -0.00296923
  0.140855    -0.149561     0.0388171    0.137652     0.179856     0.162625    -0.129202      0.0537788     0.0326497    0.157907     0.0463694   -0.0870434   -0.00370149  -0.108866      0.142514    -0.0504956   -0.0777771    -0.0255654   -0.0563559   0.0203917    -0.187055    -0.146306    -0.0105479    0.147208     -0.143431     -0.00922076
  0.0743809    0.0357361   -0.165997    -0.00581164   0.00974779   0.0544957   -0.00637815   -0.0565192     0.174705     0.0182984   -0.175563     0.167352    -0.00338964   0.0316355    -0.112284    -0.0862629   -0.152966      0.0736277    0.0150565  -0.0325162     0.161475    -0.00103301  -0.212794    -0.0567808     0.0400737     0.114624
  0.0654107    0.0512106    0.0377104   -0.235477     0.0598288    0.152478     0.111199     -0.129452      0.0949904   -0.0426715    0.0815863   -0.129759     0.0801624   -0.0236695     0.0911618   -0.00250459   0.109853     -0.141414    -0.0192229   0.0347086     0.130727     0.00598008  -0.12343      0.0760807     0.0425199    -0.0960943
  0.0257552    0.0828645    0.0447143    0.0927087   -0.068294     0.0569742   -0.0127664    -0.261079     -0.0639186    0.0770686   -0.0724161   -0.168319    -0.105478     0.0994301     0.119258    -0.00858253   0.301494      0.0203223   -0.0754219  -0.146437      0.0985943    0.117216     0.047536    -0.130883      0.0407792    -0.223998
 -0.13949     -0.20485      0.0572162   -0.0309109   -0.0610488    0.0425709    0.091884      0.0194606    -0.135552    -0.0671133    0.120969    -0.0971412   -0.0573803    0.00486417   -0.205866     0.0448824   -0.000200571  -0.0143834    0.030737   -0.0599545     0.028965     0.245151    -0.161911    -0.100789     -0.0655061     0.174467
 -0.0912464    0.0255281    0.191447     0.273084    -0.0758945   -0.0425596   -0.0909037    -0.184064      0.00467243   0.0940198    0.171841     0.103119    -0.092152    -0.149553      0.0173151    0.229839    -0.0515537     0.073489     0.13786    -0.0138898    -0.111604    -0.0677075   -0.026358     0.0396144     0.083784     -0.0551216
  0.203296     0.0434309    0.0527738   -0.0869802   -0.127238     0.185592     0.000897266   0.0753866    -0.0470027   -0.0685834    0.136914    -0.127065     0.00489216   0.0121447    -0.0812533   -0.107708    -0.0997813     0.0691809   -0.0195171  -0.0128734     0.0180617   -0.106599    -0.145425    -0.103139     -0.110067     -0.0403507
  0.0294945    0.0921468    0.0621235    0.0500372   -0.118391     0.107105     0.0160509     0.100296     -0.00653737   0.0566167   -0.028102    -0.0979057   -0.0516547   -0.0491767     0.0163993    0.0193068    0.0903592     0.0439591    0.105979   -0.0945549     0.157585    -0.0214583   -0.0605692    0.0267678     0.185443     -0.137427
  0.00900259   0.124712     0.160764    -0.015181     0.0935147   -0.00387703   0.065047      0.0362641     0.0376398   -0.14826      0.0791905    0.119491     0.0883744    0.0766211    -0.00985635   0.198301    -0.0012915     0.0950059   -0.0694366   0.14905       0.116782     0.151485     0.134954     0.0367569     0.205203      0.11896
  0.149995     0.0222733    0.130771     0.056007     0.0525936    0.0340138   -0.190154      0.000266905  -0.151608     0.14506      0.00821943  -0.0457389    0.0200969    0.166173     -0.187197    -0.0804517    0.159098      0.0637587    0.0107387   0.0713809    -0.146843    -0.0676367    0.00681612   0.000484605   0.0404777    -0.183104
  0.00375073   0.0183374    0.0486462    0.0577372   -0.0891565   -0.13337     -0.086631     -0.00695756   -0.0331917    0.114233     0.0271548    0.165553    -0.0233939    0.0208031     0.0956882    0.0671801   -0.0274093     0.0252038    0.0892989   0.123318      0.0550656   -0.0685061    0.155653     0.126465      0.035395     -0.0308256
 -0.04575     -0.00394999  -0.0126843   -0.0411946   -0.0491145   -0.122841     0.00467954   -0.120514     -0.0469933   -0.0735191    0.138246     0.122366    -0.0632222   -0.0562663     0.0437271    0.104062     0.116645      0.00583041   0.0519301  -0.0505617    -0.00590952  -0.00143034   0.180124    -0.133477      0.0724843    -0.163912
 -0.0862884   -0.253183     0.0444096    0.0236934    0.0614627   -0.0903579   -0.0930851    -0.0847071     0.0301714    0.0291553    0.227412    -0.112039     0.0567047   -0.221362     -0.0800673    0.126998     0.140456      0.0280091    0.0919507  -0.000414966  -0.154047     0.0177239    0.161388    -0.129415      0.0226312    -0.226367
 -0.00801506   0.073887     0.0181882    0.152959     0.0118596    0.118764     0.0270765     0.0156689     0.109384    -0.00289676  -0.0370955    0.216507    -0.00638038   0.000470099   0.246799     0.151373     0.0357495     0.0475392   -0.0716589   0.10277      -0.120742     0.104498    -0.0930606    0.0196044     0.0803955     0.0679062
  0.0168265   -0.00401419  -0.0498545    0.111317    -0.0125202   -0.0111272   -0.0724483     0.236443      0.182064    -0.109604     0.0861617   -0.243744     0.200099    -0.178835     -0.00129534   0.0536361    0.0371769     0.0202544    0.0723141   0.0550823    -0.0627207   -0.00115295  -0.197544     0.105937     -0.000642537  -0.0286205
  0.00238246  -0.0522854   -0.127712    -0.0622338    0.0778045   -0.0416853    0.0670697    -0.0907211     0.0377287    0.0559755   -0.234384    -0.106952    -0.0720728   -0.0456848     0.0138597    0.122107    -0.0800385     0.116621    -0.0628429   0.0509684    -0.0524692    0.137139     0.129368     0.012437      0.129006     -0.00820297
 -0.0760144    0.148777     0.0732901    0.211913    -0.033749    -0.0320625   -0.138861      0.0529064    -0.101311     0.0751539    0.194939    -0.225561     0.220577    -0.0414994     0.0297739   -0.109399     0.172692     -0.102859    -0.0381928   0.0422457     0.0124936    0.0677491    0.0947788   -0.0744186     0.0535386     0.00671957
  0.00758828   0.138044    -0.0212758    0.00112396   0.120401    -0.00878591  -0.0268187     0.0124661    -0.0756677   -0.233474     0.0236549   -0.124768    -0.00181923   0.0626717     0.0459738   -0.0530281   -0.0286531     0.142598    -0.109542   -0.000601417  -0.0961549    0.128103     0.0402077   -0.164685      0.11207       0.0511609
  0.0566456    0.123214    -0.0676206    0.0464913   -0.0146081    0.0525062    0.143774     -0.135936      0.0885188    0.0614243   -0.0483193    0.0130817    0.0408915   -0.0375764    -0.0507659   -0.0260525    0.0584607     2.58091e-5   0.0841952   0.0240918    -0.0657608   -0.00128556  -0.0291538    0.0153061    -0.00741701    0.0390095kind diag, method split
┌ Info: 0: avll = 
└   tll[1] = -1.436581492018667
[ Info: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.436668
[ Info: iteration 2, average log likelihood -1.436581
[ Info: iteration 3, average log likelihood -1.436164
[ Info: iteration 4, average log likelihood -1.432110
[ Info: iteration 5, average log likelihood -1.418814
[ Info: iteration 6, average log likelihood -1.409899
[ Info: iteration 7, average log likelihood -1.406847
[ Info: iteration 8, average log likelihood -1.404817
[ Info: iteration 9, average log likelihood -1.403386
[ Info: iteration 10, average log likelihood -1.402419
[ Info: iteration 11, average log likelihood -1.401741
[ Info: iteration 12, average log likelihood -1.401223
[ Info: iteration 13, average log likelihood -1.400837
[ Info: iteration 14, average log likelihood -1.400559
[ Info: iteration 15, average log likelihood -1.400367
[ Info: iteration 16, average log likelihood -1.400241
[ Info: iteration 17, average log likelihood -1.400156
[ Info: iteration 18, average log likelihood -1.400095
[ Info: iteration 19, average log likelihood -1.400052
[ Info: iteration 20, average log likelihood -1.400023
[ Info: iteration 21, average log likelihood -1.400005
[ Info: iteration 22, average log likelihood -1.399994
[ Info: iteration 23, average log likelihood -1.399985
[ Info: iteration 24, average log likelihood -1.399980
[ Info: iteration 25, average log likelihood -1.399975
[ Info: iteration 26, average log likelihood -1.399972
[ Info: iteration 27, average log likelihood -1.399969
[ Info: iteration 28, average log likelihood -1.399966
[ Info: iteration 29, average log likelihood -1.399964
[ Info: iteration 30, average log likelihood -1.399962
[ Info: iteration 31, average log likelihood -1.399961
[ Info: iteration 32, average log likelihood -1.399959
[ Info: iteration 33, average log likelihood -1.399958
[ Info: iteration 34, average log likelihood -1.399957
[ Info: iteration 35, average log likelihood -1.399955
[ Info: iteration 36, average log likelihood -1.399955
[ Info: iteration 37, average log likelihood -1.399954
[ Info: iteration 38, average log likelihood -1.399953
[ Info: iteration 39, average log likelihood -1.399952
[ Info: iteration 40, average log likelihood -1.399951
[ Info: iteration 41, average log likelihood -1.399951
[ Info: iteration 42, average log likelihood -1.399950
[ Info: iteration 43, average log likelihood -1.399950
[ Info: iteration 44, average log likelihood -1.399949
[ Info: iteration 45, average log likelihood -1.399949
[ Info: iteration 46, average log likelihood -1.399948
[ Info: iteration 47, average log likelihood -1.399948
[ Info: iteration 48, average log likelihood -1.399948
[ Info: iteration 49, average log likelihood -1.399947
[ Info: iteration 50, average log likelihood -1.399947
┌ Info: EM with 100000 data points 50 iterations avll -1.399947
└ 952.4 data points per parameter
┌ Info: 1
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4366675154969264
│     -1.436580578864526
│      ⋮
└     -1.3999468977902763
[ Info: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.400059
[ Info: iteration 2, average log likelihood -1.399944
[ Info: iteration 3, average log likelihood -1.399228
[ Info: iteration 4, average log likelihood -1.392251
[ Info: iteration 5, average log likelihood -1.376003
[ Info: iteration 6, average log likelihood -1.364503
[ Info: iteration 7, average log likelihood -1.359936
[ Info: iteration 8, average log likelihood -1.358187
[ Info: iteration 9, average log likelihood -1.357347
[ Info: iteration 10, average log likelihood -1.356887
[ Info: iteration 11, average log likelihood -1.356582
[ Info: iteration 12, average log likelihood -1.356342
[ Info: iteration 13, average log likelihood -1.356120
[ Info: iteration 14, average log likelihood -1.355900
[ Info: iteration 15, average log likelihood -1.355676
[ Info: iteration 16, average log likelihood -1.355457
[ Info: iteration 17, average log likelihood -1.355281
[ Info: iteration 18, average log likelihood -1.355155
[ Info: iteration 19, average log likelihood -1.355067
[ Info: iteration 20, average log likelihood -1.355002
[ Info: iteration 21, average log likelihood -1.354951
[ Info: iteration 22, average log likelihood -1.354908
[ Info: iteration 23, average log likelihood -1.354870
[ Info: iteration 24, average log likelihood -1.354832
[ Info: iteration 25, average log likelihood -1.354792
[ Info: iteration 26, average log likelihood -1.354746
[ Info: iteration 27, average log likelihood -1.354693
[ Info: iteration 28, average log likelihood -1.354628
[ Info: iteration 29, average log likelihood -1.354543
[ Info: iteration 30, average log likelihood -1.354423
[ Info: iteration 31, average log likelihood -1.354238
[ Info: iteration 32, average log likelihood -1.353966
[ Info: iteration 33, average log likelihood -1.353582
[ Info: iteration 34, average log likelihood -1.353126
[ Info: iteration 35, average log likelihood -1.352674
[ Info: iteration 36, average log likelihood -1.352304
[ Info: iteration 37, average log likelihood -1.352090
[ Info: iteration 38, average log likelihood -1.351969
[ Info: iteration 39, average log likelihood -1.351891
[ Info: iteration 40, average log likelihood -1.351834
[ Info: iteration 41, average log likelihood -1.351792
[ Info: iteration 42, average log likelihood -1.351760
[ Info: iteration 43, average log likelihood -1.351736
[ Info: iteration 44, average log likelihood -1.351718
[ Info: iteration 45, average log likelihood -1.351704
[ Info: iteration 46, average log likelihood -1.351693
[ Info: iteration 47, average log likelihood -1.351684
[ Info: iteration 48, average log likelihood -1.351677
[ Info: iteration 49, average log likelihood -1.351671
[ Info: iteration 50, average log likelihood -1.351666
┌ Info: EM with 100000 data points 50 iterations avll -1.351666
└ 473.9 data points per parameter
┌ Info: 2
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4000586169251739
│     -1.3999441607427396
│      ⋮
└     -1.351665666461813
[ Info: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.351817
[ Info: iteration 2, average log likelihood -1.351667
[ Info: iteration 3, average log likelihood -1.350917
[ Info: iteration 4, average log likelihood -1.344300
[ Info: iteration 5, average log likelihood -1.328505
[ Info: iteration 6, average log likelihood -1.312651
[ Info: iteration 7, average log likelihood -1.304487
[ Info: iteration 8, average log likelihood -1.301208
[ Info: iteration 9, average log likelihood -1.299483
[ Info: iteration 10, average log likelihood -1.298242
[ Info: iteration 11, average log likelihood -1.297114
[ Info: iteration 12, average log likelihood -1.296156
[ Info: iteration 13, average log likelihood -1.295455
[ Info: iteration 14, average log likelihood -1.294956
[ Info: iteration 15, average log likelihood -1.294569
[ Info: iteration 16, average log likelihood -1.294258
[ Info: iteration 17, average log likelihood -1.293983
[ Info: iteration 18, average log likelihood -1.293717
[ Info: iteration 19, average log likelihood -1.293467
[ Info: iteration 20, average log likelihood -1.293255
[ Info: iteration 21, average log likelihood -1.293084
[ Info: iteration 22, average log likelihood -1.292950
[ Info: iteration 23, average log likelihood -1.292850
[ Info: iteration 24, average log likelihood -1.292779
[ Info: iteration 25, average log likelihood -1.292729
[ Info: iteration 26, average log likelihood -1.292694
[ Info: iteration 27, average log likelihood -1.292671
[ Info: iteration 28, average log likelihood -1.292656
[ Info: iteration 29, average log likelihood -1.292645
[ Info: iteration 30, average log likelihood -1.292638
[ Info: iteration 31, average log likelihood -1.292633
[ Info: iteration 32, average log likelihood -1.292629
[ Info: iteration 33, average log likelihood -1.292626
[ Info: iteration 34, average log likelihood -1.292623
[ Info: iteration 35, average log likelihood -1.292622
[ Info: iteration 36, average log likelihood -1.292620
[ Info: iteration 37, average log likelihood -1.292618
[ Info: iteration 38, average log likelihood -1.292617
[ Info: iteration 39, average log likelihood -1.292616
[ Info: iteration 40, average log likelihood -1.292615
[ Info: iteration 41, average log likelihood -1.292614
[ Info: iteration 42, average log likelihood -1.292613
[ Info: iteration 43, average log likelihood -1.292612
[ Info: iteration 44, average log likelihood -1.292612
[ Info: iteration 45, average log likelihood -1.292611
[ Info: iteration 46, average log likelihood -1.292610
[ Info: iteration 47, average log likelihood -1.292609
[ Info: iteration 48, average log likelihood -1.292609
[ Info: iteration 49, average log likelihood -1.292608
[ Info: iteration 50, average log likelihood -1.292607
┌ Info: EM with 100000 data points 50 iterations avll -1.292607
└ 236.4 data points per parameter
┌ Info: 3
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.3518172541379159
│     -1.3516673423938264
│      ⋮
└     -1.2926074451376386
[ Info: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.292855
[ Info: iteration 2, average log likelihood -1.292600
[ Info: iteration 3, average log likelihood -1.291747
[ Info: iteration 4, average log likelihood -1.283285
[ Info: iteration 5, average log likelihood -1.255837
[ Info: iteration 6, average log likelihood -1.228402
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     5
│     9
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.210857
[ Info: iteration 8, average log likelihood -1.221183
[ Info: iteration 9, average log likelihood -1.207528
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.201481
[ Info: iteration 11, average log likelihood -1.207661
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     9
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 12, average log likelihood -1.199469
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 13, average log likelihood -1.206206
[ Info: iteration 14, average log likelihood -1.209965
[ Info: iteration 15, average log likelihood -1.201468
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 16, average log likelihood -1.197370
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     9
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 17, average log likelihood -1.204069
[ Info: iteration 18, average log likelihood -1.206426
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 19, average log likelihood -1.198446
[ Info: iteration 20, average log likelihood -1.204194
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     9
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 21, average log likelihood -1.195379
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 22, average log likelihood -1.200510
[ Info: iteration 23, average log likelihood -1.204091
[ Info: iteration 24, average log likelihood -1.195334
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     5
│     9
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 25, average log likelihood -1.191530
[ Info: iteration 26, average log likelihood -1.208395
[ Info: iteration 27, average log likelihood -1.196938
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 28, average log likelihood -1.192557
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     9
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 29, average log likelihood -1.200099
[ Info: iteration 30, average log likelihood -1.201671
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 31, average log likelihood -1.194413
[ Info: iteration 32, average log likelihood -1.201308
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     9
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 33, average log likelihood -1.193447
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 34, average log likelihood -1.199216
[ Info: iteration 35, average log likelihood -1.203196
[ Info: iteration 36, average log likelihood -1.194667
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     5
│     9
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 37, average log likelihood -1.191015
[ Info: iteration 38, average log likelihood -1.207974
[ Info: iteration 39, average log likelihood -1.196566
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 40, average log likelihood -1.192230
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     9
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 41, average log likelihood -1.199839
[ Info: iteration 42, average log likelihood -1.201373
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 43, average log likelihood -1.194144
[ Info: iteration 44, average log likelihood -1.201095
[ Info: iteration 45, average log likelihood -1.193275
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     5
│     9
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 46, average log likelihood -1.189452
[ Info: iteration 47, average log likelihood -1.207891
[ Info: iteration 48, average log likelihood -1.196467
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 49, average log likelihood -1.192146
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     9
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 50, average log likelihood -1.199785
┌ Info: EM with 100000 data points 50 iterations avll -1.199785
└ 118.1 data points per parameter
┌ Info: 4
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.2928550461320574
│     -1.2926001891854015
│      ⋮
└     -1.1997853137470789
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.201733
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      9
│     10
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 2, average log likelihood -1.194056
[ Info: iteration 3, average log likelihood -1.194094
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      9
│     10
│     17
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.186061
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     18
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.165470
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      9
│     10
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.137257
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.129217
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      9
│     10
│     13
│     18
│     19
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.119112
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.135840
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      9
│     10
│     19
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.105566
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     14
│     17
│     18
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 11, average log likelihood -1.115143
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      9
│     10
│     13
│     19
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 12, average log likelihood -1.131306
[ Info: iteration 13, average log likelihood -1.140088
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      9
│     10
│     18
│     19
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 14, average log likelihood -1.113793
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      5
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 15, average log likelihood -1.125987
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      9
│     10
│     17
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 16, average log likelihood -1.113396
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     13
│     18
│     25
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 17, average log likelihood -1.124584
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      9
│     10
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 18, average log likelihood -1.135628
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 19, average log likelihood -1.127883
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      9
│     10
│     17
│     18
│     19
│     25
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 20, average log likelihood -1.102013
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 21, average log likelihood -1.140242
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      9
│     10
│     13
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 22, average log likelihood -1.125328
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     18
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 23, average log likelihood -1.127532
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      9
│     10
│     17
│     19
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 24, average log likelihood -1.118860
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     14
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 25, average log likelihood -1.129230
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      5
│      9
│     10
│     18
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 26, average log likelihood -1.120508
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     13
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 27, average log likelihood -1.132783
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      9
│     10
│     17
│     19
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 28, average log likelihood -1.109914
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     18
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 29, average log likelihood -1.133992
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      9
│     10
│     19
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 30, average log likelihood -1.119646
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     14
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 31, average log likelihood -1.117348
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      5
│      9
│     10
│     13
│     17
│     18
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 32, average log likelihood -1.108338
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     25
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 33, average log likelihood -1.144082
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      9
│     10
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 34, average log likelihood -1.129088
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     18
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 35, average log likelihood -1.130988
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      9
│     10
│     19
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 36, average log likelihood -1.113773
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      5
│     14
│     17
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 37, average log likelihood -1.113879
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      9
│     10
│     13
│     18
│     19
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 38, average log likelihood -1.126645
[ Info: iteration 39, average log likelihood -1.148127
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      9
│     10
│     19
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 40, average log likelihood -1.115720
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     18
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 41, average log likelihood -1.113468
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      5
│      9
│     10
│     13
│     17
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 42, average log likelihood -1.109003
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     15
│     25
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 43, average log likelihood -1.132319
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      9
│     10
│     14
│     18
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 44, average log likelihood -1.126290
[ Info: iteration 45, average log likelihood -1.143853
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      9
│     10
│     19
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 46, average log likelihood -1.104429
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      5
│     13
│     17
│     18
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 47, average log likelihood -1.110767
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      9
│     10
│     19
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 48, average log likelihood -1.136230
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 49, average log likelihood -1.136329
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      9
│     10
│     18
│     19
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 50, average log likelihood -1.119970
┌ Info: EM with 100000 data points 50 iterations avll -1.119970
└ 59.0 data points per parameter
┌ Info: 5
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.2017329170998898
│     -1.1940560779204925
│      ⋮
└     -1.1199699332508333
┌ Info: Total log likelihood: 
│   tll =
│    251-element Array{Float64,1}:
│     -1.436581492018667
│     -1.4366675154969264
│     -1.436580578864526
│     -1.4361638419137315
│      ⋮
│     -1.1362302432332083
│     -1.1363288790690613
└     -1.1199699332508333
32×26 Array{Float64,2}:
 -0.05717     -0.140419      0.102594      0.00452082    0.042828    -0.021074    -0.0323099    -0.064698    -0.0397243    -0.00281279   0.15652     -0.0675741    0.000984925  -0.139983    -0.0715774   0.053524     0.0979443   -0.0582251    0.0267981    0.0566741    -0.0664241     0.0699548    0.120371    -0.00523135   0.0219805   -0.135219
 -0.0359702    0.116227     -0.0318887    -0.23544       0.0489083    0.0354028    0.0706882    -0.0614214   -0.00557707   -0.0223972    0.130592    -0.0061904   -0.015251      0.175806     0.136981    0.00328436   0.141087     0.155508     0.00561936  -0.0217638     0.0852376     0.0196511    0.0499278   -0.106645    -0.0824134   -0.235643
  0.180401     0.0239414     0.0456528    -0.080435     -0.166488     0.183743     0.000907838   0.0695701   -0.0412556    -0.0823098    0.127381    -0.12487      0.011026     -0.0121541   -0.0803723  -0.13818     -0.103992     0.0467024   -0.0262007   -0.0125646     0.0238435    -0.126857    -0.0506745   -0.0924467   -0.138005    -0.0334839
 -0.071044    -0.000815898   0.0527883    -0.0675965    -0.0295212    0.108864    -0.0491996     0.0252011   -0.00608128    0.00320468  -0.0262993    0.00430202  -0.159623     -0.0207282   -0.0679433  -0.0243886   -0.0449713   -0.0728661   -0.0465662   -0.0418714     0.0441426    -0.0626992   -0.00411657   0.0117548    0.00168677  -0.0364474
  0.0474199    0.129558     -0.0172131    -0.000807863   0.150974    -0.0319844   -0.073212     -0.00837114  -0.0680722    -0.195346     0.0262897   -0.122306     0.0173639     0.0498463    0.0974645  -0.0508629   -0.0116397    0.135408    -0.123384     0.000954375  -0.0923819     0.119814     0.0395642   -0.127129     0.082698     0.0507933
 -0.0254581    0.0363435    -0.0954211     0.0244078    -0.0194287    0.0768856    0.129229     -0.0935435    0.072638     -0.0258701    0.0390275    0.0787892   -0.0684616    -0.0183567    0.0190903   0.0223275    0.135441    -0.0365223   -0.00728249   0.0266495     0.0621411     0.105924    -0.136158     0.024617     0.0543955   -0.116466
 -0.0795576   -0.101313      0.000138796  -0.0367428    -0.0389393   -0.0203708    0.057048     -0.057206    -0.0803844    -0.0664543    0.125495     0.0271028   -0.0581171    -0.0297078   -0.0605728   0.0605214    0.0498753    0.00144782   0.0468365   -0.0606715     0.0120541     0.107666     0.0131427   -0.115547    -0.00547582  -0.00160986
  0.10151     -0.0497508     0.000948795   0.0631135     0.0130005    0.0274591   -0.0849204     0.0402873   -0.0319583     0.168082    -0.0403986   -0.0924268    0.0361141    -0.0353153    0.0306877  -0.0709107    0.0347624   -0.0447217    0.144107    -0.0233458    -0.109398     -0.00122472  -0.108371     0.226419    -0.0764305   -0.000377287
  0.164684     0.0966544     0.0307556     0.150563     -0.0343987   -0.0693288   -0.0939651     0.332044    -0.25434      -0.115209     0.0963209   -0.0302617    0.00666884   -0.180048    -0.219495    0.0927855    0.15542      0.0211255   -0.124983    -0.144775      0.111505     -0.219397     0.197501    -0.0335153   -0.0569252    0.157895
  0.213944     0.116352     -0.190779      0.11677      -0.0142141    0.12128     -0.0912108     0.266251     0.585314     -0.0413953    0.0663669   -0.043451     0.0161125    -0.173738    -0.192857   -0.19226      0.122177     0.0223684   -0.0890682   -0.102048      0.121454      0.400022     0.198553    -0.0315148    0.00578804   0.0226493
  0.173718     0.162483     -0.310251      0.239989     -0.0287037   -0.0314308   -0.141812      0.13607      0.0467295     0.0570349    0.193269    -0.223024     0.247279     -0.925656    -0.0418849  -0.0990442    0.13966     -0.120013    -0.0398209    0.0331761     0.0566789     0.0622489    0.0900542   -0.113161     0.14073      0.010436
 -0.398795     0.120711      0.300728      0.193541     -0.0383563   -0.0283924   -0.221011      0.032836    -0.294248      0.0927984    0.196274    -0.230135     0.168545      0.796425     0.0845718  -0.086232     0.208739    -0.0776965   -0.0517607   -0.0137879     0.00585344    0.0698232    0.0800255   -0.0386357   -0.0366868   -7.23319e-5
  0.0433312    0.103624      0.0466354     0.0447137    -0.119453     0.0711289    0.0228441     0.0842325    0.00318039    0.0469243   -0.0396185   -0.0650873   -0.0686498    -0.00187219   0.0178004  -0.0204731    0.0645331    0.0451836    0.141081    -0.105464      0.163311     -0.0432563   -0.0913698   -0.0019277    0.164193    -0.0912749
 -0.0412593   -0.143329     -0.0993775     0.0614188    -0.00292751   0.083533    -0.0393149     0.0551716    0.0349286     0.0445834    0.110153    -0.0700547    0.023696     -0.109903     0.0785289   0.0493019    0.00206144   0.0595273    0.109424     0.0173527    -0.000563507   0.131548    -0.0952834   -0.102923     0.0039477   -0.0818954
  0.101818     0.0153462    -0.158395     -0.000209315   0.034699     0.00961062  -0.0527181    -0.0506984    0.181234      0.078539    -0.0717354    0.0615203    0.0282295     0.00549049  -0.0845727  -0.0619306   -0.179668     0.181475     0.013197    -0.0163814     0.083899     -0.0296193   -0.162879    -0.0657914    0.0512782    0.0287803
  0.160236    -0.0769666    -0.0986604    -0.00461412   -0.104168     0.0843843   -0.0634097    -0.0505165   -0.0311059     0.160126     0.185891    -0.197812     0.0921943     0.0218518   -0.11171     0.0479625   -0.224017     0.0697899    0.0309343    0.110093     -0.0814118     0.0765248   -0.0794669    0.157767    -0.117489    -0.139002
  0.0858825    0.0632689    -0.130375     -0.0069402     0.0263553    0.0454549    0.0184433    -0.0544362    0.177707      0.0415659   -0.216541     0.182876     0.00168169    0.012488    -0.114833   -0.0192645   -0.117904     0.142595    -0.00167428  -0.0544839     0.0245248     0.0465684   -0.180701    -0.0975394   -0.0223926    0.156665
  0.145526     0.0220743     0.129172      0.0463048     0.048476     0.0214895   -0.177771     -0.00123041  -0.152773      0.0831337   -0.00659363  -0.0425976    0.027083      0.168411    -0.16503    -0.0660503    0.160009     0.0140698    0.0385467    0.0448976    -0.148154     -0.0750107   -0.00179494   0.0037488    0.0454378   -0.127593
 -0.0912802    0.0164625     0.186763      0.268861     -0.0748095   -0.0411505   -0.0822598    -0.184927    -0.0348707     0.0944007    0.172281     0.11165     -0.0768684    -0.155767     0.0725405   0.260141    -0.0451047    0.0719554    0.138853     0.00510454   -0.108632     -0.0542324   -0.0412735    0.0231558    0.0831144   -0.0704333
  0.0240347    0.0675069     0.084932      0.0825586    -0.0667606    0.0483404   -0.0252688    -0.240928    -0.058793      0.0625493   -0.0534009   -0.165973    -0.105088      0.110443     0.1073      0.00724507   0.306735     0.0291825   -0.0847291   -0.167037      0.0717937     0.113512     0.0819645   -0.0876586    0.0428225   -0.264533
  0.109987    -0.640711     -0.0394326     0.12143       0.142756    -0.269526     0.0284984    -0.0639715   -0.000173709   0.00494849  -0.0200426    0.0617196   -0.118529      0.0550433   -0.0508929   0.131812     0.243025     0.0936316    0.077698    -0.0581178    -0.0467758    -0.161179     0.218141    -0.180291    -0.0516722   -0.0435367
  0.191263     0.501649     -0.0790695     0.109728      0.0412908   -0.193055     0.126507     -0.13233     -0.141533      0.211795    -0.0244844    0.0634303   -0.172395      0.0626673   -0.0803538   0.147994     0.155928     0.0860509    0.0686198    0.00038801   -0.0113159    -0.0810389    0.249541    -0.287952    -0.05649     -0.0522767
 -0.0670203   -0.0548352     0.120643     -0.167266      0.0359546   -0.0351833   -0.259213      0.127152    -0.272452     -0.0611128    0.223354    -0.185996    -0.991499      0.0300539    0.0395004   0.0370811    0.0127328    0.00640044  -0.196955     0.0592071     0.0783802    -0.220822    -0.0165212   -0.0576774    0.0515348   -0.0474204
 -0.0474676   -0.0358519    -0.0572076    -0.0587001     0.0456063   -0.0118444   -0.112762      0.148648    -0.274668     -0.0850859    0.130306    -0.16425      0.8382       -0.055738     0.0418774  -0.0580243    0.00574734  -0.327761    -0.136756     0.0140248     0.249813     -0.128087    -0.0214539   -0.0299755    0.0121566   -0.244752
  0.0176547   -0.00167338   -0.0464939     0.112734     -0.00810071  -0.0202968   -0.0681159     0.224783     0.162877     -0.124036     0.097367    -0.226723     0.194049     -0.182097    -0.0145253   0.0529278    0.0107916    0.0150948    0.0729179    0.0195188    -0.0804576    -0.0109817   -0.189524     0.107936     0.00161588   0.0320722
  0.00180894   0.0185023     0.0522781     0.0485378    -0.0929556   -0.122078    -0.0956865    -0.00986527  -0.0129967     0.128137     0.027932     0.173995    -0.0227442     0.0147177    0.0980289   0.0621122   -0.039429     0.0460418    0.090763     0.125027      0.0715278    -0.0283813    0.151393     0.138802     0.0368798    0.0496016
  0.0100074    0.0913179    -0.129463      0.155903     -0.0439876    0.143807    -0.0515008     0.0631458    0.120096     -0.00307219  -0.0318047    0.168953    -0.00604401   -0.0342493   -0.082491    0.122883     0.034212     0.0156396   -0.0272424    0.104575     -0.126691      0.0893538   -0.0417688    0.0438904    0.0750653    0.0829179
 -0.00229992   0.0369408     0.120999      0.150774      0.098687     0.0808511    0.11112      -0.0186245    0.133699      0.0786309   -0.0524989    0.256154     0.0159095     0.0347876    0.628532    0.176479    -0.0017551    0.0613503    0.0062605    0.117464     -0.121609      0.140909    -0.0924901    0.00588773   0.082285     0.0340451
 -0.0149232    0.00930185   -0.0615198     0.0161715     0.0856729   -0.0416225   -0.0376135    -0.0106413   -0.0372822    -0.0444538   -0.0815001   -0.02929     -0.016435      0.0202637    0.0543074   0.0996742    0.024174     0.132621     0.0504352    0.0253574     0.017188      0.101698     0.0582424    0.072967     0.0199861    0.013823
 -0.0150746    0.0090837    -0.0269747    -0.0719718     0.0668509    0.0699764    0.0606487    -0.160844     0.107485     -0.048661    -0.0178844   -0.066342     0.0835716    -0.110661    -0.0199426   0.0135128    0.0528669   -0.0811722    0.068865    -0.0645706     0.0533456     0.0363693   -0.115726     0.121072     0.0552443   -0.00645455
 -0.0340753    0.122249      0.216062     -0.0306188     0.0971575   -0.0121887    0.0820918     0.0484691    0.0319446    -0.154575     0.0887871    0.111092     0.0195479     0.017151    -0.0924075   0.152835     0.0453635    0.0598687   -0.116972     0.166944      0.118507      0.0779978    0.133238    -0.0055442   -0.658486     0.130217
  0.0672577    0.12604       0.0532866    -0.000165762   0.115241     0.00868251   0.0168806     0.01713      0.0412209    -0.153843     0.0783264    0.12181      0.203968      0.146181     0.0784678   0.226214    -0.0270535    0.299008    -0.0426445   -0.0053433     0.0886464     0.189491     0.134062     0.0606099    0.962669     0.0919981[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 1, average log likelihood -1.128273
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      5
│      9
│     10
│     17
│     19
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 2, average log likelihood -1.094515
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     13
│     18
│     25
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 3, average log likelihood -1.108818
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      5
│      9
│     10
│     17
│     19
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.105722
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     14
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.111599
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      5
│      9
│     10
│     13
│      ⋮
│     19
│     25
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.092224
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.125464
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      5
│      9
│     10
│     17
│     19
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.093822
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│     13
│     14
│     18
│     25
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.108150
kind diag, method kmeans
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      5
│      9
│     10
│     17
│     19
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.108440
┌ Info: EM with 100000 data points 10 iterations avll -1.108440
└ 59.0 data points per parameter
[ Info: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.476446e+05
      1       7.109375e+05      -2.367071e+05 |       32
      2       6.840142e+05      -2.692326e+04 |       32
      3       6.676270e+05      -1.638725e+04 |       32
      4       6.565070e+05      -1.111996e+04 |       32
      5       6.491486e+05      -7.358442e+03 |       32
      6       6.442620e+05      -4.886629e+03 |       32
      7       6.403582e+05      -3.903759e+03 |       32
      8       6.374594e+05      -2.898768e+03 |       32
      9       6.357865e+05      -1.672939e+03 |       32
     10       6.345309e+05      -1.255621e+03 |       32
     11       6.332743e+05      -1.256596e+03 |       32
     12       6.323108e+05      -9.635187e+02 |       32
     13       6.317151e+05      -5.957024e+02 |       32
     14       6.312717e+05      -4.433297e+02 |       32
     15       6.309174e+05      -3.543495e+02 |       32
     16       6.306111e+05      -3.062960e+02 |       32
     17       6.303806e+05      -2.304632e+02 |       32
     18       6.302006e+05      -1.800448e+02 |       32
     19       6.300142e+05      -1.863960e+02 |       32
     20       6.298280e+05      -1.861870e+02 |       32
     21       6.296089e+05      -2.191413e+02 |       31
     22       6.293856e+05      -2.232431e+02 |       32
     23       6.292272e+05      -1.583709e+02 |       32
     24       6.290992e+05      -1.280939e+02 |       32
     25       6.290271e+05      -7.205665e+01 |       31
     26       6.289945e+05      -3.256749e+01 |       31
     27       6.289747e+05      -1.986415e+01 |       31
     28       6.289608e+05      -1.382305e+01 |       29
     29       6.289457e+05      -1.516578e+01 |       31
     30       6.289258e+05      -1.992268e+01 |       31
     31       6.289027e+05      -2.306781e+01 |       32
     32       6.288667e+05      -3.598366e+01 |       32
     33       6.288286e+05      -3.811431e+01 |       32
     34       6.287866e+05      -4.198041e+01 |       32
     35       6.287381e+05      -4.847272e+01 |       32
     36       6.286910e+05      -4.711369e+01 |       32
     37       6.286355e+05      -5.555341e+01 |       32
     38       6.285827e+05      -5.276941e+01 |       31
     39       6.285344e+05      -4.833523e+01 |       31
     40       6.284738e+05      -6.059418e+01 |       32
     41       6.284280e+05      -4.581140e+01 |       30
     42       6.283859e+05      -4.208873e+01 |       31
     43       6.283481e+05      -3.774905e+01 |       30
     44       6.283183e+05      -2.979318e+01 |       31
     45       6.282804e+05      -3.794336e+01 |       31
     46       6.282518e+05      -2.858104e+01 |       30
     47       6.282335e+05      -1.834136e+01 |       31
     48       6.282214e+05      -1.210520e+01 |       29
     49       6.282145e+05      -6.832402e+00 |       22
     50       6.282114e+05      -3.137905e+00 |       22
K-means terminated without convergence after 50 iterations (objv = 628211.3826204229)
┌ Info: K-means with 32000 data points using 50 iterations
└ 37.0 data points per parameter
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.341773
[ Info: iteration 2, average log likelihood -1.309793
[ Info: iteration 3, average log likelihood -1.281053
[ Info: iteration 4, average log likelihood -1.250075
[ Info: iteration 5, average log likelihood -1.203590
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      3
│     10
│     12
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.142492
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     11
│     13
│     16
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.143786
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     23
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.139619
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.120307
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      3
│      6
│     10
│     12
│     19
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.086972
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      2
│     13
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 11, average log likelihood -1.139595
[ Info: iteration 12, average log likelihood -1.130274
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      3
│     26
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 13, average log likelihood -1.077748
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      6
│     10
│     11
│     12
│     23
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 14, average log likelihood -1.072139
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      2
│     13
│     16
│     19
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 15, average log likelihood -1.117229
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 16, average log likelihood -1.140519
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 17, average log likelihood -1.101278
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      2
│     10
│     12
│     23
│     26
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 18, average log likelihood -1.063040
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      6
│     13
│     16
│     19
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 19, average log likelihood -1.115646
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      3
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 20, average log likelihood -1.137017
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 21, average log likelihood -1.117428
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     10
│     12
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 22, average log likelihood -1.081060
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      3
│      6
│     11
│     13
│     16
│     23
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 23, average log likelihood -1.083636
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      2
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 24, average log likelihood -1.129430
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     12
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 25, average log likelihood -1.109649
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      3
│     10
│     27
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 26, average log likelihood -1.076795
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│     11
│     13
│     16
│     23
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 27, average log likelihood -1.097698
[ Info: iteration 28, average log likelihood -1.120225
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      2
│      3
│      6
│     12
│     19
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 29, average log likelihood -1.055416
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     10
│     13
│     14
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 30, average log likelihood -1.090143
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     11
│     16
│     23
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 31, average log likelihood -1.107334
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      3
│     12
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 32, average log likelihood -1.112288
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      2
│      6
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 33, average log likelihood -1.085708
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     10
│     13
│     14
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 34, average log likelihood -1.079648
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      3
│     11
│     12
│     16
│     23
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 35, average log likelihood -1.097566
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     27
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 36, average log likelihood -1.116187
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      2
│      6
│     14
│     19
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 37, average log likelihood -1.072279
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      3
│     10
│     13
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 38, average log likelihood -1.101609
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     11
│     12
│     23
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 39, average log likelihood -1.105644
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     14
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 40, average log likelihood -1.093088
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      2
│      3
│      6
│     19
│     27
│     28
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 41, average log likelihood -1.051509
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     10
│     12
│     13
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 42, average log likelihood -1.107979
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     11
│     23
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 43, average log likelihood -1.114240
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      2
│      3
│      6
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 44, average log likelihood -1.082221
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     12
│     16
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 45, average log likelihood -1.087538
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│     10
│     13
│     27
│     28
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 46, average log likelihood -1.089782
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      3
│     11
│     23
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 47, average log likelihood -1.094349
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      2
│      6
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 48, average log likelihood -1.096394
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     12
│     16
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 49, average log likelihood -1.087755
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      3
│     10
│     13
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 50, average log likelihood -1.085174
┌ Info: EM with 100000 data points 50 iterations avll -1.085174
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
  0.0767966    0.0687801    -0.0256123    0.0207571   -0.118113    -0.0616873   -0.0667326    0.040202     -0.087418     0.180083    -0.110009   -0.101339     0.0910882    0.0207843   -0.0337603   -0.0651936    0.0938974   -0.0540073    0.298378    -0.06498     -0.0659541    0.109908    -0.192245      0.277737     0.00192029   -0.00351564
 -0.0684094   -0.0461924     0.0824278   -0.0487556   -0.0693595    0.106446    -0.00968791  -0.0358127    -0.00702177   0.0191491   -0.0228567   0.0873566   -0.0997065   -0.166298    -0.0810935    0.014709     0.0160187   -0.12517      0.0322415   -0.0186763    0.0594441   -0.0761729    0.0154352    -0.00742447  -0.161765     -0.0742985
 -0.0920404    0.0172978     0.192022     0.272665    -0.0759135   -0.0432522   -0.0852984   -0.186723     -0.0325328    0.0962573    0.174413    0.112379    -0.082257    -0.163119     0.0722006    0.259884    -0.0456625    0.0732818    0.136906     0.00485932  -0.108077    -0.0570629   -0.0445629     0.0254255    0.0831369    -0.0689411
  0.085521    -0.0572963     0.0408669    0.0577188    0.0570023   -0.103822     0.0540571   -0.069102     -0.092348     0.0351946    0.036955    0.0333979   -0.112744     0.00989424  -0.0646547    0.0626546    0.141527    -0.00541891   0.0308773    0.0380932   -0.00163127  -0.00715112   0.165462     -0.0542906   -0.00904613   -0.0500067
  0.182699     0.0201812     0.0501629   -0.0785532   -0.169023     0.185202     0.00102613   0.0728667    -0.0397648   -0.0847568    0.127575   -0.131154     0.0112987   -0.00632382  -0.0805536   -0.136736    -0.0984541    0.0466918   -0.024937    -0.0134015    0.0245552   -0.125996    -0.0465657    -0.0919078   -0.138329     -0.0371638
 -0.0902873    0.0445035     0.00287711  -0.0915618   -0.0167825    0.100134    -0.0682363    0.0759589     0.00734026   0.00418632  -0.0493464  -0.0774643   -0.214239     0.186157    -0.0701621   -0.0643303   -0.177924    -0.0207019   -0.194888    -0.091581     0.0397907   -0.046628    -0.0420029     0.0158408    0.278724      0.0266772
 -0.0442503    0.000744301  -0.0202025   -0.0445385   -0.0161924   -0.104778     0.0199502   -0.133715     -0.0359497   -0.0704503    0.141096    0.122537    -0.0624271   -0.0510305    0.0441131    0.0954868    0.11285      0.00574044   0.0523374   -0.0630109    0.00190205  -0.00373828   0.177501     -0.111247     0.0383327    -0.160813
  0.00164827   0.0613788     0.00286545   0.151805     0.0232389    0.0993765    0.0332502    0.0187381     0.12675      0.0520073   -0.0333197   0.217931     0.00375637   0.00149502   0.292074     0.14893      0.0132258    0.0410133   -0.0100421    0.110142    -0.123366     0.11267     -0.0559067     0.0316297    0.078765      0.0623671
 -0.0390777    0.11982      -0.033717    -0.240476     0.0477201    0.0361085    0.0705756   -0.0594466    -0.00466173  -0.0214389    0.129469   -0.0082861   -0.0153247    0.179578     0.137804     0.00355451   0.141871     0.157632     0.00445012  -0.0221919    0.0855892    0.0204632    0.0515375    -0.106242    -0.0813696    -0.236971
  0.145935     0.0221993     0.128417     0.0457254    0.0482487    0.0205503   -0.177244     0.000467898  -0.151526     0.0828914   -0.0099111  -0.0416721    0.0235681    0.169533    -0.164218    -0.0649653    0.159834     0.0162627    0.0418335    0.0456971   -0.147885    -0.0741061   -0.00100043    0.00355899   0.0456832    -0.135811
 -0.0629456    0.00028237   -0.113967     0.00787904  -0.0252616    0.050105     0.109664    -0.0674153     0.0601852   -0.0727507    0.0963744   0.142392    -0.123023    -0.00879862   0.0414237    0.0897621    0.172958    -0.0597649   -0.0515199    0.011135     0.133157     0.16588     -0.180407      0.0151765    0.0839097    -0.224939
  0.192216     0.0939443    -0.0300937    0.140713    -0.0265482   -0.0110905   -0.0930164    0.288307     -0.04542     -0.0955399    0.0922683  -0.0358848    0.00456608  -0.202403    -0.2132       0.0205546    0.129711     0.0206654   -0.113974    -0.127833     0.1125      -0.054575     0.189387     -0.0273282   -0.0358788     0.130822
  0.0173141   -0.000980916  -0.0471209    0.112805    -0.00605489  -0.0227319   -0.0726404    0.232618      0.160539    -0.133008     0.100783   -0.228713     0.196573    -0.181787    -0.0154489    0.0536168    0.0149035    0.0197444    0.0723068    0.0181678   -0.0786052   -0.00767594  -0.192471      0.106647     0.000886267   0.0288618
  0.0679028    0.0256634    -0.199628    -0.00043662   0.0419661    0.0490106   -0.00642874  -0.0669116     0.181917     0.0261429   -0.200226    0.195788    -0.00531434  -0.00235954  -0.107465    -0.0718845   -0.146987     0.103776     0.104887    -0.0289595    0.18807     -0.00590341  -0.204341     -0.094088     0.0269186     0.133355
  0.156877    -0.0650593    -0.0990375   -0.00912982  -0.097044     0.0642292   -0.0696388   -0.047298      0.00501191   0.150054     0.179418   -0.184312     0.0892357    0.0220945   -0.103589     0.0364527   -0.219189     0.0837012    0.0282055    0.102212    -0.0762164    0.0672304   -0.0814246     0.14364     -0.0978491    -0.126228
 -0.0240653    0.0112245     0.0612263    0.0367322   -0.0834589   -0.116394    -0.104942    -0.0206498    -0.0263259    0.139997     0.0800582   0.205549    -0.0181672    0.0127816    0.0896658    0.0665634   -0.0387449    0.0235936    0.0960274    0.125543     0.0755884   -0.040368     0.153086      0.136228     0.0411745     0.0611823
 -0.0934438    0.142914     -0.02403      0.216689    -0.0331417   -0.0280715   -0.178733     0.0910527    -0.11689      0.0724267    0.194158   -0.224315     0.207676    -0.117346     0.018252    -0.0930261    0.175121    -0.0997189   -0.0473466    0.010073     0.0330827    0.0659219    0.0863131    -0.0767912    0.0545284     0.00913566
 -0.111569    -0.219212      0.0540878    0.0414515    0.0568333   -0.0967232   -0.0929559   -0.0812876     0.0318769    0.0460324    0.226591   -0.14069      0.058524    -0.223297    -0.0905579    0.139095     0.127137     0.0167391    0.0885149   -0.0031328   -0.154196     0.0232085    0.165735     -0.154731    -0.00150062   -0.229842
  0.00526936  -0.0394329    -0.138502    -0.0589862    0.061881    -0.0427067    0.0561027   -0.0851283     0.0415252    0.0548586   -0.328047   -0.108356    -0.0657345   -0.034641     0.00590602   0.111463    -0.0730948    0.121339    -0.0608704    0.0391905   -0.100143     0.195804     0.12957       0.0200673    0.079208      0.025129
  0.0512882    0.104334     -0.0635752    0.0402836   -0.017501     0.0830816    0.146315    -0.144191      0.0851203    0.0514033   -0.0609979  -0.00430217   0.0384515   -0.0327904   -0.0438346   -0.0908736    0.0579425    0.00443356   0.071082     0.0435684   -0.0668119    0.0400342   -0.0538041     0.028699    -0.00469842    0.0579411
  0.0153555    0.123807      0.137252    -0.0151059    0.107563    -0.00177025   0.0522801    0.0345217     0.035688    -0.151947     0.0835493   0.115906     0.106779     0.0817385   -0.010024     0.188502     0.00806584   0.176178    -0.0804941    0.0840212    0.104352     0.13412      0.133661      0.0271838    0.133945      0.112795
  0.0273075    0.0647912     0.0866036    0.0838876   -0.0672277    0.0486217   -0.0261579   -0.239692     -0.0605615    0.0614735   -0.0548547  -0.167136    -0.105357     0.112585     0.104445     0.00558257   0.309245     0.029951    -0.0868766   -0.16705      0.0728853    0.11353      0.0844869    -0.0896308    0.0427691    -0.264247
  0.0364076    0.138184     -0.02235     -0.0133047    0.141435    -0.0481392   -0.0826562   -0.0013197    -0.0741851   -0.219788     0.033454   -0.14184      0.00336032   0.0615354    0.0951499   -0.0505817   -0.0136839    0.147362    -0.143233    -0.00649453  -0.0909187    0.14215      0.0411267    -0.147703     0.104969      0.0516183
  0.0449454    0.0467161     0.0416684   -0.235985     0.0300367    0.146936     0.0943022   -0.118857      0.0792933   -0.042787     0.0652405  -0.124773     0.0732373   -0.0214951    0.091511     0.0230156    0.100527    -0.132402    -0.0282732    0.0227139    0.13799      0.0215747   -0.109591      0.0761299    0.0542623    -0.0809758
 -0.133098    -0.202157      0.0430922   -0.0250213   -0.0656636    0.0666173    0.0932343    0.0162201    -0.134946    -0.0685872    0.119254   -0.100219    -0.0700182   -0.016419    -0.192533     0.0314868   -0.0155397   -0.00945727   0.0402012   -0.055397     0.0272302    0.238068    -0.151454     -0.0933702   -0.0585778     0.172944
  0.144917    -0.146706      0.035648     0.0585682    0.228666     0.154278    -0.121193     0.0374611     0.0289394    0.120014     0.0418629  -0.10152     -0.0444683   -0.0966068    0.127908    -0.049014    -0.0575779   -0.0170835   -0.0613706    0.0168252   -0.16634     -0.128202     0.00142448    0.0693357   -0.170858      0.000332518
  0.0324279    0.0968351     0.0720244    0.048521    -0.116658     0.067082     0.0134451    0.101039     -0.00706123   0.0431221   -0.0287354  -0.096169    -0.067865    -0.0202467    0.0423184    0.00169261   0.0840646    0.0478008    0.109741    -0.108099     0.15813     -0.0259743   -0.0699458     0.0235376    0.183604     -0.126622
 -0.0374319   -0.153102     -0.109659     0.0578494   -0.00152684   0.0779325   -0.049966     0.0794727     0.0396192    0.0502679    0.115071   -0.0917777    0.033891    -0.107337     0.0954015    0.0578645    0.0101681    0.0698793    0.103654     0.00661378   0.00666104   0.134698    -0.0813847    -0.10825      0.00903593   -0.0967252
 -0.111449    -0.0319925    -0.0552378    0.0705029    0.113364     0.00104557   0.0388001   -0.221108      0.11942     -0.0781032   -0.0287302  -0.0677054    0.118823    -0.221421    -0.0793252    0.0353399    0.0792593   -0.0575656    0.15281     -0.153893    -0.0621219    0.0480499   -0.0803653     0.167385     0.054033      0.000132673
  0.134241    -0.186401      0.041221     0.133287     0.148029     0.157174    -0.116036     0.0666719     0.0354192    0.158672     0.0463507  -0.0827153   -0.0569727   -0.10696      0.1376      -0.0524548   -0.0565514   -0.0457833   -0.0974748    0.0378556   -0.180932    -0.164364     0.000393932   0.220752    -0.133984     -0.022371
 -0.0415615    0.0747683     0.0339381    0.112782     0.105399    -0.0445179   -0.128276     0.103058     -0.142373    -0.168161     0.0958399   0.0482267    0.0247688    0.0935057    0.107859     0.0639149    0.170022     0.137631     0.207127     0.0166951    0.0899421    0.0253751   -0.0292728     0.137938    -0.0601291     0.000884926
 -0.0581007   -0.0460032     0.0339205   -0.116774     0.0398666   -0.0253642   -0.189518     0.139646     -0.276303    -0.0722549    0.177136   -0.176383    -0.105563    -0.00858679   0.0407536   -0.0088531    0.00941025  -0.155488    -0.169876     0.0366163    0.160488    -0.175563    -0.0150677    -0.0444849    0.0331024    -0.149724[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│     11
│     23
│     26
│     27
│     28
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 1, average log likelihood -1.081705
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      2
│      6
│     11
│     12
│      ⋮
│     27
│     28
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 2, average log likelihood -1.027444
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      3
│     10
│     11
│     13
│      ⋮
│     27
│     28
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 3, average log likelihood -1.028578
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      2
│      6
│     11
│     12
│      ⋮
│     27
│     28
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.059803
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│     11
│     14
│     19
│     23
│     26
│     27
│     28
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.038924
┌ Warning: Variances had to be floored 
│   ind =
│    14-element Array{Int64,1}:
│      2
│      3
│      6
│     10
│      ⋮
│     27
│     28
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.012287
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│     11
│     19
│     23
│     26
│     27
│     28
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.075477
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      2
│      6
│     11
│     12
│      ⋮
│     27
│     28
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.032041
┌ Warning: Variances had to be floored 
│   ind =
│    12-element Array{Int64,1}:
│      3
│     10
│     11
│     13
│      ⋮
│     27
│     28
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.022501
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      2
│      6
│     11
│     12
│      ⋮
│     27
│     28
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.065420
┌ Info: EM with 100000 data points 10 iterations avll -1.065420
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.0570279    0.0339493    0.0211917    0.0767858   -0.14598      -0.03703     -0.0941888     0.00619629   -0.0869319     0.0800279    0.0256471    0.0385173    0.091474    -0.0676992    0.0980114    0.140509     0.189364     0.0862333    0.190795     -0.0487982  -0.0225101   -0.0943711    0.0107661    0.0747806    0.162651    -0.138395
 -0.0929691   -0.0431504   -0.0343874   -0.0974672    0.0325977    -0.101597    -0.000163957   0.00534527    0.0543684     0.0170315   -0.123476    -0.043102    -0.00521339   0.00719539  -0.078396     0.114156     0.151997    -0.00384486   0.000436028  -0.0752869  -0.0903688    0.0966121   -0.223226     0.0653418   -0.0889118   -0.00204126
 -0.0782965    0.144073    -0.0943847   -0.0664959   -0.000642928  -0.00273976   0.0026315     0.0754729    -0.0251744    -0.00144665  -0.0297361    0.0322884   -0.0745046   -0.0831909    0.0384677   -0.0649915   -0.0738829   -0.0431698    0.079206      0.244874   -0.0752335    0.174989     0.0420471    0.192003     0.0374702    0.198741
 -0.10165      0.0774608   -0.00214143   0.0425805    0.21574      -0.0497956   -0.0321157     0.167545      0.000431436  -0.0714963    0.041648     0.106501    -0.0135143   -0.02786      0.202372    -0.0656636   -0.0220424   -0.136916     0.045181     -0.149255   -0.112289     0.12199     -0.0433847    0.00838074   0.128652    -0.0631198
  0.030167     0.0652289   -0.10307      0.0661859   -0.179143     -0.0196035   -0.1605       -0.00985087   -0.024505      0.0414716    0.01308      0.0608843   -0.0694486    0.0160757   -0.00149158   0.164684     0.156577     0.156616    -0.0272714     0.0423283   0.104598    -0.00591842  -0.0233136   -0.0376814    0.00455808  -0.161469
 -0.0233387   -0.085953     0.0353164   -0.0101052   -0.04173       0.0903213    0.0530887    -0.0337379    -0.183503     -0.23411      0.0582507    0.0794451   -0.0477929   -0.0469871   -0.0501926   -0.112764     0.0148202    0.0961159   -0.237064     -0.110102    0.0525502    0.154626    -0.059202     0.108861    -0.0609751    0.0355579
 -0.0513025   -0.0174122   -0.0284956    0.166151     0.155529      0.0404992    0.0834657     0.0982077     0.0990562     0.0732093   -0.206763    -0.114156     0.0292071   -0.0882956    0.0135365   -0.150052    -0.0174615   -0.0408379   -0.10719       0.0776752   0.066084    -0.0549886   -0.18345      0.101091    -0.00111423   0.00283453
 -0.0186189   -0.0802229    0.087738     0.0618003   -0.0344016    -0.0837209   -0.0556667     0.0208753    -0.0306726    -0.00850444  -0.0183146    0.0665281   -0.0118742   -0.0407782    0.239234    -0.00726818  -0.0195405   -0.133411     0.158059      0.0766162  -0.0699388   -0.0851399    0.197792     0.0186479    0.135898     0.026648
 -0.0150521   -0.0182163    0.0808033    0.033769    -0.0365416    -0.179375    -0.00518964   -0.000384609   0.0695566    -0.250859     0.201598     0.138934    -0.0547161    0.0290729    0.149554    -0.195931    -0.0273156    0.0680768   -0.24039      -0.102168    0.195999    -0.107603    -0.215447    -0.0695124   -0.13344     -0.102053
 -0.111        0.0516016   -0.181414    -0.00346886  -0.123421     -0.055493    -0.041022     -0.0990726    -0.0421802    -0.0402523   -0.0908568    0.0647471    0.00464885   0.0121501   -0.0205847   -0.125801     0.137239    -0.187619    -0.0169758     0.0280255   0.0418991   -0.0257722    0.0753504    0.13811     -0.00936952   0.0278266
 -0.179019    -0.0814692   -0.00988401  -0.0779937    0.0682108     0.169194     0.0890229     0.0476424    -0.0814187     0.077291     0.0606881   -0.0358447   -0.048093     0.127065    -0.0865403   -0.170533     0.0310494   -0.0803348   -0.0946273    -0.065746    0.00706224   0.0764307    0.0903515    0.101953    -0.090264     0.162919
 -0.0954923   -0.13314      0.189513    -0.0772894   -0.0816542    -0.0851567    0.0476246     0.0132114    -0.073114     -0.0581411   -0.0626637   -0.0971792   -0.00428001  -0.0124176    0.066374    -0.126033     0.147496    -0.189776     0.13651       0.0298111  -0.0382141    0.00557931   0.0916402    0.1515      -0.104256    -0.222143
 -0.191271    -0.0623737   -0.0686305    0.0710392   -0.0618979     0.0750475   -0.0410977    -0.246681     -0.00360297   -0.0839112   -2.67116e-5  -0.119889     0.00975788  -0.0102863   -0.0722965   -0.0265047   -0.137242     0.0950509   -0.0833992     0.0781393   0.0513971   -0.0452924    0.20648      0.166476    -0.0355171   -0.0886134
  0.0550953   -0.0640137   -0.0197854   -0.0438386    0.0306067    -0.20769     -0.038825      0.10007       0.0944004    -0.23478      0.217949     0.158157    -0.187593     0.0970009   -0.0869301   -0.10721      0.0396506   -0.0361279    0.0959732    -0.0819921   0.140876     0.119912    -0.055941    -0.0645717   -0.17585     -0.0330688
  0.0243793    0.122591     0.0755368   -0.110903    -0.171716     -0.215432     0.12882      -0.0211892    -0.129216     -0.0338843   -0.0444777    0.0782612    0.0468666    0.0732076    0.0422867    0.0267541   -0.118676     0.0292079   -0.0604602     0.167268   -0.138232    -0.0683844    0.0178552    0.0533657    0.0869761   -0.192731
 -0.266052     0.0817628    0.00412858  -0.081201     0.0112805     0.0303002    0.0720076     0.0867461     0.0711594    -0.0522492   -0.00662116   0.00744828   0.071758    -0.0506992   -0.0798255    0.0892206   -0.138077    -0.0451395   -0.0827711    -0.141717   -0.131455     0.108223     0.17961     -0.0711241    0.0464112    0.0254801
 -0.0301529    0.131324    -0.22609      0.0459103   -0.152906     -0.00272924  -0.0625531     0.0987437     0.0745932    -0.0518388    0.0391092   -0.0249854    0.226238     0.115036     0.121313    -0.017497     0.0516941   -0.106998     0.0460954     0.151869    0.00360568  -0.126265     0.0960726   -0.0740904   -0.212657     0.025508
  0.0296842    0.0420056   -0.0258565    0.266006     0.068453     -0.0261542   -0.0304569    -0.276856     -0.124588      0.0413494   -0.0102078    0.0845036    0.0762324   -0.278731     0.100713     0.0180246   -0.0870029   -0.100583    -0.0349286     0.0684495  -0.075677    -0.00925014  -0.178259     0.0232692   -0.0724717   -0.126669
 -0.172722    -0.0452563   -0.0697586   -0.141313    -0.0873597    -0.0340811    0.020552     -0.0503135    -0.0146602    -0.0832051    0.0378533    0.0933945   -0.00472968  -0.100122    -0.0570932    0.0191712   -0.198618    -0.00712772   0.120914      0.19408    -0.128792    -0.027144    -0.0105478    0.118911     0.0634298   -0.0595163
 -0.206602     0.0239526   -0.107063    -0.103763    -0.0265214    -0.122091     0.0475296    -0.124953      0.0321831    -0.109279    -0.0818414    0.0864859    0.137647     0.216055    -0.0516613   -0.14291     -0.0861939   -0.0488862   -0.147085      0.03282     0.0871809   -0.0377691    0.0536807   -0.0494214   -0.00945503   0.0306129
 -0.216744     0.0792681    0.0234583    0.0205671   -0.000970967   0.0461454   -0.0925068    -0.180915     -0.0134013    -0.0100737   -0.0936459    0.00344029   0.175944     0.0298552   -0.29882      0.0704781   -0.152428     0.0143297   -0.0336642    -0.0426029  -0.0443976    0.0324507   -0.00548983  -0.0505032   -0.141802    -0.231983
 -0.0971335    0.0389499    0.0074128   -0.0380512   -0.226453     -0.0286378   -0.175122      0.0223693    -0.0417332    -0.00911155  -0.114609    -0.0578438    0.0856605   -0.0755114    0.0530299    0.0309621   -0.00703617  -0.0552933   -0.0318207    -0.116232    0.0289517   -0.0141342    0.105811    -0.127697     0.146212    -0.0358785
 -0.00171524   0.062399    -0.0383957    0.0174272    0.0751016     0.0229125    0.135501      0.0291796     0.123004      0.0168194   -0.0172884    0.0702009   -0.187779    -0.00539407   0.0145167    0.0914536   -0.0290813   -0.192902     0.119639     -0.0235966   0.057972    -0.00215629  -0.00900428  -0.0132572    0.0737349    0.0826292
  0.0307766   -0.00717239  -0.105839    -0.0792264    0.0592716     0.0101557    0.171633     -0.0274366    -0.00560459   -0.0754556    0.140295    -0.191661    -0.15097      0.0626333   -0.110479     0.115076     0.109414    -0.0753207    0.229377     -0.0491454  -0.00985144   0.0741905   -0.0677826    0.0145467    0.0316781    0.065333
  0.040819     0.0627345   -0.108697     0.0147911    0.0174602     0.0336235   -0.151808     -0.0194711    -0.0879269     0.0109024    0.0104011   -0.101413    -0.0843595    0.0265529   -0.0229229   -0.0172377   -0.0269212    0.118661     0.107809     -0.014121   -0.126997     0.0314855   -0.0801303    0.0327541   -0.0781625    0.0751455
  0.0839027    0.0685208   -0.264642     0.010393    -0.0334924    -0.0916608   -0.0103848    -0.187124     -0.0203583     0.0609723   -0.0694746    0.138688    -0.0389999    0.00469716   0.0215042   -0.00961107   0.109619    -0.0757819   -0.000173444   0.0621769  -0.0556563   -0.0390107    0.100939    -0.090975    -0.0551385   -0.0329757
  0.0658586    0.0558772   -0.0260213   -0.182158    -0.201514     -0.199617    -0.0074971    -0.0567664    -0.106984     -0.0211127    0.0206727    0.146415    -0.0230921   -0.0300334    0.0202878    0.060337     0.10505     -0.193554     0.148163     -0.0696876  -0.0273278   -0.190763     0.0704832    0.0459909    0.0917309   -0.0231724
 -0.0845308   -0.0471903   -0.135584    -0.0201373   -0.0694982    -0.0673026   -0.036977     -0.049181      0.0227459     0.0745044   -0.0969048    0.11462     -0.126808    -0.147792     0.0214799   -0.0753265   -0.19551     -0.0790914    0.0799864    -0.0989208  -0.12472     -0.0632836    0.00849444   0.128257    -0.144015    -0.0537758
  0.0877001    0.0152303    0.10681     -0.026407    -0.132177      0.12266     -0.00636506    0.0946548     0.0354378     0.0947066   -0.0989843   -0.109879    -0.169782     0.0202373    0.0646143   -0.0860454   -0.015582     0.118658    -0.09633       0.11864     0.240267     0.0328764    0.0418354    0.0332829    0.0447313    0.077165
 -0.119961    -0.0630752    0.0520686    0.0145476    0.0171091    -0.117576    -0.0122527    -0.0263739     0.0539056    -0.0758853   -0.107455     0.106389     0.0103747    0.137553    -0.19362      0.0106079    0.157679    -0.0411495    0.047737      0.0716146  -0.0463651    0.185842    -0.0393271    0.119522     0.0269439    0.13299
  0.00330165   0.0285901   -0.164422     0.0601681    0.0857255     0.10334     -0.106235     -0.040111      0.0369919     0.0915253   -0.171708    -0.241508     0.00139971  -0.0360722    0.0724549    0.142806    -0.0480662   -0.22326     -0.0354296    -0.0269401   0.138945     0.132117    -2.94612e-6   0.0400023   -0.0139368   -0.0955322
 -0.226721    -0.0436232   -0.0740826   -0.183129     0.0139519     0.0310248   -0.197557     -0.0523498    -0.109041     -0.0268979   -0.0638514    0.0185215   -0.0420673   -0.0248622    0.116619     0.0276791   -0.0435309    0.0681898   -0.15999      -0.110852   -0.135336    -0.013525    -0.0708224   -0.131382    -0.0976309    0.0376254kind full, method split
┌ Info: 0: avll = 
└   tll[1] = -1.430048059316119
[ Info: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.430067
[ Info: iteration 2, average log likelihood -1.430003
[ Info: iteration 3, average log likelihood -1.429962
[ Info: iteration 4, average log likelihood -1.429922
[ Info: iteration 5, average log likelihood -1.429876
[ Info: iteration 6, average log likelihood -1.429821
[ Info: iteration 7, average log likelihood -1.429742
[ Info: iteration 8, average log likelihood -1.429601
[ Info: iteration 9, average log likelihood -1.429306
[ Info: iteration 10, average log likelihood -1.428688
[ Info: iteration 11, average log likelihood -1.427618
[ Info: iteration 12, average log likelihood -1.426348
[ Info: iteration 13, average log likelihood -1.425416
[ Info: iteration 14, average log likelihood -1.424971
[ Info: iteration 15, average log likelihood -1.424803
[ Info: iteration 16, average log likelihood -1.424743
[ Info: iteration 17, average log likelihood -1.424721
[ Info: iteration 18, average log likelihood -1.424712
[ Info: iteration 19, average log likelihood -1.424708
[ Info: iteration 20, average log likelihood -1.424707
[ Info: iteration 21, average log likelihood -1.424706
[ Info: iteration 22, average log likelihood -1.424705
[ Info: iteration 23, average log likelihood -1.424704
[ Info: iteration 24, average log likelihood -1.424704
[ Info: iteration 25, average log likelihood -1.424704
[ Info: iteration 26, average log likelihood -1.424703
[ Info: iteration 27, average log likelihood -1.424703
[ Info: iteration 28, average log likelihood -1.424703
[ Info: iteration 29, average log likelihood -1.424703
[ Info: iteration 30, average log likelihood -1.424702
[ Info: iteration 31, average log likelihood -1.424702
[ Info: iteration 32, average log likelihood -1.424702
[ Info: iteration 33, average log likelihood -1.424702
[ Info: iteration 34, average log likelihood -1.424702
[ Info: iteration 35, average log likelihood -1.424702
[ Info: iteration 36, average log likelihood -1.424702
[ Info: iteration 37, average log likelihood -1.424701
[ Info: iteration 38, average log likelihood -1.424701
[ Info: iteration 39, average log likelihood -1.424701
[ Info: iteration 40, average log likelihood -1.424701
[ Info: iteration 41, average log likelihood -1.424701
[ Info: iteration 42, average log likelihood -1.424701
[ Info: iteration 43, average log likelihood -1.424701
[ Info: iteration 44, average log likelihood -1.424701
[ Info: iteration 45, average log likelihood -1.424701
[ Info: iteration 46, average log likelihood -1.424701
[ Info: iteration 47, average log likelihood -1.424701
[ Info: iteration 48, average log likelihood -1.424701
[ Info: iteration 49, average log likelihood -1.424701
[ Info: iteration 50, average log likelihood -1.424701
┌ Info: EM with 100000 data points 50 iterations avll -1.424701
└ 952.4 data points per parameter
┌ Info: 1
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4300673489131654
│     -1.4300028623495458
│      ⋮
└     -1.4247007936666016
[ Info: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.424716
[ Info: iteration 2, average log likelihood -1.424656
[ Info: iteration 3, average log likelihood -1.424614
[ Info: iteration 4, average log likelihood -1.424570
[ Info: iteration 5, average log likelihood -1.424520
[ Info: iteration 6, average log likelihood -1.424461
[ Info: iteration 7, average log likelihood -1.424395
[ Info: iteration 8, average log likelihood -1.424323
[ Info: iteration 9, average log likelihood -1.424248
[ Info: iteration 10, average log likelihood -1.424173
[ Info: iteration 11, average log likelihood -1.424104
[ Info: iteration 12, average log likelihood -1.424044
[ Info: iteration 13, average log likelihood -1.423995
[ Info: iteration 14, average log likelihood -1.423957
[ Info: iteration 15, average log likelihood -1.423928
[ Info: iteration 16, average log likelihood -1.423907
[ Info: iteration 17, average log likelihood -1.423891
[ Info: iteration 18, average log likelihood -1.423878
[ Info: iteration 19, average log likelihood -1.423866
[ Info: iteration 20, average log likelihood -1.423856
[ Info: iteration 21, average log likelihood -1.423846
[ Info: iteration 22, average log likelihood -1.423837
[ Info: iteration 23, average log likelihood -1.423827
[ Info: iteration 24, average log likelihood -1.423817
[ Info: iteration 25, average log likelihood -1.423807
[ Info: iteration 26, average log likelihood -1.423796
[ Info: iteration 27, average log likelihood -1.423785
[ Info: iteration 28, average log likelihood -1.423773
[ Info: iteration 29, average log likelihood -1.423761
[ Info: iteration 30, average log likelihood -1.423748
[ Info: iteration 31, average log likelihood -1.423735
[ Info: iteration 32, average log likelihood -1.423721
[ Info: iteration 33, average log likelihood -1.423707
[ Info: iteration 34, average log likelihood -1.423693
[ Info: iteration 35, average log likelihood -1.423678
[ Info: iteration 36, average log likelihood -1.423664
[ Info: iteration 37, average log likelihood -1.423650
[ Info: iteration 38, average log likelihood -1.423636
[ Info: iteration 39, average log likelihood -1.423622
[ Info: iteration 40, average log likelihood -1.423609
[ Info: iteration 41, average log likelihood -1.423597
[ Info: iteration 42, average log likelihood -1.423585
[ Info: iteration 43, average log likelihood -1.423574
[ Info: iteration 44, average log likelihood -1.423563
[ Info: iteration 45, average log likelihood -1.423553
[ Info: iteration 46, average log likelihood -1.423543
[ Info: iteration 47, average log likelihood -1.423535
[ Info: iteration 48, average log likelihood -1.423526
[ Info: iteration 49, average log likelihood -1.423519
[ Info: iteration 50, average log likelihood -1.423511
┌ Info: EM with 100000 data points 50 iterations avll -1.423511
└ 473.9 data points per parameter
┌ Info: 2
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4247161895503955
│     -1.4246564632622476
│      ⋮
└     -1.4235114693054691
[ Info: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.423516
[ Info: iteration 2, average log likelihood -1.423450
[ Info: iteration 3, average log likelihood -1.423393
[ Info: iteration 4, average log likelihood -1.423328
[ Info: iteration 5, average log likelihood -1.423248
[ Info: iteration 6, average log likelihood -1.423151
[ Info: iteration 7, average log likelihood -1.423040
[ Info: iteration 8, average log likelihood -1.422921
[ Info: iteration 9, average log likelihood -1.422807
[ Info: iteration 10, average log likelihood -1.422706
[ Info: iteration 11, average log likelihood -1.422621
[ Info: iteration 12, average log likelihood -1.422552
[ Info: iteration 13, average log likelihood -1.422496
[ Info: iteration 14, average log likelihood -1.422451
[ Info: iteration 15, average log likelihood -1.422415
[ Info: iteration 16, average log likelihood -1.422386
[ Info: iteration 17, average log likelihood -1.422362
[ Info: iteration 18, average log likelihood -1.422342
[ Info: iteration 19, average log likelihood -1.422325
[ Info: iteration 20, average log likelihood -1.422310
[ Info: iteration 21, average log likelihood -1.422297
[ Info: iteration 22, average log likelihood -1.422284
[ Info: iteration 23, average log likelihood -1.422272
[ Info: iteration 24, average log likelihood -1.422261
[ Info: iteration 25, average log likelihood -1.422250
[ Info: iteration 26, average log likelihood -1.422240
[ Info: iteration 27, average log likelihood -1.422230
[ Info: iteration 28, average log likelihood -1.422219
[ Info: iteration 29, average log likelihood -1.422210
[ Info: iteration 30, average log likelihood -1.422200
[ Info: iteration 31, average log likelihood -1.422190
[ Info: iteration 32, average log likelihood -1.422181
[ Info: iteration 33, average log likelihood -1.422172
[ Info: iteration 34, average log likelihood -1.422163
[ Info: iteration 35, average log likelihood -1.422155
[ Info: iteration 36, average log likelihood -1.422147
[ Info: iteration 37, average log likelihood -1.422139
[ Info: iteration 38, average log likelihood -1.422131
[ Info: iteration 39, average log likelihood -1.422124
[ Info: iteration 40, average log likelihood -1.422117
[ Info: iteration 41, average log likelihood -1.422110
[ Info: iteration 42, average log likelihood -1.422104
[ Info: iteration 43, average log likelihood -1.422097
[ Info: iteration 44, average log likelihood -1.422092
[ Info: iteration 45, average log likelihood -1.422086
[ Info: iteration 46, average log likelihood -1.422080
[ Info: iteration 47, average log likelihood -1.422075
[ Info: iteration 48, average log likelihood -1.422070
[ Info: iteration 49, average log likelihood -1.422065
[ Info: iteration 50, average log likelihood -1.422061
┌ Info: EM with 100000 data points 50 iterations avll -1.422061
└ 236.4 data points per parameter
┌ Info: 3
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4235161022228366
│     -1.4234499912644787
│      ⋮
└     -1.4220607872492088
[ Info: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.422066
[ Info: iteration 2, average log likelihood -1.422004
[ Info: iteration 3, average log likelihood -1.421947
[ Info: iteration 4, average log likelihood -1.421880
[ Info: iteration 5, average log likelihood -1.421797
[ Info: iteration 6, average log likelihood -1.421695
[ Info: iteration 7, average log likelihood -1.421575
[ Info: iteration 8, average log likelihood -1.421444
[ Info: iteration 9, average log likelihood -1.421311
[ Info: iteration 10, average log likelihood -1.421184
[ Info: iteration 11, average log likelihood -1.421068
[ Info: iteration 12, average log likelihood -1.420964
[ Info: iteration 13, average log likelihood -1.420872
[ Info: iteration 14, average log likelihood -1.420789
[ Info: iteration 15, average log likelihood -1.420716
[ Info: iteration 16, average log likelihood -1.420651
[ Info: iteration 17, average log likelihood -1.420592
[ Info: iteration 18, average log likelihood -1.420540
[ Info: iteration 19, average log likelihood -1.420493
[ Info: iteration 20, average log likelihood -1.420452
[ Info: iteration 21, average log likelihood -1.420413
[ Info: iteration 22, average log likelihood -1.420379
[ Info: iteration 23, average log likelihood -1.420347
[ Info: iteration 24, average log likelihood -1.420318
[ Info: iteration 25, average log likelihood -1.420290
[ Info: iteration 26, average log likelihood -1.420265
[ Info: iteration 27, average log likelihood -1.420241
[ Info: iteration 28, average log likelihood -1.420219
[ Info: iteration 29, average log likelihood -1.420198
[ Info: iteration 30, average log likelihood -1.420179
[ Info: iteration 31, average log likelihood -1.420161
[ Info: iteration 32, average log likelihood -1.420144
[ Info: iteration 33, average log likelihood -1.420128
[ Info: iteration 34, average log likelihood -1.420113
[ Info: iteration 35, average log likelihood -1.420099
[ Info: iteration 36, average log likelihood -1.420085
[ Info: iteration 37, average log likelihood -1.420073
[ Info: iteration 38, average log likelihood -1.420061
[ Info: iteration 39, average log likelihood -1.420050
[ Info: iteration 40, average log likelihood -1.420039
[ Info: iteration 41, average log likelihood -1.420029
[ Info: iteration 42, average log likelihood -1.420019
[ Info: iteration 43, average log likelihood -1.420010
[ Info: iteration 44, average log likelihood -1.420001
[ Info: iteration 45, average log likelihood -1.419992
[ Info: iteration 46, average log likelihood -1.419984
[ Info: iteration 47, average log likelihood -1.419976
[ Info: iteration 48, average log likelihood -1.419968
[ Info: iteration 49, average log likelihood -1.419961
[ Info: iteration 50, average log likelihood -1.419954
┌ Info: EM with 100000 data points 50 iterations avll -1.419954
└ 118.1 data points per parameter
┌ Info: 4
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4220658469998373
│     -1.4220041331860813
│      ⋮
└     -1.4199535496926845
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.419955
[ Info: iteration 2, average log likelihood -1.419890
[ Info: iteration 3, average log likelihood -1.419828
[ Info: iteration 4, average log likelihood -1.419756
[ Info: iteration 5, average log likelihood -1.419667
[ Info: iteration 6, average log likelihood -1.419557
[ Info: iteration 7, average log likelihood -1.419426
[ Info: iteration 8, average log likelihood -1.419278
[ Info: iteration 9, average log likelihood -1.419123
[ Info: iteration 10, average log likelihood -1.418967
[ Info: iteration 11, average log likelihood -1.418816
[ Info: iteration 12, average log likelihood -1.418675
[ Info: iteration 13, average log likelihood -1.418544
[ Info: iteration 14, average log likelihood -1.418425
[ Info: iteration 15, average log likelihood -1.418318
[ Info: iteration 16, average log likelihood -1.418222
[ Info: iteration 17, average log likelihood -1.418137
[ Info: iteration 18, average log likelihood -1.418061
[ Info: iteration 19, average log likelihood -1.417993
[ Info: iteration 20, average log likelihood -1.417931
[ Info: iteration 21, average log likelihood -1.417876
[ Info: iteration 22, average log likelihood -1.417825
[ Info: iteration 23, average log likelihood -1.417778
[ Info: iteration 24, average log likelihood -1.417735
[ Info: iteration 25, average log likelihood -1.417695
[ Info: iteration 26, average log likelihood -1.417658
[ Info: iteration 27, average log likelihood -1.417622
[ Info: iteration 28, average log likelihood -1.417589
[ Info: iteration 29, average log likelihood -1.417558
[ Info: iteration 30, average log likelihood -1.417528
[ Info: iteration 31, average log likelihood -1.417500
[ Info: iteration 32, average log likelihood -1.417473
[ Info: iteration 33, average log likelihood -1.417447
[ Info: iteration 34, average log likelihood -1.417422
[ Info: iteration 35, average log likelihood -1.417398
[ Info: iteration 36, average log likelihood -1.417376
[ Info: iteration 37, average log likelihood -1.417354
[ Info: iteration 38, average log likelihood -1.417333
[ Info: iteration 39, average log likelihood -1.417312
[ Info: iteration 40, average log likelihood -1.417293
[ Info: iteration 41, average log likelihood -1.417274
[ Info: iteration 42, average log likelihood -1.417255
[ Info: iteration 43, average log likelihood -1.417238
[ Info: iteration 44, average log likelihood -1.417220
[ Info: iteration 45, average log likelihood -1.417204
[ Info: iteration 46, average log likelihood -1.417188
[ Info: iteration 47, average log likelihood -1.417172
[ Info: iteration 48, average log likelihood -1.417157
[ Info: iteration 49, average log likelihood -1.417142
[ Info: iteration 50, average log likelihood -1.417127
┌ Info: EM with 100000 data points 50 iterations avll -1.417127
└ 59.0 data points per parameter
┌ Info: 5
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4199545276941852
│     -1.4198899115355181
│      ⋮
└     -1.4171273392409691
┌ Info: Total log likelihood: 
│   tll =
│    251-element Array{Float64,1}:
│     -1.430048059316119
│     -1.4300673489131654
│     -1.4300028623495458
│     -1.429962381284707
│      ⋮
│     -1.4171565412416702
│     -1.4171417180872274
└     -1.4171273392409691
32×26 Array{Float64,2}:
  0.393926    0.136307     0.0489048   0.566999    0.0327207   -0.512786     0.0171599  -0.738576    0.186769   -0.287943   -0.643498   -0.0886383   -1.09047     0.669227      0.00089357  -0.0660491    0.470457   -0.430512    -0.0203772   -0.62019     -0.362829    -0.317027    -0.689231    -0.13375     0.350764    -0.210748
  0.0776638   0.575393     0.131245   -0.075878    0.028148     0.241428    -0.189691    0.0667569  -0.11572    -0.140511   -0.0645892  -0.102171    -0.562002    0.247844     -0.556875     0.179495     0.169739    0.158904     0.0302555   -0.302067     0.0239755   -0.223654    -0.101147     0.737126    0.00148569  -0.421206
 -0.0760591   0.213352    -0.074273   -0.0765722   0.182615    -0.205043     0.0498743   0.137752   -0.3973      0.293578    0.388122    0.131695    -0.0935462  -0.37151       0.306146    -0.282028     0.222002   -0.714824    -0.424848    -0.297084    -0.202055     0.128299    -0.675388     0.161658    0.240669     0.561125
  0.0825247   0.33915      0.42983    -0.111761    0.42566     -0.126636     0.641627   -0.0150384  -0.0445767   0.232336    0.0198772   0.0445771    0.0649011   0.115428      0.724202    -0.2139      -0.186585   -0.38674      0.659392    -0.236034     0.0534202   -0.528159     0.122573     0.0359514  -0.145093     0.782099
 -0.570941    0.510286     0.147377    0.106609   -0.00208916   0.349474     0.337733   -0.100432   -0.386249    0.275756    0.150133   -0.551787     0.0930895   0.11813      -0.0684493   -0.222713     0.175858    0.177997    -0.502573    -0.118276    -0.405197    -0.108454    -0.388611    -0.265392   -0.303024    -0.42119
  0.0102021   0.497516     0.761291   -0.262164   -0.15024      0.14479      0.375247   -0.116956   -0.366123    0.0182941  -0.109171    0.166294     0.887148    0.46355      -0.0261752    0.115012    -0.0801257   0.0894917   -0.341906     0.0841898   -0.506827     0.0623625   -0.0732742    0.620575    0.121743    -0.0433149
  0.245106    0.12129     -0.169596   -0.325732    0.179956     0.219463     0.0792781  -0.244473   -0.525751   -0.161419   -0.758742   -0.396678     0.333744   -0.380207      0.31318     -0.122575    -0.371533   -0.00304047   0.457878    -0.00358785   0.00543648  -0.175741    -0.414199     0.0676515  -0.427077    -0.465274
 -0.28354    -0.116179    -0.318462    0.0730058  -0.715654    -0.0816951   -0.253419    0.0969905  -0.224858    0.64185    -0.40842    -0.564569     0.222434    0.576938      0.344687     0.844156    -0.568914    0.0462718    0.274786    -0.455458    -0.040811    -0.417844    -0.0656538   -0.0245013  -0.393891    -0.0166805
  0.126093   -0.245803     0.046445    0.35592    -0.139874    -0.0698556   -1.00291    -0.402921    0.0936085   0.0589286  -0.0696065   0.356472    -0.157502   -0.0958049     0.102558     0.126771    -0.268288   -0.135281    -0.169009     0.188834     0.0788589    0.00499541   0.0691636   -0.367616    0.277395     0.323969
  0.33531    -0.243637    -0.293454    0.531136   -0.242401     0.164455    -0.959213   -0.0951805   0.0412278  -0.229528   -0.100617    0.0209189    0.109574   -0.310892     -0.0396485    0.5566      -0.194781   -0.0910725    0.563494     0.422054     0.159147     0.100128     0.243434     0.0924344  -0.23062     -0.376443
 -0.431209   -0.887703    -0.416503   -0.0639289  -0.175073    -0.0670567    0.210211   -0.148543    0.127018   -0.0545416   0.255263    0.0780399   -0.198979   -0.173105      0.305219    -0.243498     0.0279046  -0.132079     0.12701     -0.203536     0.324278     0.30232     -0.423448    -0.873507   -0.00476116   0.275333
  0.230077   -0.180901    -0.675457    0.203323   -0.0877575   -0.426405     0.2733     -0.337361    0.138111    0.294385   -0.294392   -0.152659     0.330423   -0.405127      0.507804     0.157107     0.234483   -0.480681     0.131904     0.317009     0.142675    -0.379708     0.488207    -0.875701    0.0233449    0.290422
 -0.059667    0.0642045    0.0778564  -0.0765032   0.118139     0.0123505    0.193186   -0.272039    0.115824   -0.115817   -0.229372   -0.0449286    0.0110064   0.0901767    -0.0821173   -0.00537482  -0.0264735  -0.0191136   -0.0614869   -0.0151164    0.0327185   -0.00758382  -0.0473172   -0.0273007  -0.0554604   -0.016016
 -0.0905866  -0.00720843  -0.259459    0.10555    -0.194822    -0.0859474   -0.108457    0.308114   -0.0294832   0.0630326   0.163      -0.0104304    0.0359531  -0.0722049     0.106649     0.0398842   -0.0284003  -0.0323878    0.102635     0.0466394   -0.0404008    0.0168297    0.141399     0.032198   -0.0194524   -0.0802599
  0.202289   -0.708355    -0.306677   -0.761363   -0.198962    -0.0706241   -0.117202    0.380302   -0.367319    0.130468   -0.522654   -0.00865244  -0.275972   -0.0777971     0.267226     0.0621472   -0.158842    0.216149     0.545612    -0.69681      0.0447721    0.4673      -0.401882     0.503027    0.484259     0.302258
 -0.062778   -0.757284    -0.104391   -0.396204   -0.0462586   -0.338597    -0.322111    0.229525   -0.288774   -0.241415   -0.0426575   0.257338    -0.126839   -0.314041      0.0121836   -0.522249    -0.169825    0.46517      0.23996      0.407256     0.158894     0.128934     0.17293     -0.0629541  -0.129019     0.0296415
 -0.875169   -0.313379    -0.331843    0.0373525   0.501378    -0.299861     0.163319   -0.368063    0.340245   -0.64103    -0.342059    0.427695     0.236471   -0.0373248    -1.00662     -0.0351894   -0.428299   -0.232468    -0.472077     0.297508    -0.190696    -0.619532     0.126111     0.314978   -0.327345    -0.393607
 -0.336863    0.699424     0.754494    1.03664     0.0870108   -0.0187765    0.0847606  -0.372444    0.488973   -0.554918    0.49559    -0.092175     0.0902488   0.281732     -0.527738    -0.0533519    0.2566     -0.46905     -0.658226     0.384824     0.325069    -0.210976     0.548235     0.109451   -0.297076     0.013243
 -0.109263    0.0512998    0.23624    -0.364607   -0.19731      0.542451     0.119117   -1.07622     0.305841   -0.469886   -0.504265    0.177987     0.0524053   0.213683     -0.156015    -0.13775     -0.178636    0.703781    -0.0156913    0.103455     0.174709     0.0392125    0.20419     -0.37767    -0.288145    -0.328603
  0.0165368  -0.269425     0.45037     0.184868   -0.0579723    0.0205114    0.214214    0.404315    0.391177   -0.899935   -0.381735   -0.099662    -0.0409701   0.538274     -0.260217     0.245406    -0.215327    0.683655     0.312683     0.199982     0.297847     0.536774     0.538223     0.325536   -0.208566    -0.515976
 -0.369682   -0.274234    -0.675645   -0.350108   -0.39823     -0.29299      0.195083    0.0532533   0.275924   -0.620625    0.539251    0.343217     0.289404    0.415441      0.0206439    0.133915     0.586651   -0.477006    -0.254941    -0.294882    -0.312801     0.0451126    0.137575     0.0837751  -0.285877    -0.481752
 -0.187412   -0.142885    -0.0336421  -0.0847341  -0.0561306   -0.501288     0.124395    0.107044    0.662881   -0.151302    0.416458    0.592391    -0.405051    0.0286743    -0.0895671   -0.0122131    0.450072   -0.339441    -0.0852148    0.0285071    0.0664025    0.0430019    0.451933     0.224176    0.341684     0.580134
 -0.457678    0.612519    -0.76661     0.122146   -0.224647     0.334793    -0.0873132  -0.227116   -0.265148    0.211757    0.348779    0.448746     0.197578   -0.188473     -0.224734    -0.179289     0.977187    0.602382     0.347183    -0.319073    -0.488892     0.323181    -0.165043     0.0783331   0.355701    -0.16391
  0.359116    0.569016    -0.101242    0.230351    0.364293     0.00664686  -0.235913   -0.345431   -0.142834   -0.765875    0.723423    0.400025    -0.219365   -0.887694     -0.298469    -1.01327      0.67499    -0.0589813    0.178793     0.63255     -0.0706165    0.0114931   -0.143489    -0.214055    0.261479    -0.213291
  0.0711038  -0.837465    -0.366149    0.0323319   0.0161364   -0.365281    -0.352198   -0.0155675   0.43192    -0.239003   -0.0327105   0.0949842   -0.337841   -0.178659      0.157309     0.0906432   -0.0377874  -0.0740676    0.23026      0.0206392    0.177463     0.0163003    0.217895    -0.410768    0.0099873   -0.0214757
 -0.0624222   0.663946     0.0669715  -0.0220126  -0.138041     0.30065      0.176193   -0.229326   -0.281209    0.254848   -0.197269   -0.133669     0.250068    0.0404277     0.1058       0.0414629    0.0786496   0.0514521   -0.0515278   -0.0341717   -0.166127    -0.122989     0.00281386   0.0576375  -0.0645172   -0.058774
 -0.22667    -0.344779     0.0738797   0.741664    0.256294    -0.417561    -0.011611    1.0049      0.565597    0.173992    0.414674   -0.151573    -0.264608    0.000450089   0.147057    -0.583289    -0.052645   -0.301293    -0.180382     0.176932    -0.43227      0.700772     0.139244     0.114753    0.797746    -0.0595374
  0.324994   -0.30741      0.418484   -0.357781    0.379006    -0.515808    -0.37908     0.138215    0.434455   -0.138708   -0.297491   -0.550373    -0.188271    0.0669593     0.0815175    0.709314    -0.279601   -0.505296    -0.664985    -0.0438438    0.114803     0.00212076   0.0129455   -0.0790798  -0.548361     0.0784425
 -0.445723    0.201679    -0.0973233   0.0208318   0.0346057   -0.222131    -0.0386299   0.326234   -0.4934      0.500046    0.232449    0.227997    -0.214805   -0.0322911    -0.250702     0.0855285   -0.105034   -0.23194      0.123521     0.0880538    0.227966    -0.898353    -0.228354     0.82293     0.294457    [ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
-0.0694401
 -0.0353439   0.117426     0.494999   -0.451783    0.0798942    0.0304844   -0.0287955   0.54408    -0.373078   -0.269029    0.301211    0.278617    -0.414366    0.487838     -0.229763    -0.501896    -0.126752    0.0680559   -0.222533    -0.236054    -0.00912714   0.738877    -0.859156     0.742308   -0.0553317   -0.135983
  0.404579    0.713718     0.389893    0.259738    0.0409839    0.3725      -0.0707105   0.143292   -0.391384    0.965675   -0.346192   -0.279752    -0.0293878  -0.178092      0.158881    -0.0243588   -0.352982    0.135822    -0.0259046    0.0329777    0.0271923    0.34646     -0.0329349   -0.105355    0.45829      0.375281
 -0.364067    0.250122    -0.297433   -0.532591   -0.0639396    0.745562     0.0111545   0.7299     -0.231586    0.368995    0.533789   -0.0354523    0.530544   -0.521938     -0.104211    -0.0393308   -0.278289    0.167993    -0.00126827   0.483928     0.428479     0.239098     0.773857     0.41375    -0.493683     0.273385[ Info: iteration 1, average log likelihood -1.417113
[ Info: iteration 2, average log likelihood -1.417100
[ Info: iteration 3, average log likelihood -1.417087
[ Info: iteration 4, average log likelihood -1.417074
[ Info: iteration 5, average log likelihood -1.417062
[ Info: iteration 6, average log likelihood -1.417050
[ Info: iteration 7, average log likelihood -1.417038
[ Info: iteration 8, average log likelihood -1.417027
[ Info: iteration 9, average log likelihood -1.417017
[ Info: iteration 10, average log likelihood -1.417006
kind full, method kmeans
┌ Info: EM with 100000 data points 10 iterations avll -1.417006
└ 59.0 data points per parameter
[ Info: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.342957e+05
      1       7.168883e+05      -2.174074e+05 |       32
      2       7.032533e+05      -1.363503e+04 |       32
      3       6.980832e+05      -5.170061e+03 |       32
      4       6.952235e+05      -2.859675e+03 |       32
      5       6.932975e+05      -1.926084e+03 |       32
      6       6.919273e+05      -1.370193e+03 |       32
      7       6.908320e+05      -1.095262e+03 |       32
      8       6.899642e+05      -8.678384e+02 |       32
      9       6.892892e+05      -6.749562e+02 |       32
     10       6.887632e+05      -5.260335e+02 |       32
     11       6.883109e+05      -4.522830e+02 |       32
     12       6.878905e+05      -4.203585e+02 |       32
     13       6.875291e+05      -3.614459e+02 |       32
     14       6.872118e+05      -3.172952e+02 |       32
     15       6.869188e+05      -2.929576e+02 |       32
     16       6.866555e+05      -2.633317e+02 |       32
     17       6.864260e+05      -2.295064e+02 |       32
     18       6.862200e+05      -2.059650e+02 |       32
     19       6.860327e+05      -1.873611e+02 |       32
     20       6.858637e+05      -1.689530e+02 |       32
     21       6.857136e+05      -1.500805e+02 |       32
     22       6.855681e+05      -1.455065e+02 |       32
     23       6.854501e+05      -1.180802e+02 |       32
     24       6.853255e+05      -1.245788e+02 |       32
     25       6.852030e+05      -1.224329e+02 |       32
     26       6.850953e+05      -1.077771e+02 |       32
     27       6.849895e+05      -1.058004e+02 |       32
     28       6.848921e+05      -9.731656e+01 |       32
     29       6.847960e+05      -9.618838e+01 |       32
     30       6.847119e+05      -8.405703e+01 |       32
     31       6.846373e+05      -7.456980e+01 |       32
     32       6.845654e+05      -7.192314e+01 |       32
     33       6.844942e+05      -7.117877e+01 |       32
     34       6.844327e+05      -6.154113e+01 |       32
     35       6.843715e+05      -6.114374e+01 |       32
     36       6.843045e+05      -6.703150e+01 |       32
     37       6.842404e+05      -6.410310e+01 |       32
     38       6.841823e+05      -5.810761e+01 |       32
     39       6.841209e+05      -6.143544e+01 |       32
     40       6.840541e+05      -6.673541e+01 |       32
     41       6.839976e+05      -5.651450e+01 |       32
     42       6.839535e+05      -4.407743e+01 |       32
     43       6.839077e+05      -4.579908e+01 |       32
     44       6.838559e+05      -5.180927e+01 |       32
     45       6.838028e+05      -5.310832e+01 |       32
     46       6.837473e+05      -5.555872e+01 |       32
     47       6.836985e+05      -4.871821e+01 |       32
     48       6.836569e+05      -4.164358e+01 |       32
     49       6.836191e+05      -3.780069e+01 |       32
     50       6.835830e+05      -3.614784e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 683582.9572821646)
┌ Info: K-means with 32000 data points using 50 iterations
└ 37.0 data points per parameter
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.428932
[ Info: iteration 2, average log likelihood -1.424010
[ Info: iteration 3, average log likelihood -1.422687
[ Info: iteration 4, average log likelihood -1.421737
[ Info: iteration 5, average log likelihood -1.420765
[ Info: iteration 6, average log likelihood -1.419841
[ Info: iteration 7, average log likelihood -1.419145
[ Info: iteration 8, average log likelihood -1.418715
[ Info: iteration 9, average log likelihood -1.418461
[ Info: iteration 10, average log likelihood -1.418298
[ Info: iteration 11, average log likelihood -1.418181
[ Info: iteration 12, average log likelihood -1.418090
[ Info: iteration 13, average log likelihood -1.418015
[ Info: iteration 14, average log likelihood -1.417952
[ Info: iteration 15, average log likelihood -1.417897
[ Info: iteration 16, average log likelihood -1.417849
[ Info: iteration 17, average log likelihood -1.417807
[ Info: iteration 18, average log likelihood -1.417769
[ Info: iteration 19, average log likelihood -1.417735
[ Info: iteration 20, average log likelihood -1.417703
[ Info: iteration 21, average log likelihood -1.417675
[ Info: iteration 22, average log likelihood -1.417648
[ Info: iteration 23, average log likelihood -1.417623
[ Info: iteration 24, average log likelihood -1.417599
[ Info: iteration 25, average log likelihood -1.417577
[ Info: iteration 26, average log likelihood -1.417556
[ Info: iteration 27, average log likelihood -1.417535
[ Info: iteration 28, average log likelihood -1.417515
[ Info: iteration 29, average log likelihood -1.417495
[ Info: iteration 30, average log likelihood -1.417476
[ Info: iteration 31, average log likelihood -1.417456
[ Info: iteration 32, average log likelihood -1.417438
[ Info: iteration 33, average log likelihood -1.417419
[ Info: iteration 34, average log likelihood -1.417400
[ Info: iteration 35, average log likelihood -1.417382
[ Info: iteration 36, average log likelihood -1.417364
[ Info: iteration 37, average log likelihood -1.417346
[ Info: iteration 38, average log likelihood -1.417328
[ Info: iteration 39, average log likelihood -1.417310
[ Info: iteration 40, average log likelihood -1.417293
[ Info: iteration 41, average log likelihood -1.417276
[ Info: iteration 42, average log likelihood -1.417259
[ Info: iteration 43, average log likelihood -1.417242
[ Info: iteration 44, average log likelihood -1.417226
[ Info: iteration 45, average log likelihood -1.417211
[ Info: iteration 46, average log likelihood -1.417196
[ Info: iteration 47, average log likelihood -1.417181
[ Info: iteration 48, average log likelihood -1.417167
[ Info: iteration 49, average log likelihood -1.417154
32×26 Array{Float64,2}:
  0.355858      0.935428    0.444462      0.00346281   0.128418     0.535155      0.206959    -0.0941697  -0.478764     0.621793   -0.272295   -0.325373     0.164478    0.0303388   0.0960012   -0.00677169  -0.106435     0.0826577  -0.162894    -0.21858     -0.160457     0.3571      -0.163993   -0.00987439   0.236611    0.209082
  0.252997     -0.156137   -0.384133     -0.6851      -0.231881     0.207545     -0.470574    -0.0187615  -0.760057     0.170422   -0.371746    0.226196     0.164937   -0.630472    0.254595    -0.522735    -0.315965     0.112445    0.249469    -0.0646353   -0.615727     0.21126     -0.857633    0.0774827    0.0671627  -0.122147
 -0.242852      0.268945    0.536416     [ Info: iteration 50, average log likelihood -1.417141
┌ Info: EM with 100000 data points 50 iterations avll -1.417141
└ 59.0 data points per parameter
-0.365573     0.152745     0.0966167     0.178256     0.454267   -0.570044    -0.016037    0.228232   -0.0277366   -0.149959    0.519367   -0.308719    -0.292048    -0.0130182    0.212681   -0.255746    -0.15258      0.0947174    0.260266    -0.793376    0.751308    -0.0989892  -0.264729
 -0.100682      0.111283   -0.251082     -0.466811    -0.124184     0.512983      0.15652     -0.633186   -0.118427    -0.143215   -0.810474   -0.136975    -0.101213   -0.16054    -0.186607     0.0219274   -0.246914     0.58332     0.305683     0.144525     0.518749    -0.559922    -0.132095   -0.0295383   -0.512071   -0.437892
 -0.210487      0.214063    0.204565      0.0630367    0.212791    -0.415204      0.128176     0.306706    0.232132     0.185519    0.731888    0.351973    -0.291733   -0.0418872   0.146894    -0.400831     0.378216    -0.52535    -0.52901     -0.162123    -0.31818      0.278688    -0.163475    0.183457     0.397315    0.79178
 -0.2827       -0.750015   -0.331773     -0.300629    -0.263803    -0.348572      0.32382      0.172224   -0.0869412    0.13774    -0.373449   -0.320374    -0.322515    0.202591    0.674296     0.0318984   -0.0627575   -0.11146     0.0856477   -0.596775    -0.100923     0.387779    -0.518286   -0.120997     0.60378     0.155559
  0.165106     -0.170979   -0.167394      0.124742    -0.568764    -0.0360468    -0.492093    -0.0256407  -0.358106     0.378495   -0.576887   -0.559404     0.361642    0.384554    0.666606     0.933896    -0.446014    -0.138095    0.281877    -0.185818     0.0300122   -0.126706     0.0208047  -0.141498    -0.556783   -0.133389
  0.271934      0.0419264  -0.0491663     0.881786     0.53703     -0.427178     -0.297718     0.919627    0.16973      0.299003    0.0139429  -0.390469     0.0113261  -0.292664    0.378893    -0.022005    -0.155283    -0.751473   -0.0179862    0.361142    -0.610607     0.3401       0.0944994   0.113954     0.253867   -0.257727
  0.406453      0.274573   -0.0780164     0.521894     0.00623284  -0.351835     -0.160844    -0.770783    0.19228     -0.193609   -0.511944   -0.118107    -0.984858    0.557942   -0.224791    -0.00233439   0.409406    -0.659708   -0.183168    -0.67018     -0.228101    -0.477166    -0.540999   -0.105647     0.134561   -0.124887
 -0.648413     -0.035226   -0.721475     -0.875076    -0.295504    -0.00356994    0.577396     0.103393    0.112231    -0.650536    0.429291    0.297942     0.493161    0.322025    0.0492003    0.241902     0.516011    -0.537953   -0.332594    -0.473853    -0.345328    -0.0499551   -0.0318454   0.338829    -0.671349   -0.296863
  0.0289679    -0.228798    0.270145      0.0704404    0.185321    -0.0672436    -0.401387    -0.0122672  -0.143309     0.0824351  -0.23436     0.318387    -0.0697735  -0.320895    0.153159     0.143701    -0.260671    -0.344725    0.182667    -0.0231288    0.277954    -0.0128768   -0.152403    0.0961464    0.187528    0.5291
  0.14044      -0.0940674   0.723066      0.0748266   -0.0945728    0.148918      0.343163    -0.107891    0.314155    -0.979987   -0.517468   -0.0294388    0.10056     0.340743   -0.0189985    0.0283289   -0.303994     0.597063    0.106803     0.161623     0.0667465    0.757852     0.326023    0.161152    -0.151732   -0.332404
  0.366749     -0.0310884   0.541488      0.871395     0.0642391   -0.22441      -0.769819    -0.122103    0.334245     0.351437   -0.491636   -0.179764    -0.30535    -0.0626656  -0.00952798  -0.099227    -0.56865      0.490326    0.101808     0.476832     0.367586    -0.0131991    0.346767   -0.274204     0.516901    0.00503095
 -0.366076      0.596347   -0.174638      0.120299    -0.581699     0.280881      0.331891     0.554078   -0.356096     0.775968   -0.106176    0.677598     0.101152   -0.152503    0.0816377    0.0207553   -0.598141     0.135361    0.3485       0.404235     0.151468     0.0166952    0.0990796   0.522769     0.937195    0.535371
 -0.000468566   0.159178   -0.0827072     0.0244229   -0.0514414   -0.000532676  -0.00728082  -0.0532331  -0.139291     0.0864941   0.0780603  -0.117159     0.0375084  -0.086635    0.132583    -0.0773855    0.00366458  -0.0496142  -0.080097     0.00623383  -0.0674799    0.0211685   -0.109172   -0.0888371   -0.066146   -0.0571971
  0.00971509   -0.424487   -0.000125173  -0.00313374   0.053637    -0.285745     -0.476039     0.11071     0.500853    -0.596883   -0.146301   -0.210558    -0.237548    0.294472   -0.0765075    0.855461     0.226364     0.0167882  -0.191403    -0.151871    -0.00230264   0.00329033   0.423418   -0.0339852   -0.271126   -0.439779
 -0.132885     -0.104363   -0.0942644    -0.0774727   -0.0482507   -0.0317894     0.234884    -0.0231165   0.00173876  -0.0303718   0.0504924   0.127597    -0.138487   -0.0569413   0.0908957   -0.473618    -0.0948392    0.141667    0.217724     0.0333985    0.0927285    0.0947331   -0.126208   -0.0303593    0.0300583   0.0610397
  0.0575537     0.854236    0.0801493     0.00253941  -0.0804187    0.316751     -0.418745     0.339892   -0.151293     0.294545   -0.0632335  -0.344917    -0.122497    0.137184   -0.401968     0.427179    -0.0658727    0.151127    0.249586    -0.267254    -0.504803    -0.540923     0.280308    0.830783     0.0600148  -0.378737
 -0.262202     -0.176688   -0.0376375     0.407526    -0.0799399   -0.488919      0.0981429   -0.431558    0.554417    -0.492857    0.353609    0.241289    -0.136294   -0.0114694  -0.369038    -0.409611     0.68062     -0.163536   -0.5346       0.277548    -0.180503     0.344665     0.280279   -0.237765     0.26186    -0.357465
 -0.252588      0.678541    0.309817      0.589608     0.316986     0.201693     -0.0742651   -0.407951   -0.0805987   -0.936774    0.47653     0.489572     0.166183   -0.0254417  -0.746523    -0.205614     0.212126    -0.344792   -0.233141     0.362733     0.276397    -0.537917     0.0832866   0.466048    -0.324422   -0.341095
  0.182382     -0.267947   -0.596903      0.0981906    0.0518084   -0.365309      0.0805343   -0.47739     0.222926     0.22785    -0.147945   -0.00882804   0.144511   -0.616698    0.445223     0.130352     0.271045    -0.554725    0.190518     0.184454     0.280752    -0.318063     0.268687   -0.787819     0.0867221   0.563367
 -0.275088      0.0637198   0.00417018   -0.156196     0.0524863   -0.0375579     0.323611    -0.0266957   0.277203    -0.140073   -0.283109    0.04647     -0.0187398   0.331378   -0.384207     0.103846     0.131965     0.12556    -0.0180596   -0.0469648   -0.0639753   -0.0857897    0.122818    0.263583     0.0632231  -0.087642
 -0.0538518     0.533738   -0.0456095    -0.156197    -0.259378     0.819023     -0.140712     0.48268    -0.122536     0.337527    0.47716    -0.234403     0.557823   -0.481687   -0.139294     0.123804    -0.151582    -0.0983428  -0.184722     0.619137     0.336855     0.190388     0.896325    0.268667    -0.550147    0.207792
  1.04483      -0.213123    0.12955      -0.900066     0.180283    -0.247983     -0.302937     0.144151    0.0832432   -0.556057    0.0612564   0.277992    -0.653298   -0.0456199   0.0159161   -0.174        0.111185    -0.193692    0.378589    -0.23389      0.382811     0.446598    -0.165376    0.314072    -0.07842     0.117354
 -1.00366       0.52175     0.397137      0.60089     -0.027505     0.316335      0.466932    -0.545886   -0.185731     0.302837   -0.150271   -0.298278     0.387424    0.0355609  -0.191337     0.205676    -0.227939    -0.142131   -0.503293    -0.136978    -0.42382     -0.468021    -0.40154     0.0402009   -0.337635   -0.230636
 -0.548487     -0.429792    0.0549574    -0.517444    -0.126977     0.407701     -0.158602    -1.16145     0.103616    -0.440448   -0.0235214   0.617354     0.494234    0.648671   -0.379319    -0.252739    -0.157906     0.405686   -0.258019    -0.0167433   -0.290539     0.0613616    0.303393   -0.694367     0.11099     0.01949
  0.102485      0.333488    0.0990346    -0.184196     0.219857    -0.300468      0.917924     0.0127873  -0.128949     0.247674    0.085632   -0.144915     0.322716    0.345814    0.812265    -0.337465     0.0527454   -0.246403    0.238312    -0.129533    -0.0314247   -0.651457     0.129235   -0.197486    -0.25978     0.189138
 -0.0484656    -0.328592    0.345007     -0.347282     0.514054    -0.585624     -0.0664338   -0.0108706   0.256249    -0.13622    -0.379599   -0.113969     0.121046    0.101026   -0.268209     0.364857    -0.77983     -0.671113   -0.542392     0.352607     0.330622    -0.561357     0.180276    0.122235    -0.589567   -0.0706796
 -0.289613      0.679181   -0.731736      0.218437    -0.163684     0.29296      -0.115602    -0.289819   -0.293879     0.143086    0.445685    0.346675     0.114984   -0.236431   -0.171675    -0.220213     1.04535      0.459116    0.332244    -0.240248    -0.399006     0.287881    -0.276835   -0.0222164    0.334103   -0.210123
 -0.455267     -0.759967   -0.529698      0.120503    -0.350728    -0.377152     -0.395872     0.634784    0.154461     0.0590872   0.679335    0.40194     -0.194859    0.115522   -0.241316     0.167169     0.164003    -0.42266    -0.00777438  -0.0825452    0.129979    -0.0829268    0.0725758   0.00960178   0.182901    0.0834139
  0.0231043    -0.50113    -0.471468      0.123684    -0.325165    -0.136863     -0.425863    -0.026255    0.148417    -0.474459    0.0880111   0.14881     -0.0566037  -0.139516    0.0254316    0.0712068   -0.0799413    0.0794883   0.35403      0.260536     0.124953     0.0419984    0.280146   -0.101       -0.334084   -0.332271
 -0.390694     -0.533099   -0.256571     -0.399635     0.308953     0.154451      0.0823893    0.461885   -0.206883     0.195591    0.289391   -0.152384     0.0303096  -0.597457    0.189497    -0.545365    -0.181684     0.618365    0.290073     0.199572     0.129355     0.411478     0.443483   -0.124593    -0.12303     0.267666[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.417129
[ Info: iteration 2, average log likelihood -1.417117
[ Info: iteration 3, average log likelihood -1.417106
[ Info: iteration 4, average log likelihood -1.417095
[ Info: iteration 5, average log likelihood -1.417085
[ Info: iteration 6, average log likelihood -1.417075
[ Info: iteration 7, average log likelihood -1.417066
[ Info: iteration 8, average log likelihood -1.417057
[ Info: iteration 9, average log likelihood -1.417048
[ Info: iteration 10, average log likelihood -1.417040
┌ Info: EM with 100000 data points 10 iterations avll -1.417040
└ 59.0 data points per parameter
[ Info: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.678561e+05
┌ Info: K-means with 900 data points using 3 iterations
└ 150.0 data points per parameter
[ Info: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
      1       2.230230e+04      -1.455538e+05 |        2
      2       7.823675e+03      -1.447862e+04 |        0
      3       7.823675e+03       0.000000e+00 |        0
K-means converged with 3 iterations (objv = 7823.67549422947)
[ Info: iteration 1, average log likelihood -2.043155
[ Info: iteration 2, average log likelihood -2.043154
[ Info: iteration 3, average log likelihood -2.043154
[ Info: iteration 4, average log likelihood -2.043154
[ Info: iteration 5, average log likelihood -2.043154
[ Info: iteration 6, average log likelihood -2.043154
[ Info: iteration 7, average log likelihood -2.043154
[ Info: iteration 8, average log likelihood -2.043154
[ Info: iteration 9, average log likelihood -2.043154
[ Info: iteration 10, average log likelihood -2.043154
┌ Info: EM with 900 data points 10 iterations avll -2.043154
└ 81.8 data points per parameter
   Testing GaussianMixtures tests passed 
