Julia Version 1.4.0-DEV.662
Commit ce498d65f8 (2019-12-25 11:44 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-8.0.1 (ORCJIT, skylake)
Environment:
  JULIA_DEPOT_PATH = ::/usr/local/share/julia

 Resolving package versions...
 Installed SortingAlgorithms ── v0.3.1
 Installed GaussianMixtures ─── v0.3.0
 Installed CMakeWrapper ─────── v0.2.3
 Installed Compat ───────────── v2.2.0
 Installed CMake ────────────── v1.1.2
 Installed Arpack ───────────── v0.4.0
 Installed OrderedCollections ─ v1.1.0
 Installed BinaryProvider ───── v0.5.8
 Installed URIParser ────────── v0.4.0
 Installed ScikitLearnBase ──── v0.5.0
 Installed PDMats ───────────── v0.9.10
 Installed NearestNeighbors ─── v0.4.4
 Installed FileIO ───────────── v1.2.0
 Installed Rmath ────────────── v0.6.0
 Installed FillArrays ───────── v0.8.2
 Installed BinDeps ──────────── v1.0.0
 Installed StatsFuns ────────── v0.9.3
 Installed Missings ─────────── v0.4.3
 Installed LegacyStrings ────── v0.4.1
 Installed Distances ────────── v0.8.2
 Installed QuadGK ───────────── v2.3.1
 Installed Parameters ───────── v0.12.0
 Installed StatsBase ────────── v0.32.0
 Installed JLD ──────────────── v0.9.1
 Installed Clustering ───────── v0.13.3
 Installed DataStructures ───── v0.17.6
 Installed Blosc ────────────── v0.5.1
 Installed DataAPI ──────────── v1.1.0
 Installed StaticArrays ─────── v0.12.1
 Installed SpecialFunctions ─── v0.9.0
 Installed HDF5 ─────────────── v0.12.5
 Installed Distributions ────── v0.21.11
 Installed OpenSpecFun_jll ──── v0.5.3+1
 Installed Arpack_jll ───────── v3.5.0+2
 Installed OpenBLAS_jll ─────── v0.3.7+1
  Updating `~/.julia/environments/v1.4/Project.toml`
  [cc18c42c] + GaussianMixtures v0.3.0
  Updating `~/.julia/environments/v1.4/Manifest.toml`
  [7d9fca2a] + Arpack v0.4.0
  [68821587] + Arpack_jll v3.5.0+2
  [9e28174c] + BinDeps v1.0.0
  [b99e7846] + BinaryProvider v0.5.8
  [a74b3585] + Blosc v0.5.1
  [631607c0] + CMake v1.1.2
  [d5fb7624] + CMakeWrapper v0.2.3
  [aaaa29a8] + Clustering v0.13.3
  [34da2185] + Compat v2.2.0
  [9a962f9c] + DataAPI v1.1.0
  [864edb3b] + DataStructures v0.17.6
  [b4f34e82] + Distances v0.8.2
  [31c24e10] + Distributions v0.21.11
  [5789e2e9] + FileIO v1.2.0
  [1a297f60] + FillArrays v0.8.2
  [cc18c42c] + GaussianMixtures v0.3.0
  [f67ccb44] + HDF5 v0.12.5
  [4138dd39] + JLD v0.9.1
  [1b4a561d] + LegacyStrings v0.4.1
  [e1d29d7a] + Missings v0.4.3
  [b8a86587] + NearestNeighbors v0.4.4
  [4536629a] + OpenBLAS_jll v0.3.7+1
  [efe28fd5] + OpenSpecFun_jll v0.5.3+1
  [bac558e1] + OrderedCollections v1.1.0
  [90014a1f] + PDMats v0.9.10
  [d96e819e] + Parameters v0.12.0
  [1fd47b50] + QuadGK v2.3.1
  [79098fc4] + Rmath v0.6.0
  [6e75b9c4] + ScikitLearnBase v0.5.0
  [a2af1166] + SortingAlgorithms v0.3.1
  [276daf66] + SpecialFunctions v0.9.0
  [90137ffa] + StaticArrays v0.12.1
  [2913bbd2] + StatsBase v0.32.0
  [4c63d2b9] + StatsFuns v0.9.3
  [30578b45] + URIParser v0.4.0
  [2a0f44e3] + Base64 
  [ade2ca70] + Dates 
  [8bb1440f] + DelimitedFiles 
  [8ba89e20] + Distributed 
  [b77e0a4c] + InteractiveUtils 
  [76f85450] + LibGit2 
  [8f399da3] + Libdl 
  [37e2e46d] + LinearAlgebra 
  [56ddb016] + Logging 
  [d6f4376e] + Markdown 
  [a63ad114] + Mmap 
  [44cfe95a] + Pkg 
  [de0858da] + Printf 
  [9abbd945] + Profile 
  [3fa0cd96] + REPL 
  [9a3f8284] + Random 
  [ea8e919c] + SHA 
  [9e88b42a] + Serialization 
  [1a1011a3] + SharedArrays 
  [6462fe0b] + Sockets 
  [2f01184e] + SparseArrays 
  [10745b16] + Statistics 
  [4607b0f0] + SuiteSparse 
  [8dfed614] + Test 
  [cf7118a7] + UUIDs 
  [4ec0a83e] + Unicode 
  Building CMake → `~/.julia/packages/CMake/nSK2r/deps/build.log`
  Updating `/tmp/jl_YfT3it/Project.toml`
 [no changes]
  Updating `/tmp/jl_YfT3it/Manifest.toml`
 [no changes]
  Building Rmath → `~/.julia/packages/Rmath/BoBag/deps/build.log`
  Updating `/tmp/jl_Z2g0Uk/Project.toml`
 [no changes]
  Updating `/tmp/jl_Z2g0Uk/Manifest.toml`
 [no changes]
  Building Blosc → `~/.julia/packages/Blosc/lzFr0/deps/build.log`
  Updating `/tmp/jl_Iin3gJ/Project.toml`
 [no changes]
  Updating `/tmp/jl_Iin3gJ/Manifest.toml`
 [no changes]
  Building HDF5 ─→ `~/.julia/packages/HDF5/Zh9on/deps/build.log`
  Updating `/tmp/jl_ZtPiGQ/Project.toml`
 [no changes]
  Updating `/tmp/jl_ZtPiGQ/Manifest.toml`
 [no changes]
   Testing GaussianMixtures
  Updating `/tmp/jl_Ysjg3p/Project.toml`
 [no changes]
  Updating `/tmp/jl_Ysjg3p/Manifest.toml`
 [no changes]
Running sandbox
Status `/tmp/jl_Ysjg3p/Project.toml`
  [aaaa29a8] Clustering v0.13.3
  [34da2185] Compat v2.2.0
  [31c24e10] Distributions v0.21.11
  [5789e2e9] FileIO v1.2.0
  [cc18c42c] GaussianMixtures v0.3.0
  [4138dd39] JLD v0.9.1
  [90014a1f] PDMats v0.9.10
  [6e75b9c4] ScikitLearnBase v0.5.0
  [276daf66] SpecialFunctions v0.9.0
  [2913bbd2] StatsBase v0.32.0
  [8bb1440f] DelimitedFiles 
  [8ba89e20] Distributed 
  [37e2e46d] LinearAlgebra 
  [56ddb016] Logging 
  [de0858da] Printf 
  [9a3f8284] Random 
  [10745b16] Statistics 
[ Info: Testing Data
(100000, -1.325403921052714e6, [538.900081823446, 99461.09991817655], [1005.8636636978936 -828.0703491887253 -809.8284426362499; -845.2160453937405 1303.4105752674368 757.6088991757225], [[2303.436549056761 -1302.7142526543012 -1275.4962176659126; -1302.7142526543012 1735.278696157987 1137.9106021873213; -1275.4962176659128 1137.9106021873213 1686.1394891318198], [98076.17371636984 1782.2283899658148 1824.0279720567637; 1782.2283899658148 98000.88310820538 -1302.625176266565; 1824.0279720567642 -1302.625176266565 98443.87955254572]])
┌ Warning: rmprocs: process 1 not removed
└ @ Distributed /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.4/Distributed/src/cluster.jl:1015
[ Info: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.765816e+03
      1       1.307625e+03      -4.581904e+02 |        8
      2       1.104571e+03      -2.030540e+02 |        4
      3       1.062710e+03      -4.186183e+01 |        2
      4       1.030226e+03      -3.248337e+01 |        2
      5       9.839213e+02      -4.630499e+01 |        2
      6       9.714153e+02      -1.250597e+01 |        4
      7       9.611912e+02      -1.022415e+01 |        2
      8       9.489279e+02      -1.226329e+01 |        2
      9       9.427486e+02      -6.179232e+00 |        0
     10       9.427486e+02       0.000000e+00 |        0
K-means converged with 10 iterations (objv = 942.7486358108254)
┌ Info: K-means with 272 data points using 10 iterations
└ 11.3 data points per parameter
[ Info: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
┌ Info: EM with 272 data points 0 iterations avll -2.075177
└ 5.8 data points per parameter
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:221
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:221
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:221
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:221
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:630 [inlined]
└ @ Core ./broadcast.jl:630
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:230
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:230
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:630 [inlined]
└ @ Core ./broadcast.jl:630
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:630 [inlined]
└ @ Core ./broadcast.jl:630
[ Info: iteration 1, lowerbound -3.730440
[ Info: iteration 2, lowerbound -3.597565
[ Info: iteration 3, lowerbound -3.472374
[ Info: iteration 4, lowerbound -3.339289
[ Info: iteration 5, lowerbound -3.204934
[ Info: iteration 6, lowerbound -3.081454
[ Info: dropping number of Gaussions to 7
[ Info: iteration 7, lowerbound -2.974958
[ Info: dropping number of Gaussions to 5
[ Info: iteration 8, lowerbound -2.882434
[ Info: iteration 9, lowerbound -2.819369
[ Info: dropping number of Gaussions to 4
[ Info: iteration 10, lowerbound -2.786784
[ Info: iteration 11, lowerbound -2.770115
[ Info: dropping number of Gaussions to 3
[ Info: iteration 12, lowerbound -2.754163
[ Info: iteration 13, lowerbound -2.731528
[ Info: iteration 14, lowerbound -2.702144
[ Info: iteration 15, lowerbound -2.659150
[ Info: iteration 16, lowerbound -2.601658
[ Info: iteration 17, lowerbound -2.534269
[ Info: iteration 18, lowerbound -2.467313
[ Info: iteration 19, lowerbound -2.410667
[ Info: iteration 20, lowerbound -2.367356
[ Info: iteration 21, lowerbound -2.335528
[ Info: iteration 22, lowerbound -2.314743
[ Info: iteration 23, lowerbound -2.307397
[ Info: dropping number of Gaussions to 2
[ Info: iteration 24, lowerbound -2.302946
[ Info: iteration 25, lowerbound -2.299261
[ Info: iteration 26, lowerbound -2.299257
[ Info: iteration 27, lowerbound -2.299255
[ Info: iteration 28, lowerbound -2.299254
[ Info: iteration 29, lowerbound -2.299253
[ Info: iteration 30, lowerbound -2.299253
[ Info: iteration 31, lowerbound -2.299253
[ Info: iteration 32, lowerbound -2.299253
[ Info: iteration 33, lowerbound -2.299253
[ Info: iteration 34, lowerbound -2.299253
[ Info: iteration 35, lowerbound -2.299253
[ Info: iteration 36, lowerbound -2.299253
[ Info: iteration 37, lowerbound -2.299253
[ Info: iteration 38, lowerbound -2.299253
[ Info: iteration 39, lowerbound -2.299253
[ Info: iteration 40, lowerbound -2.299253
[ Info: iteration 41, lowerbound -2.299253
[ Info: iteration 42, lowerbound -2.299253
[ Info: iteration 43, lowerbound -2.299253
[ Info: iteration 44, lowerbound -2.299253
[ Info: iteration 45, lowerbound -2.299253
[ Info: iteration 46, lowerbound -2.299253
[ Info: iteration 47, lowerbound -2.299253
[ Info: iteration 48, lowerbound -2.299253
[ Info: iteration 49, lowerbound -2.299253
[ Info: iteration 50, lowerbound -2.299253
[ Info: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
History[Thu Dec 26 06:56:50 2019: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
, Thu Dec 26 06:56:58 2019: K-means with 272 data points using 10 iterations
11.3 data points per parameter
, Thu Dec 26 06:57:01 2019: EM with 272 data points 0 iterations avll -2.075177
5.8 data points per parameter
, Thu Dec 26 06:57:03 2019: GMM converted to Variational GMM
, Thu Dec 26 06:57:13 2019: iteration 1, lowerbound -3.730440
, Thu Dec 26 06:57:13 2019: iteration 2, lowerbound -3.597565
, Thu Dec 26 06:57:13 2019: iteration 3, lowerbound -3.472374
, Thu Dec 26 06:57:13 2019: iteration 4, lowerbound -3.339289
, Thu Dec 26 06:57:13 2019: iteration 5, lowerbound -3.204934
, Thu Dec 26 06:57:13 2019: iteration 6, lowerbound -3.081454
, Thu Dec 26 06:57:13 2019: dropping number of Gaussions to 7
, Thu Dec 26 06:57:13 2019: iteration 7, lowerbound -2.974958
, Thu Dec 26 06:57:13 2019: dropping number of Gaussions to 5
, Thu Dec 26 06:57:13 2019: iteration 8, lowerbound -2.882434
, Thu Dec 26 06:57:13 2019: iteration 9, lowerbound -2.819369
, Thu Dec 26 06:57:13 2019: dropping number of Gaussions to 4
, Thu Dec 26 06:57:13 2019: iteration 10, lowerbound -2.786784
, Thu Dec 26 06:57:13 2019: iteration 11, lowerbound -2.770115
, Thu Dec 26 06:57:13 2019: dropping number of Gaussions to 3
, Thu Dec 26 06:57:13 2019: iteration 12, lowerbound -2.754163
, Thu Dec 26 06:57:13 2019: iteration 13, lowerbound -2.731528
, Thu Dec 26 06:57:13 2019: iteration 14, lowerbound -2.702144
, Thu Dec 26 06:57:13 2019: iteration 15, lowerbound -2.659150
, Thu Dec 26 06:57:13 2019: iteration 16, lowerbound -2.601658
, Thu Dec 26 06:57:13 2019: iteration 17, lowerbound -2.534269
, Thu Dec 26 06:57:13 2019: iteration 18, lowerbound -2.467313
, Thu Dec 26 06:57:13 2019: iteration 19, lowerbound -2.410667
, Thu Dec 26 06:57:13 2019: iteration 20, lowerbound -2.367356
, Thu Dec 26 06:57:13 2019: iteration 21, lowerbound -2.335528
, Thu Dec 26 06:57:13 2019: iteration 22, lowerbound -2.314743
, Thu Dec 26 06:57:13 2019: iteration 23, lowerbound -2.307397
, Thu Dec 26 06:57:13 2019: dropping number of Gaussions to 2
, Thu Dec 26 06:57:13 2019: iteration 24, lowerbound -2.302946
, Thu Dec 26 06:57:14 2019: iteration 25, lowerbound -2.299261
, Thu Dec 26 06:57:14 2019: iteration 26, lowerbound -2.299257
, Thu Dec 26 06:57:14 2019: iteration 27, lowerbound -2.299255
, Thu Dec 26 06:57:14 2019: iteration 28, lowerbound -2.299254
, Thu Dec 26 06:57:14 2019: iteration 29, lowerbound -2.299253
, Thu Dec 26 06:57:14 2019: iteration 30, lowerbound -2.299253
, Thu Dec 26 06:57:14 2019: iteration 31, lowerbound -2.299253
, Thu Dec 26 06:57:14 2019: iteration 32, lowerbound -2.299253
, Thu Dec 26 06:57:14 2019: iteration 33, lowerbound -2.299253
, Thu Dec 26 06:57:14 2019: iteration 34, lowerbound -2.299253
, Thu Dec 26 06:57:14 2019: iteration 35, lowerbound -2.299253
, Thu Dec 26 06:57:14 2019: iteration 36, lowerbound -2.299253
, Thu Dec 26 06:57:14 2019: iteration 37, lowerbound -2.299253
, Thu Dec 26 06:57:14 2019: iteration 38, lowerbound -2.299253
, Thu Dec 26 06:57:14 2019: iteration 39, lowerbound -2.299253
, Thu Dec 26 06:57:14 2019: iteration 40, lowerbound -2.299253
, Thu Dec 26 06:57:14 2019: iteration 41, lowerbound -2.299253
, Thu Dec 26 06:57:14 2019: iteration 42, lowerbound -2.299253
, Thu Dec 26 06:57:14 2019: iteration 43, lowerbound -2.299253
, Thu Dec 26 06:57:14 2019: iteration 44, lowerbound -2.299253
, Thu Dec 26 06:57:14 2019: iteration 45, lowerbound -2.299253
, Thu Dec 26 06:57:14 2019: iteration 46, lowerbound -2.299253
, Thu Dec 26 06:57:14 2019: iteration 47, lowerbound -2.299253
, Thu Dec 26 06:57:14 2019: iteration 48, lowerbound -2.299253
, Thu Dec 26 06:57:14 2019: iteration 49, lowerbound -2.299253
, Thu Dec 26 06:57:14 2019: iteration 50, lowerbound -2.299253
, Thu Dec 26 06:57:14 2019: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [95.95490777396927, 178.04509222603073]
β = [95.95490777396927, 178.04509222603073]
m = [2.0002292577752288 53.851987172460554; 4.250300733269772 79.28686694435982]
ν = [97.95490777396927, 180.04509222603073]
W = LinearAlgebra.UpperTriangular{Float64,Array{Float64,2}}[[0.3758763611950727 -0.00895312382734871; 0.0 0.012748664777410143], [0.18404155547482606 -0.0076440490423288895; 0.0 0.008581705166330849]]
Kind: diag, size256
nx: 100000 sum(zeroth order stats): 100000.00000000001
avll from stats: -1.0020002695133021
avll from llpg:  -1.0020002695133028
avll direct:     -1.0020002695133028
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 100000.0
avll from stats: -1.000730727445288
avll from llpg:  -1.000730727445288
avll direct:     -1.000730727445288
sum posterior: 100000.0
32×26 Array{Float64,2}:
  0.0417079     0.0657074     0.105702     0.120896    -0.235043    -0.0530146  -0.0946317    0.015522    -0.00346406  -0.0436467    0.0260609    0.0749894    0.0190857    -0.192523     0.104901    -0.00598265  -0.0857543   -0.0686166   -0.0530934   -0.0688411   -0.12586       0.0655768   -0.0182382    0.212216     0.117575     0.0626925
  0.149043     -0.069517      0.156863    -0.0400108   -0.0727468    0.192945    0.191517    -0.0153286    0.0534409   -0.0996653   -0.0614374    0.0350363    0.114386      0.227368    -0.0418204    0.0679999    0.0224762   -0.0930364   -0.0269018   -0.0331282   -0.0336965    -0.0835812   -0.0732922    0.198834    -0.160076     0.00247724
 -0.0105387     0.0335716    -0.0241612    0.0345445   -0.0378907    0.167025    0.148883     0.0938814   -0.00924568   0.0803882   -0.0114683    0.0263791    0.104362     -0.133686     0.272979     0.106588    -0.0805268    0.0592329    0.109446     0.0475871   -0.00979234    0.0418975   -0.061912     0.231837    -0.0745106    0.0733498
  0.0592905    -0.0380029     0.00427025   0.104357     0.0569966   -0.055817    0.244046    -0.0015356    0.140723    -0.0595492   -0.102338    -0.045775    -0.0452402     0.0147715    0.0400977    0.0951156    0.0372699    8.24112e-5   0.039678    -0.0602424   -0.0208237    -0.00982707   0.00968915  -0.00245285   0.178792    -0.0493419
 -0.0646357    -0.00916885   -0.00989345   0.124519     0.0558246    0.161277    0.00380362  -0.0352559   -0.13795      0.0354442    0.00468091   0.0394399    0.0124814    -0.0325472   -0.0439394   -0.080703    -0.0643396   -0.0172633    0.0223585    0.213139    -0.0138261    -0.120607    -0.0524824    0.062642    -0.0956068   -0.156239
  0.0744437     0.0424847     0.0897166    0.307854     0.173472    -0.113033    0.112877     0.0512689    0.0941078   -0.0857812   -0.296308     0.0630682    0.0160008     0.066485     0.0468201   -0.0138213    0.0638058    0.029204    -0.0754193   -0.00784077   0.0920708    -0.00601717   0.212482     0.00656031  -0.0320323    0.0127248
 -0.11237       0.0240827    -0.00361549  -0.0360081    0.0551263    0.114344    0.0664691    0.0432102   -0.115993     0.0974696    0.0741226    0.00611215   0.106553     -0.120282    -0.0288187   -0.0360263   -0.178654     0.109856    -0.14492     -0.0190975   -0.0530925     0.177942     0.0561789   -0.0588431    0.106245    -0.09582
 -0.000787741   0.16845      -0.10515     -0.024465     0.0459818    0.0436406   0.0794484   -0.00745248   0.0901086   -0.102415    -0.0118414   -0.0311564    0.173632     -0.13635      0.308037    -0.0176172    0.00798889  -0.0605796   -0.118408    -0.126488     0.119399     -0.230822     0.0233059   -0.0149195   -0.0550333    0.174088
 -0.134954      0.046563      0.207639     0.150482    -0.0130438   -0.0940591  -0.0272328    0.173974     0.00849827   0.155167    -0.0199827   -0.137028     0.000914635   0.0948223    0.0646412    0.136483    -0.078997    -0.213402     0.063628     0.0289898    0.0143443     0.0719189    0.0400944    0.0793041   -0.0131024    0.0378215
 -0.00809215   -0.0861285     0.082537     0.0781906    0.00961269   0.110345   -0.0320793    0.070749     0.157228     0.0437781   -0.00930374   0.00493633   0.0108145     0.11278      0.0405718    0.111632    -0.0600504    0.00260286  -0.161344     0.171042     0.0484256    -0.0923686   -0.177625     0.145672     0.0268508    0.0749809
  0.31644      -0.0973881     0.0349075    0.0554528   -0.0389054   -0.0554312   0.0165808   -0.00158658   0.0168454   -0.107864    -0.0820644    0.0238282   -0.123533     -0.155955     0.0195385   -0.126132     0.0447218   -0.244093     0.0652504   -0.0130848   -0.0788449     0.114482     0.133892     0.0173175   -0.0703229    0.0696364
  0.0142757     0.0162673    -0.171968    -0.131413    -0.0906946   -0.180536   -0.075       -0.0467465   -0.026733    -0.0585125    0.0450285   -0.130071     0.142529      0.300124     0.00668251   0.0226437   -0.0217239    0.169509    -0.123311     0.0532643    0.134001      0.151772    -0.0950771    0.0997041    0.0381945    0.103585
 -0.0432609     0.0564295    -0.0840394   -0.0632903   -0.0456918   -0.190946    0.0615181   -0.00678029   0.0555359    0.0753012    0.0210433    0.134442    -0.0284786     0.137319     0.0879288   -0.0119679    0.019966     0.227832     0.0896526    0.0384154    0.0157792     0.234512     0.20808     -0.0932225    0.0240956    0.206545
  0.183654      0.0453586    -0.148216    -0.0927222    0.212677    -0.141421    0.0835697   -0.113652     0.106933     0.00240547  -0.103547     0.0826225   -0.0507764     0.269344     0.126999     0.039839     0.0166475    0.0813291    0.163535     0.0759895    0.00427838   -0.115326     0.138412     0.0948601   -0.218655     0.0428634
  0.104859     -0.109361     -0.0368288   -0.050197    -0.0304072   -0.0798367  -0.0860789    0.0606948    0.0933969   -0.0628848   -0.089129     0.07901     -0.0888923    -0.145683    -0.0199771   -0.0264143   -0.0703655    0.148952    -0.0893177    0.179834     0.105749     -0.0179207   -0.0569843    0.0602322   -0.0656186    0.102824
  0.0197502     0.000381333  -0.0354872    0.0322225   -0.16745     -0.0508416   0.123073     0.0501804    0.0192964   -0.103226     0.056496    -0.0675544   -0.087931     -0.0168745   -0.0206124    0.112823    -0.0640136    0.035081    -0.0146214   -0.0431723   -0.051728     -0.0444422   -0.048873    -0.197408    -0.0414761    0.144713
  0.186442     -0.0191665     0.104929    -0.0361211    0.0202449    0.0358498  -0.147991    -0.0496425    0.154588    -0.00797304   0.110948     0.0356154    0.154959      0.085598     0.0584157    0.0774797    0.0204828    0.168877    -0.0975886    0.127039     0.022055     -0.108003    -0.176575    -0.0192934   -0.163789     0.171598
  0.105898     -0.0124086    -0.0952771   -0.0125211   -0.136706    -0.139363   -0.00431729  -0.0822013    0.0255356   -0.0352269   -0.104159    -0.0102917    0.125478      0.0209555    0.0212646   -0.0104085   -0.0714604   -0.0821302    0.0363319    0.0125678   -0.158575      0.0342741   -0.129773     0.228125    -0.0271614    0.0229324
  0.0215089     0.1231        0.0220912   -0.0119267    0.0666532   -0.068772   -0.035754    -5.93072e-5  -0.0501411   -0.355384     0.00129489  -0.0219989    0.0416154    -0.025659    -0.0651797   -0.195353    -0.111485    -0.0785154   -0.00115958  -0.0368946    0.00736726    0.136478    -0.035481     0.00643277  -0.0727588    0.124546
  0.00884741    0.0128876     0.0440811    0.0584619   -0.0451961   -0.0543518  -0.0768713   -0.0548775   -0.0714356    0.0100356    0.213015    -0.158988    -0.079385     -0.0467755    0.0302627   -0.00712403   0.11454      0.0522623   -0.0250861    0.0687685   -0.133105      0.0554805   -0.187663    -0.107493     0.0921194   -0.109113
  0.358771     -0.0842831     0.121352     0.0537428   -0.113335    -0.0492688  -0.0559638    0.275406     0.0128786    0.121647     0.107732    -0.117714    -0.0286769     0.0084713    0.12654     -0.0164579    0.055383     0.052739    -0.0236874   -0.00473229  -0.0722739     0.0133118    0.0884592    0.096277    -0.00870845  -0.0414348
 -0.00410836    0.187105      0.0333185    0.0375814    0.0236265    0.0860069  -0.0328325    0.0489832    0.0726995    0.166356     0.0440011    0.0916627    0.18565       0.226695     0.152956    -0.026094    -0.114228     0.0544402    0.0389255    0.0955075   -0.135055      0.00343691   0.091847    -0.0177869    0.0627297    0.03013
 -0.0955306     0.0790693    -0.151053    -0.129824     0.0706321   -0.0998679  -0.0589948    0.109413     0.0327554    0.0478345    0.00632526  -0.0880787    0.165677     -0.245888    -0.0974678    0.142739     0.035353     0.145193     0.00583594   0.0414021    0.134552      0.138769     0.0968133   -0.0317025   -0.0796054   -0.067203
  0.0667951    -0.0999527    -0.0355996    0.0567571   -0.0319945   -0.0648312   0.0168224   -0.00971029   0.0101863    0.0618373    0.0712551   -0.145365     0.0396353     0.10723      0.172282     0.157443    -0.0441971    0.0967365   -0.098829    -0.185787     0.0166629     0.00700756  -0.0542666    0.0219539    0.0275731   -0.0479736
  0.263929      0.0605605     0.117195    -0.0145014   -0.0221313   -0.0627343   0.110482    -0.116534     0.22268      0.165148    -0.0104399   -0.0606234    0.0147988    -0.00206256  -0.153518    -0.0568625    0.0384274    0.0501572    0.00368437   0.0729855    0.0982082     0.151239    -0.116477    -0.105514    -0.0894677   -0.213469
 -0.221598     -0.111633     -0.0338895    0.00475564   0.0451316   -0.0942402   0.0543523    0.0331588    0.0167344    0.0995638   -0.108274    -0.114256    -0.113235      0.179199    -0.0580601   -0.026522     0.144539    -0.0511393   -0.067875    -0.00895034  -0.110354      0.180174     0.0248443   -0.165757     0.0409459   -0.0287603
 -0.0197231     0.09122       0.0961472   -0.108221    -0.0579485    0.107323    0.156452    -0.173423    -0.201329     0.105357    -0.0255153   -0.145511     0.0858693     0.0794978   -0.101789     0.0658381   -0.0533968   -0.0812206    0.0829482    0.0158499   -0.0466204     0.0261932    0.103387     0.0283498   -0.114164    -0.0589295
 -0.197055     -0.146877      0.108293     0.0807659   -0.101741     0.019033    0.0987629    0.131506    -0.0513248    0.0930423   -0.0593679    0.0381658    0.149319     -0.0184132   -0.0964336    0.180885    -0.0986329    0.151601    -0.00722858   0.0638811    0.0895316     0.0207771    0.0849262   -0.0950628    0.00372761   0.00477475
  0.0246034    -0.0842578     0.00840986   0.0445346    0.237435    -0.0609236  -0.0877253    0.0941441    0.104925     0.13799      0.131968    -0.171877    -0.0345734    -0.00754154  -0.146058    -0.0608267    0.130732    -0.0205921    0.0419406    0.0683596   -0.0556888    -0.0391152    0.073892     0.0419946   -0.255412     0.0347176
  0.0812357     0.0120223     0.00981677  -0.111786    -0.0442953   -0.153925    0.174492    -0.0612191    0.107974    -0.171831    -0.0157939   -0.0264196    0.06543       0.0377383   -0.0156871    0.1046      -0.0508599   -0.0941984   -0.113242     0.0527746   -0.0633235     0.00717077   0.12285     -0.117483     0.112483    -0.0925533
  0.0145552     0.104734     -0.0402773    0.0839632    0.112192     0.178638   -0.079752    -0.138905    -0.290006     0.0539496   -0.0146039    0.113749     0.130547     -0.0435483    0.0575336    0.0804045    0.0617012   -0.0104766   -0.00744901   0.0941483    0.000182903   0.0893658   -0.00279682   0.00562573  -0.0169079    0.0393552
  0.0885704     0.0764703     0.134216    -0.144208     0.201454    -0.043341   -0.136853     0.162825    -0.166207    -0.168063     0.0944835    0.085119    -0.0717233     0.0868331   -0.105284     0.0476801    0.0181764    0.107946     0.0462078   -0.0490534    0.00680686    0.0541843   -0.0750402    0.0857767   -0.0470759   -0.218006kind diag, method split
┌ Info: 0: avll = 
└   tll[1] = -1.4031684604684445
[ Info: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.403230
[ Info: iteration 2, average log likelihood -1.403150
[ Info: iteration 3, average log likelihood -1.402271
[ Info: iteration 4, average log likelihood -1.392598
[ Info: iteration 5, average log likelihood -1.373016
[ Info: iteration 6, average log likelihood -1.367634
[ Info: iteration 7, average log likelihood -1.366681
[ Info: iteration 8, average log likelihood -1.366282
[ Info: iteration 9, average log likelihood -1.366071
[ Info: iteration 10, average log likelihood -1.365936
[ Info: iteration 11, average log likelihood -1.365831
[ Info: iteration 12, average log likelihood -1.365737
[ Info: iteration 13, average log likelihood -1.365646
[ Info: iteration 14, average log likelihood -1.365551
[ Info: iteration 15, average log likelihood -1.365445
[ Info: iteration 16, average log likelihood -1.365319
[ Info: iteration 17, average log likelihood -1.365176
[ Info: iteration 18, average log likelihood -1.365065
[ Info: iteration 19, average log likelihood -1.365002
[ Info: iteration 20, average log likelihood -1.364965
[ Info: iteration 21, average log likelihood -1.364940
[ Info: iteration 22, average log likelihood -1.364920
[ Info: iteration 23, average log likelihood -1.364904
[ Info: iteration 24, average log likelihood -1.364890
[ Info: iteration 25, average log likelihood -1.364878
[ Info: iteration 26, average log likelihood -1.364865
[ Info: iteration 27, average log likelihood -1.364853
[ Info: iteration 28, average log likelihood -1.364840
[ Info: iteration 29, average log likelihood -1.364825
[ Info: iteration 30, average log likelihood -1.364809
[ Info: iteration 31, average log likelihood -1.364789
[ Info: iteration 32, average log likelihood -1.364763
[ Info: iteration 33, average log likelihood -1.364730
[ Info: iteration 34, average log likelihood -1.364681
[ Info: iteration 35, average log likelihood -1.364603
[ Info: iteration 36, average log likelihood -1.364475
[ Info: iteration 37, average log likelihood -1.364314
[ Info: iteration 38, average log likelihood -1.364159
[ Info: iteration 39, average log likelihood -1.364023
[ Info: iteration 40, average log likelihood -1.363906
[ Info: iteration 41, average log likelihood -1.363805
[ Info: iteration 42, average log likelihood -1.363714
[ Info: iteration 43, average log likelihood -1.363630
[ Info: iteration 44, average log likelihood -1.363548
[ Info: iteration 45, average log likelihood -1.363468
[ Info: iteration 46, average log likelihood -1.363384
[ Info: iteration 47, average log likelihood -1.363293
[ Info: iteration 48, average log likelihood -1.363186
[ Info: iteration 49, average log likelihood -1.363071
[ Info: iteration 50, average log likelihood -1.362973
┌ Info: EM with 100000 data points 50 iterations avll -1.362973
└ 952.4 data points per parameter
┌ Info: 1
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4032301216556453
│     -1.4031496360811417
│      ⋮
└     -1.3629730810541072
[ Info: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.363025
[ Info: iteration 2, average log likelihood -1.362881
[ Info: iteration 3, average log likelihood -1.362544
[ Info: iteration 4, average log likelihood -1.359082
[ Info: iteration 5, average log likelihood -1.344279
[ Info: iteration 6, average log likelihood -1.330783
[ Info: iteration 7, average log likelihood -1.326339
[ Info: iteration 8, average log likelihood -1.324055
[ Info: iteration 9, average log likelihood -1.322381
[ Info: iteration 10, average log likelihood -1.321132
[ Info: iteration 11, average log likelihood -1.320030
[ Info: iteration 12, average log likelihood -1.318852
[ Info: iteration 13, average log likelihood -1.317550
[ Info: iteration 14, average log likelihood -1.316189
[ Info: iteration 15, average log likelihood -1.314841
[ Info: iteration 16, average log likelihood -1.313536
[ Info: iteration 17, average log likelihood -1.312205
[ Info: iteration 18, average log likelihood -1.311213
[ Info: iteration 19, average log likelihood -1.310700
[ Info: iteration 20, average log likelihood -1.310460
[ Info: iteration 21, average log likelihood -1.310343
[ Info: iteration 22, average log likelihood -1.310284
[ Info: iteration 23, average log likelihood -1.310251
[ Info: iteration 24, average log likelihood -1.310228
[ Info: iteration 25, average log likelihood -1.310212
[ Info: iteration 26, average log likelihood -1.310197
[ Info: iteration 27, average log likelihood -1.310183
[ Info: iteration 28, average log likelihood -1.310168
[ Info: iteration 29, average log likelihood -1.310153
[ Info: iteration 30, average log likelihood -1.310135
[ Info: iteration 31, average log likelihood -1.310114
[ Info: iteration 32, average log likelihood -1.310088
[ Info: iteration 33, average log likelihood -1.310052
[ Info: iteration 34, average log likelihood -1.310000
[ Info: iteration 35, average log likelihood -1.309925
[ Info: iteration 36, average log likelihood -1.309863
[ Info: iteration 37, average log likelihood -1.309829
[ Info: iteration 38, average log likelihood -1.309802
[ Info: iteration 39, average log likelihood -1.309774
[ Info: iteration 40, average log likelihood -1.309747
[ Info: iteration 41, average log likelihood -1.309721
[ Info: iteration 42, average log likelihood -1.309699
[ Info: iteration 43, average log likelihood -1.309681
[ Info: iteration 44, average log likelihood -1.309668
[ Info: iteration 45, average log likelihood -1.309658
[ Info: iteration 46, average log likelihood -1.309650
[ Info: iteration 47, average log likelihood -1.309645
[ Info: iteration 48, average log likelihood -1.309641
[ Info: iteration 49, average log likelihood -1.309639
[ Info: iteration 50, average log likelihood -1.309636
┌ Info: EM with 100000 data points 50 iterations avll -1.309636
└ 473.9 data points per parameter
┌ Info: 2
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.3630246561938308
│     -1.3628814341372393
│      ⋮
└     -1.309636485076661
[ Info: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.309822
[ Info: iteration 2, average log likelihood -1.309639
[ Info: iteration 3, average log likelihood -1.308741
[ Info: iteration 4, average log likelihood -1.300484
[ Info: iteration 5, average log likelihood -1.277147
[ Info: iteration 6, average log likelihood -1.260825
[ Info: iteration 7, average log likelihood -1.255119
[ Info: iteration 8, average log likelihood -1.252422
[ Info: iteration 9, average log likelihood -1.250193
[ Info: iteration 10, average log likelihood -1.247763
[ Info: iteration 11, average log likelihood -1.244948
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 12, average log likelihood -1.242088
[ Info: iteration 13, average log likelihood -1.254112
[ Info: iteration 14, average log likelihood -1.247142
[ Info: iteration 15, average log likelihood -1.244246
[ Info: iteration 16, average log likelihood -1.241921
[ Info: iteration 17, average log likelihood -1.239546
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     4
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 18, average log likelihood -1.237670
[ Info: iteration 19, average log likelihood -1.251319
[ Info: iteration 20, average log likelihood -1.243663
[ Info: iteration 21, average log likelihood -1.240840
[ Info: iteration 22, average log likelihood -1.238426
[ Info: iteration 23, average log likelihood -1.235760
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     3
│     4
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 24, average log likelihood -1.233353
[ Info: iteration 25, average log likelihood -1.258192
[ Info: iteration 26, average log likelihood -1.252024
[ Info: iteration 27, average log likelihood -1.242605
[ Info: iteration 28, average log likelihood -1.238373
[ Info: iteration 29, average log likelihood -1.235786
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     4
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 30, average log likelihood -1.234537
[ Info: iteration 31, average log likelihood -1.248750
[ Info: iteration 32, average log likelihood -1.241558
[ Info: iteration 33, average log likelihood -1.239113
[ Info: iteration 34, average log likelihood -1.237161
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 35, average log likelihood -1.234985
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     4
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 36, average log likelihood -1.244087
[ Info: iteration 37, average log likelihood -1.252716
[ Info: iteration 38, average log likelihood -1.242803
[ Info: iteration 39, average log likelihood -1.239435
[ Info: iteration 40, average log likelihood -1.237127
[ Info: iteration 41, average log likelihood -1.235197
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     4
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 42, average log likelihood -1.234426
[ Info: iteration 43, average log likelihood -1.248725
[ Info: iteration 44, average log likelihood -1.241568
[ Info: iteration 45, average log likelihood -1.239142
[ Info: iteration 46, average log likelihood -1.237174
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 47, average log likelihood -1.234960
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     4
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 48, average log likelihood -1.244034
[ Info: iteration 49, average log likelihood -1.252701
[ Info: iteration 50, average log likelihood -1.242787
┌ Info: EM with 100000 data points 50 iterations avll -1.242787
└ 236.4 data points per parameter
┌ Info: 3
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.309821907648242
│     -1.3096386191646636
│      ⋮
└     -1.2427871454185793
[ Info: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.239662
[ Info: iteration 2, average log likelihood -1.236999
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 3, average log likelihood -1.233107
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.218356
[ Info: iteration 5, average log likelihood -1.198831
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.171368
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     2
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.159178
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     6
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.153591
[ Info: iteration 9, average log likelihood -1.150685
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     2
│     5
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.132957
[ Info: iteration 11, average log likelihood -1.151373
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     6
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 12, average log likelihood -1.131086
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     2
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 13, average log likelihood -1.137470
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 14, average log likelihood -1.141799
[ Info: iteration 15, average log likelihood -1.137582
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     2
│     6
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 16, average log likelihood -1.123137
[ Info: iteration 17, average log likelihood -1.150224
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 18, average log likelihood -1.129700
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     2
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 19, average log likelihood -1.131031
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     6
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 20, average log likelihood -1.136265
[ Info: iteration 21, average log likelihood -1.139893
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     2
│     5
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 22, average log likelihood -1.124148
[ Info: iteration 23, average log likelihood -1.145512
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     6
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 24, average log likelihood -1.126744
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     2
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 25, average log likelihood -1.134841
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 26, average log likelihood -1.138637
[ Info: iteration 27, average log likelihood -1.135930
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     2
│     6
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 28, average log likelihood -1.122163
[ Info: iteration 29, average log likelihood -1.149616
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 30, average log likelihood -1.129227
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     2
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 31, average log likelihood -1.131029
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     6
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 32, average log likelihood -1.137009
[ Info: iteration 33, average log likelihood -1.140095
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     2
│     5
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 34, average log likelihood -1.124404
[ Info: iteration 35, average log likelihood -1.145636
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     6
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 36, average log likelihood -1.127944
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     2
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 37, average log likelihood -1.134449
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 38, average log likelihood -1.138348
[ Info: iteration 39, average log likelihood -1.135505
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     2
│     6
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 40, average log likelihood -1.123020
[ Info: iteration 41, average log likelihood -1.148734
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 42, average log likelihood -1.128681
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     2
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 43, average log likelihood -1.130469
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     6
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 44, average log likelihood -1.137160
[ Info: iteration 45, average log likelihood -1.139189
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     2
│     5
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 46, average log likelihood -1.123827
[ Info: iteration 47, average log likelihood -1.145024
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     6
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 48, average log likelihood -1.127668
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     2
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 49, average log likelihood -1.134117
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 50, average log likelihood -1.138141
┌ Info: EM with 100000 data points 50 iterations avll -1.138141
└ 118.1 data points per parameter
┌ Info: 4
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.2396617100077871
│     -1.2369986034817548
│      ⋮
└     -1.138140930631458
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.135585
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      3
│      4
│     11
│     12
│     15
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 2, average log likelihood -1.122646
[ Info: iteration 3, average log likelihood -1.126003
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      3
│      4
│      9
│     11
│     12
│     15
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.097177
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     10
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.083626
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      1
│      3
│      4
│     11
│      ⋮
│     16
│     21
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.043347
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      5
│     16
│     20
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.053750
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      3
│      4
│     10
│     11
│      ⋮
│     22
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.017229
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      1
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.048342
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      3
│      4
│      9
│     11
│      ⋮
│     20
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.013144
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      5
│     10
│     16
│     21
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 11, average log likelihood -1.039235
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      1
│      3
│      4
│     11
│      ⋮
│     22
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 12, average log likelihood -1.018235
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      9
│     10
│     16
│     20
│     21
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 13, average log likelihood -1.035801
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      3
│      4
│     11
│     12
│     15
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 14, average log likelihood -1.031338
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      1
│      5
│     16
│     21
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 15, average log likelihood -1.028728
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      3
│      4
│     10
│     11
│      ⋮
│     20
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 16, average log likelihood -1.022146
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      9
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 17, average log likelihood -1.046474
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      1
│      3
│      4
│     10
│      ⋮
│     21
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 18, average log likelihood -1.007526
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      5
│     16
│     20
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 19, average log likelihood -1.048358
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      3
│      4
│      9
│     10
│      ⋮
│     21
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 20, average log likelihood -1.011571
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      1
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 21, average log likelihood -1.049504
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      3
│      4
│     11
│     12
│      ⋮
│     22
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 22, average log likelihood -1.015026
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     10
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 23, average log likelihood -1.043183
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      1
│      3
│      4
│      5
│      ⋮
│     15
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 24, average log likelihood -1.008272
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     10
│     16
│     20
│     21
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 25, average log likelihood -1.048325
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      3
│      4
│     11
│     12
│     15
│     22
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 26, average log likelihood -1.026701
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      1
│      9
│     10
│     16
│     21
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 27, average log likelihood -1.026347
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      3
│      4
│      5
│     11
│      ⋮
│     20
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 28, average log likelihood -1.031055
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     16
│     21
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 29, average log likelihood -1.043420
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      1
│      3
│      4
│     10
│      ⋮
│     15
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 30, average log likelihood -1.008081
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      5
│      9
│     16
│     20
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 31, average log likelihood -1.046277
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      3
│      4
│     10
│     11
│      ⋮
│     21
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 32, average log likelihood -1.020078
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      1
│     16
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 33, average log likelihood -1.042701
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      3
│      4
│     10
│     11
│      ⋮
│     21
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 34, average log likelihood -1.009042
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      5
│      9
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 35, average log likelihood -1.039560
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      1
│      3
│      4
│     10
│      ⋮
│     22
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 36, average log likelihood -1.006363
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     16
│     20
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 37, average log likelihood -1.060271
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      3
│      4
│      9
│     10
│      ⋮
│     15
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 38, average log likelihood -1.014825
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      1
│      5
│     16
│     21
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 39, average log likelihood -1.040291
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      3
│      4
│     11
│     12
│      ⋮
│     22
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 40, average log likelihood -1.028128
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     10
│     16
│     21
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 41, average log likelihood -1.035910
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      1
│      3
│      4
│      5
│      ⋮
│     15
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 42, average log likelihood -1.005497
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│     10
│     16
│     20
│     21
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 43, average log likelihood -1.044286
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      3
│      4
│     11
│     12
│     15
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 44, average log likelihood -1.033193
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      1
│      7
│     10
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 45, average log likelihood -1.033948
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      3
│      4
│      5
│      9
│      ⋮
│     21
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 46, average log likelihood -1.007700
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     10
│     16
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 47, average log likelihood -1.053582
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      1
│      3
│      4
│     11
│      ⋮
│     21
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 48, average log likelihood -1.016191
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     10
│     16
│     20
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 49, average log likelihood -1.039661
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      3
│      4
│      5
│      9
│      ⋮
│     22
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 50, average log likelihood -1.001537
┌ Info: EM with 100000 data points 50 iterations avll -1.001537
└ 59.0 data points per parameter
┌ Info: 5
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.1355853779174048
│     -1.122646347864957
│      ⋮
└     -1.0015365155379627
┌ Info: Total log likelihood: 
│   tll =
│    251-element Array{Float64,1}:
│     -1.4031684604684445
│     -1.4032301216556453
│     -1.4031496360811417
│     -1.4022709432740434
│      ⋮
│     -1.0161910834804566
│     -1.0396607814811196
└     -1.0015365155379627
32×26 Array{Float64,2}:
  0.0265937     0.110681     0.0286766  -0.00143046   0.0699747  -0.0696483   -0.0212326   -0.000356186  -0.0398586   -0.330939     0.00383211  -0.0278444    0.0627186   -0.0212135   -0.0682391   -0.169633    -0.100395   -0.0922799   -0.00488677  -0.0375696   -0.0131961    0.0901565  -0.0326103     0.0090787   -0.0768964   0.126972
  0.362671     -0.0484023    0.092249    0.063454    -0.113251   -0.045195    -0.0494602    0.277509      0.0301934    0.153514     0.108497    -0.108548    -0.0461444   -0.0133566    0.134732    -0.00462113   0.0773828   0.0491353   -0.0218028   -0.108851    -0.0753106    0.0129612   0.0924884     0.0987367   -0.0269578  -0.0494559
 -0.190895     -1.26746     -0.0675512   0.134669    -0.0932968  -0.093823     0.00172502   0.0173785     0.2405       0.0840326   -0.252827    -0.0612308   -0.112472     0.0767639   -0.0585079   -0.00770911   0.157937   -0.0527114   -0.101854    -0.0311123   -0.10661      0.189642    0.0266495    -0.110836     0.0625511  -0.00525153
 -0.26668       0.435512    -0.0383588  -0.0342906    0.0980633  -0.0942646    0.0430379    0.0102159    -0.0835816    0.094942     0.0087354   -0.138113    -0.112336     0.258058    -0.0583862   -0.046218     0.115182   -0.0480853   -0.0386123   -0.0119623   -0.104225     0.189549    0.0293949    -0.20079      0.0379917  -0.0464505
  0.00874917    0.00139242   0.0330749   0.0594078   -0.0563627  -0.0546629   -0.0755518   -0.0495246    -0.107793     0.0122095    0.214218    -0.11963     -0.0749787   -0.066445     0.0277008   -0.00826519   0.0916165   0.050669    -0.0576571    0.072338    -0.0838746    0.0557408  -0.203519     -0.109792     0.092259   -0.108928
 -0.0161929     0.0851878    0.0720158  -0.0381071   -0.057742    0.0998402    0.143982    -0.166766     -0.206314     0.100392    -0.0217484   -0.143269     0.083615     0.0797338   -0.108118     0.0594821   -0.0452302  -0.0815156    0.0695747    0.0213392   -0.055625     0.0252263   0.0922701     0.040412    -0.104686   -0.0557328
 -0.0296696    -0.136914    -0.0012751  -0.012012     0.214241    0.0204195    0.0542935    0.0442505    -0.00265027   0.093078     0.0331834    0.0230479   -0.0390933   -0.0432551   -0.0150935   -0.0831348   -0.142003    0.0641728   -0.0843546    0.0949697   -0.0309151    0.0654014   0.122892     -0.0154527    0.0501379   0.06846
 -0.117036     -0.00752295   0.0022713  -0.0430641    0.0767475   0.126391     0.0800845    0.0402532    -0.122679     0.0564002    0.0853129    0.00608846   0.119855    -0.149353    -0.0284115   -0.033556    -0.177445    0.11        -0.184919     0.0350247   -0.0552507    0.197688    0.0760874    -0.0689455    0.112108   -0.116255
  0.10171      -0.124331    -0.0410387  -0.049979    -0.0312961  -0.0892805   -0.105488     0.0631124     0.0934278   -0.0678015   -0.0846119    0.075025    -0.0888138   -0.145414    -0.0234016   -0.0251554   -0.075325    0.143658    -0.0895124    0.185452     0.111773    -0.0262632  -0.0278746     0.0507412   -0.0657661   0.0871128
  0.110886      0.065302     0.133251   -0.15431      0.170828   -0.0501125   -0.0824559    0.155202     -0.157505    -0.163186     0.0859611    0.0851124   -0.0505013    0.0763738   -0.0965775    0.0450063    0.0194204   0.0718379    0.0440383   -0.0458364   -0.0880873    0.0597214  -0.0506317     0.0865924   -0.0422751  -0.18627
  0.189385      0.0446055   -0.126913   -0.1439       0.0957751  -0.20826      0.0765341   -0.113429      0.103265     0.00655872  -0.147395     0.0832731   -0.0482604    0.268525     0.0990844    0.0343626    0.0184736   0.0479957    0.151995     0.0849018   -0.0623933   -0.119931    0.13258      -1.14004     -0.206028    0.0257025
  0.183963      0.0458087   -0.151222   -0.0986639    0.290317   -0.0710819    0.0355643   -0.113228      0.107881     0.0102367   -0.150483     0.0826722   -0.0522097    0.27019      0.201067     0.0467494    0.017828    0.0909934    0.145303     0.0803638    0.0133262   -0.113504    0.142181      0.70504     -0.234542    0.0420115
  0.0949667     0.0934775   -0.0902908  -0.663832    -0.118221   -0.136796     0.0199797   -0.0812641     0.0200688   -0.0358957   -0.12128     -0.0107839    0.0976026    0.0132273    0.0559875    0.00352686  -0.0852275  -0.0819172    0.0153274    0.157528    -0.140321     0.046682   -0.172406      0.0973454    0.130234    0.0294989
  0.0712723    -0.0927787   -0.0878122   0.735409    -0.098398   -0.139885    -0.051001    -0.0753246     0.0414929   -0.0336189   -0.0986395   -0.0248819    0.151211     0.0310558    0.0979913   -0.0102477   -0.0710738  -0.0123298    0.0488119   -0.149841    -0.179622     0.024902   -0.0889411     0.349875    -0.0842112   0.00153271
 -0.108603      0.0826738   -0.152576   -0.175582     0.0648973  -0.0730169   -0.0572797    0.162349     -0.0622936    0.0484336   -0.030601    -0.0876717    0.16805     -0.244957    -0.153247     0.165971     0.0299993   0.145441     0.0050118   -0.186125     0.0947028    0.171261    0.105317     -0.0395005   -0.107918   -0.0965609
 -0.130798      0.0847365   -0.138054   -0.0507211    0.120493   -0.199308    -0.0619484   -0.0276344     0.175289     0.0479807    0.354196    -0.084877     0.161265    -0.242808    -0.0134893    0.118579     0.0150246   0.144329    -0.0522813    0.851015     0.187169     0.0880505   0.0401185     0.155043    -0.235778   -0.0399374
 -0.0155467     0.113685    -0.0110455   0.0446761    0.0105979   0.122076     0.0653522    0.0828044     0.0359711    0.127255     0.0134291    0.0593841    0.151311     0.0344852    0.230351     0.0390836   -0.0917226   0.0725916    0.069168     0.0852192   -0.059635     0.0173866   0.00591719    0.0848557   -0.0231284   0.0713214
  0.00562816    0.0193994   -0.0858138  -0.117588    -0.0879592  -0.167795     0.0380954   -0.0517852     0.05751     -0.101029     0.00368682  -0.0869976    0.108974     0.178586     0.00750902   0.0384975   -0.0378285   0.0638895   -0.115216     0.0676785    0.0438906    0.0882189   0.0109035    -0.00662539   0.0531268   0.0115618
  0.000583681  -0.0395185    0.0831004   0.0480712    0.0194841   0.110809    -0.0353899    0.0531939     0.155314     0.00710109   0.0488731    0.00895868   0.00949908   0.109325     0.0293797    0.123157    -0.0387985   0.00554626  -0.153837     0.154204     0.0837828   -0.0895564  -0.174888      0.120951     0.0228857   0.0734699
  0.00805385    0.0346866   -0.0800377  -0.0461069   -0.0636847  -0.188947     0.0620196   -0.0066846     0.0551286    0.0362788    0.0227766    0.151942    -0.0242991    0.11947      0.0701149   -0.0190596    0.0273209   0.237886     0.0915977    0.0219743   -0.00269914   0.226104    0.221177     -0.0852874    0.0189942   0.135253
 -0.0644628    -0.00751497  -0.0291266   0.144129     0.0791522   0.152416     0.00135174  -0.0366994    -0.139545     0.0541826    0.00445812   0.0669273    0.0115561   -0.0324824   -0.0446502   -0.0794555   -0.019705   -0.0194029    0.00313839   0.231822    -0.0228406   -0.110424   -0.0543212     0.00801626  -0.0845998  -0.17099
 -0.0119535     0.111311    -0.0217771   0.0745401    0.108211    0.184687    -0.0835174   -0.14348      -0.290142     0.0796035   -0.0263297    0.115604     0.136492    -0.045282     0.0526116    0.0686187    0.0892185  -0.00688139  -0.0187487    0.0982044   -0.0332493    0.0876444   0.00307047    0.00319532  -0.0422161   0.0522297
  0.0896929    -0.0236719    0.140299    0.111215    -0.0284444  -0.0947635   -0.00696047   0.0907426     0.0140141    0.041766    -0.0372519   -0.0726108   -0.0443315   -0.0165014    0.0452133    0.0288792   -0.0302207  -0.225326     0.080242     0.00991482  -0.0273558    0.0968611   0.0769948     0.051557    -0.0189533   0.0388855
  0.0600836     0.00966048   0.0341153   0.0973088   -0.0789167  -0.0450532    0.0778581    0.01665       0.069135    -0.0499169   -0.0244573    0.0156553   -0.00355457  -0.0889784    0.075514     0.0445936   -0.0270206  -0.0327235   -0.0227294   -0.0518077   -0.0740829    0.0212034   0.000241948   0.0867984    0.156285    0.0182
  0.0218352     0.171318    -0.104092   -0.0272895    0.0563662   0.0498078    0.0799739    0.00887601    0.09717     -0.120531    -0.00787041  -0.0306864    0.168101    -0.140822     0.331287    -0.00881057   0.028312   -0.059402    -0.0970528   -0.124803     0.107465    -0.212872    0.0265491    -0.00753666  -0.0486492   0.172932
  0.0122238    -0.0882667    0.0133565   0.0192282    0.230045   -0.0903445   -0.0877999    0.0746851     0.104616     0.121621     0.137506    -0.176449    -0.0338302    0.0213739   -0.133011    -0.057451     0.122893   -0.0156897    0.0365305    0.0731672   -0.0700525   -0.0412464   0.0921208     0.0275354   -0.249523    0.0347202
  0.104352     -0.00740205   0.119637    0.155461     0.0617236   0.0280821    0.142338     0.0273848     0.0750441   -0.101666    -0.204539     0.0648274    0.0640161    0.152021    -0.00826616   0.0318222    0.0340097  -0.0530389   -0.0585671   -0.0335428    0.0313873   -0.0386965   0.0877814     0.107404    -0.0885567   0.00450798
  0.0914963     0.00240753   0.0209777  -0.00575966  -0.0848714   0.00220048  -0.00507245   0.00358093    0.0713761   -0.0571902    0.0792088   -0.00876559   0.0301599   -0.002539     0.0250632    0.0980352   -0.0342958   0.075166    -0.0393136    0.0290773   -0.0187345   -0.0974489  -0.109786     -0.114603    -0.0973868   0.138795
 -0.220008     -0.136803     0.109609    0.121647    -0.0861874  -0.0129956    0.0243461    0.176121     -0.0712359   -0.0352136   -0.0669953    0.0180317    0.136675    -0.00318398   0.00536959  -0.375829    -0.0990647   0.00126299  -0.0289831   -0.0166346    0.0224215    0.0304327   0.0177606    -0.0913618   -0.132016   -0.0510361
 -0.179761     -0.180936     0.106005    0.105906    -0.126772    0.0586038    0.120596     0.117089     -0.0281552    0.243669    -0.0473153    0.0141964    0.150471     0.0719672   -0.0983104    0.84373     -0.0973174   0.504157    -0.00212277   0.102244     0.112709     0.0226114   0.154724     -0.0945463    0.0847542   0.0170893
  0.254321      0.0590869    0.0946634  -0.00936516   0.0256515  -0.0615322    0.110215    -0.112965      0.222351     0.149896    -0.0135474   -0.0552662    0.0324836   -0.030312    -0.144039    -0.0570575    0.0473353   0.0684009    0.00523726   0.0918759    0.0944481    0.160478   -0.118247     -0.101521    -0.0871451  -0.204007
  0.0382558    -0.0947663   -0.0209642   0.0695454   -0.0351239  -0.0781705    0.0172858   -0.010391      0.00773349   0.0397829    0.0465395   -0.0703103    0.091013     0.116669     0.210348     0.210232    -0.0429981   0.0674296   -0.102604    -0.182329     0.0234307    0.0183763  -0.0601294     0.0126728    0.0258775  -0.0486288[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      1
│     10
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 1, average log likelihood -1.051330
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      1
│      3
│      4
│     10
│      ⋮
│     20
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 2, average log likelihood -1.013976
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      1
│     10
│     16
│     21
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 3, average log likelihood -1.022432
┌ Warning: Variances had to be floored 
│   ind =
│    14-element Array{Int64,1}:
│      1
│      3
│      4
│      5
│      ⋮
│     22
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -0.993365
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      1
│     10
│     16
│     21
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.042438
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      1
│      3
│      4
│     10
│      ⋮
│     20
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.009676
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      1
│      9
│     10
│     16
│     21
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.017645
┌ Warning: Variances had to be floored 
│   ind =
│    12-element Array{Int64,1}:
│      1
│      3
│      4
│      5
│      ⋮
│     20
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.017580
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      1
│     10
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.040966
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      1
│      3
│      4
│      7
│      ⋮
│     21
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
kind diag, method kmeans
[ Info: iteration 10, average log likelihood -0.992131
┌ Info: EM with 100000 data points 10 iterations avll -0.992131
└ 59.0 data points per parameter
[ Info: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.504280e+05
      1       6.708578e+05      -1.795701e+05 |       32
      2       6.408954e+05      -2.996240e+04 |       32
      3       6.244374e+05      -1.645799e+04 |       32
      4       6.152911e+05      -9.146385e+03 |       32
      5       6.093438e+05      -5.947270e+03 |       32
      6       6.041693e+05      -5.174530e+03 |       32
      7       6.004514e+05      -3.717855e+03 |       32
      8       5.977637e+05      -2.687715e+03 |       32
      9       5.961155e+05      -1.648199e+03 |       32
     10       5.955336e+05      -5.818382e+02 |       32
     11       5.953172e+05      -2.164271e+02 |       32
     12       5.952087e+05      -1.084959e+02 |       32
     13       5.951193e+05      -8.939782e+01 |       32
     14       5.950291e+05      -9.024791e+01 |       32
     15       5.948896e+05      -1.394987e+02 |       32
     16       5.946487e+05      -2.408484e+02 |       32
     17       5.941991e+05      -4.496424e+02 |       32
     18       5.935961e+05      -6.029497e+02 |       32
     19       5.930438e+05      -5.522998e+02 |       32
     20       5.926006e+05      -4.432202e+02 |       32
     21       5.922512e+05      -3.493694e+02 |       32
     22       5.920352e+05      -2.160760e+02 |       32
     23       5.919086e+05      -1.265447e+02 |       32
     24       5.918041e+05      -1.044921e+02 |       31
     25       5.916704e+05      -1.337056e+02 |       32
     26       5.914765e+05      -1.939555e+02 |       32
     27       5.912261e+05      -2.503871e+02 |       32
     28       5.909326e+05      -2.935258e+02 |       32
     29       5.906015e+05      -3.310999e+02 |       32
     30       5.903589e+05      -2.425595e+02 |       32
     31       5.901851e+05      -1.738022e+02 |       32
     32       5.901112e+05      -7.389229e+01 |       32
     33       5.900735e+05      -3.769606e+01 |       32
     34       5.900525e+05      -2.105654e+01 |       31
     35       5.900385e+05      -1.395853e+01 |       32
     36       5.900279e+05      -1.057408e+01 |       31
     37       5.900204e+05      -7.473069e+00 |       28
     38       5.900142e+05      -6.210861e+00 |       23
     39       5.900074e+05      -6.881374e+00 |       27
     40       5.899987e+05      -8.650211e+00 |       27
     41       5.899896e+05      -9.149954e+00 |       29
     42       5.899829e+05      -6.633248e+00 |       28
     43       5.899772e+05      -5.752029e+00 |       31
     44       5.899734e+05      -3.812292e+00 |       26
     45       5.899702e+05      -3.142450e+00 |       22
     46       5.899674e+05      -2.776430e+00 |       21
     47       5.899653e+05      -2.157630e+00 |       23
     48       5.899634e+05      -1.870949e+00 |       25
     49       5.899621e+05      -1.344823e+00 |       17
     50       5.899610e+05      -1.070717e+00 |       16
K-means terminated without convergence after 50 iterations (objv = 589960.9944590982)
┌ Info: K-means with 32000 data points using 50 iterations
└ 37.0 data points per parameter
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.295817
[ Info: iteration 2, average log likelihood -1.255139
[ Info: iteration 3, average log likelihood -1.213457
[ Info: iteration 4, average log likelihood -1.156536
[ Info: iteration 5, average log likelihood -1.091531
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      8
│     10
│     16
│     23
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.025832
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      1
│     11
│     12
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.058350
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      2
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.056767
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      8
│     15
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.021113
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│     10
│     16
│     17
│     18
│     23
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.006983
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     11
│     12
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 11, average log likelihood -1.054545
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      1
│      2
│      8
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 12, average log likelihood -1.031211
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│     10
│     15
│     17
│     23
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 13, average log likelihood -1.022226
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      7
│     11
│     12
│     18
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 14, average log likelihood -1.049118
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      8
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 15, average log likelihood -1.054443
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      1
│      2
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 16, average log likelihood -1.020505
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     10
│     17
│     19
│     23
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 17, average log likelihood -1.007678
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      8
│     11
│     12
│     15
│     18
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 18, average log likelihood -1.032483
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 19, average log likelihood -1.066094
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      2
│      7
│     10
│     17
│     23
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 20, average log likelihood -1.017007
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      1
│      8
│     11
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 21, average log likelihood -1.031307
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      4
│     12
│     18
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 22, average log likelihood -1.025055
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     16
│     17
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 23, average log likelihood -1.033408
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      2
│     10
│     15
│     23
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 24, average log likelihood -1.027185
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      8
│     12
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 25, average log likelihood -1.036032
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      1
│     11
│     16
│     17
│     18
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 26, average log likelihood -1.016814
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     4
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 27, average log likelihood -1.053112
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      2
│      8
│     10
│     12
│     15
│     19
│     23
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 28, average log likelihood -1.005003
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     17
│     18
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 29, average log likelihood -1.078991
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     11
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 30, average log likelihood -1.040061
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      1
│      4
│      8
│     10
│     17
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 31, average log likelihood -0.987638
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      2
│     12
│     18
│     23
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 32, average log likelihood -1.028707
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     15
│     19
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 33, average log likelihood -1.061908
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      8
│     10
│     11
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 34, average log likelihood -1.039752
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     16
│     17
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 35, average log likelihood -1.022641
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      1
│      2
│      4
│     12
│     18
│     23
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 36, average log likelihood -0.991732
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      8
│     10
│     11
│     17
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 37, average log likelihood -1.020978
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     15
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 38, average log likelihood -1.069843
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     16
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 39, average log likelihood -1.027948
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      1
│      2
│      4
│      8
│      ⋮
│     18
│     23
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 40, average log likelihood -0.973745
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 41, average log likelihood -1.098761
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 42, average log likelihood -1.053225
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      8
│     10
│     15
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 43, average log likelihood -0.993435
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      1
│      2
│      4
│     11
│      ⋮
│     18
│     23
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 44, average log likelihood -0.990222
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     19
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 45, average log likelihood -1.075295
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      8
│     10
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 46, average log likelihood -1.054388
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     15
│     17
│     18
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 47, average log likelihood -1.017322
┌ Warning: Variances had to be floored 
│   ind =
│    12-element Array{Int64,1}:
│      1
│      2
│      4
│      8
│      ⋮
│     28
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 48, average log likelihood -0.974451
[ Info: iteration 49, average log likelihood -1.116740
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     17
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 50, average log likelihood -1.045929
┌ Info: EM with 100000 data points 50 iterations avll -1.045929
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
  0.163896     0.0411507   -0.194291     -0.0809108   0.270452    -0.0904812    0.0339388   -0.0864218    0.0970406    0.0162294   -0.152922     0.078971    -0.0498159     0.232805    0.174292     0.0441308    0.0165971    0.129136     0.142941     0.086981     -0.0158997   -0.109251     0.131587     0.273741    -0.181002     0.0320804
  0.00332602   0.174972    -0.115905     -0.0289717   0.0726592    0.0592868    0.0745943    0.0199049    0.108813    -0.145258    -0.00531996  -0.0140981    0.16764      -0.114365    0.543949    -0.0185473    0.0230587   -0.0989538   -0.0868209   -0.110313      0.100609    -0.252483     0.0344706   -0.0143402   -0.0440335    0.157457
  0.0733077    0.0431225    0.088859      0.341548    0.167708    -0.108671     0.105297     0.0458118    0.0881848   -0.102672    -0.306937     0.0767009    0.0200985     0.0842056   0.0480538   -0.0018138    0.0415758   -0.0128745   -0.0794253   -0.0170358     0.0839768   -0.00615182   0.224054     0.00224822  -0.00121552   0.0108305
  0.0188309    0.0445867    0.0543601     0.0610031  -0.0358222   -0.0509942   -0.0856875   -0.0967719   -0.178269     0.0289491    0.225106    -0.229525    -0.0680348    -0.0398696   0.0248189    0.011996     0.16244      0.0439804   -0.0265822    0.0682533    -0.191251     0.0540479   -0.184386    -0.27211      0.103129    -0.101985
 -0.10675      0.0451165    0.206306      0.146079   -0.00837143  -0.10879     -0.0207116    0.16864      0.0149259    0.145378     0.00884974  -0.137639     0.000184714   0.104688    0.0689258    0.157083    -0.0841118   -0.219332     0.110869     0.0278781    -0.00109404   0.0863153    0.039562     0.0798699    0.0119476   -0.0206023
  0.359829    -0.0446281    0.0929367     0.0622566  -0.112881    -0.0455436   -0.0486958    0.274132     0.0266181    0.141645     0.108954    -0.10788     -0.0428513    -0.0133937   0.132862    -0.0119424    0.0746673    0.049557    -0.0215781   -0.104529     -0.073577     0.0186974    0.0905916    0.0977748   -0.0285873   -0.0447499
  0.149044    -0.0415791    0.129078     -0.0585399  -0.0690495    0.143308     0.172514    -0.00402601   0.054601    -0.0815367   -0.0780618    0.0428592    0.0982351     0.201841   -0.0367752    0.0624118    0.0230962   -0.0790308   -0.0344448   -0.0471921    -0.0330515   -0.0902047   -0.0551283    0.205289    -0.177425     0.015949
  0.219938     0.0750508    0.0742603    -0.0096228   0.03562     -0.0484443    0.100605    -0.100869     0.212147     0.126614    -0.0100497   -0.0513931    0.0521108    -0.0405213  -0.130946    -0.0516287    0.0411002    0.0667408    0.00837739   0.0717789     0.0900503    0.127898    -0.0937744   -0.0926289   -0.0832095   -0.147877
 -0.0120785    0.0618438   -0.0265601     0.0426617  -0.0339264    0.159196     0.147124     0.113364     0.00418193   0.0874681   -0.0108942    0.0267923    0.10219      -0.122143    0.273402     0.0999598   -0.0741417    0.0602142    0.0965687    0.0619077     0.0060146    0.0377156   -0.0636405    0.218957    -0.0868365    0.0934251
  0.0970796    0.094869     0.14062      -0.178697    0.197929    -0.0543437   -0.117051     0.164947    -0.183018    -0.189264     0.0934791    0.0834475   -0.0393279     0.0806559  -0.106049     0.0485043    0.0149997    0.107971     0.043149    -0.0622463    -0.106405     0.055293    -0.0667113    0.0872553   -0.050202    -0.239546
  0.0206967    0.0653703    0.0842323     0.0046553   0.0753246   -0.0509649   -0.00416071  -0.0140441   -0.162008    -0.646378     0.00845627  -0.0470604    0.0381472    -0.0150075  -0.092762    -0.266504    -0.113178    -0.148573     0.00610769  -0.0441097     0.0492315    0.122317    -0.0280803   -0.013366    -0.116711     0.118347
 -0.0047024    0.160637     0.0222506     0.0312865   0.0832979    0.00284892  -0.0369351    0.0151313    0.0725525    0.0927602    0.0214859    0.0549657    0.154054      0.0831159   0.0532136   -0.050037    -0.110138     0.00275751   0.0101383    0.0321005    -0.108823    -0.014225     0.0211317   -0.0942159    0.0492903    0.0891739
 -0.00561003  -0.0345693    0.0826861     0.0454802   0.0188155    0.110783    -0.0335508    0.0571779    0.154985     0.00850669   0.0466277    0.00470101   0.00859976    0.110043    0.0289507    0.12105     -0.0418965    0.0101307   -0.153959     0.154548      0.0855646   -0.0890811   -0.169329     0.119087     0.0248152    0.0775256
 -0.108876    -0.0212228   -0.000985651  -0.0402202   0.0905892    0.110032     0.0740495    0.0412287   -0.106132     0.0553259    0.0776081    0.0113851    0.0975436    -0.134231   -0.0269464   -0.0371983   -0.171543     0.106074    -0.171589     0.047099     -0.0524728    0.178248     0.0822218   -0.0613175    0.102175    -0.0914724
 -0.0108493    0.115859    -0.0141217     0.0657667   0.108253     0.172719    -0.0694407   -0.127197    -0.263695     0.0712226   -0.0190578    0.103101     0.14172      -0.0330979   0.0585919    0.0547382    0.0775351   -0.011328    -0.0421029    0.0842622    -0.0239584    0.0625298    0.00958314   0.00334292  -0.0437159    0.0598742
  0.00465971   0.0502729   -0.0863377    -0.0485918  -0.0424161   -0.190914     0.0550535   -0.00862928   0.0600598    0.0519744    0.0151677    0.145365    -0.00941406    0.14903     0.0777186    0.00139422   0.0149394    0.264081     0.0929218    0.0419216     0.00804891   0.214441     0.205048    -0.105988     0.00996394   0.142507
 -0.236633    -0.216597    -0.0492803     0.030169    0.0262143   -0.0941919    0.0283637    0.0139511    0.0395674    0.0901172   -0.09309     -0.107459    -0.112197      0.186738   -0.058434    -0.0323914    0.129555    -0.0489949   -0.063589    -0.018955     -0.105031     0.189128     0.028271    -0.166616     0.0470363   -0.0303797
 -0.0339557    0.0397125   -0.166913     -0.102555   -0.111098    -0.14162     -0.0670811   -0.00957269   0.0384064   -0.0378945    0.0336093   -0.105396     0.168823      0.286154    0.0457895   -0.00499117  -0.037398     0.21267     -0.0999919    0.0648759     0.119545     0.131066    -0.0680176    0.106951     0.0300066    0.100548
  0.00491071   0.00200618  -0.057523      0.0206069  -0.169034    -0.0332895    0.116241     0.0416943    0.00276614  -0.10295      0.051036    -0.0741854   -0.0861093    -0.103779    0.00640786   0.112727    -0.0684628   -0.00132283   0.019051    -0.0522584    -0.0513754   -0.053646    -0.0505787   -0.19595     -0.0523321    0.118924
  0.043933     0.0421797    0.0602785     0.102189   -0.177877     0.00374767  -0.0395139    0.0186404    0.00366521  -0.0327224    0.024818     0.0479913    0.0114486    -0.140663    0.0792205   -0.0108644   -0.0845031   -0.0532747   -0.0414712   -0.0387994    -0.110641     0.0494203   -0.0101436    0.13438      0.140817     0.0451942
  0.0829674   -0.0030182   -0.0881254     0.0710315  -0.105928    -0.138987    -0.019738    -0.0760815    0.0318748   -0.0346692   -0.110233    -0.0163192    0.119419      0.021361    0.0785916   -0.00388421  -0.0775285   -0.0419395    0.0310668    0.000283017  -0.157134     0.0346521   -0.130549     0.22736      0.0139657    0.0137267
  0.0463878    0.00883043   0.00604553   -0.112838   -0.0461954   -0.154361     0.170411    -0.0804956    0.0868667   -0.151784    -0.0321761   -0.053179     0.0532174     0.0387465  -0.0137054    0.0826385   -0.0530445   -0.0849616   -0.110541     0.0540477    -0.0897858    0.0287006    0.12859     -0.131608     0.0863942   -0.0980345
 -0.100282     0.083858    -0.150952     -0.148033    0.080419    -0.0915814   -0.055339     0.115831    -0.0121912    0.0458448    0.0422617   -0.0789611    0.157053     -0.218337   -0.125377     0.150661     0.023772     0.142113    -0.00174793   0.0178024     0.101703     0.146004     0.0935347   -0.00233948  -0.131509    -0.0761739
  0.0375038   -0.0929802   -0.0213626     0.0666241  -0.0344301   -0.0799857    0.0134585   -0.0111659    0.00892597   0.0374722    0.0449376   -0.0703858    0.0890085     0.11776     0.20858      0.208027    -0.042757     0.066872    -0.102587    -0.181501      0.0238066    0.018771    -0.0600063    0.0114153    0.0249822   -0.0486021
  0.0142639   -0.0831229    0.00868855    0.0173316   0.233158    -0.0931548   -0.0869204    0.0733159    0.101968     0.123539     0.136021    -0.169566    -0.0337602     0.0218022  -0.132534    -0.0580507    0.116712    -0.0168296    0.0359226    0.0751705    -0.0644126   -0.0434092    0.0972687    0.0228874   -0.246557     0.0360981
  0.187542     0.00177964   0.099749     -0.0334565   0.013354     0.0283904   -0.148485    -0.0425083    0.151677    -0.00890468   0.108376     0.0636422    0.169389      0.0960969   0.061986     0.0809114    0.00904181   0.168839    -0.0969089    0.126466      0.0209849   -0.133307    -0.158093    -0.0123678   -0.135137     0.154183
 -0.198946    -0.157129     0.107192      0.112262   -0.104475     0.0211737    0.0679641    0.145202    -0.0505217    0.0990966   -0.0565935    0.017356     0.144316      0.0359866  -0.0447678    0.210704    -0.0981797    0.243156    -0.0166268    0.0408405     0.0663448    0.0260417    0.0836773   -0.0921657   -0.0301069   -0.0180883
 -0.0604557   -0.00885861  -0.0784221     0.176509    0.092207     0.124847     0.0156939   -0.0364234   -0.133417     0.106662     0.0258145    0.136856     0.00116132   -0.0396183  -0.0379435   -0.0693656   -0.0331948   -0.0184977   -0.0159469    0.359906     -0.0172623   -0.119495    -0.0661578   -0.0329981   -0.202011    -0.260956
  0.300708    -0.0961165    0.0359859     0.0467842  -0.015728    -0.0816203    0.0244475   -0.0141939    0.0307062   -0.0945559   -0.0907906    0.0377669   -0.114671     -0.100618    0.00699217  -0.0948606    0.0426898   -0.236182     0.0519453   -0.00567307   -0.0436309    0.0812895    0.127569     0.00507804  -0.0837637    0.0800183
 -0.0197352    0.0939264    0.0660268    -0.0505136  -0.0510087    0.0963951    0.146148    -0.149038    -0.197891     0.0988309   -0.0220231   -0.131346     0.0747052     0.0732694  -0.105439     0.0583663   -0.0430309   -0.0762812    0.063205     0.0284007    -0.0533744    0.0222785    0.0861062    0.0454858   -0.102654    -0.0531927
  0.0717978   -0.110382    -0.0105469    -0.0139209  -0.0371754   -0.075976    -0.110684     0.0529658    0.0422519   -0.0518059    0.0213879    0.0513573   -0.0822754    -0.12462    -0.00621852  -0.0272219   -0.0392984    0.126368    -0.0840353    0.146638      0.0992261    0.00574573  -0.121313     0.066857    -0.0118228    0.0232676
  0.0211119   -0.0351161   -0.00605763    0.108561    0.0854169   -0.0376061    0.234672     0.0036563    0.0891008   -0.0456831   -0.0972686   -0.0623564   -0.0478854    -0.0047402   0.0128912    0.0650386    0.06298     -0.00664898   0.0426404   -0.0341421    -0.0171999   -0.0716039   -0.00311243   0.016978     0.16054     -0.0678515[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     10
│     18
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 1, average log likelihood -1.009242
┌ Warning: Variances had to be floored 
│   ind =
│    14-element Array{Int64,1}:
│      1
│      2
│      4
│      8
│      ⋮
│     23
│     28
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 2, average log likelihood -0.935894
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      8
│     10
│     16
│     18
│     19
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 3, average log likelihood -0.983318
┌ Warning: Variances had to be floored 
│   ind =
│    12-element Array{Int64,1}:
│      1
│      2
│      4
│     10
│      ⋮
│     23
│     28
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -0.950425
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      8
│     10
│     16
│     18
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -0.983796
┌ Warning: Variances had to be floored 
│   ind =
│    15-element Array{Int64,1}:
│      1
│      2
│      4
│      8
│      ⋮
│     23
│     28
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -0.935987
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     10
│     18
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -0.999966
┌ Warning: Variances had to be floored 
│   ind =
│    14-element Array{Int64,1}:
│      1
│      2
│      4
│      8
│      ⋮
│     23
│     28
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -0.936223
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      8
│     10
│     16
│     18
│     19
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -0.986018
┌ Warning: Variances had to be floored 
│   ind =
│    12-element Array{Int64,1}:
│      1
│      2
│      4
│     10
│      ⋮
│     23
│     28
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -0.950249
┌ Info: EM with 100000 data points 10 iterations avll -0.950249
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.0847163    -0.0505174   -0.117929    -0.0277983   -0.0123621   -0.0614904    0.0443672     0.0208428    0.063668     -0.154511    -0.0904398  -0.055126    -0.276296    -0.0187834    -0.00211456  -0.00957288  -0.0775728   -0.0586999    0.15053      0.162358      0.0748327    0.0858722    -0.0182565   -0.0289449    -0.0433361   -0.0186286
  0.151687     -0.0982731   -0.149089     0.00600144   0.0733414   -0.120198     0.0303773     0.208291    -0.0201866     0.0113374    0.265564    0.190495     0.0886023    0.103138     -0.199869     0.0725357   -0.00547422  -0.00657073  -0.0377641   -0.0548672    -0.023314     0.32273       0.0948178    0.0566127    -0.0752053    0.0326354
 -0.044216     -0.145266    -0.0851124   -0.165524    -0.0791553    0.0766222   -0.101886     -0.0520408   -0.169981      0.044218     0.0900861  -0.0151338    0.00560641   0.0483751     0.085648    -0.0568154    0.0353478   -0.128374    -0.059767     0.0346594     0.0675358    0.146911      0.0269042   -0.0650905     0.0410624    0.131833
  0.0872382     0.0728527    0.0612279   -0.12337      0.10608     -0.0272066   -0.0468068    -0.079928     0.0404034    -0.161096     0.041069    0.020492    -0.0503589   -0.0226852    -0.00325574   0.0415294    0.105946     0.158898     0.0382199   -0.00675241    0.208197     0.0430374     0.00516404   0.0107911     0.164304    -0.167784
  0.0413794    -0.0378318   -0.102656    -0.00498746  -0.157514     0.0229181    0.0051504     0.056297     0.0256989     0.0230004    0.0505751   0.162036     0.0579902   -0.0421915     0.156744     0.136782     0.0765309   -0.140258    -0.0125487   -0.0109034     0.0367136    0.108376     -0.132528    -0.0796799    -0.082921    -0.0749326
 -0.108921     -0.0922174    0.0587645    0.0774451   -0.142847    -0.0236261   -0.0718824    -0.135229     0.0881966     0.029538     0.0630077  -0.038942    -0.0884643    0.0398391     0.00431942  -0.145845     0.129468     0.156431    -0.193274    -0.13376      -0.0660026   -0.000451694  -0.0769395    0.000754674   0.0298999    0.0570237
 -0.0668808    -0.0768192   -0.0028418   -0.063922     0.074979     0.0850232    0.102374     -0.0407711    0.0589087     0.0126785   -0.0134287   0.0312351   -0.146344    -0.0151173     0.231722    -0.0524966    0.0478201    0.0431884   -0.0442218    0.0493356     0.103764     0.0557542    -0.0560035   -0.00539332   -0.14027      0.00172092
  0.0100983     0.102269    -0.0265958   -0.0776576   -0.0224179    0.202515    -0.0221043     0.0125152   -0.0476125     0.0109903    0.0484724  -0.121271     0.0429751   -0.012068     -0.0811592    0.0905413    0.167829     0.0792823   -0.0267262   -0.0507131    -0.0924137    0.129394      0.0584202   -0.0856499     0.147992     0.00669333
 -0.147237      0.0777565   -0.0487507    0.0547071    0.0810776    0.0252563   -0.0312773    -0.134982     0.0586508     0.0483484    0.159348    0.0175828   -0.0685248    0.112041     -0.0269909    0.161315    -0.166608     0.00643065  -0.0233888    0.000512854  -0.0224142    0.178422      0.130239    -0.0607435     0.0336392    0.112946
  0.186728      0.027509     0.0188793   -0.0250448    0.0357748    0.00951442  -0.160301      0.0372029    0.00044058   -0.1433       0.091469    0.109474     0.0908194   -0.0148468     0.0430649   -0.0152828   -0.153524     0.0973072   -0.00236294  -0.136083     -0.00748962   0.138598      0.0953794    0.0522504    -0.0300878    0.0824678
  0.0217135     0.124767     0.116651     0.104487     0.265625     0.0917416    0.0345491     0.0740422    0.000825608   0.00141716  -0.0620029   0.0310835    0.101527    -0.0565437    -0.0431854    0.114963     0.0186595    0.0479465   -0.0330223   -0.0282682     0.0278043    0.0818154     0.0870492   -0.109706     -1.6303e-5   -0.07645
  0.0121876    -0.0898545    0.0736652    0.0175647    0.102966    -0.172723    -0.0700397    -0.131788     0.136059     -0.0638241    0.0386189   0.0557296   -0.0573973    0.034736      0.122318    -0.191482     0.0736777    0.154208     0.0771728   -0.23839       0.0820371    0.0682743     0.0420337    0.0443545    -0.135443    -0.104886
 -0.0901575    -0.220292     0.0610743   -0.0774417   -0.15339      0.0690141   -0.152415     -0.0360735    0.0715801    -0.0939464   -0.11687     0.124892     0.0857625   -0.0130335    -0.0165105    0.104957     0.0633251   -0.0602479   -0.0414791    0.111082     -0.0674653   -0.144666      0.00688149   0.0391612    -0.173197     0.166263
 -0.0391806     0.0961016    0.00389716  -0.103725    -0.0105556    0.0124957   -0.0330427    -0.0416818    0.0756465     0.110274     0.0591841   0.154082    -0.018916     0.105514     -0.0296433    0.168835     0.142452    -0.160588    -0.0311844   -0.0156653     0.0119942    0.0621387    -0.208314     0.255337     -0.092157     0.221742
  0.0639642    -0.0970569    0.0492297   -0.0174729   -0.146434    -0.104595     0.0328862     0.0782503   -0.059784     -0.0405295   -0.0145357   0.216358    -0.0587417    0.0236441    -0.111553     0.00630798   0.0345093    0.212804    -0.0584985   -0.0268868    -0.0521785   -0.0843537    -0.0272537   -0.0455832     0.118524    -0.125897
  0.101563     -0.0685207    0.014698     0.0104569    0.0488115   -0.0570925    0.0946799    -0.00364882  -0.0286577    -0.114282     0.0350736  -0.102205     0.00356697   0.192965     -0.0307959   -0.0433167   -0.131022    -0.203809    -0.0885027    0.0958481    -0.0473401   -0.0766487    -0.0224253    0.19291       0.0454703    0.181547
 -0.0635915     0.033764    -0.0177439    0.0411218    0.0538135   -0.0485508   -0.0864454     0.0835123    0.0459077    -0.0652513    0.098821   -0.029882     0.026452    -0.0595644    -0.0919919   -0.0757064    0.0911369   -0.010902     0.153957    -0.0576204    -0.206291    -0.0262922     0.161176     0.00774737    0.120371    -0.121003
 -0.000831645  -0.0218648   -0.065071     0.0272547    0.0363942   -0.0830745    0.0418599    -0.184198    -0.00447803    0.0756168    0.0169195   0.00177018  -0.0239317    0.0563253     0.113629     0.0730148    0.149986     0.0110926   -0.123018     0.199178     -0.0554256   -0.0329422     0.131586    -0.0129421     0.228846     0.195501
 -0.183334     -0.0122151    0.182055     0.0356588    0.00355283  -0.0655748    0.187418      0.124431    -0.053637      0.11433      0.0689584   0.0825869   -0.00390676   0.0770812     0.0374076    0.0181383    0.233259     0.237963    -0.0379885    0.116439      0.00103382  -0.0726954    -0.0418071   -0.00413435   -0.0262626    0.0108618
  0.00288097   -0.00928961  -0.0345566    0.0840273   -0.00465249  -0.0665212   -0.0723356     0.0811646    0.0250258     0.00292133  -0.0614126  -0.152872    -0.0156968    0.000750794  -0.14232      0.0729677    0.0865467   -0.167447     0.037016     0.0277428    -0.235574    -0.277806     -0.0157757    0.0100442     0.0837941   -0.00318395
  0.032902      0.0551846    0.178809     0.129203    -0.0352391    0.194885    -0.0991859     0.0738771    0.0677889     0.0489491   -0.0330022   0.0986572    0.0531653   -0.12792       0.134752    -0.0380429   -0.0229852    0.134611    -0.187499    -0.00980655   -0.0391313    0.0635133     0.100171    -0.0912007     0.0725089    0.00770554
  0.100603     -0.13568      0.0490249   -0.0262105   -0.113585    -0.112147    -0.0245207     0.00551329  -0.0628232     0.032932    -0.1154     -0.0460406    0.136914     0.159236     -0.0119861    0.0237625   -0.111031     0.20004      0.0478116   -0.0240193    -0.0108857    0.0795666    -0.255236     0.0783279    -0.0291167    0.115461
  0.00493703    0.118058     0.0967902   -0.170317    -0.126969    -0.0953844   -0.0990685    -0.0777014    0.139904     -0.153196     0.111932   -0.0783049    0.180732     0.0869166    -0.00333744  -0.0789711    0.0582081   -0.0209485    0.141886     0.0235875    -0.059846     0.126067     -0.0362204   -0.179437     -9.66079e-5   0.057706
  0.129421      0.0351285    0.0883093    0.0560951    0.0333829   -0.0681191    0.0104269    -0.144099    -0.0539674    -0.179298     0.0171618   0.0742688    0.0561915   -0.0187196    -0.0671274   -0.108209     0.00871817   0.0891496   -0.018401     0.0306282    -0.0895238    0.0469318     0.0430078    0.00199196   -0.0599952   -0.105858
 -0.0735873    -0.0402559    0.157214     0.0855338    0.00308526   0.0305755    0.00983436    0.0315501   -0.111119     -0.0765924    0.0723994  -0.127409     0.360673     0.0636293     0.156609     0.149801    -0.110559     0.00563326   0.0558012    0.0216669     0.134499    -0.0413808     0.0459054   -0.0265283     0.0251388   -0.0042782
 -0.0252239    -0.0390384    0.00339726   0.144898    -0.00505179  -0.134046    -0.00767011   -0.0607677   -0.0504322     0.161553    -0.0199166   0.0815227   -0.0190606    0.0651612    -0.0863604   -0.0507501    0.067288    -0.234703     0.0203615    0.0308675    -0.144679     0.0930274     0.094491    -0.0138633    -0.146309     0.263217
 -0.193655      0.0507104    0.226179     0.0344021    0.112671     0.047897    -0.0783314    -0.0693208    0.0466087     0.141065    -0.0112674  -0.00867039  -0.0390488   -0.0186546    -0.0407497    0.0377826   -0.0255473   -0.0171688    0.179221     0.0602663    -0.203168    -0.0413291    -0.0584453    0.119        -0.143379     0.0292992
  0.111517      0.110718     0.00377623  -0.0387724    0.0484271    0.028631    -0.0781039    -0.0503954    0.0811358    -0.0107821    0.0770546   0.073415     0.0682461    0.0854978     0.0871556    0.0503954    0.0280708    0.0155732    0.194981    -0.0446299     0.00195277  -0.052702     -0.0361993    0.0382565     0.131364    -0.0277358
 -0.152691     -0.0442791   -0.0104735    0.0386735    0.0091108    0.195622    -0.000204725   0.0573336    0.0364865    -0.0708044   -0.135753    0.0085647    0.107928    -0.0377554     0.0848472   -0.108963     0.123376     0.00837059  -0.0537321   -0.0184588     0.0751435   -0.0806502     0.0287026   -0.0634802     0.0621536    0.116897
 -0.122783      0.0596561   -0.0995871   -0.0192346   -0.0826375    0.146084     0.0517743     0.222716     0.0814793    -0.0668575   -0.135311   -0.0756043    0.0481793    0.112504     -0.0872384   -0.188392     0.140684     0.0136191    0.0932621    0.229785      0.0480826    0.181303     -0.148816     0.1688        0.115166    -0.0770692
  0.0758942    -0.0426432   -0.100043     0.025874    -0.0207374   -0.0572418    0.163041     -0.117807    -0.00668672   -0.0835363    0.0285689  -0.04276     -0.0555367    0.0222834    -0.1245      -0.0482928   -0.0845195    0.0638429   -0.257382    -0.00393193   -0.0202811    0.082489      0.176617     0.00407175   -0.00303421  -0.0264901
 -0.0102841     0.0830349   -0.0108182    0.161767     0.0797408    0.138361    -0.00605128   -0.00381307  -0.0864924    -0.118624    -0.028338    0.0394549    0.0164908    0.0653642     0.0701511   -0.0471651   -0.057901     0.0894446   -0.195734    -0.123952     -0.0400968   -0.0649948     0.148265     0.0187502     0.0811043   -0.20878kind full, method split
┌ Info: 0: avll = 
└   tll[1] = -1.4247554090529504
[ Info: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.424776
[ Info: iteration 2, average log likelihood -1.424713
[ Info: iteration 3, average log likelihood -1.424666
[ Info: iteration 4, average log likelihood -1.424610
[ Info: iteration 5, average log likelihood -1.424541
[ Info: iteration 6, average log likelihood -1.424450
[ Info: iteration 7, average log likelihood -1.424319
[ Info: iteration 8, average log likelihood -1.424100
[ Info: iteration 9, average log likelihood -1.423695
[ Info: iteration 10, average log likelihood -1.422969
[ Info: iteration 11, average log likelihood -1.421907
[ Info: iteration 12, average log likelihood -1.420810
[ Info: iteration 13, average log likelihood -1.420055
[ Info: iteration 14, average log likelihood -1.419682
[ Info: iteration 15, average log likelihood -1.419528
[ Info: iteration 16, average log likelihood -1.419468
[ Info: iteration 17, average log likelihood -1.419444
[ Info: iteration 18, average log likelihood -1.419434
[ Info: iteration 19, average log likelihood -1.419430
[ Info: iteration 20, average log likelihood -1.419429
[ Info: iteration 21, average log likelihood -1.419428
[ Info: iteration 22, average log likelihood -1.419427
[ Info: iteration 23, average log likelihood -1.419427
[ Info: iteration 24, average log likelihood -1.419427
[ Info: iteration 25, average log likelihood -1.419427
[ Info: iteration 26, average log likelihood -1.419427
[ Info: iteration 27, average log likelihood -1.419427
[ Info: iteration 28, average log likelihood -1.419426
[ Info: iteration 29, average log likelihood -1.419426
[ Info: iteration 30, average log likelihood -1.419426
[ Info: iteration 31, average log likelihood -1.419426
[ Info: iteration 32, average log likelihood -1.419426
[ Info: iteration 33, average log likelihood -1.419426
[ Info: iteration 34, average log likelihood -1.419426
[ Info: iteration 35, average log likelihood -1.419426
[ Info: iteration 36, average log likelihood -1.419426
[ Info: iteration 37, average log likelihood -1.419426
[ Info: iteration 38, average log likelihood -1.419426
[ Info: iteration 39, average log likelihood -1.419426
[ Info: iteration 40, average log likelihood -1.419426
[ Info: iteration 41, average log likelihood -1.419426
[ Info: iteration 42, average log likelihood -1.419426
[ Info: iteration 43, average log likelihood -1.419426
[ Info: iteration 44, average log likelihood -1.419426
[ Info: iteration 45, average log likelihood -1.419426
[ Info: iteration 46, average log likelihood -1.419426
[ Info: iteration 47, average log likelihood -1.419426
[ Info: iteration 48, average log likelihood -1.419426
[ Info: iteration 49, average log likelihood -1.419426
[ Info: iteration 50, average log likelihood -1.419426
┌ Info: EM with 100000 data points 50 iterations avll -1.419426
└ 952.4 data points per parameter
┌ Info: 1
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4247756447057973
│     -1.4247125621752612
│      ⋮
└     -1.419425514142056
[ Info: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.419446
[ Info: iteration 2, average log likelihood -1.419380
[ Info: iteration 3, average log likelihood -1.419332
[ Info: iteration 4, average log likelihood -1.419275
[ Info: iteration 5, average log likelihood -1.419207
[ Info: iteration 6, average log likelihood -1.419129
[ Info: iteration 7, average log likelihood -1.419045
[ Info: iteration 8, average log likelihood -1.418964
[ Info: iteration 9, average log likelihood -1.418890
[ Info: iteration 10, average log likelihood -1.418827
[ Info: iteration 11, average log likelihood -1.418772
[ Info: iteration 12, average log likelihood -1.418722
[ Info: iteration 13, average log likelihood -1.418674
[ Info: iteration 14, average log likelihood -1.418625
[ Info: iteration 15, average log likelihood -1.418574
[ Info: iteration 16, average log likelihood -1.418522
[ Info: iteration 17, average log likelihood -1.418470
[ Info: iteration 18, average log likelihood -1.418419
[ Info: iteration 19, average log likelihood -1.418372
[ Info: iteration 20, average log likelihood -1.418331
[ Info: iteration 21, average log likelihood -1.418296
[ Info: iteration 22, average log likelihood -1.418268
[ Info: iteration 23, average log likelihood -1.418246
[ Info: iteration 24, average log likelihood -1.418230
[ Info: iteration 25, average log likelihood -1.418217
[ Info: iteration 26, average log likelihood -1.418208
[ Info: iteration 27, average log likelihood -1.418200
[ Info: iteration 28, average log likelihood -1.418195
[ Info: iteration 29, average log likelihood -1.418190
[ Info: iteration 30, average log likelihood -1.418187
[ Info: iteration 31, average log likelihood -1.418184
[ Info: iteration 32, average log likelihood -1.418182
[ Info: iteration 33, average log likelihood -1.418180
[ Info: iteration 34, average log likelihood -1.418178
[ Info: iteration 35, average log likelihood -1.418176
[ Info: iteration 36, average log likelihood -1.418175
[ Info: iteration 37, average log likelihood -1.418174
[ Info: iteration 38, average log likelihood -1.418173
[ Info: iteration 39, average log likelihood -1.418172
[ Info: iteration 40, average log likelihood -1.418171
[ Info: iteration 41, average log likelihood -1.418170
[ Info: iteration 42, average log likelihood -1.418169
[ Info: iteration 43, average log likelihood -1.418168
[ Info: iteration 44, average log likelihood -1.418168
[ Info: iteration 45, average log likelihood -1.418167
[ Info: iteration 46, average log likelihood -1.418166
[ Info: iteration 47, average log likelihood -1.418166
[ Info: iteration 48, average log likelihood -1.418165
[ Info: iteration 49, average log likelihood -1.418165
[ Info: iteration 50, average log likelihood -1.418164
┌ Info: EM with 100000 data points 50 iterations avll -1.418164
└ 473.9 data points per parameter
┌ Info: 2
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4194455025222128
│     -1.4193803228004758
│      ⋮
└     -1.4181641805140404
[ Info: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.418176
[ Info: iteration 2, average log likelihood -1.418127
[ Info: iteration 3, average log likelihood -1.418086
[ Info: iteration 4, average log likelihood -1.418038
[ Info: iteration 5, average log likelihood -1.417980
[ Info: iteration 6, average log likelihood -1.417909
[ Info: iteration 7, average log likelihood -1.417827
[ Info: iteration 8, average log likelihood -1.417736
[ Info: iteration 9, average log likelihood -1.417643
[ Info: iteration 10, average log likelihood -1.417553
[ Info: iteration 11, average log likelihood -1.417471
[ Info: iteration 12, average log likelihood -1.417399
[ Info: iteration 13, average log likelihood -1.417337
[ Info: iteration 14, average log likelihood -1.417283
[ Info: iteration 15, average log likelihood -1.417237
[ Info: iteration 16, average log likelihood -1.417197
[ Info: iteration 17, average log likelihood -1.417161
[ Info: iteration 18, average log likelihood -1.417129
[ Info: iteration 19, average log likelihood -1.417100
[ Info: iteration 20, average log likelihood -1.417073
[ Info: iteration 21, average log likelihood -1.417049
[ Info: iteration 22, average log likelihood -1.417026
[ Info: iteration 23, average log likelihood -1.417006
[ Info: iteration 24, average log likelihood -1.416986
[ Info: iteration 25, average log likelihood -1.416969
[ Info: iteration 26, average log likelihood -1.416953
[ Info: iteration 27, average log likelihood -1.416938
[ Info: iteration 28, average log likelihood -1.416924
[ Info: iteration 29, average log likelihood -1.416911
[ Info: iteration 30, average log likelihood -1.416899
[ Info: iteration 31, average log likelihood -1.416888
[ Info: iteration 32, average log likelihood -1.416878
[ Info: iteration 33, average log likelihood -1.416868
[ Info: iteration 34, average log likelihood -1.416859
[ Info: iteration 35, average log likelihood -1.416850
[ Info: iteration 36, average log likelihood -1.416842
[ Info: iteration 37, average log likelihood -1.416834
[ Info: iteration 38, average log likelihood -1.416827
[ Info: iteration 39, average log likelihood -1.416820
[ Info: iteration 40, average log likelihood -1.416813
[ Info: iteration 41, average log likelihood -1.416807
[ Info: iteration 42, average log likelihood -1.416801
[ Info: iteration 43, average log likelihood -1.416795
[ Info: iteration 44, average log likelihood -1.416789
[ Info: iteration 45, average log likelihood -1.416784
[ Info: iteration 46, average log likelihood -1.416778
[ Info: iteration 47, average log likelihood -1.416773
[ Info: iteration 48, average log likelihood -1.416769
[ Info: iteration 49, average log likelihood -1.416764
[ Info: iteration 50, average log likelihood -1.416760
┌ Info: EM with 100000 data points 50 iterations avll -1.416760
└ 236.4 data points per parameter
┌ Info: 3
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4181757788712068
│     -1.418126872671997
│      ⋮
└     -1.4167597305261619
[ Info: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.416766
[ Info: iteration 2, average log likelihood -1.416710
[ Info: iteration 3, average log likelihood -1.416662
[ Info: iteration 4, average log likelihood -1.416608
[ Info: iteration 5, average log likelihood -1.416543
[ Info: iteration 6, average log likelihood -1.416465
[ Info: iteration 7, average log likelihood -1.416374
[ Info: iteration 8, average log likelihood -1.416273
[ Info: iteration 9, average log likelihood -1.416168
[ Info: iteration 10, average log likelihood -1.416064
[ Info: iteration 11, average log likelihood -1.415966
[ Info: iteration 12, average log likelihood -1.415875
[ Info: iteration 13, average log likelihood -1.415793
[ Info: iteration 14, average log likelihood -1.415718
[ Info: iteration 15, average log likelihood -1.415649
[ Info: iteration 16, average log likelihood -1.415586
[ Info: iteration 17, average log likelihood -1.415528
[ Info: iteration 18, average log likelihood -1.415474
[ Info: iteration 19, average log likelihood -1.415423
[ Info: iteration 20, average log likelihood -1.415377
[ Info: iteration 21, average log likelihood -1.415333
[ Info: iteration 22, average log likelihood -1.415293
[ Info: iteration 23, average log likelihood -1.415254
[ Info: iteration 24, average log likelihood -1.415219
[ Info: iteration 25, average log likelihood -1.415185
[ Info: iteration 26, average log likelihood -1.415154
[ Info: iteration 27, average log likelihood -1.415124
[ Info: iteration 28, average log likelihood -1.415095
[ Info: iteration 29, average log likelihood -1.415069
[ Info: iteration 30, average log likelihood -1.415043
[ Info: iteration 31, average log likelihood -1.415019
[ Info: iteration 32, average log likelihood -1.414996
[ Info: iteration 33, average log likelihood -1.414974
[ Info: iteration 34, average log likelihood -1.414952
[ Info: iteration 35, average log likelihood -1.414932
[ Info: iteration 36, average log likelihood -1.414912
[ Info: iteration 37, average log likelihood -1.414893
[ Info: iteration 38, average log likelihood -1.414875
[ Info: iteration 39, average log likelihood -1.414857
[ Info: iteration 40, average log likelihood -1.414840
[ Info: iteration 41, average log likelihood -1.414823
[ Info: iteration 42, average log likelihood -1.414807
[ Info: iteration 43, average log likelihood -1.414791
[ Info: iteration 44, average log likelihood -1.414775
[ Info: iteration 45, average log likelihood -1.414761
[ Info: iteration 46, average log likelihood -1.414746
[ Info: iteration 47, average log likelihood -1.414732
[ Info: iteration 48, average log likelihood -1.414718
[ Info: iteration 49, average log likelihood -1.414705
[ Info: iteration 50, average log likelihood -1.414692
┌ Info: EM with 100000 data points 50 iterations avll -1.414692
└ 118.1 data points per parameter
┌ Info: 4
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4167656551700951
│     -1.4167098926040265
│      ⋮
└     -1.4146917750765295
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.414688
[ Info: iteration 2, average log likelihood -1.414614
[ Info: iteration 3, average log likelihood -1.414542
[ Info: iteration 4, average log likelihood -1.414459
[ Info: iteration 5, average log likelihood -1.414356
[ Info: iteration 6, average log likelihood -1.414230
[ Info: iteration 7, average log likelihood -1.414082
[ Info: iteration 8, average log likelihood -1.413919
[ Info: iteration 9, average log likelihood -1.413749
[ Info: iteration 10, average log likelihood -1.413581
[ Info: iteration 11, average log likelihood -1.413422
[ Info: iteration 12, average log likelihood -1.413277
[ Info: iteration 13, average log likelihood -1.413147
[ Info: iteration 14, average log likelihood -1.413031
[ Info: iteration 15, average log likelihood -1.412929
[ Info: iteration 16, average log likelihood -1.412838
[ Info: iteration 17, average log likelihood -1.412757
[ Info: iteration 18, average log likelihood -1.412684
[ Info: iteration 19, average log likelihood -1.412619
[ Info: iteration 20, average log likelihood -1.412561
[ Info: iteration 21, average log likelihood -1.412508
[ Info: iteration 22, average log likelihood -1.412461
[ Info: iteration 23, average log likelihood -1.412419
[ Info: iteration 24, average log likelihood -1.412381
[ Info: iteration 25, average log likelihood -1.412346
[ Info: iteration 26, average log likelihood -1.412314
[ Info: iteration 27, average log likelihood -1.412284
[ Info: iteration 28, average log likelihood -1.412257
[ Info: iteration 29, average log likelihood -1.412231
[ Info: iteration 30, average log likelihood -1.412207
[ Info: iteration 31, average log likelihood -1.412184
[ Info: iteration 32, average log likelihood -1.412162
[ Info: iteration 33, average log likelihood -1.412141
[ Info: iteration 34, average log likelihood -1.412121
[ Info: iteration 35, average log likelihood -1.412101
[ Info: iteration 36, average log likelihood -1.412082
[ Info: iteration 37, average log likelihood -1.412064
[ Info: iteration 38, average log likelihood -1.412047
[ Info: iteration 39, average log likelihood -1.412030
[ Info: iteration 40, average log likelihood -1.412013
[ Info: iteration 41, average log likelihood -1.411997
[ Info: iteration 42, average log likelihood -1.411982
[ Info: iteration 43, average log likelihood -1.411967
[ Info: iteration 44, average log likelihood -1.411952
[ Info: iteration 45, average log likelihood -1.411938
[ Info: iteration 46, average log likelihood -1.411925
[ Info: iteration 47, average log likelihood -1.411912
[ Info: iteration 48, average log likelihood -1.411899
[ Info: iteration 49, average log likelihood -1.411887
[ Info: iteration 50, average log likelihood -1.411875
┌ Info: EM with 100000 data points 50 iterations avll -1.411875
└ 59.0 data points per parameter
┌ Info: 5
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4146880731810845
│     -1.4146137443407567
│      ⋮
└     -1.4118746077527489
┌ Info: Total log likelihood: 
│   tll =
│    251-element Array{Float64,1}:
│     -1.4247554090529504
│     -1.4247756447057973
│     -1.4247125621752612
│     -1.4246658106935193
│      ⋮
│     -1.4118988117195588
│     -1.4118865119072848
└     -1.4118746077527489
32×26 Array{Float64,2}:
 -0.106372   -0.181802   -0.185732   -0.0463296  -0.138941   -0.0141468   -0.248647    0.692059   -0.637565     0.0966353   0.386471    -0.471358    -0.490741    0.0893904   -0.349042   -0.195922    0.330541   -0.237391    0.464787    0.0973746    0.666231    0.00354289  -0.226826    -0.0741681  -0.735225    0.093737
 -0.11426    -0.0674306  -0.924186   -0.0284482  -0.0659272   0.242785     0.484587    0.0851833   0.446052     0.170872    0.258575     0.34144     -0.851042    0.774207    -0.429534    0.436618    0.843925   -0.476007   -0.129842    0.431018     0.014301    0.224413     0.408459    -0.614308   -0.195197   -0.135715
 -0.0906124  -0.387512   -0.0227365  -0.0977464   0.70655     0.333552    -0.0505465   0.26108     0.453531    -0.0240921  -0.881821    -0.169561    -0.180023    0.180778    -0.171251    0.652384    0.433529   -0.324136    0.0929559  -0.116774     0.356506   -0.481335    -0.566964     0.185605    0.163772    0.671281
 -0.0159535  -0.601167   -0.469169   -0.585289   -0.0927552  -0.0492667   -0.289651    0.460324    0.388927     0.0557486   0.510227    -0.161456     0.343841   -0.053831    -0.203732    0.0495973   0.0432589   0.565799    0.213599    0.425408    -0.552901   -0.243692    -0.290077    -0.636177    0.50074     0.133324
 -0.233656    0.203163   -0.0231871  -0.135547    0.142412    0.217405    -0.656851   -0.0968542   0.00313987  -0.112204    0.711981     0.411521     0.0855022  -0.247757    -0.579439   -1.20131    -0.169018    0.170782    0.292615   -0.193804     0.092997    0.753267     0.0600231    0.41525     0.154061    0.153254
 -0.420328    0.164322   -0.434147   -0.451667    1.35115     0.118695    -0.569462   -0.890896   -0.246478    -0.194514    0.244141    -0.0303217    0.185797    0.0538917   -0.63744    -0.535971    0.452345   -0.178512   -0.0272561   0.417596    -0.163021   -0.356295     0.00964927   0.24054    -0.168058    0.121551
 -0.326535   -0.254271   -0.274932   -0.17035    -0.0744246  -0.0483138    0.0358578  -0.673573    0.145655     0.910465   -0.365745     0.682868    -0.162534    0.113264     0.434687   -0.0913031   0.390401   -0.353887    0.0527833  -0.825226     0.371834    0.433066    -0.159717    -0.232065   -0.197977    0.115152
  0.222071    0.374531   -0.848675   -0.218512    0.736005   -0.352404     0.214842    0.0495602   0.179886     0.725173    0.286143    -0.275192     0.0773672   0.436999     0.27615    -0.485598    0.599235   -0.0968349  -0.142576    0.0664225    0.0307843  -0.0387151    0.585405     0.302443   -0.520587   -0.212879
 -0.050515   -0.292211   -0.141302    0.0496915  -0.274608    0.106378    -0.0468677  -0.017725   -0.594125     0.239323    0.00407869   0.504944    -0.192205   -0.0287299   -0.230942    0.0629487  -0.926978    0.225938   -0.465985   -0.246171     0.429578   -0.385681     0.171267     0.668779   -0.523053    0.668459
  0.0175383   0.0461916  -0.0538364  -0.127206   -0.510973   -0.229473    -0.193861    0.125419    0.696146     0.532387   -0.271014    -0.281656     0.133438    0.379057     0.27074     0.0845797  -1.09821     0.182335   -0.394363   -0.0187298   -0.0838317  -0.0937759   -0.283896     0.0782597   0.0787519   0.672611
  0.139871    0.0129014   0.430888    0.0168886  -1.1014     -0.00207835   0.411139   -0.0283195   0.0680978   -0.483655   -0.0151334    0.433823    -0.41939    -0.202046    -0.334887    0.108414   -0.241406   -0.048039   -0.111253    0.163624     0.302786    0.503681    -0.404723    -0.491908    0.218113    0.254762
  0.250237    0.0241728   0.352055    0.551343   -0.966305   -0.00726138   0.389442    0.514478    0.211227     0.0758443  -0.112853    -0.00775858  -0.0517752   0.384776     0.938172    0.695987   -0.555156    0.18039     0.104038   -0.265793     0.161308    0.234804     0.106526    -0.44834    -0.0840327  -0.246092
  0.308926   -0.204839    0.893147    0.549133   -0.521741   -0.533001    -0.319005   -0.359452   -0.951303    -0.807781    0.0873826   -0.0668658    0.650427   -0.382899     0.248969    0.471101   -0.167214    0.228073    0.14855    -0.453463    -0.272955   -0.201525    -0.12334      0.350991    0.299696   -0.244518
  0.0180129  -0.0277788   0.0479585   0.0596523   0.108822    0.0684291    0.0405395  -0.0270407  -0.279095    -0.0979      0.117326     0.197844    -0.0866851   0.0357292   -0.0995676  -0.129273    0.200867   -0.0225299   0.108996    0.0330649    0.0143535   0.150793     0.162576     0.119253   -0.066738   -0.280258
  0.15421     0.403332    0.256504    0.679045    0.0194128   0.445249    -0.501946    0.610261   -0.133636    -0.808272    0.345073    -0.305022    -0.225633   -0.137258    -0.255124   -0.0740654  -0.214882    0.286428   -0.476977    0.519746    -0.271732   -0.632407     0.0778549   -0.0116619   0.581342   -0.281604
  0.14859     0.438967    0.20319     0.13169     0.119352   -0.142334     0.169934    0.132251    0.358229    -0.217907   -0.174514    -0.370858     0.2134      0.211762     0.359748   -0.430561    0.970334   -0.213761    0.343927    0.287074    -0.262715    0.386319    -0.225546    -0.568704    0.595289   -0.586724
 -0.28538     0.0744773   0.0752288   0.311629    0.309929   -0.0201493   -0.179453   -0.0568526  -0.0572498    0.0379594  -0.185193     0.259599    -0.183995    0.244128     0.302452    0.12408     0.129065   -0.0794156   0.139281   -0.479911     0.0593987  -0.0325117   -0.0946212    0.167763   -0.0530141  -0.132131
 -0.029055    0.0348777  -0.0649599   0.0055272  -0.0394648  -0.0393338    0.0529007  -0.150398    0.015075     0.0083577   0.0533781    0.105201     0.100895   -0.0131791   -0.0482924   0.0355491  -0.034947    0.0377446  -0.129313    0.0396568   -0.0030975   0.069502     0.174251     0.0152946  -0.0502448   0.022841
  0.252493    0.211682    0.120267    0.221755    0.150251    0.333412     0.0481548   0.182461   -0.259967    -0.637752   -0.160256     0.288498    -0.340435    0.444287     0.107211   -0.106118   -0.204966    0.0151264   0.279      -0.0286729    0.164059   -0.346653    -0.845185     0.290944    0.140103    0.13582
  0.103738    0.23027     0.183999    0.450008    0.322134    0.0903924    0.0833495  -0.304651   -0.633412    -0.358437   -0.0184493    0.440558    -0.403307    0.108532    -0.174884   -0.306698    0.368186   -0.213685   -0.0854382  -0.00122923   0.286963    0.110315     0.564567     0.524624    0.0141424  -0.514951
 -0.243928   -0.0117547   0.116223    0.147841   -0.187682    0.345407     0.0570902   0.362747   -0.0106988   -0.168196    0.527149    -0.206089    -0.52689    -0.00705096  -0.0804413  -0.207788    0.185174    0.173331    0.140818    0.248995    -0.172011    0.122352    -0.0268521   -0.149736   -0.199931   -0.265114
  0.419919   -0.173724    0.40658    -0.0206312  -0.463349    0.0149848   -0.107777    0.384367   -0.00869829  -0.364261    0.104924    -0.344718     0.215116   -0.113268    -0.107775    0.204829   -0.345752    0.070714   -0.0584994   0.0754464   -0.14518     0.0358954   -0.204491     0.122331    0.039626    0.193533
  0.506189   -0.233424    0.17261    -0.481355   -0.656595   -0.253931     0.572699    0.0341813   0.334471    -0.273852    0.62688     -0.833613     0.228      -0.540204    -0.134293   -0.211706   -0.157785   -0.0956924  -0.0401461  -0.146078    -0.662574   -0.0597147    0.317567    -0.277054   -0.149707    0.242614
 -0.118759   -0.158701    0.120386    0.109591   -0.368837   -0.292522     0.183681   -0.350239   -0.338783     0.0345731   0.204438     0.286647     0.53772    -0.312987    -0.159017   -0.0354085   0.068248    0.487559    0.0768148   0.239439    -0.039532    0.33596      0.897201    -0.345054   -0.188035   -0.390362
 -0.157719   -0.495546   -0.0225183  -0.879812    0.469199    0.334629    -0.14801    -0.142622   -0.0971081    0.154688    0.298163     0.190691     0.132298   -0.481899    -0.484279    0.290629    0.161247   -0.399196    0.208961   -0.0853507   -0.03614    -0.214086     0.22502      0.0922392  -0.286918    0.120018
 -0.411928   -0.163622    0.12433     0.393354    0.201056   -0.148153    -0.452907    0.192576    0.254387     0.303747   -0.0494269   -0.163736     0.452609    0.24319     -0.49654     0.552191    0.109776   -0.098572   -0.45752    -0.0982615   -0.136705    0.0634268    0.503947     0.102582   -0.524115    0.0520636
 -0.274993   -0.313847   -0.134958   -0.119803   -0.104403   -0.171885    -0.380148    0.106857    0.260043    -0.207828   -0.0847339   -0.452452     0.805455   -0.244802     0.474036    0.696543    0.0812197   0.123521    0.385884   -0.262746    -0.342004   -0.171219    -0.189541    -0.818336   -0.0556146  -0.0238352
 -0.0920617  -0.150229    0.0658102  -0.153448    0.385217    0.0514502   -0.492284   -0.0522216  -0.0530634    0.152436   -0.503665    -0.157193     0.511793   -0.442253     0.787618   -0.0355258  -0.405355    0.394167    0.22984    -0.203533    -0.134808    0.141541    -0.0675511    0.801347   -0.0027068  -0.192799
  0.0028854  -0.040646   -0.309914   -0.252682    0.192054   -0.207576    -0.289164   -0.116087    0.472113     0.319603   -0.0973228    0.082476     0.250206    0.24828     -0.0776953  -0.0581093   0.144056   -0.141195   -0.234323    0.198172    -0.238722    0.0845496   -0.19276     -0.104513    0.454888    0.314826
  0.359873    0.0783565   0.0183147  -0.306142    0.078379   -0.146445     0.625645   -0.0206202  -0.0822626    0.170823   -0.240481    -0.21788     -0.181624    0.080902     0.0478493  -0.331146    0.210362    0.136584    0.138182    0.147044     0.362453    0.0569396   -0.114128    -0.212045    0.239781    0.478271
 -0.100185    0.107408    0.179433    0.253698    0.117791   -0.0803354    0.410321   -0.623514    0.607888    -0.274378   -0.293994     0.262693    -0.029909    0.0790938    0.43802     0.176412   -0.229359    0.387736   -0.467879    0.187372    -0.820095   -0.013387     0.225769     0.216098    0.554528   -0.342384
  0.113024    1.13763     0.268883    0.0585305   0.262635   -0.056135     0.374365   -0.736594    0.147768     0.138269   -0.0727179    0.597558     0.592856   -0.264312     0.564488   -0.0773628  -0.158971   -0.170292   -1.01999     0.0310265   -0.0870346   0.184721     0.0224812    0.214102    0.529797    0.423026[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.411863
[ Info: iteration 2, average log likelihood -1.411852
[ Info: iteration 3, average log likelihood -1.411841
[ Info: iteration 4, average log likelihood -1.411831
[ Info: iteration 5, average log likelihood -1.411821
[ Info: iteration 6, average log likelihood -1.411811
[ Info: iteration 7, average log likelihood -1.411801
[ Info: iteration 8, average log likelihood -1.411792
[ Info: iteration 9, average log likelihood -1.411783
[ Info: iteration 10, average log likelihood -1.411774
┌ Info: EM with 100000 data points 10 iterations avll -1.411774
└ 59.0 data points per parameter
[ Info: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
kind full, method kmeans
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.369462e+05
      1       7.073888e+05      -2.295574e+05 |       32
      2       6.940607e+05      -1.332819e+04 |       32
      3       6.883283e+05      -5.732374e+03 |       32
      4       6.854185e+05      -2.909830e+03 |       32
      5       6.835194e+05      -1.899052e+03 |       32
      6       6.821672e+05      -1.352225e+03 |       32
      7       6.811007e+05      -1.066440e+03 |       32
      8       6.802669e+05      -8.338546e+02 |       32
      9       6.796056e+05      -6.613165e+02 |       32
     10       6.790676e+05      -5.379638e+02 |       32
     11       6.785773e+05      -4.903463e+02 |       32
     12       6.781250e+05      -4.522740e+02 |       32
     13       6.777274e+05      -3.975768e+02 |       32
     14       6.773755e+05      -3.519094e+02 |       32
     15       6.770525e+05      -3.230022e+02 |       32
     16       6.767782e+05      -2.743075e+02 |       32
     17       6.765487e+05      -2.294750e+02 |       32
     18       6.763374e+05      -2.113363e+02 |       32
     19       6.761338e+05      -2.036163e+02 |       32
     20       6.759637e+05      -1.700379e+02 |       32
     21       6.758030e+05      -1.606860e+02 |       32
     22       6.756405e+05      -1.625505e+02 |       32
     23       6.754930e+05      -1.474816e+02 |       32
     24       6.753682e+05      -1.247836e+02 |       32
     25       6.752584e+05      -1.097715e+02 |       32
     26       6.751410e+05      -1.174164e+02 |       32
     27       6.750258e+05      -1.151808e+02 |       32
     28       6.749225e+05      -1.033343e+02 |       32
     29       6.748245e+05      -9.806386e+01 |       32
     30       6.747322e+05      -9.229355e+01 |       32
     31       6.746378e+05      -9.437699e+01 |       32
     32       6.745631e+05      -7.470322e+01 |       32
     33       6.744981e+05      -6.493971e+01 |       32
     34       6.744371e+05      -6.107989e+01 |       32
     35       6.743859e+05      -5.113160e+01 |       32
     36       6.743410e+05      -4.491071e+01 |       32
     37       6.743011e+05      -3.990946e+01 |       32
     38       6.742577e+05      -4.338979e+01 |       32
     39       6.742160e+05      -4.168869e+01 |       32
     40       6.741721e+05      -4.393629e+01 |       32
     41       6.741308e+05      -4.129264e+01 |       32
     42       6.740911e+05      -3.974511e+01 |       32
     43       6.740560e+05      -3.504016e+01 |       32
     44       6.740228e+05      -3.321729e+01 |       32
     45       6.739880e+05      -3.474591e+01 |       32
     46       6.739541e+05      -3.399790e+01 |       32
     47       6.739165e+05      -3.757910e+01 |       32
     48       6.738792e+05      -3.726853e+01 |       32
     49       6.738490e+05      -3.019074e+01 |       32
     50       6.738254e+05      -2.363072e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 673825.3822830141)
┌ Info: K-means with 32000 data points using 50 iterations
└ 37.0 data points per parameter
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.424067
[ Info: iteration 2, average log likelihood -1.418989
[ Info: iteration 3, average log likelihood -1.417673
[ Info: iteration 4, average log likelihood -1.416763
[ Info: iteration 5, average log likelihood -1.415815
[ Info: iteration 6, average log likelihood -1.414860
[ Info: iteration 7, average log likelihood -1.414094
[ Info: iteration 8, average log likelihood -1.413602
[ Info: iteration 9, average log likelihood -1.413310
[ Info: iteration 10, average log likelihood -1.413124
[ Info: iteration 11, average log likelihood -1.412990
[ Info: iteration 12, average log likelihood -1.412883
[ Info: iteration 13, average log likelihood -1.412794
[ Info: iteration 14, average log likelihood -1.412716
[ Info: iteration 15, average log likelihood -1.412647
[ Info: iteration 16, average log likelihood -1.412584
[ Info: iteration 17, average log likelihood -1.412528
[ Info: iteration 18, average log likelihood -1.412476
[ Info: iteration 19, average log likelihood -1.412428
[ Info: iteration 20, average log likelihood -1.412384
[ Info: iteration 21, average log likelihood -1.412343
[ Info: iteration 22, average log likelihood -1.412305
[ Info: iteration 23, average log likelihood -1.412269
[ Info: iteration 24, average log likelihood -1.412236
[ Info: iteration 25, average log likelihood -1.412205
[ Info: iteration 26, average log likelihood -1.412176
[ Info: iteration 27, average log likelihood -1.412149
[ Info: iteration 28, average log likelihood -1.412124
[ Info: iteration 29, average log likelihood -1.412100
[ Info: iteration 30, average log likelihood -1.412078
[ Info: iteration 31, average log likelihood -1.412057
[ Info: iteration 32, average log likelihood -1.412037
[ Info: iteration 33, average log likelihood -1.412019
[ Info: iteration 34, average log likelihood -1.412001
[ Info: iteration 35, average log likelihood -1.411985
[ Info: iteration 36, average log likelihood -1.411969
[ Info: iteration 37, average log likelihood -1.411955
[ Info: iteration 38, average log likelihood -1.411940
[ Info: iteration 39, average log likelihood -1.411927
[ Info: iteration 40, average log likelihood -1.411914
[ Info: iteration 41, average log likelihood -1.411902
[ Info: iteration 42, average log likelihood -1.411891
[ Info: iteration 43, average log likelihood -1.411880
[ Info: iteration 44, average log likelihood -1.411869
[ Info: iteration 45, average log likelihood -1.411859
[ Info: iteration 46, average log likelihood -1.411849
[ Info: iteration 47, average log likelihood -1.411839
[ Info: iteration 48, average log likelihood -1.411830
[ Info: iteration 49, average log likelihood -1.411821
[ Info: iteration 50, average log likelihood -1.411813
┌ Info: EM with 100000 data points 50 iterations avll -1.411813
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
  0.297204    0.449718    0.178604    -0.00188833   0.192806    -0.0324842    0.0987622    0.0634955   0.34232     -0.2041     -0.0806329   -0.230939    0.408815     0.158421    0.256702   -0.428492    1.06073     -0.550669     0.40088      0.306549   -0.319973    0.489148    -0.18067    -0.577064    0.481627    -0.483921
  0.0264955  -0.357131    0.13236      0.135356    -0.00279867   0.075413    -0.00558032   0.0468879   0.00150924  -0.138956   -0.598274    -0.287273    0.632977     0.0610367   0.70574     0.940117   -0.165401     0.0660169    0.465338    -0.513143    0.133778   -0.237654    -0.144924   -0.241441   -0.313354     0.037451
  0.265641    0.238832   -1.0136      -0.130652     0.647855    -0.473616     0.249076     0.267998    0.370219     0.889076    0.147414    -0.583802    0.293822     0.533605    0.375278   -0.281445    0.418906    -0.0430274   -0.221471     0.0159192  -0.271589   -0.0850687    0.635222    0.576235   -0.73914     -0.0511418
 -0.104577   -0.827654   -0.27087     -0.364184    -0.00768085  -0.0872557   -0.596191     0.216726    0.236366    -0.153487    0.425695    -0.138484    0.698599    -0.145809    0.0306828   0.216977    0.174811     0.570282     0.263205     0.18322    -1.09691    -0.164036    -0.0504936  -0.449908    0.371519    -0.209907
 -0.281179   -0.168393   -0.342759    -0.189852    -0.231987    -0.0745044   -0.229517     0.319841    0.0702092    0.259526    0.0831159    0.267323    0.114554     0.474659   -0.745135    0.1499      0.394875    -0.0318       0.00124181  -0.0220206   0.529948    0.285341     0.128545   -0.525342   -0.440946     0.130347
  0.0741192   0.0141312   0.440041     0.484388    -1.07069      0.00510263   0.292313     0.62749     0.423223    -0.0487128   0.0891001    0.207463   -0.282873     0.386874    0.471507    0.682516   -0.526907     0.14063     -0.0336215   -0.090819    0.0146821   0.386113     0.114933   -0.462973    0.0988619   -0.200398
  0.229176    0.0325627   0.373976     0.113316    -0.282502     0.135808     0.072069    -0.125767   -0.449048    -0.374086    0.00711059   0.330999   -0.033884    -0.132368   -0.0103572  -0.147028   -0.21019      0.0699052    0.123802     0.0264264   0.31677     0.0158853    0.0158286   0.177127    0.0673922   -0.144554
 -0.084612   -0.205363    0.142554    -0.0584138    0.183785    -0.324948     0.0643709   -0.312361    0.611812     0.555201   -0.561141     0.259726    0.689507     0.106341   -0.255987    0.426254   -0.120942     0.0500205   -0.40927      0.0554901  -0.396705    0.277023     0.297082    0.175177    0.480819     0.47734
  0.55028     0.329354    0.146396    -0.216577     0.15105      0.0133426   -0.152154     0.121159   -0.466002     0.148322    0.583534     0.280477   -0.520887     0.11775    -0.395249   -1.2709      0.207126     0.321809     0.0189328    0.174958    0.379288    0.279435     0.192118    0.672282    0.391009     0.0544056
  0.496832   -0.221767    0.0900477   -0.612228    -0.919654    -0.223919     0.48689      0.11887     0.352967    -0.415242    0.316236    -0.463553    0.127996    -0.406671   -0.25009    -0.0932308  -0.362488     0.00390863  -0.10531      0.0524446  -0.285952   -0.0129266   -0.194023   -0.447808    0.228324     0.579429
  0.106892   -0.0514577  -0.139372    -0.172281    -0.0137121   -0.0780261    0.0902533   -0.0544971   0.152646     0.0314886   0.0908864   -0.038772    0.0468554    0.0193189  -0.0578396  -0.0494245   0.031647     0.0587723   -0.0641916    0.108434   -0.0690499   0.0854818   -0.0417884  -0.104878    0.111488     0.234298
  0.167578    0.0329656  -0.399547    -0.0872032    0.461176    -0.104216     0.609141    -0.147622    0.0603906   -0.105058    0.192338    -0.0383802  -0.57081      0.454518   -0.134055    0.16555     0.832642    -0.502525    -0.135509     0.0632444  -0.0494164  -0.317708     0.218167   -0.311235    0.202771    -0.138377
 -0.120285   -0.211407   -0.198185    -0.0694612    0.177147    -0.0177664   -0.00419307   0.100509    0.414584     0.185492   -0.170109    -0.151752   -0.12902      0.295626   -0.0502984   0.282198    0.290093    -0.223579    -0.0272777   -0.0429225   0.01338    -0.0207407   -0.15249    -0.136313   -0.0197981    0.134029
 -0.111969    0.0566863   0.0612548    0.12004     -0.34896     -0.124287     0.50304      0.19272     0.115916     0.122582   -0.0807598   -0.651068   -0.0925887   -0.113799    0.392129   -0.354298    0.401439     0.899469     0.40346      0.547297    0.215263    0.299829    -0.102809   -0.725754    0.111523    -0.146712
  0.173791    0.371092    0.177426     0.317437    -0.297401    -0.0755628    0.539529    -0.131691   -0.184251     0.109028   -0.535709     0.157356   -0.518711     0.202377    0.110018    0.0489837  -0.0481486   -0.394708    -0.374339    -0.248437    1.02385     0.0459473   -0.385414    0.188712   -0.234948     0.64748
  0.170018    0.601967    0.293029     0.28444      0.0907981   -0.206925     0.534107    -0.781868    0.26847     -0.321957   -0.26766      0.28814     0.233891    -0.0708195   0.798797   -0.0380474  -0.211441     0.284775    -0.69886      0.219738   -0.643796   -0.0215135    0.16954     0.235179    0.612303    -0.187564
  0.0715282   0.205631   -0.317326    -0.465651    -0.0639311   -0.268745    -0.578981    -0.01354     0.420597     0.738542   -0.46039     -0.230416    0.240093     0.108202    0.642021   -0.0141614  -0.237019    -0.496174    -0.0701569   -0.0856377   0.241222   -0.0633057   -0.31021    -0.242868    0.337505     0.372141
  0.227424   -0.173243    0.219596    -0.0932647    0.396359     0.480274     0.0156124    0.282777    0.166556    -0.526426   -0.413805     0.126659   -0.392402     0.23897     0.0634442   0.0139735  -0.11724      0.125509     0.346194     0.0620958  -0.102945   -0.228395    -0.861641    0.332191    0.459817     0.171259
  0.022282    0.0833369  -0.168017     0.179043    -0.574485    -0.410472    -0.0843341   -0.33801    -0.357085     0.16208     0.703188    -0.0286313   0.508123    -0.108037    0.342559    0.0502885  -0.508422     0.053551    -0.376452    -0.165255   -0.190496    0.213213     0.761387   -0.017475   -0.487818    -0.576965
 -0.423971   -0.198063    0.129525    -0.401219     0.19665      0.35047      0.113369    -0.48429     0.227078     0.095296    0.588766     0.0642669   0.0826411   -0.522307   -0.585691   -0.232541    0.143109    -0.0733081   -0.0321428    0.155761   -0.414775    0.361084     0.694291    0.0313691  -0.239231    -0.269886
 -0.113984   -0.24359     0.358939     0.0687221   -0.206978    -0.447384     0.308234    -0.307624   -0.709531     0.157678    0.412072     0.385526    0.453084    -0.738973   -0.34985     0.0221067   0.385263    -0.0153961    0.507184    -0.18731     0.129357    0.270204     0.615035   -0.213209   -0.361302    -0.181592
 -0.0165361  -0.278029    0.216574    -0.0853555   -0.355796     0.103853    -0.395237     0.0998045  -0.0152769    0.446009   -0.261678    -0.0139741   0.31174     -0.124351    0.194299   -0.211738   -1.28613      0.81296     -0.103089    -0.0784294   0.0829015   0.00513714  -0.227667    0.547798   -0.253378     0.525922
 -0.0687855  -0.283367   -0.202068    -0.160408     0.00271702   0.174651    -0.155544     0.749232   -0.667112    -0.093998    0.554124    -0.499226   -0.805676    -0.0172056  -0.21465    -0.15509     0.341447    -0.363861     0.59593      0.163575    0.329089   -0.0621655   -0.18938     0.133008   -0.699753    -0.0761528
 -0.154831    0.113706   -0.906157    -0.351704     0.0715419    0.884465     0.0546986    0.248302    0.14154      0.326326    0.395754     0.840317   -0.642303     0.0234408  -0.547651   -0.463553    0.0431005    0.117215    -0.21351      0.932512    0.421694   -0.447197     0.384428   -0.815265   -0.0465111   -0.167474
 -0.198007    0.20493     0.255128     0.985713     0.317704     0.0622717   -0.070911    -0.312637   -0.372668    -0.417252   -0.105214     0.287064   -0.355963     0.271983   -0.377459   -0.185167    0.583802     0.236668    -0.122438     0.15567    -0.0595721   0.255808     0.656683    0.406683   -0.276486    -0.614136
 -0.35156     0.0853087  -0.0164905    0.19639      0.291931    -0.0291288   -0.248615    -0.0708309   0.0942483    0.14031    -0.161757     0.158738    0.00755021   0.131226    0.350695    0.0700713   0.0955851   -0.0162728    0.0171734   -0.203303   -0.141091    0.0397625    0.0149257   0.230098   -0.0321729   -0.210268
  0.196009   -0.102444    0.513781     0.181053    -0.217352    -0.19023     -0.225684     0.486258   -0.0433527   -0.360277    0.186285    -0.726762    0.219514    -0.115838   -0.0901632   0.317906   -0.00918031   0.00604047  -0.101691     0.0024589  -0.530902   -0.0834021   -0.0291886   0.0677666  -0.103835    -0.0474688
 -0.328571   -0.182314   -0.347353    -0.044686     0.201151     0.15425     -0.0180597   -0.629463   -0.00842487   0.641776   -0.284399     0.825109   -0.330738     0.0765208   0.571568   -0.243403    0.390562    -0.140716     0.131664    -0.684196    0.210952    0.539689    -0.0183513   0.0825606  -0.00181991  -0.173535
 -0.228497   -0.328778    0.00733255  -0.0534762    0.163397     0.276028    -0.243867     0.0815466  -0.227005     0.0189865   0.199931     0.305237   -0.117612    -0.222687   -0.562824    0.386714   -0.467109    -0.0239221   -0.394294    -0.279542    0.0384526  -0.354082     0.226706    0.619772   -0.473065     0.51106
 -0.307346    0.0560443  -0.359826    -0.425702     1.24513      0.0926926   -0.565065    -0.540904   -0.234787    -0.0598285  -0.0415164    0.0149136   0.360204    -0.0652238  -0.483705   -0.374074    0.424216    -0.310595     0.101477     0.0925494   0.0155652  -0.228857    -0.0432343   0.364259   -0.126564     0.261077
 -0.341598    0.689951   -0.191565     0.520879     0.225373     0.10988     -0.366224     0.369101    0.356374    -0.220549    0.549461    -0.198373   -0.0687255    0.293379   -0.0832207  -0.332644   -0.383714     0.301294    -0.357284     0.290126   -0.166746   -0.400638    -0.371152   -0.236517    0.318216     0.297665
  0.2192      0.17851     0.591233     0.457527    -0.25272      0.10203     -0.308522     0.0754351  -0.551257    -1.09908     0.0987688    0.0313229   0.0717245   -0.158068   -0.0130045   0.112761   -0.166906     0.0680454   -0.0727687    0.0648124  -0.229091   -0.13774     -0.0321025   0.220595    0.467242    -0.457978[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.411804
[ Info: iteration 2, average log likelihood -1.411796
[ Info: iteration 3, average log likelihood -1.411788
[ Info: iteration 4, average log likelihood -1.411780
[ Info: iteration 5, average log likelihood -1.411773
[ Info: iteration 6, average log likelihood -1.411765
[ Info: iteration 7, average log likelihood -1.411758
[ Info: iteration 8, average log likelihood -1.411751
[ Info: iteration 9, average log likelihood -1.411744
[ Info: iteration 10, average log likelihood -1.411737
┌ Info: EM with 100000 data points 10 iterations avll -1.411737
└ 59.0 data points per parameter
[ Info: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
┌ Info: K-means with 900 data points using 3 iterations
└ 150.0 data points per parameter
[ Info: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.678561e+05
      1       2.230230e+04      -1.455538e+05 |        2
      2       7.823675e+03      -1.447862e+04 |        0
      3       7.823675e+03       0.000000e+00 |        0
K-means converged with 3 iterations (objv = 7823.67549422947)
[ Info: iteration 1, average log likelihood -2.043155
[ Info: iteration 2, average log likelihood -2.043154
[ Info: iteration 3, average log likelihood -2.043154
[ Info: iteration 4, average log likelihood -2.043154
[ Info: iteration 5, average log likelihood -2.043154
[ Info: iteration 6, average log likelihood -2.043154
[ Info: iteration 7, average log likelihood -2.043154
[ Info: iteration 8, average log likelihood -2.043154
[ Info: iteration 9, average log likelihood -2.043154
[ Info: iteration 10, average log likelihood -2.043154
┌ Info: EM with 900 data points 10 iterations avll -2.043154
└ 81.8 data points per parameter
   Testing GaussianMixtures tests passed 
