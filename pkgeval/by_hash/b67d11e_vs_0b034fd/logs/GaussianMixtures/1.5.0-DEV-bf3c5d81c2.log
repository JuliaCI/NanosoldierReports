Julia Version 1.5.0-DEV.11
Commit bf3c5d81c2 (2020-01-03 14:17 UTC)
Platform Info:
  OS: Linux (x86_64-linux-gnu)
  CPU: Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-8.0.1 (ORCJIT, skylake)
Environment:
  JULIA_DEPOT_PATH = ::/usr/local/share/julia

 Resolving package versions...
 Installed GaussianMixtures ─── v0.3.0
 Installed SortingAlgorithms ── v0.3.1
 Installed Distances ────────── v0.8.2
 Installed Rmath ────────────── v0.6.0
 Installed OrderedCollections ─ v1.1.0
 Installed FileIO ───────────── v1.2.1
 Installed StaticArrays ─────── v0.12.1
 Installed Clustering ───────── v0.13.3
 Installed DataAPI ──────────── v1.1.0
 Installed Missings ─────────── v0.4.3
 Installed URIParser ────────── v0.4.0
 Installed CMake ────────────── v1.1.2
 Installed PDMats ───────────── v0.9.10
 Installed BinDeps ──────────── v1.0.0
 Installed Arpack_jll ───────── v3.5.0+2
 Installed LegacyStrings ────── v0.4.1
 Installed Arpack ───────────── v0.4.0
 Installed Parameters ───────── v0.12.0
 Installed JLD ──────────────── v0.9.1
 Installed DataStructures ───── v0.17.7
 Installed ScikitLearnBase ──── v0.5.0
 Installed Distributions ────── v0.21.12
 Installed OpenSpecFun_jll ──── v0.5.3+1
 Installed Compat ───────────── v2.2.0
 Installed StatsBase ────────── v0.32.0
 Installed NearestNeighbors ─── v0.4.4
 Installed Blosc ────────────── v0.5.1
 Installed StatsFuns ────────── v0.9.3
 Installed HDF5 ─────────────── v0.12.5
 Installed QuadGK ───────────── v2.3.1
 Installed CMakeWrapper ─────── v0.2.3
 Installed OpenBLAS_jll ─────── v0.3.7+2
 Installed BinaryProvider ───── v0.5.8
 Installed FillArrays ───────── v0.8.2
 Installed SpecialFunctions ─── v0.9.0
  Updating `~/.julia/environments/v1.5/Project.toml`
  [cc18c42c] + GaussianMixtures v0.3.0
  Updating `~/.julia/environments/v1.5/Manifest.toml`
  [7d9fca2a] + Arpack v0.4.0
  [68821587] + Arpack_jll v3.5.0+2
  [9e28174c] + BinDeps v1.0.0
  [b99e7846] + BinaryProvider v0.5.8
  [a74b3585] + Blosc v0.5.1
  [631607c0] + CMake v1.1.2
  [d5fb7624] + CMakeWrapper v0.2.3
  [aaaa29a8] + Clustering v0.13.3
  [34da2185] + Compat v2.2.0
  [9a962f9c] + DataAPI v1.1.0
  [864edb3b] + DataStructures v0.17.7
  [b4f34e82] + Distances v0.8.2
  [31c24e10] + Distributions v0.21.12
  [5789e2e9] + FileIO v1.2.1
  [1a297f60] + FillArrays v0.8.2
  [cc18c42c] + GaussianMixtures v0.3.0
  [f67ccb44] + HDF5 v0.12.5
  [4138dd39] + JLD v0.9.1
  [1b4a561d] + LegacyStrings v0.4.1
  [e1d29d7a] + Missings v0.4.3
  [b8a86587] + NearestNeighbors v0.4.4
  [4536629a] + OpenBLAS_jll v0.3.7+2
  [efe28fd5] + OpenSpecFun_jll v0.5.3+1
  [bac558e1] + OrderedCollections v1.1.0
  [90014a1f] + PDMats v0.9.10
  [d96e819e] + Parameters v0.12.0
  [1fd47b50] + QuadGK v2.3.1
  [79098fc4] + Rmath v0.6.0
  [6e75b9c4] + ScikitLearnBase v0.5.0
  [a2af1166] + SortingAlgorithms v0.3.1
  [276daf66] + SpecialFunctions v0.9.0
  [90137ffa] + StaticArrays v0.12.1
  [2913bbd2] + StatsBase v0.32.0
  [4c63d2b9] + StatsFuns v0.9.3
  [30578b45] + URIParser v0.4.0
  [2a0f44e3] + Base64 
  [ade2ca70] + Dates 
  [8bb1440f] + DelimitedFiles 
  [8ba89e20] + Distributed 
  [b77e0a4c] + InteractiveUtils 
  [76f85450] + LibGit2 
  [8f399da3] + Libdl 
  [37e2e46d] + LinearAlgebra 
  [56ddb016] + Logging 
  [d6f4376e] + Markdown 
  [a63ad114] + Mmap 
  [44cfe95a] + Pkg 
  [de0858da] + Printf 
  [9abbd945] + Profile 
  [3fa0cd96] + REPL 
  [9a3f8284] + Random 
  [ea8e919c] + SHA 
  [9e88b42a] + Serialization 
  [1a1011a3] + SharedArrays 
  [6462fe0b] + Sockets 
  [2f01184e] + SparseArrays 
  [10745b16] + Statistics 
  [4607b0f0] + SuiteSparse 
  [8dfed614] + Test 
  [cf7118a7] + UUIDs 
  [4ec0a83e] + Unicode 
  Building Rmath → `~/.julia/packages/Rmath/BoBag/deps/build.log`
  Updating `/tmp/jl_dji3Vt/Project.toml`
 [no changes]
  Updating `/tmp/jl_dji3Vt/Manifest.toml`
 [no changes]
  Building CMake → `~/.julia/packages/CMake/nSK2r/deps/build.log`
  Updating `/tmp/jl_yd6m6M/Project.toml`
 [no changes]
  Updating `/tmp/jl_yd6m6M/Manifest.toml`
 [no changes]
  Building Blosc → `~/.julia/packages/Blosc/lzFr0/deps/build.log`
  Updating `/tmp/jl_zNrJRI/Project.toml`
 [no changes]
  Updating `/tmp/jl_zNrJRI/Manifest.toml`
 [no changes]
  Building HDF5 ─→ `~/.julia/packages/HDF5/Zh9on/deps/build.log`
  Updating `/tmp/jl_u0yNxm/Project.toml`
 [no changes]
  Updating `/tmp/jl_u0yNxm/Manifest.toml`
 [no changes]
   Testing GaussianMixtures
  Updating `/tmp/jl_nVAYIr/Project.toml`
 [no changes]
  Updating `/tmp/jl_nVAYIr/Manifest.toml`
 [no changes]
Running sandbox
Status `/tmp/jl_nVAYIr/Project.toml`
  [aaaa29a8] Clustering v0.13.3
  [34da2185] Compat v2.2.0
  [31c24e10] Distributions v0.21.12
  [5789e2e9] FileIO v1.2.1
  [cc18c42c] GaussianMixtures v0.3.0
  [4138dd39] JLD v0.9.1
  [90014a1f] PDMats v0.9.10
  [6e75b9c4] ScikitLearnBase v0.5.0
  [276daf66] SpecialFunctions v0.9.0
  [2913bbd2] StatsBase v0.32.0
  [8bb1440f] DelimitedFiles 
  [8ba89e20] Distributed 
  [37e2e46d] LinearAlgebra 
  [56ddb016] Logging 
  [de0858da] Printf 
  [9a3f8284] Random 
  [10745b16] Statistics 
[ Info: Testing Data
(100000, -2.1675097414316195e6, [18144.91809092731, 81855.0819090727], [-1834.8293177957369 -11009.402721945256 717.6159155192843; 1867.9470244196157 10727.697073233288 -397.7825017180601], [[20137.607346214492 -12240.772227267815 -13326.680730927508; -12240.772227267815 38542.669108122376 7704.250219681507; -13326.680730927508 7704.250219681507 18812.46529672769], [79627.25373045787 12173.807851363625 13300.580043761189; 12173.807851363625 61375.092946123455 -7880.277570085147; 13300.580043761189 -7880.277570085147 81623.52946582444]])
┌ Warning: rmprocs: process 1 not removed
└ @ Distributed /workspace/srcdir/usr/share/julia/stdlib/v1.5/Distributed/src/cluster.jl:1015
[ Info: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.449729e+03
      1       1.040568e+03      -4.091607e+02 |        4
      2       1.001733e+03      -3.883539e+01 |        5
      3       9.718484e+02      -2.988422e+01 |        2
      4       9.617631e+02      -1.008536e+01 |        2
      5       9.557364e+02      -6.026671e+00 |        0
      6       9.557364e+02       0.000000e+00 |        0
K-means converged with 6 iterations (objv = 955.7363811737619)
┌ Info: K-means with 272 data points using 6 iterations
└ 11.3 data points per parameter
[ Info: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
┌ Info: EM with 272 data points 0 iterations avll -2.076102
└ 5.8 data points per parameter
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:221
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:221
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:221
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:221
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:630 [inlined]
└ @ Core ./broadcast.jl:630
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:230
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:230
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:630 [inlined]
└ @ Core ./broadcast.jl:630
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:630 [inlined]
└ @ Core ./broadcast.jl:630
[ Info: iteration 1, lowerbound -3.697355
[ Info: iteration 2, lowerbound -3.534916
[ Info: iteration 3, lowerbound -3.374323
[ Info: iteration 4, lowerbound -3.208647
[ Info: iteration 5, lowerbound -3.056899
[ Info: dropping number of Gaussions to 7
[ Info: iteration 6, lowerbound -2.932267
[ Info: iteration 7, lowerbound -2.854718
[ Info: dropping number of Gaussions to 4
[ Info: iteration 8, lowerbound -2.809287
[ Info: iteration 9, lowerbound -2.781910
[ Info: dropping number of Gaussions to 3
[ Info: iteration 10, lowerbound -2.773171
[ Info: iteration 11, lowerbound -2.762168
[ Info: iteration 12, lowerbound -2.750330
[ Info: iteration 13, lowerbound -2.731368
[ Info: iteration 14, lowerbound -2.701994
[ Info: iteration 15, lowerbound -2.658997
[ Info: iteration 16, lowerbound -2.601499
[ Info: iteration 17, lowerbound -2.534110
[ Info: iteration 18, lowerbound -2.467170
[ Info: iteration 19, lowerbound -2.410550
[ Info: iteration 20, lowerbound -2.367265
[ Info: iteration 21, lowerbound -2.335461
[ Info: iteration 22, lowerbound -2.314707
[ Info: iteration 23, lowerbound -2.307397
[ Info: dropping number of Gaussions to 2
[ Info: iteration 24, lowerbound -2.302946
[ Info: iteration 25, lowerbound -2.299261
[ Info: iteration 26, lowerbound -2.299257
[ Info: iteration 27, lowerbound -2.299255
[ Info: iteration 28, lowerbound -2.299254
[ Info: iteration 29, lowerbound -2.299253
[ Info: iteration 30, lowerbound -2.299253
[ Info: iteration 31, lowerbound -2.299253
[ Info: iteration 32, lowerbound -2.299253
[ Info: iteration 33, lowerbound -2.299253
[ Info: iteration 34, lowerbound -2.299253
[ Info: iteration 35, lowerbound -2.299253
[ Info: iteration 36, lowerbound -2.299253
[ Info: iteration 37, lowerbound -2.299253
[ Info: iteration 38, lowerbound -2.299253
[ Info: iteration 39, lowerbound -2.299253
[ Info: iteration 40, lowerbound -2.299253
[ Info: iteration 41, lowerbound -2.299253
[ Info: iteration 42, lowerbound -2.299253
[ Info: iteration 43, lowerbound -2.299253
[ Info: iteration 44, lowerbound -2.299253
[ Info: iteration 45, lowerbound -2.299253
[ Info: iteration 46, lowerbound -2.299253
[ Info: iteration 47, lowerbound -2.299253
[ Info: iteration 48, lowerbound -2.299253
[ Info: iteration 49, lowerbound -2.299253
[ Info: iteration 50, lowerbound -2.299253
[ Info: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
History[Fri Jan  3 18:14:45 2020: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
, Fri Jan  3 18:14:54 2020: K-means with 272 data points using 6 iterations
11.3 data points per parameter
, Fri Jan  3 18:14:56 2020: EM with 272 data points 0 iterations avll -2.076102
5.8 data points per parameter
, Fri Jan  3 18:14:58 2020: GMM converted to Variational GMM
, Fri Jan  3 18:15:07 2020: iteration 1, lowerbound -3.697355
, Fri Jan  3 18:15:07 2020: iteration 2, lowerbound -3.534916
, Fri Jan  3 18:15:07 2020: iteration 3, lowerbound -3.374323
, Fri Jan  3 18:15:07 2020: iteration 4, lowerbound -3.208647
, Fri Jan  3 18:15:07 2020: iteration 5, lowerbound -3.056899
, Fri Jan  3 18:15:07 2020: dropping number of Gaussions to 7
, Fri Jan  3 18:15:07 2020: iteration 6, lowerbound -2.932267
, Fri Jan  3 18:15:07 2020: iteration 7, lowerbound -2.854718
, Fri Jan  3 18:15:07 2020: dropping number of Gaussions to 4
, Fri Jan  3 18:15:07 2020: iteration 8, lowerbound -2.809287
, Fri Jan  3 18:15:07 2020: iteration 9, lowerbound -2.781910
, Fri Jan  3 18:15:07 2020: dropping number of Gaussions to 3
, Fri Jan  3 18:15:07 2020: iteration 10, lowerbound -2.773171
, Fri Jan  3 18:15:07 2020: iteration 11, lowerbound -2.762168
, Fri Jan  3 18:15:07 2020: iteration 12, lowerbound -2.750330
, Fri Jan  3 18:15:07 2020: iteration 13, lowerbound -2.731368
, Fri Jan  3 18:15:07 2020: iteration 14, lowerbound -2.701994
, Fri Jan  3 18:15:07 2020: iteration 15, lowerbound -2.658997
, Fri Jan  3 18:15:07 2020: iteration 16, lowerbound -2.601499
, Fri Jan  3 18:15:07 2020: iteration 17, lowerbound -2.534110
, Fri Jan  3 18:15:07 2020: iteration 18, lowerbound -2.467170
, Fri Jan  3 18:15:07 2020: iteration 19, lowerbound -2.410550
, Fri Jan  3 18:15:07 2020: iteration 20, lowerbound -2.367265
, Fri Jan  3 18:15:07 2020: iteration 21, lowerbound -2.335461
, Fri Jan  3 18:15:07 2020: iteration 22, lowerbound -2.314707
, Fri Jan  3 18:15:07 2020: iteration 23, lowerbound -2.307397
, Fri Jan  3 18:15:07 2020: dropping number of Gaussions to 2
, Fri Jan  3 18:15:07 2020: iteration 24, lowerbound -2.302946
, Fri Jan  3 18:15:07 2020: iteration 25, lowerbound -2.299261
, Fri Jan  3 18:15:07 2020: iteration 26, lowerbound -2.299257
, Fri Jan  3 18:15:07 2020: iteration 27, lowerbound -2.299255
, Fri Jan  3 18:15:07 2020: iteration 28, lowerbound -2.299254
, Fri Jan  3 18:15:07 2020: iteration 29, lowerbound -2.299253
, Fri Jan  3 18:15:07 2020: iteration 30, lowerbound -2.299253
, Fri Jan  3 18:15:07 2020: iteration 31, lowerbound -2.299253
, Fri Jan  3 18:15:07 2020: iteration 32, lowerbound -2.299253
, Fri Jan  3 18:15:07 2020: iteration 33, lowerbound -2.299253
, Fri Jan  3 18:15:07 2020: iteration 34, lowerbound -2.299253
, Fri Jan  3 18:15:07 2020: iteration 35, lowerbound -2.299253
, Fri Jan  3 18:15:07 2020: iteration 36, lowerbound -2.299253
, Fri Jan  3 18:15:07 2020: iteration 37, lowerbound -2.299253
, Fri Jan  3 18:15:07 2020: iteration 38, lowerbound -2.299253
, Fri Jan  3 18:15:07 2020: iteration 39, lowerbound -2.299253
, Fri Jan  3 18:15:07 2020: iteration 40, lowerbound -2.299253
, Fri Jan  3 18:15:07 2020: iteration 41, lowerbound -2.299253
, Fri Jan  3 18:15:07 2020: iteration 42, lowerbound -2.299253
, Fri Jan  3 18:15:07 2020: iteration 43, lowerbound -2.299253
, Fri Jan  3 18:15:07 2020: iteration 44, lowerbound -2.299253
, Fri Jan  3 18:15:07 2020: iteration 45, lowerbound -2.299253
, Fri Jan  3 18:15:07 2020: iteration 46, lowerbound -2.299253
, Fri Jan  3 18:15:07 2020: iteration 47, lowerbound -2.299253
, Fri Jan  3 18:15:07 2020: iteration 48, lowerbound -2.299253
, Fri Jan  3 18:15:07 2020: iteration 49, lowerbound -2.299253
, Fri Jan  3 18:15:07 2020: iteration 50, lowerbound -2.299253
, Fri Jan  3 18:15:07 2020: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [95.95490777396944, 178.04509222603053]
β = [95.95490777396944, 178.04509222603053]
m = [2.000229257775231 53.851987172460554; 4.250300733269775 79.28686694435984]
ν = [97.95490777396944, 180.04509222603053]
W = LinearAlgebra.UpperTriangular{Float64,Array{Float64,2}}[[0.3758763611950756 -0.008953123827348513; 0.0 0.012748664777409824], [0.18404155547482748 -0.007644049042328947; 0.0 0.008581705166331005]]
Kind: diag, size256
nx: 100000 sum(zeroth order stats): 100000.00000000004
avll from stats: -0.988945709700939
avll from llpg:  -0.9889457097009386
avll direct:     -0.9889457097009386
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 100000.00000000003
avll from stats: -0.9901488823421527
avll from llpg:  -0.9901488823421525
avll direct:     -0.9901488823421526
sum posterior: 100000.0
32×26 Array{Float64,2}:
 -0.0450756   -0.0531935  -0.0233275     0.0531462   -0.0138386   -0.00324405  -0.010473     0.0675597    0.0248007   0.176523     0.204957     -0.144426    -0.0574773    0.0289392   -0.0309272   -0.137221     0.040217     -0.109643    -0.0397226    0.120784     -0.0519219   -0.0894376    -0.11776      0.0561877    0.0292862    0.0748555
 -0.051095     0.101495    0.133608     -0.0205994   -0.0143199    0.0860959   -0.129729     0.0959503    0.0628737  -0.042852     0.180319      0.0692097   -0.0590616   -0.0857341    0.0898647    0.0227365   -0.0702583    -0.054215     0.0514115   -0.0703044     0.0956333   -0.000634858   0.0499137    0.034131    -0.00274246  -0.147956
 -0.00669418   0.0214663  -0.0453098    -0.0966703    0.0462105   -0.00198228   0.0233756    0.115358     0.0518822   0.201189     0.0425477    -0.0324958    0.20124     -0.0961231    0.0457512    0.269506     0.0546471     0.112209     0.0672174    0.140773      0.136839     0.0110644     0.138713    -0.125408    -0.0941178   -0.0129491
  0.121708    -0.0572133  -0.0800376    -0.138106     0.0116085   -0.0397827   -0.0255311    0.099062    -0.065914    0.200241    -0.146333     -0.0781413   -0.143703     0.0273546   -0.0606772   -0.095033     0.0943735     0.0690507   -0.0163961    0.189501     -0.0304584    0.0763056     0.0987918    0.0238092   -0.0297015    0.005753
  0.110034    -0.0877903   0.0834439     0.0779691    0.0806263   -0.00978829   0.0640477    0.173512     0.0517823   0.145125     0.0819503    -0.096927    -0.0951974   -0.0258868   -0.019043     0.0931802   -0.0657335    -0.0773351    0.16941     -0.206515     -0.0388588    0.179895     -0.125527    -0.141313     0.078374    -0.172471
 -0.0114368    0.152374   -0.14617      -0.0223845    0.135231    -0.0816701    0.0421404   -0.100087    -0.174819    0.0821736    0.0833398    -0.0588603   -0.0173462    0.0619434    0.100746     0.0119986   -0.134198     -0.163468    -0.00474154  -0.000505033  -0.0449151   -0.0818717    -0.11433      0.119761     0.194943     0.0591582
  0.00951689   0.110124   -0.182132      0.00221657   0.0720789    0.0852487    0.0179092   -0.224539    -0.195061    0.0148359    0.00482952    0.059818     0.00119816  -0.0104353   -0.0505774   -0.0839985    0.0519089    -0.00182147   0.00549115  -0.116471     -0.120072    -0.0547529    -0.0167459   -0.0228116    0.130414     0.0275605
 -0.158221     0.121979   -0.000739059   0.0744252    0.0656756   -0.0214905   -0.00661787  -0.0280585    0.224194   -0.0464387    0.0993589    -0.0187964    0.110916    -0.0107669   -0.0537982   -0.0665054    0.000176981  -0.0491789   -0.0739646   -0.0602091    -0.0946304    0.0460551    -0.00197819  -0.24872     -0.0136336   -0.190392
  0.0236952    0.148772   -0.0998759    -0.118278     0.0328634    0.12083     -0.0443464   -0.126015    -0.238935    0.0640328   -0.156296     -0.0386599   -0.166963     0.0247943   -0.0738189    0.0123202    0.185943      0.0221402    0.0152097    0.118966      0.0466354   -0.192959      0.0565099   -0.0952956   -0.0825259   -0.0342265
  0.0165322    0.0892903  -0.0347477     0.136946     0.00588142   0.183619    -0.072185    -0.138752    -0.0531994   0.0577567   -0.0503798     0.0279047    0.10511     -0.148325    -0.0762158   -0.00653721   0.206465      0.00992493  -0.0499847    0.151509      0.103318    -0.0255512    -0.0447552   -0.128557     0.0646853   -0.0566976
  0.0816321    0.0848753   0.0983855    -0.0780658   -0.031219    -0.00707305  -0.0139711    0.239065    -0.0936681  -0.0353192   -0.105833     -0.0206072   -0.109354    -0.162848    -0.0453577    0.257648    -0.0415999     0.0935392   -0.0248033    0.132033      0.2074       0.105966      0.0213925   -0.0158731   -0.006172    -0.0365002
 -0.066709    -0.0305912   0.0646608    -0.0347933    0.196432    -0.213431    -0.0215363   -0.0880191   -0.186871    0.0579741    0.205391     -0.154347     0.0867452   -0.0297811    0.0784832    0.0462678   -0.0707718    -0.0298467    0.169608     0.00445911   -0.115502    -0.110015      0.0268415    0.0291866    0.146706    -0.00486062
 -0.18654      0.0688069  -0.254341      0.0531257    0.0765601    0.0639117   -0.0150035    0.0684387    0.0467135  -0.0654064    0.148877     -0.00717758   0.273934    -0.00957176  -0.0716698   -0.0719626    0.0641836    -0.102148    -0.196697     0.0492211     0.0845334    0.0662865     0.0363326    0.00117321   0.0253073    0.0957041
 -0.00335098   0.12689     0.120657      0.0658498   -0.0303328   -0.0197466    0.13094     -0.201828    -0.0461163  -0.0900251    0.0175967    -0.0460789    0.066979     0.0710121   -0.17948     -0.278245    -0.0707469    -0.00394514   0.129205     0.0259401    -0.134585    -0.14708      -0.0285558   -0.011511    -0.11458     -0.0425551
  0.109416     0.0585439  -0.0519944    -0.0930994   -0.0266629   -0.0355453   -0.0852494    0.0666399   -0.127005    0.0778165   -0.105722     -0.0184372    0.0113724   -0.00857365  -0.170014    -0.1107       0.0887493    -0.125292    -0.145237    -0.047003     -0.0905368    0.0475067    -0.0734336   -0.0843473   -0.0324014   -0.145293
 -0.0692327    0.0928217   0.065639     -0.0640218   -0.00169647  -0.00482329  -0.139959    -0.0147613   -0.0540749   0.0393427    0.23819       0.0322266   -0.188835    -0.0342212    0.187148    -0.1051      -0.0781975     0.0231617    0.0574207    0.095473     -0.0816523   -0.1983       -0.0869546    0.0647237    0.0572196    0.00602329
 -0.100236    -0.164728   -0.0126321     0.0209704   -0.0311547    0.0965458   -0.055615    -0.117005     0.0249865   0.0365826   -0.204038     -0.0652924    0.0131149   -0.0222456   -0.235631    -0.160224    -0.0519574    -0.0744533    0.00141444  -0.186275     -0.0816652   -0.258212      0.0719907   -0.166406     0.109354     0.0473827
  0.0557642   -0.174225   -0.00447011   -0.0420763   -0.153391     0.0202445    0.0827024   -0.0715236   -0.0721292   0.055406    -0.0162494    -0.143372    -0.0170052   -0.036676     0.00593795  -0.00512517   0.0841989     0.177798    -0.0365362   -0.0330689    -0.0340711   -0.066807      0.121142    -0.0824801    0.0403751   -0.0498558
  0.0872652    0.0895297  -0.163128     -0.277525    -0.0546622    0.0774851   -0.0334055    0.0510257    0.0607727   0.00802564  -0.0243114     0.136681    -0.142814     0.064463    -0.00433828  -0.24576     -0.0791842     0.332716    -0.166427     0.0968705     0.013888    -0.137046      0.0317123    0.0924627    0.191883    -0.0877079
  0.00160387  -0.0487643   0.0534317     0.160989    -0.104708     0.123435    -0.261242    -0.0614872   -0.0297316  -0.00427469   0.0682308     0.0544793   -0.0664798   -0.117524    -0.0983144    0.0141488    0.116769      0.0255043    0.0823365    0.0264062     0.00220385  -0.101032      0.111515    -0.00964397   0.145014    -0.19365
 -0.096131     0.139302   -0.0622878    -0.00999781   0.0217007    0.0034424    0.0305907   -0.0968068    0.0432731  -0.0587468   -0.120438      0.0344395   -0.194834    -0.0234024    0.133316     0.256117    -0.0946431     0.209703     0.092069    -0.0387933    -0.053257    -0.0779138    -0.0240904    0.00286782  -0.042636    -0.0307256
  0.0224364    0.0324891   0.119987      0.0443046    0.0110537   -0.020358    -0.121369    -0.00715617   0.0725052  -0.0669596   -0.0663824     0.0908084    0.0312786    0.121224    -0.0623046   -0.147674     0.119143     -0.165473    -0.0789552    0.00522723   -0.0355548    0.0602892     0.0923701   -0.30058      0.18779      0.0898247
 -0.0714666    0.136521   -0.0692578    -0.0744773   -0.130304    -0.0819261    0.0598305   -0.0469626   -0.124296    0.0490604   -0.000699473   0.0065552   -0.0515457   -0.00587629  -0.0602692    0.0297803    0.0833821     0.0808266   -0.0579235    0.0424635    -0.168498     0.0741716    -0.0837666    0.110158    -0.186727    -0.110719
 -0.0070981    0.0571483   0.00315099    0.0317487    0.117919    -0.0463893    0.0802739    0.170138    -0.0394033   0.0148797    0.0186996    -0.0799543    0.0467238   -0.0747511    0.120344    -0.00510985   0.0433604    -0.0173448    0.0858333   -0.237163     -0.0437293    0.1009        0.122152     0.0333243    0.0805217   -0.194023
 -0.148176     0.0485825   0.0534549     0.0936554   -0.187008    -0.0108371    0.0290453   -0.0187784   -0.195059   -0.0209466    0.00612838   -0.0222774   -0.0979833    0.169673    -0.0418649   -0.0961545    0.0847591     0.180463    -0.0175039   -0.143186     -0.00973017   0.0657777     0.0618787    0.122294     0.082433    -0.097181
 -0.0491361   -0.145858   -0.000913244   0.0162974    0.148526    -0.0817164    0.0229628    0.0657959    0.10912     0.0772503    0.131036      0.00233092  -0.0978048   -0.0780788   -0.0983008   -0.00917985   0.0398888    -0.0439905    0.0583204    0.2172       -0.00633054   0.0404042     0.00173558   0.0403796   -0.0102084   -0.199935
 -0.0262908    0.0238889  -0.122361     -8.84326e-5   0.00539243   0.180406     0.0358267    0.316739    -0.0696495   0.0460621    0.104855     -0.024014     0.0102648    0.0199461   -0.107277    -0.0160736   -0.0488943    -0.0304765    0.0657534    0.0801567    -0.00741904   0.0902569     0.15393     -0.0654758   -0.0279229    0.00226834
 -0.0785341    0.0837157  -0.0229138    -0.0992276    0.0867892   -0.0897514    0.00362249  -0.0771759    0.17308    -0.0405648    0.098163     -0.102224     0.159931     0.0624183   -0.009632    -0.0357938    0.259834     -0.126441     0.0336156   -0.164975     -0.170115    -0.111991     -0.116261     0.00273405  -0.0842558   -0.157628
  0.0772772    0.113316   -0.121539     -0.0363089   -0.00957226   0.0485485    0.0950363   -8.18516e-5   0.179119   -0.0440332   -0.0323593    -0.0734111   -0.190091    -0.0465386   -0.0460449   -0.115504     0.0141586    -0.12307      0.202558    -0.168124     -0.0364179    0.0620639    -0.263822     0.0543075   -0.163855     0.0244585
 -0.0873487   -0.0802567   0.0134652     0.217324    -0.0802105   -0.12475      0.167603     0.00135737  -0.0463814   0.0663205   -0.0054152     0.016241     0.0776754   -0.0249909   -0.101679    -0.0179319    0.154316      0.0744      -0.0024225    0.0576616    -0.0139312    0.0684865    -0.0101413   -0.0672538    0.0945943   -0.0498557
  0.0865173    0.119078    0.0242426    -0.244811     0.0945789    0.0125039    0.139771     0.0934612    0.0404475  -0.0185555    0.0358876     0.0580118    0.0108195   -0.00802056  -0.108289     0.018136     0.0184059    -0.0535929    0.0672892    0.027149      0.0548681    0.0942227     0.13502     -0.0900896    0.0443268   -0.0433196
 -0.243681    -0.11811     0.0261275    -0.0256538   -0.0187742    0.0140481    0.158328     0.128157    -0.2311      0.0468132    0.141572      0.105667    -0.100586     0.0480543    0.0406619   -0.0294662    0.0846985     0.0112121    0.0602829   -0.0775166     0.125873     0.0796452     0.0754316   -0.139138     0.0201019    0.0448235kind diag, method split
┌ Info: 0: avll = 
└   tll[1] = -1.382661351504606
[ Info: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.382725
[ Info: iteration 2, average log likelihood -1.382655
[ Info: iteration 3, average log likelihood -1.381974
[ Info: iteration 4, average log likelihood -1.374112
[ Info: iteration 5, average log likelihood -1.356398
[ Info: iteration 6, average log likelihood -1.349947
[ Info: iteration 7, average log likelihood -1.348410
[ Info: iteration 8, average log likelihood -1.347579
[ Info: iteration 9, average log likelihood -1.346973
[ Info: iteration 10, average log likelihood -1.346367
[ Info: iteration 11, average log likelihood -1.345533
[ Info: iteration 12, average log likelihood -1.344433
[ Info: iteration 13, average log likelihood -1.343823
[ Info: iteration 14, average log likelihood -1.343461
[ Info: iteration 15, average log likelihood -1.343188
[ Info: iteration 16, average log likelihood -1.342972
[ Info: iteration 17, average log likelihood -1.342805
[ Info: iteration 18, average log likelihood -1.342681
[ Info: iteration 19, average log likelihood -1.342592
[ Info: iteration 20, average log likelihood -1.342529
[ Info: iteration 21, average log likelihood -1.342485
[ Info: iteration 22, average log likelihood -1.342452
[ Info: iteration 23, average log likelihood -1.342427
[ Info: iteration 24, average log likelihood -1.342408
[ Info: iteration 25, average log likelihood -1.342392
[ Info: iteration 26, average log likelihood -1.342380
[ Info: iteration 27, average log likelihood -1.342369
[ Info: iteration 28, average log likelihood -1.342360
[ Info: iteration 29, average log likelihood -1.342353
[ Info: iteration 30, average log likelihood -1.342347
[ Info: iteration 31, average log likelihood -1.342342
[ Info: iteration 32, average log likelihood -1.342338
[ Info: iteration 33, average log likelihood -1.342334
[ Info: iteration 34, average log likelihood -1.342330
[ Info: iteration 35, average log likelihood -1.342326
[ Info: iteration 36, average log likelihood -1.342322
[ Info: iteration 37, average log likelihood -1.342318
[ Info: iteration 38, average log likelihood -1.342313
[ Info: iteration 39, average log likelihood -1.342308
[ Info: iteration 40, average log likelihood -1.342302
[ Info: iteration 41, average log likelihood -1.342295
[ Info: iteration 42, average log likelihood -1.342287
[ Info: iteration 43, average log likelihood -1.342278
[ Info: iteration 44, average log likelihood -1.342268
[ Info: iteration 45, average log likelihood -1.342256
[ Info: iteration 46, average log likelihood -1.342240
[ Info: iteration 47, average log likelihood -1.342219
[ Info: iteration 48, average log likelihood -1.342192
[ Info: iteration 49, average log likelihood -1.342160
[ Info: iteration 50, average log likelihood -1.342128
┌ Info: EM with 100000 data points 50 iterations avll -1.342128
└ 952.4 data points per parameter
┌ Info: 1
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.3827251534569
│     -1.3826545714449714
│      ⋮
└     -1.342127597939906
[ Info: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.342238
[ Info: iteration 2, average log likelihood -1.342061
[ Info: iteration 3, average log likelihood -1.341057
[ Info: iteration 4, average log likelihood -1.332891
[ Info: iteration 5, average log likelihood -1.316166
[ Info: iteration 6, average log likelihood -1.308141
[ Info: iteration 7, average log likelihood -1.304924
[ Info: iteration 8, average log likelihood -1.303002
[ Info: iteration 9, average log likelihood -1.301745
[ Info: iteration 10, average log likelihood -1.300859
[ Info: iteration 11, average log likelihood -1.300187
[ Info: iteration 12, average log likelihood -1.299638
[ Info: iteration 13, average log likelihood -1.299177
[ Info: iteration 14, average log likelihood -1.298797
[ Info: iteration 15, average log likelihood -1.298487
[ Info: iteration 16, average log likelihood -1.298226
[ Info: iteration 17, average log likelihood -1.298003
[ Info: iteration 18, average log likelihood -1.297816
[ Info: iteration 19, average log likelihood -1.297658
[ Info: iteration 20, average log likelihood -1.297526
[ Info: iteration 21, average log likelihood -1.297415
[ Info: iteration 22, average log likelihood -1.297323
[ Info: iteration 23, average log likelihood -1.297246
[ Info: iteration 24, average log likelihood -1.297181
[ Info: iteration 25, average log likelihood -1.297125
[ Info: iteration 26, average log likelihood -1.297075
[ Info: iteration 27, average log likelihood -1.297031
[ Info: iteration 28, average log likelihood -1.296992
[ Info: iteration 29, average log likelihood -1.296957
[ Info: iteration 30, average log likelihood -1.296926
[ Info: iteration 31, average log likelihood -1.296898
[ Info: iteration 32, average log likelihood -1.296872
[ Info: iteration 33, average log likelihood -1.296847
[ Info: iteration 34, average log likelihood -1.296823
[ Info: iteration 35, average log likelihood -1.296800
[ Info: iteration 36, average log likelihood -1.296778
[ Info: iteration 37, average log likelihood -1.296757
[ Info: iteration 38, average log likelihood -1.296737
[ Info: iteration 39, average log likelihood -1.296717
[ Info: iteration 40, average log likelihood -1.296698
[ Info: iteration 41, average log likelihood -1.296680
[ Info: iteration 42, average log likelihood -1.296662
[ Info: iteration 43, average log likelihood -1.296645
[ Info: iteration 44, average log likelihood -1.296629
[ Info: iteration 45, average log likelihood -1.296614
[ Info: iteration 46, average log likelihood -1.296600
[ Info: iteration 47, average log likelihood -1.296586
[ Info: iteration 48, average log likelihood -1.296573
[ Info: iteration 49, average log likelihood -1.296560
[ Info: iteration 50, average log likelihood -1.296547
┌ Info: EM with 100000 data points 50 iterations avll -1.296547
└ 473.9 data points per parameter
┌ Info: 2
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.3422380361289041
│     -1.3420610410892493
│      ⋮
└     -1.2965469719733251
[ Info: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.296720
[ Info: iteration 2, average log likelihood -1.296523
[ Info: iteration 3, average log likelihood -1.295958
[ Info: iteration 4, average log likelihood -1.291289
[ Info: iteration 5, average log likelihood -1.275315
[ Info: iteration 6, average log likelihood -1.260340
[ Info: iteration 7, average log likelihood -1.253414
[ Info: iteration 8, average log likelihood -1.249140
[ Info: iteration 9, average log likelihood -1.245949
[ Info: iteration 10, average log likelihood -1.243230
[ Info: iteration 11, average log likelihood -1.241305
[ Info: iteration 12, average log likelihood -1.240008
[ Info: iteration 13, average log likelihood -1.239015
[ Info: iteration 14, average log likelihood -1.238147
[ Info: iteration 15, average log likelihood -1.237344
[ Info: iteration 16, average log likelihood -1.236602
[ Info: iteration 17, average log likelihood -1.236012
[ Info: iteration 18, average log likelihood -1.235567
[ Info: iteration 19, average log likelihood -1.235232
[ Info: iteration 20, average log likelihood -1.234953
[ Info: iteration 21, average log likelihood -1.234705
[ Info: iteration 22, average log likelihood -1.234481
[ Info: iteration 23, average log likelihood -1.234282
[ Info: iteration 24, average log likelihood -1.234114
[ Info: iteration 25, average log likelihood -1.233987
[ Info: iteration 26, average log likelihood -1.233895
[ Info: iteration 27, average log likelihood -1.233829
[ Info: iteration 28, average log likelihood -1.233784
[ Info: iteration 29, average log likelihood -1.233754
[ Info: iteration 30, average log likelihood -1.233736
[ Info: iteration 31, average log likelihood -1.233725
[ Info: iteration 32, average log likelihood -1.233717
[ Info: iteration 33, average log likelihood -1.233711
[ Info: iteration 34, average log likelihood -1.233705
[ Info: iteration 35, average log likelihood -1.233701
[ Info: iteration 36, average log likelihood -1.233696
[ Info: iteration 37, average log likelihood -1.233691
[ Info: iteration 38, average log likelihood -1.233685
[ Info: iteration 39, average log likelihood -1.233679
[ Info: iteration 40, average log likelihood -1.233673
[ Info: iteration 41, average log likelihood -1.233665
[ Info: iteration 42, average log likelihood -1.233656
[ Info: iteration 43, average log likelihood -1.233646
[ Info: iteration 44, average log likelihood -1.233635
[ Info: iteration 45, average log likelihood -1.233623
[ Info: iteration 46, average log likelihood -1.233610
[ Info: iteration 47, average log likelihood -1.233595
[ Info: iteration 48, average log likelihood -1.233577
[ Info: iteration 49, average log likelihood -1.233556
[ Info: iteration 50, average log likelihood -1.233529
┌ Info: EM with 100000 data points 50 iterations avll -1.233529
└ 236.4 data points per parameter
┌ Info: 3
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.2967198642044615
│     -1.2965232628542254
│      ⋮
└     -1.2335291653530842
[ Info: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.233767
[ Info: iteration 2, average log likelihood -1.233444
[ Info: iteration 3, average log likelihood -1.232465
[ Info: iteration 4, average log likelihood -1.220630
[ Info: iteration 5, average log likelihood -1.184441
[ Info: iteration 6, average log likelihood -1.158685
[ Info: iteration 7, average log likelihood -1.145609
[ Info: iteration 8, average log likelihood -1.138888
[ Info: iteration 9, average log likelihood -1.134941
[ Info: iteration 10, average log likelihood -1.131897
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 11, average log likelihood -1.129329
[ Info: iteration 12, average log likelihood -1.139229
[ Info: iteration 13, average log likelihood -1.133722
[ Info: iteration 14, average log likelihood -1.132396
[ Info: iteration 15, average log likelihood -1.131035
[ Info: iteration 16, average log likelihood -1.129463
[ Info: iteration 17, average log likelihood -1.128305
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 18, average log likelihood -1.127273
[ Info: iteration 19, average log likelihood -1.138494
[ Info: iteration 20, average log likelihood -1.133451
[ Info: iteration 21, average log likelihood -1.132074
[ Info: iteration 22, average log likelihood -1.130515
[ Info: iteration 23, average log likelihood -1.128851
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 24, average log likelihood -1.127717
[ Info: iteration 25, average log likelihood -1.138305
[ Info: iteration 26, average log likelihood -1.133276
[ Info: iteration 27, average log likelihood -1.131809
[ Info: iteration 28, average log likelihood -1.130097
[ Info: iteration 29, average log likelihood -1.128246
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 30, average log likelihood -1.126875
[ Info: iteration 31, average log likelihood -1.137448
[ Info: iteration 32, average log likelihood -1.131649
[ Info: iteration 33, average log likelihood -1.128620
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 34, average log likelihood -1.124818
[ Info: iteration 35, average log likelihood -1.133153
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 36, average log likelihood -1.125774
[ Info: iteration 37, average log likelihood -1.135684
[ Info: iteration 38, average log likelihood -1.129733
[ Info: iteration 39, average log likelihood -1.127700
[ Info: iteration 40, average log likelihood -1.125916
[ Info: iteration 41, average log likelihood -1.123437
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 42, average log likelihood -1.120390
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 43, average log likelihood -1.129803
[ Info: iteration 44, average log likelihood -1.135413
[ Info: iteration 45, average log likelihood -1.129185
[ Info: iteration 46, average log likelihood -1.127282
[ Info: iteration 47, average log likelihood -1.125466
[ Info: iteration 48, average log likelihood -1.123816
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 49, average log likelihood -1.122521
[ Info: iteration 50, average log likelihood -1.133792
┌ Info: EM with 100000 data points 50 iterations avll -1.133792
└ 118.1 data points per parameter
┌ Info: 4
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.2337674051321348
│     -1.2334436737728798
│      ⋮
└     -1.1337918958187152
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.128371
[ Info: iteration 2, average log likelihood -1.125589
[ Info: iteration 3, average log likelihood -1.122212
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     12
│     13
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.107114
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     11
│     14
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.066676
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│     10
│     12
│     13
│     25
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.041743
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      7
│      9
│     11
│     14
│     17
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.038988
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     12
│     13
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.048619
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│     10
│     11
│     14
│     26
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.029932
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      9
│     11
│     12
│     13
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.036307
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      7
│     10
│     12
│     14
│     17
│     28
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 11, average log likelihood -1.026742
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     11
│     13
│     25
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 12, average log likelihood -1.051226
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      9
│     12
│     14
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 13, average log likelihood -1.039907
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│     10
│     11
│     12
│     13
│     25
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 14, average log likelihood -1.021731
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      7
│     12
│     14
│     17
│     27
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 15, average log likelihood -1.031113
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      9
│     11
│     13
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 16, average log likelihood -1.045282
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     10
│     14
│     26
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 17, average log likelihood -1.041855
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     11
│     12
│     13
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 18, average log likelihood -1.032846
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      7
│      9
│     10
│     12
│      ⋮
│     27
│     28
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 19, average log likelihood -1.020726
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     11
│     13
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 20, average log likelihood -1.061506
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     12
│     14
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 21, average log likelihood -1.047398
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│     10
│     11
│     12
│     13
│     25
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 22, average log likelihood -1.028838
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      7
│      9
│     12
│     14
│     17
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 23, average log likelihood -1.033920
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│     10
│     11
│     12
│     13
│     25
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 24, average log likelihood -1.038542
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     14
│     26
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 25, average log likelihood -1.045254
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      9
│     11
│     12
│     13
│     25
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 26, average log likelihood -1.034586
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      7
│     10
│     12
│     14
│     17
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 27, average log likelihood -1.032762
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│     11
│     12
│     13
│     25
│     27
│     28
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 28, average log likelihood -1.030442
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      9
│     14
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 29, average log likelihood -1.050389
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     10
│     12
│     13
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 30, average log likelihood -1.040931
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      7
│     11
│     14
│     17
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 31, average log likelihood -1.030976
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      9
│     10
│     11
│     12
│     13
│     25
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 32, average log likelihood -1.032893
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     12
│     14
│     26
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 33, average log likelihood -1.043138
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     11
│     13
│     25
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 34, average log likelihood -1.040777
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      7
│     10
│     12
│     14
│     17
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 35, average log likelihood -1.031574
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      9
│     11
│     12
│     13
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 36, average log likelihood -1.035512
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│     10
│     12
│     14
│     26
│     27
│     28
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 37, average log likelihood -1.029541
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     11
│     13
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 38, average log likelihood -1.051833
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      7
│      9
│     12
│     14
│     17
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 39, average log likelihood -1.036324
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│     10
│     11
│     13
│     25
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 40, average log likelihood -1.038898
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│     11
│     12
│     14
│     26
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 41, average log likelihood -1.035049
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      9
│     10
│     12
│     13
│     25
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 42, average log likelihood -1.030625
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      7
│     11
│     14
│     17
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 43, average log likelihood -1.041361
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     12
│     13
│     25
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 44, average log likelihood -1.047297
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│     10
│     11
│     12
│     14
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 45, average log likelihood -1.025102
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      9
│     12
│     13
│     25
│     27
│     28
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 46, average log likelihood -1.027408
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      7
│     11
│     14
│     17
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 47, average log likelihood -1.047772
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     10
│     12
│     13
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 48, average log likelihood -1.042460
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      9
│     11
│     12
│     14
│     26
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 49, average log likelihood -1.036272
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│     10
│     12
│     13
│     25
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 50, average log likelihood -1.037383
┌ Info: EM with 100000 data points 50 iterations avll -1.037383
└ 59.0 data points per parameter
┌ Info: 5
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.1283710257925055
│     -1.1255893095380338
│      ⋮
└     -1.0373831240032017
┌ Info: Total log likelihood: 
│   tll =
│    251-element Array{Float64,1}:
│     -1.382661351504606
│     -1.3827251534569
│     -1.3826545714449714
│     -1.3819738656275176
│      ⋮
│     -1.0424602449203761
│     -1.0362720403003312
└     -1.0373831240032017
32×26 Array{Float64,2}:
 -0.0785104    -0.25475    -0.0154475   -0.097399     0.234711     -0.180544     0.0103304    -0.102545     0.0607335   -0.0421027    0.0177606   -0.106067      0.159379      0.0707359    0.0148769    0.0232797    0.263946    -0.12623      0.052222     -0.101235     0.201097    -0.0814726   -0.197181    -0.0472644   -0.157023     -0.145249
 -0.0547571     0.682863   -0.029885    -0.104804    -0.0408248    -0.012787     0.000207374  -0.0591578    0.262258    -0.0439466    0.24396     -0.0966382     0.158046      0.0484894   -0.038385    -0.0928889    0.230911    -0.127593    -0.00112672   -0.243699    -0.660323    -0.128936     0.026684     0.00112172   0.0229055    -0.0958842
 -0.138083      0.0320637   0.0388684    0.121765    -0.156554     -0.0254952    0.0366995    -0.0415472   -0.193629    -0.254455     0.0943885   -0.0238305    -0.103117      0.133035    -0.0644787   -0.0818295    0.0786729    0.180987    -0.0116633    -0.2102      -0.683223     0.267121     0.0611024    0.129823     0.0759429    -0.112919
 -0.131236      0.0577947   0.0598434    0.0286176   -0.234536      0.0181797    0.0297364    -0.0182311   -0.177118     0.0462565   -0.0189251   -0.0158017    -0.127815      0.277175     0.019215    -0.135572     0.0695788    0.176091     3.4438e-5    -0.106372     0.51426     -0.0960967    0.0602263    0.061913     0.0961376    -0.0714544
 -0.0498091    -0.139782    0.0480262    0.089586    -0.117202      0.123347    -0.300686     -0.0599146   -0.071268     0.0088883    0.0378305    0.0541939    -0.053588      0.00654174   0.00201153   0.0262451    0.138167     0.0355548    0.0784202    -0.635059    -0.129394    -0.116375     0.212146    -0.0336661    0.202213     -0.276787
  0.0470671    -0.0372607   0.061517     0.214907     0.0266669     0.119602    -0.216366     -0.0660409    0.0350714   -0.0314498    0.0847035    0.055973     -0.0888686    -0.182486    -0.15608      0.00670867   0.112717     0.0222126    0.0719798     0.65623      0.125539    -0.0696483   -0.0274243    0.0107215    0.0762017    -0.0566812
  0.0128516     0.157743   -0.119569    -0.11528      0.0325904     0.111368    -0.0726453    -0.130395    -0.235518     0.0800348   -0.154034    -0.0293037    -0.119324      0.0238212   -0.0272852    0.030207     0.193818     0.0225141   -0.00898119    0.105456     0.0536643   -0.208564     0.0540476   -0.118552    -0.0816599    -0.0206146
 -0.0496538     0.0731361  -0.136299     0.102437     0.053994      0.117504    -0.0434497    -0.0411275   -0.00469348   0.00252198   0.0569567    0.00840318    0.196895     -0.10229     -0.061808    -0.0416311    0.132541    -0.0290434   -0.121882      0.118026     0.0804442    0.0283837   -0.0108235   -0.064038     0.0445768     0.00224269
 -0.0714813    -0.0782053   0.00139885   0.214913    -0.0784228    -0.114931     0.166736      0.00400225  -0.0198901    0.0492494   -0.0145898    0.0153919     0.107015     -0.0222487   -0.0949615   -0.0472639    0.151276     0.112313    -0.0416162     0.0484165   -0.0170493    0.0713746   -0.0196427   -0.0499021    0.0996734    -0.0473118
  0.0947314     0.0882538  -0.162829    -0.271021    -0.0512139     0.0743475   -0.0485985     0.0550604    0.0379931    0.0220473   -0.0289411    0.12659      -0.13047       0.0653517    0.00857221  -0.258249    -0.0797868    0.330889    -0.174959      0.0897265    0.0253976   -0.149878     1.38219e-5   0.0888288    0.191935     -0.0632978
 -0.0897037    -0.0292378   0.0601788   -0.0419796    0.202275     -0.214296    -0.010334     -0.0930372   -0.184307     0.0649558    0.200646    -0.157208      0.0919995    -0.028474     0.0394753    0.0712696   -0.0789402   -0.0329554    0.212785     -0.00105679  -0.120189    -0.119309     0.013173     0.0239323    0.160387     -0.0075183
  0.0200136     0.128403    0.146545     0.0769783   -0.0297232    -0.0357656    0.153782     -0.18044     -0.0439522   -0.0861767    0.0241128   -0.0446584     0.102981      0.0641093   -0.16827     -0.21898     -0.0693156    0.0183982    0.116077      0.012599    -0.128743    -0.138359    -0.0276455   -0.00112289  -0.106157     -0.0421048
 -0.100668     -0.1638     -0.00966931   0.0204981   -0.0340693     0.0811561   -0.055091     -0.111402     0.0271559    0.0387388   -0.20698     -0.0774192     0.0329881    -0.0423184   -0.229938    -0.153361    -0.0495503   -0.0635812    0.00158278   -0.140204    -0.115899    -0.250427     0.0824602   -0.171768     0.113696      0.0367607
 -0.000612667  -0.140901   -0.00211686   0.0197976    0.148978     -0.073195     0.0322736     0.0647725    0.0714372    0.0487292    0.140849     0.00225692   -0.102216     -0.0677551   -0.104954    -0.0186958    0.0423625   -0.0435929    0.0591531     0.173316     0.00846406   0.0627124    0.0387464    0.0217218   -0.0475894    -0.166777
 -0.063907      0.0122245  -0.023878    -0.00723231   0.0178357     0.0611577   -0.0277775     0.132073    -0.0340723    0.0993659    0.160336    -0.0813137    -0.0817819     0.00377803   0.0239854   -0.0836085   -0.0399732   -0.0385409    0.0173906     0.115592    -0.0465406   -0.0886082   -0.022034     0.0299513    0.0166822     0.0180172
 -0.119738      0.128107   -0.0359888    0.0431058    0.0343054    -0.00732758  -0.0116494    -0.0728168    0.114025    -0.0483484   -0.0282943    0.0332441    -0.0363754    -0.0135505    0.0389745    0.116293    -0.0508502    0.0880969    0.00652281   -0.0556623   -0.0886365   -0.0324564   -0.0141689   -0.122985    -0.0120722    -0.0949604
 -0.0813363     0.128707   -0.0674746   -0.060477    -0.108522     -0.0613797    0.0606947    -0.0411483   -0.115283     0.0438409   -0.00137906  -0.0223716    -0.057774     -0.00595704  -0.0560697    0.0471561    0.0689661    0.0921633   -0.0569343     0.0446069   -0.147108     0.0754384   -0.0799321    0.0893829   -0.182476     -0.137035
  0.112495     -0.0990787  -0.00605014  -0.0515733    0.0455531    -0.0334522    0.0104718     0.136583    -0.00830295   0.179958    -0.04705     -0.0845389    -0.146146      0.0031248   -0.0416683    0.00275094   0.0115585   -0.00575041   0.0590533     0.0653433   -0.035962     0.11954      0.00506019  -0.0680297    0.0334682    -0.0658268
  0.0351989    -0.202385   -0.056366    -0.0250202   -0.148782      0.019588     0.0703016    -0.0987982   -0.067342     0.0886241    0.0900621   -0.127122      0.0685723     0.0222619   -0.01863      0.0513371    0.10782      0.154806    -0.332954     -0.0632347   -0.191751    -0.140007     0.10678     -0.138623     0.0466446    -0.0495465
  0.0472562    -0.151773    0.0450872   -0.115338    -0.158195      0.0611587    0.0804338    -0.0298409   -0.0743757    0.0754322   -0.194958    -0.158974     -0.0407437    -0.0963414    0.0325143   -0.0273301    0.0861496    0.212832     0.312402      0.0143953    0.144808    -0.0480089    0.143466    -0.0306637    0.0550335    -0.0510996
 -0.264846     -0.147475    0.0249557   -0.0148068   -0.0422616     0.0146394    0.166528      0.0965729   -0.231586     0.0462132    0.155375     0.105144     -0.102689      0.0492895    0.0394966   -0.0289804    0.0843607    0.0338478    0.0504614    -0.0761415    0.107047     0.0794921    0.0768417   -0.132382     0.0303613     0.0119387
  0.0318121     0.146829   -0.148994    -0.0233265    0.134504     -0.0225402    0.050488     -0.0928494   -0.170736     0.0605081    0.0870067   -0.0608254    -0.0128927     0.0686651    0.109741     0.00126087  -0.134362    -0.145781     0.000530112   0.025451    -0.0433257   -0.0731819   -0.120677     0.124642     0.192187      0.0873148
  0.129666      0.120152   -0.59117     -0.0487984    0.000607802   0.0173052    0.0967101     0.231486     0.213015    -0.0537504   -0.0232318   -0.0709842    -0.14375      -0.0246489   -0.066       -0.106209     0.0178773   -0.0942771    0.230275     -0.158348    -0.106484     0.0814196   -0.257364     0.054494    -0.279703     -0.00186586
 -0.0272818     0.0852054   0.364394     0.0200465   -0.0244056     0.00827462   0.0921369    -0.181056     0.156889    -0.0248152   -0.0513249   -0.0641603    -0.202962     -0.0518682   -0.00728795  -0.120529     0.00918999  -0.14204      0.14432      -0.154994     0.0685192    0.00359623  -0.264477     0.0453518    0.00107146    0.0680924
  0.0911096     0.0321528  -0.0421705   -0.127771    -0.0271319    -0.0353062   -0.0743574     0.0791299   -0.108091     0.0451972   -0.1069       0.000714211  -0.000433359  -0.00336172  -0.170476    -0.116645     0.0532934   -0.150821    -0.119029     -0.0461706   -0.0938066    0.0368956   -0.10082     -0.0804396   -0.0119206    -0.146276
 -0.00653318   -0.033085   -0.00578529  -0.0547836    0.0742516    -0.00421076   0.0129754     0.137735     0.0483204    0.18502      0.0393932    0.00307497    0.200236     -0.0886931    0.0450976    0.271966     0.0514547    0.101794     0.0602321     0.135921     0.137362     0.0311877    0.118592    -0.122372    -0.0858642    -0.0344377
  0.0749025     0.111789    0.0362643   -0.259641     0.0927287     0.00977749   0.127332      0.0963271   -0.0232519    0.00203582   0.0228511    0.0519368     0.0141889    -0.00505577  -0.09066      0.00713991   0.00477092  -0.0274374    0.072459      0.0372864    0.0570514    0.116985     0.135854    -0.095735     0.0536705    -0.0638176
 -0.0396615     0.0887465   0.145819    -0.0208705   -0.0142412     0.0837149   -0.141532      0.111963     0.0226323   -0.0450156    0.212474     0.0675482    -0.0756748    -0.0741041    0.0695803    0.0222675   -0.0672572   -0.0279902    0.0511578    -0.0789412    0.095171     0.00188331   0.0831639    0.0456567    0.00117694   -0.150706
  0.0240664     0.107728   -0.148811    -0.0127828    0.0402181     0.0873363    0.0151494    -0.222033    -0.185159    -0.00347783   0.00584961   0.0604858     0.00153038   -0.0100511   -0.024513    -0.0803737    0.0959456    0.045907     0.0729614    -0.103133    -0.124668    -0.0444756   -0.0465551   -0.00237006   0.119981      0.0391012
  0.0864621     0.0794914   0.0981326   -0.0770877   -0.0369689    -0.0326404   -0.0318764     0.277617    -0.0788522   -0.0318808   -0.0971773   -0.0568377    -0.117297     -0.155718    -0.048961     0.253355     0.00958189   0.100149    -0.0751633     0.150102     0.174721     0.10699      0.12397     -0.0430119    0.000652065  -0.0320281
 -0.00435677    0.0615364   0.00386728   0.0200708    0.101167     -0.0398246    0.0758491     0.169245    -0.0550601    0.0286697    0.017332    -0.0691675     0.0427641    -0.0652643    0.119687     0.00671578   0.106592     0.00963814   0.0558757    -0.230961    -0.0450833    0.103165     0.126369     0.0285112    0.0800411    -0.202468
  0.0215106     0.0351617   0.120225     0.0529568    0.00761413   -0.033384    -0.0914953    -0.0215603    0.078482    -0.0591558   -0.0626832    0.0836955     0.0339581     0.0767436   -0.0603286   -0.142951     0.245518    -0.165305    -0.0823237     0.00213813  -0.0286946    0.0626653    0.087533    -0.320386     0.199029      0.0960512[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      7
│     11
│     12
│     14
│     17
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 1, average log likelihood -1.030150
┌ Warning: Variances had to be floored 
│   ind =
│    12-element Array{Int64,1}:
│      7
│      9
│     10
│     11
│      ⋮
│     27
│     28
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 2, average log likelihood -1.000530
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      7
│     11
│     12
│     14
│     17
│     26
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 3, average log likelihood -1.027857
┌ Warning: Variances had to be floored 
│   ind =
│    12-element Array{Int64,1}:
│      7
│      9
│     10
│     11
│      ⋮
│     27
│     28
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.002380
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      7
│     11
│     12
│     14
│     17
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.028074
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      7
│      9
│     10
│     11
│      ⋮
│     27
│     28
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.000222
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      7
│     11
│     12
│     14
│     17
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.030009
┌ Warning: Variances had to be floored 
│   ind =
│    12-element Array{Int64,1}:
│      7
│      9
│     10
│     11
│      ⋮
│     27
│     28
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.000382
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      7
│     11
│     12
│     14
│     17
│     26
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.027854
┌ Warning: Variances had to be floored 
│   ind =
│    12-element Array{Int64,1}:
│      7
│      9
│     10
│     11
│      ⋮
│     27
│     28
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.002375
┌ Info: EM with 100000 data points 10 iterations avll -1.002375
└ 59.0 data points per parameter
[ Info: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
kind diag, method kmeans
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.364416e+05
      1       6.452814e+05      -1.911603e+05 |       32
      2       6.122374e+05      -3.304400e+04 |       32
      3       5.949881e+05      -1.724928e+04 |       32
      4       5.845949e+05      -1.039319e+04 |       32
      5       5.790150e+05      -5.579925e+03 |       32
      6       5.766192e+05      -2.395790e+03 |       32
      7       5.754538e+05      -1.165374e+03 |       32
      8       5.746741e+05      -7.797201e+02 |       32
      9       5.740100e+05      -6.641184e+02 |       32
     10       5.731773e+05      -8.326951e+02 |       32
     11       5.721101e+05      -1.067241e+03 |       32
     12       5.707033e+05      -1.406736e+03 |       32
     13       5.692176e+05      -1.485771e+03 |       32
     14       5.681613e+05      -1.056246e+03 |       32
     15       5.676626e+05      -4.986656e+02 |       32
     16       5.674393e+05      -2.233208e+02 |       32
     17       5.673008e+05      -1.384879e+02 |       31
     18       5.672039e+05      -9.688824e+01 |       32
     19       5.671030e+05      -1.009452e+02 |       32
     20       5.669393e+05      -1.636637e+02 |       32
     21       5.667098e+05      -2.294974e+02 |       32
     22       5.663832e+05      -3.266239e+02 |       32
     23       5.660170e+05      -3.661721e+02 |       31
     24       5.657345e+05      -2.825153e+02 |       31
     25       5.655932e+05      -1.413476e+02 |       30
     26       5.655248e+05      -6.841410e+01 |       32
     27       5.654987e+05      -2.603179e+01 |       29
     28       5.654849e+05      -1.384370e+01 |       30
     29       5.654737e+05      -1.122975e+01 |       27
     30       5.654677e+05      -5.931055e+00 |       29
     31       5.654628e+05      -4.936432e+00 |       25
     32       5.654586e+05      -4.150756e+00 |       22
     33       5.654557e+05      -2.952975e+00 |       24
     34       5.654527e+05      -2.951881e+00 |       22
     35       5.654494e+05      -3.343152e+00 |       23
     36       5.654460e+05      -3.396121e+00 |       22
     37       5.654430e+05      -2.950828e+00 |       22
     38       5.654401e+05      -2.981017e+00 |       23
     39       5.654369e+05      -3.155065e+00 |       20
     40       5.654343e+05      -2.629062e+00 |       18
     41       5.654319e+05      -2.358359e+00 |       18
     42       5.654288e+05      -3.162920e+00 |       20
     43       5.654236e+05      -5.182631e+00 |       18
     44       5.654143e+05      -9.246516e+00 |       25
     45       5.653987e+05      -1.562031e+01 |       24
     46       5.653664e+05      -3.233194e+01 |       29
     47       5.653235e+05      -4.285578e+01 |       31
     48       5.652572e+05      -6.636220e+01 |       30
     49       5.651672e+05      -8.999389e+01 |       28
     50       5.650587e+05      -1.084725e+02 |       31
K-means terminated without convergence after 50 iterations (objv = 565058.6959123886)
┌ Info: K-means with 32000 data points using 50 iterations
└ 37.0 data points per parameter
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.283096
[ Info: iteration 2, average log likelihood -1.255331
[ Info: iteration 3, average log likelihood -1.231829
[ Info: iteration 4, average log likelihood -1.202676
[ Info: iteration 5, average log likelihood -1.157963
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     15
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.094709
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      7
│     20
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.058111
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      1
│      2
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.048689
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.054765
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     12
│     15
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.031447
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      4
│      7
│     13
│     20
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 11, average log likelihood -1.021431
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      1
│     10
│     14
│     22
│     23
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 12, average log likelihood -1.020011
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      2
│      8
│     12
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 13, average log likelihood -1.052923
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     15
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 14, average log likelihood -1.041946
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      1
│      7
│     13
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 15, average log likelihood -1.010664
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      4
│     10
│     12
│     14
│     20
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 16, average log likelihood -1.014193
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 17, average log likelihood -1.050534
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      2
│     15
│     22
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 18, average log likelihood -1.018753
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      1
│      7
│     12
│     13
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 19, average log likelihood -1.034222
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      4
│     10
│     20
│     23
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 20, average log likelihood -1.037725
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 21, average log likelihood -1.039893
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      1
│     12
│     13
│     15
│     22
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 22, average log likelihood -1.003027
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      7
│     14
│     20
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 23, average log likelihood -1.050580
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      4
│     10
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 24, average log likelihood -1.038834
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     2
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 25, average log likelihood -1.017733
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      1
│     12
│     13
│     15
│     22
│     23
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 26, average log likelihood -1.003630
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      7
│     14
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 27, average log likelihood -1.057456
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      4
│     10
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 28, average log likelihood -1.029897
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      8
│     12
│     17
│     20
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 29, average log likelihood -1.006299
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      1
│      2
│     13
│     15
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 30, average log likelihood -1.016420
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      7
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 31, average log likelihood -1.055840
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      4
│     10
│     12
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 32, average log likelihood -1.017473
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      1
│      8
│     22
│     23
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 33, average log likelihood -1.019830
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     13
│     15
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 34, average log likelihood -1.037829
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      2
│      7
│     14
│     20
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 35, average log likelihood -1.025075
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      1
│      4
│     10
│     12
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 36, average log likelihood -1.021449
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      8
│     17
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 37, average log likelihood -1.042319
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     13
│     15
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 38, average log likelihood -1.034999
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      7
│     12
│     20
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 39, average log likelihood -1.023213
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      1
│      2
│      4
│     10
│     22
│     23
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 40, average log likelihood -1.014388
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 41, average log likelihood -1.045888
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│     12
│     13
│     14
│     15
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 42, average log likelihood -1.009344
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      1
│      7
│     20
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 43, average log likelihood -1.040274
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     17
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 44, average log likelihood -1.050577
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      2
│      4
│      8
│     10
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 45, average log likelihood -0.998131
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      1
│     12
│     13
│     15
│     22
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 46, average log likelihood -1.014229
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      7
│     20
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 47, average log likelihood -1.068782
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     10
│     17
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 48, average log likelihood -1.033880
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      8
│     23
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 49, average log likelihood -1.007578
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      1
│      2
│     12
│     13
│     14
│     15
│     22
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 50, average log likelihood -0.994456
32×26 Array{Float64,┌ Info: EM with 100000 data points 50 iterations avll -0.994456
└ 59.0 data points per parameter
2}:
 -0.098118    -0.164353    -0.00868774   0.0200299    -0.0332049    0.0816181   -0.0559761    -0.113829     0.0256727    0.0392468    -0.209393    -0.083984     0.0323988   -0.0409494   -0.232919    -0.154631    -0.0507637   -0.0612981    0.00314276  -0.136838     -0.115423    -0.254007     0.080139     -0.171457     0.114784      0.0343989
 -0.0683283   -0.0799979   -0.00170686   0.222805     -0.0766979   -0.113399     0.166898      0.00140832  -0.0212705    0.0520745    -0.0103869    0.0160616    0.10091     -0.0254055   -0.0960411   -0.0457577    0.151016     0.11526     -0.0392221    0.0455662    -0.0268624    0.0705934   -0.0147968    -0.0600774    0.0995648    -0.0520695
 -0.0851785    0.135502    -0.0675912    0.000781994   0.00531456   0.00436463  -0.00170773   -0.105024     0.0127392   -0.0556654    -0.0934831    0.0630699   -0.175543    -0.0211773    0.12912      0.245367    -0.0881595    0.198286     0.0762541   -0.0342591    -0.0744494   -0.0851998   -0.0314527     0.0175947   -0.0426581     0.0167496
 -0.0825288    0.072229     0.110648     0.0112152    -0.0206857    0.0784061   -0.190459      0.0797072    0.0376948    0.0447504     0.111702     0.0645392   -0.132802    -0.0827119    0.0817386    0.0226735   -0.0290273    0.0511595    0.0699974   -0.0586845     0.0584842   -0.0112064    0.0866265     0.0577875    0.102979     -0.10409
 -0.0832056    0.0923589    0.0716348   -0.0736909     0.0453802   -0.0063757   -0.119879     -0.029851    -0.0494301    0.0542171     0.230523     0.0147631   -0.187582    -0.0325026    0.194384    -0.105084    -0.0904985    0.0219821    0.0478131    0.108004     -0.0854572   -0.192668    -0.0890426     0.0647785    0.0413505    -0.0355931
 -0.153689     0.0667601   -0.222132     0.050218      0.0839682    0.0668741   -0.0193598     0.0673324    0.041522    -0.0663154     0.136432     0.00573426   0.270265    -0.0569374   -0.0657145   -0.0707138    0.0550555   -0.101234    -0.185271     0.042566      0.0832083    0.047616     0.0402601    -0.00512206   0.0207966     0.0663844
  0.0170818    0.106999    -0.132176    -0.013259      0.0448442    0.08482      0.00646695   -0.225528    -0.187586    -0.000885938   0.00184122   0.0575195    0.00139067  -0.0109027   -0.0234194   -0.0780455    0.0879759    0.039365     0.0688093   -0.102034     -0.120962    -0.0394908   -0.0583558     0.0032248    0.117579      0.0326321
 -0.0586733   -0.0501819    0.043163     0.047755     -0.0222188    0.0340284   -0.0267168     0.138094     0.0599146    0.189989      0.185804    -0.0876815   -0.0384532   -0.00213653   0.00598728  -0.0545916    0.034543    -0.151229    -0.0171684    0.0546271    -0.00262361  -0.068106    -0.0958277     0.0869701    0.00316103   -0.00133629
  0.0282834    0.0803965   -0.0536273    0.130499      0.0158363    0.160759    -0.0598736    -0.141291    -0.0542781    0.0617861    -0.0175128    0.0147429    0.0958002   -0.144153    -0.0597971   -0.0168117    0.194243     0.0215872   -0.0632049    0.16608       0.0814053   -0.00498166  -0.0467334    -0.0991563    0.0619097    -0.0534719
 -0.0811681   -0.023849     0.0580777   -0.0307979     0.168862    -0.160071    -0.0250625    -0.0733618   -0.177865     0.0621071     0.223105    -0.120312     0.0562706   -0.0303898    0.0656876    0.0454989   -0.0797212   -0.0378867    0.166305     0.000959088  -0.0848175   -0.105434     0.0201772     0.0384753    0.14579      -0.0176617
  0.0545887    0.101456    -0.12845     -0.0168526    -0.0136193    0.0131842    0.0944519     0.0287838    0.18593     -0.0393116    -0.0388774   -0.0670144   -0.172113    -0.0380744   -0.0369196   -0.113185     0.0133998   -0.118621     0.189108    -0.156687     -0.0209229    0.0444671   -0.261484      0.0497429   -0.143678      0.0314716
 -0.0902306    0.137897    -0.0720313   -0.0808551    -0.126763    -0.0656805    0.0588932    -0.0482783   -0.128918     0.0397031    -0.00669067  -0.016637    -0.0495454   -0.00845302  -0.0590014    0.0438354    0.0803363    0.103942    -0.0765096    0.0449905    -0.168468     0.0652536   -0.0774716     0.100961    -0.186814     -0.131855
  0.0141179    0.16099     -0.1226      -0.115763      0.0329708    0.11368     -0.0758846    -0.130339    -0.237309     0.0794834    -0.155711    -0.0289529   -0.124485     0.0273776   -0.0245939    0.0301493    0.193004     0.0227638   -0.0124759    0.10627       0.0538747   -0.208621     0.0550555    -0.118053    -0.0836089    -0.0173379
  0.00854968   0.128639     0.154259     0.0692522    -0.0256525   -0.0377043    0.154326     -0.189496    -0.0400232   -0.0887197     0.0156885   -0.0463568    0.107829     0.0671333   -0.176292    -0.201388    -0.0727708    0.0283479    0.126694     0.0144793    -0.134907    -0.14682     -0.0289801    -0.00298873  -0.0998795    -0.0467677
  0.083678     0.112809     0.0334542   -0.258213      0.0931484    0.00700304   0.122787      0.0932419   -0.0190916    0.011008      0.0204391    0.0554598    0.012778    -0.00446743  -0.0931004    0.00328173   0.00721938  -0.0303665    0.0700577    0.0322021     0.0548646    0.113228     0.137006     -0.0938185    0.059597     -0.0679334
  0.00946548   0.0479526    0.0625664    0.0388632     0.0526649   -0.0360447   -0.00689313    0.0754737    0.0131525   -0.0185786    -0.0232486    0.00838401   0.0381616    0.0072099    0.0302865   -0.0700113    0.18611     -0.080877    -0.0130072   -0.109874     -0.0362531    0.0817555    0.103637     -0.146598     0.139994     -0.0494575
  0.0958023    0.0859929   -0.162109    -0.279868     -0.0510027    0.0760222   -0.044121      0.0602998    0.0531085    0.0302706    -0.0229506    0.121648    -0.140766     0.059038     0.0046113   -0.250795    -0.0784389    0.356787    -0.16985      0.0845376     0.029852    -0.159329     0.00200763    0.0894126    0.188757     -0.0677772
 -0.00106482  -0.08945      0.0549238    0.149596     -0.0480605    0.121151    -0.258545     -0.0631267   -0.0153919   -0.0101369     0.0597634    0.0551638   -0.0708759   -0.0874692   -0.0767556    0.0168293    0.125572     0.0287487    0.0746143    0.00263211   -0.00202862  -0.0937529    0.0960535    -0.0122743    0.137876     -0.167601
  0.109879    -0.0891258   -0.0900528   -0.13866       0.0215053   -0.0484454   -0.0230249     0.0991843   -0.0624418    0.205034     -0.147451    -0.0700114   -0.139997     0.0200815   -0.0599161   -0.0783786    0.0949161    0.0585897   -0.0170976    0.209689     -0.0197012    0.0788337    0.108983      0.00929919  -0.0239109     0.00394517
  0.0185943    0.129455     0.0278996    0.0199201    -0.0282656    0.0495302   -0.0579307    -0.0431531    0.00878138  -0.087376      0.0495912    0.0356023   -0.0399952   -0.0161398   -0.0803323   -0.0317219   -0.0638193    0.00029129   0.0297561    0.0103116     0.035194    -0.0741946   -0.0275461     0.0102978   -0.0538216    -0.0884735
  0.0328972    0.14729     -0.148896    -0.0254582     0.133578    -0.0230128    0.0494396    -0.092398    -0.167197     0.0599999     0.0891894   -0.0606942   -0.0136858    0.0662547    0.112306     0.00142546  -0.134214    -0.144402     0.00200084   0.0296867    -0.0431175   -0.0751929   -0.121266      0.125292     0.19194       0.0848689
  0.00126227  -0.145623    -0.00138929   0.0138541     0.151284    -0.0765531    0.0269938     0.0654921    0.0763536    0.0463373     0.144289    -0.00135416  -0.110829    -0.0685318   -0.105558    -0.0163753    0.0433623   -0.0477337    0.0533584    0.188296      0.00670387   0.0614068    0.0367512     0.0357136   -0.0418452    -0.167411
 -0.0223463    0.0533752    0.130346    -0.0218701     0.015259     0.0463377   -0.171378      0.15484      0.0487189   -0.029342      0.222593    -0.0262584   -0.0568012   -0.0380231    0.0792825   -0.0320882   -0.0521916   -0.0497045    0.0158263    0.00108717    0.081032    -0.0231846    0.0869138     0.0397458   -0.0105765    -0.0735236
  0.0435767   -0.177171    -0.00861661  -0.0672916    -0.152898     0.0383145    0.0742572    -0.0702947   -0.0706322    0.0832104    -0.0453302   -0.1416       0.0161895   -0.032911     0.00664789   0.0135228    0.0961418    0.184852    -0.0319833   -0.0274244    -0.0364693   -0.0990425    0.125214     -0.0859588    0.0488674    -0.0499426
  0.104894    -0.0993428    0.0967704    0.0806404     0.075879    -0.0148393    0.0567441     0.169162     0.0578741    0.146522      0.0762818   -0.0973992   -0.143346    -0.014878    -0.0186512    0.091815    -0.0834244   -0.0702329    0.16675     -0.0888261    -0.0366197    0.183259    -0.12042      -0.159147     0.0850593    -0.169008
 -0.263401    -0.148427     0.0248004   -0.0124955    -0.0393753    0.0142384    0.165323      0.0988494   -0.231848     0.0459448     0.156334     0.104988    -0.102541     0.0493426    0.0386545   -0.0295639    0.0841517    0.0338132    0.0501907   -0.0754925     0.106874     0.0793477    0.0768939    -0.132419     0.0295279     0.0106957
 -0.0233337    0.0200767   -0.122532    -0.00367871    0.0107597    0.182441     0.0353977     0.319337    -0.0686637    0.0390862     0.0916875   -0.0943763    0.00979564   0.0191085   -0.122048    -0.0163123   -0.0641477   -0.0341574    0.0532729    0.100719     -0.0146604    0.0499748    0.157712     -0.0598539   -0.0247251     0.0126596
 -0.0675175    0.171415    -0.0215666   -0.100577      0.109476    -0.104789     0.00576533   -0.0840982    0.152985    -0.0436853     0.120863    -0.101648     0.158799     0.0605036   -0.0106415   -0.0305418    0.248359    -0.126842     0.027777    -0.16767      -0.193253    -0.10281     -0.0944918    -0.0258032   -0.0758286    -0.123506
  0.0385971    0.00160365  -0.0123381   -0.0936578     0.0176373   -0.0175947   -0.0335927     0.107564    -0.0277023    0.113078     -0.0190598   -0.00070857   0.091367    -0.0457041   -0.064977     0.069158     0.0505715   -0.0179869   -0.0277036    0.0418171     0.0241834    0.0229844    0.00611846   -0.100984    -0.0474699    -0.0872274
  0.0865154    0.0789858    0.10234     -0.0771046    -0.037263    -0.0342618   -0.0337893     0.276135    -0.0863922   -0.0300931    -0.0986986   -0.0591104   -0.116018    -0.156218    -0.0475858    0.250072     0.00694691   0.0994979   -0.0694147    0.148183      0.176291     0.107595     0.122858     -0.0402899   -0.000375916  -0.0302048
 -0.136365     0.0454694    0.0482048    0.0722658    -0.19887     -0.00149378   0.0331848    -0.0301053   -0.18553     -0.0927015     0.0352529   -0.019496    -0.115059     0.209455    -0.0188946   -0.11119      0.0729204    0.178234    -0.00751384  -0.151563     -0.0449494    0.0733356    0.0611038     0.0939723 [ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
   0.0847204    -0.090732
 -0.153015     0.108385    -0.0023297    0.086972      0.0557065   -0.0176849   -0.000536508  -0.00188562   0.197565    -0.0403118     0.038405    -0.00781578   0.110647    -0.00438467  -0.0542665   -0.0570653    0.0052555   -0.0480368   -0.073294    -0.0707957    -0.085552     0.0363887   -0.000969721  -0.250093     0.0187938    -0.193115┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 1, average log likelihood -1.081284
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      4
│      7
│     10
│     17
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 2, average log likelihood -1.009591
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      7
│      8
│     10
│     12
│     13
│     15
│     17
│     20
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 3, average log likelihood -0.979719
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      1
│      4
│      7
│     10
│     17
│     22
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.020117
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      2
│      7
│     10
│     17
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.022881
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      4
│      7
│      8
│     10
│     12
│     15
│     17
│     20
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -0.991881
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      7
│     10
│     13
│     14
│     17
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.010432
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      1
│      4
│      7
│     10
│     17
│     22
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -0.988695
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      2
│      7
│      8
│     10
│     12
│     17
│     20
│     23
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.004437
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      4
│      7
│     13
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.041564
┌ Info: EM with 100000 data points 10 iterations avll -1.041564
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.132102     -0.0524977    -0.10225      0.0124996   -0.0940103   -0.1034       0.0488342    0.108819     -0.0127865   -0.0888889    -0.0223674   0.0559722   0.0384457    -0.0412163   -0.0840519  -0.0500115     0.123768     0.117059     0.160079     -0.0583363     0.059769     0.114485     -0.127216     0.0295869    0.0721099    -0.151551
  0.08312      -0.137892     -0.0739025   -0.0592734    0.196763    -0.00507446  -0.11853     -0.030224     -0.244102    -0.0233281     0.0388506  -0.11753    -0.0300655    -0.126746     0.144386    0.0945594    -0.0145048    0.104089    -0.0553602     0.0788697     0.0102858    0.0916046     0.0422434   -0.0217357   -0.00877439   -0.109136
  0.0871746     0.158971     -0.0236133    0.0200125   -0.0649145    0.434601    -0.092852     0.000368121   0.144072    -0.0974713     0.0820655  -0.0253208   0.0126749    -0.00886524   0.0449604   0.0148551    -0.146473    -0.0758894   -0.0274259     0.0297447    -0.0864923   -0.0397286    -0.103321    -0.0125175    0.057387     -0.0271461
  0.234745      0.117401      0.159165    -0.0625802   -0.206803     0.026119     0.0343592   -0.0765054     0.0983535   -0.0443962    -0.0167236   0.15348    -0.16463      -0.0567201   -0.1122     -0.034425      0.105294     0.0569849    0.0242182    -0.0360368    -0.133355    -0.212414      0.103785    -0.0967108   -0.112407      0.0934906
 -0.0742346     0.00193276    0.0562482    0.0920002   -0.163578     0.111314     0.0535315   -0.0654329     0.110092     0.0716355    -0.113898    0.0783337   0.0347464     0.0145687    0.0169081   0.0179055     0.201709     0.00242261  -0.0458096    -0.133552     -0.0744838    0.0273255     0.208206    -0.0319705   -0.0167122    -0.100476
  0.189455     -0.0791421     0.0830361    0.0112251    0.0799299   -0.0856409    0.0914134    0.139721      0.133783    -0.100438      0.0472933  -0.066203   -0.172701      0.124893    -0.197549    0.0588055     0.039451    -0.148767     0.113016      0.0576968    -0.161987     0.196878     -0.0357029    0.0461863   -0.025277     -0.035461
 -0.00342786    0.0755495     0.110169    -0.070089    -0.0745579   -0.0631407   -0.0872153   -0.193647      0.149881    -0.166707     -0.0996261   0.0500593  -0.000264259   0.108013     0.0425394   0.0658253     0.163177     0.0324638   -0.093383     -0.0847908    -0.0739238    0.0199707     0.0728262   -0.00761602   0.00789503   -0.130122
  0.182369      0.059905      0.139949    -0.2828       0.0173571    0.0618645   -0.0588089    0.104081     -0.0373382   -0.218951     -0.167047   -0.0224345   0.136477     -0.230547     0.0207181  -0.0341465    -0.0947966    0.094846    -0.110068      0.0719214    -0.0083232   -0.0120348    -0.172999     0.00734107   0.0470869    -0.0497548
  0.000562319  -0.168273      0.00234962  -0.0975574   -0.0715763    0.0600325   -0.186423    -0.0871394    -0.0307419   -0.0849976    -0.0377001  -0.057393   -0.0151527     0.0500525   -0.0350766  -0.182173     -0.0720223   -0.224512    -0.182475      0.039945      0.058929    -0.046639     -0.121531     0.111492     0.0711444     0.0667029
  0.0154665     0.0827978    -0.124824     0.155937     0.0742887   -0.172324     0.12977     -0.118447     -0.114727     0.0978298     0.0201739  -0.12216    -0.0854427    -0.135422    -0.158666    0.212746     -0.073307    -0.0481784    0.0233597    -0.0324289     0.141257    -0.0428643    -0.190402     0.0628954   -0.0815734     0.105537
 -0.00242291   -0.016378     -0.00213591  -0.121145    -0.075032     0.12244      0.0331152    0.135259     -0.0611716   -0.000175145   0.0375988  -0.0354321  -0.00186912   -0.118015    -0.109619    0.0513192     0.0610908   -0.0750136    0.000891036   0.0132633     0.0577186   -0.00352618    0.0817385   -0.0206552   -0.0988073     0.189968
  0.0106863    -0.0510998     0.0385521   -0.0423122   -0.100885     0.126058     0.0993029    0.0319693     0.115524    -0.042538      0.122522   -0.0176848   0.000759676   0.0559483   -0.0616145  -0.00402442    0.00507087   0.032585     0.16006      -0.0273287    -0.108744    -0.151202      0.0628344    0.101281     0.0527044    -0.0046324
  0.174581      0.134606     -0.02381      0.17982      0.151984    -0.173244    -0.0992703   -0.0901571     0.150063    -0.0716447    -0.132795   -0.0693828  -0.04977       0.195234    -0.0464572   0.0173436     0.0137244   -0.0429451    0.0344673     0.0206102     0.0222282    0.0108046     0.0140985    0.0223289   -0.200501      0.0737184
 -0.128872     -0.136782     -0.0956987   -0.020234    -0.0698842    0.200722     0.109244    -0.105193      0.00837809   0.115046     -0.04702     0.0214531  -0.132599     -0.12415     -0.0732633   0.140644     -0.116528     0.0862174   -0.056234     -0.129814     -0.0734009    0.00399214    0.0298199   -0.184721    -0.0727051     0.0240529
 -0.0585376     0.122949     -0.109315     0.0444748   -0.0510458   -0.122863    -0.0470446   -0.157335     -0.212089    -0.0308108    -0.0702541   0.0891572  -0.37239      -0.0259454   -0.0395299   0.000439028  -0.0281469    0.0625343    0.113267      0.0387464     0.0738861    0.188541     -0.0269276    0.0970689    0.0194213    -0.00796854
 -0.125472      0.00568953    0.083009    -0.260284    -0.0626025    0.0131531   -0.0299392    0.0724956     0.0922936   -0.0215165    -0.0217711   0.0870065  -0.0483203    -0.00528911  -0.108594    0.0800351    -0.00195543   0.0207186    0.00263529    0.127605      0.0309662   -0.124776      0.26221     -0.017258     0.140007     -0.000620372
 -0.0980149     0.0293917    -0.059275     0.0316065    0.152485    -0.0940745    0.403795    -0.0775545    -0.136999    -0.189892     -0.161238    0.089348   -0.062485      0.00173875  -0.0163109  -0.0325895     0.00776275  -0.112597    -0.0256419     0.0977857    -0.0363037   -0.100961     -0.095882     0.0444389    0.0596552     0.141141
  0.158952      0.148714     -0.0857451    0.0390174   -0.0599341   -0.0662857   -0.209609    -0.107735      0.306345     0.138609      0.0714206  -0.0274046   0.167573      0.12273      0.0316553  -0.0345899    -0.0156578   -0.0494354    0.0575996     0.0749904     0.103721     0.0635312     0.0940982   -0.00287995  -0.0942285    -0.0287121
 -0.0143856     0.0126108    -0.0802444    0.161382    -0.136233     0.0826499   -0.0571362   -0.072901      0.278887    -0.00995903   -0.0121543   0.158009    0.00860901    0.0693892   -0.123035   -0.177467     -0.0230656    0.00477505   0.0587444    -0.0751143     0.0475588    0.000117303   0.00624791   0.0317774    0.118764     -0.114794
  0.134004      0.0200616     0.135219    -0.0277105    0.00556123   0.14349      0.0940836    0.110944      0.00276044  -0.0909361    -0.174225    0.122664    0.00545086   -0.0668231    0.0232015   0.16672       0.138146    -0.00135508   0.095131      0.0645129    -0.0671897   -0.0905203     0.0580338    0.00750334   0.110056      0.057393
 -0.00631608   -0.0107619    -0.0851921    0.0267276   -0.179409     0.0831346   -0.121528     0.133426     -0.103574     0.138915     -0.112399   -0.0924921   0.0608832    -0.0599216    0.128323    0.0339862    -0.0612367    0.109625     0.0571213     0.141414     -0.00640099   0.153664     -0.0946631   -0.0577379   -0.149477      0.0293775
 -0.175369     -0.128669     -0.0972299   -0.0495454    0.0418679    0.0168078   -0.143025    -0.0207035    -0.0782721    0.0787334    -0.164689    0.0854152  -0.0142917     0.0182387   -0.120542    0.138387      0.018273     0.0407933   -0.179955     -0.178576      0.081463    -0.0773157    -0.1589      -0.0343154    0.104163      0.0884692
  0.079551     -0.0574246    -0.186388    -0.00106761   0.15304     -0.0325504   -0.0951057   -0.0217115    -0.171785    -0.133371     -0.195963    0.200922    0.150546      0.0933442   -0.155249    0.096601     -0.0461442   -0.172522    -0.048295      0.0867478    -0.186823    -0.0327942     0.00391345  -0.00311268  -0.0532536    -0.126802
  0.123392     -0.148391      0.108172     0.109536    -0.193704     0.0657629   -0.00470885  -0.128125     -0.264196    -0.154258      0.0269732  -0.035335    0.0834929    -0.194383     0.119956   -0.0896553     0.0741944    0.0488461   -0.078671     -0.160523     -0.00550522  -0.226961     -0.0535964    0.0237236    0.0318702     0.0440525
 -0.0678568     0.23895      -0.0858876    0.0714131    0.0616011   -0.00306015   0.078669     0.0323596    -0.0420745   -0.188855     -0.0625016   0.059741    0.0316082     0.00957088  -0.0324554  -0.116934      0.0249179   -0.20448     -0.0700203     0.0313248     0.0538811   -0.0447725    -0.208729     0.00937011  -0.0972715     0.0605609
  0.00586423   -0.139168      0.0215508    0.0325277    0.0890781    0.106361    -0.0721736   -0.106091      0.0148402    0.0902002     0.0138408   0.121081    0.0312714     0.15583     -0.073712    0.065238      0.0215603   -0.059565    -0.112482     -0.0938517    -0.255067    -0.0853795    -0.0215911   -0.0525729   -0.00660937    0.0919028
 -0.230707     -0.0755254     0.0506177    0.25482     -0.157559     0.0832428   -0.0819447   -0.0601061    -0.0834228    0.112088      0.0924691  -0.0308156   0.0313511    -0.0674971    0.0854831   0.087435      0.148429    -0.074538     0.204601     -0.0212263    -0.0678955   -0.189466      0.109596     0.014406    -0.000314716   0.0091446
 -0.0204563     0.00501615    0.137614    -0.167028     0.0760064   -0.0729779    0.195617     0.02419       0.0157468    0.00794572    0.0944511   0.0578559  -0.0135905    -0.0285331    0.0588791   0.121482     -0.0240166   -0.130544     0.10606      -0.0829265    -0.0281349    0.0861652     0.0382724    0.165593     0.0927247     0.181147
 -0.00622716    0.0111542    -0.0300776    0.076687     0.103424     0.0288885   -0.0305581   -0.0816437     0.0623105    0.0668505     0.0176672  -0.110619    0.295259     -0.00687742   0.079019    0.0182641     0.214158    -0.0464922   -0.026604      0.000528605  -0.0250531   -0.00985689   -0.0620574   -0.0116608    0.0240607     0.0761137
  0.16818      -0.000917667   0.0112925   -0.0797088   -0.14119      0.0563023    0.0226025   -0.0190113    -0.0709603   -0.039004      0.074294   -0.0502119  -0.0397345    -0.133687    -0.197374   -0.0120222     0.103823     0.0557782    0.0582314    -0.0599354    -0.0941957   -0.053021     -0.00408621  -0.145431    -0.0810611    -0.0965234
  0.0174954    -0.0646485    -0.132143     0.0361738   -0.0378031    0.0981909   -0.103366     0.0496146     0.131404    -0.159156     -0.0834285   0.102768   -0.0380244     0.00418168  -0.0128041  -0.0846664     0.0071547   -0.250203     0.176398     -0.0180277     0.0190163   -0.0201772    -0.0520275   -0.0124286    0.0905284     0.0132
  0.175229      0.013103      0.0418947    0.141895    -0.00983941  -0.0385969    0.00625542   0.0907659    -0.0219235    0.0783884    -0.0152337   0.119728    0.161377      0.0425746    0.148991   -0.105631      0.0716795   -0.0614236   -0.134335      0.0959872     0.170879    -0.00340127    0.0918857   -0.0252665    0.0648249     0.244898kind full, method split
┌ Info: 0: avll = 
└   tll[1] = -1.4208402893437697
[ Info: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.420859
[ Info: iteration 2, average log likelihood -1.420791
[ Info: iteration 3, average log likelihood -1.420737
[ Info: iteration 4, average log likelihood -1.420668
[ Info: iteration 5, average log likelihood -1.420564
[ Info: iteration 6, average log likelihood -1.420384
[ Info: iteration 7, average log likelihood -1.420035
[ Info: iteration 8, average log likelihood -1.419358
[ Info: iteration 9, average log likelihood -1.418274
[ Info: iteration 10, average log likelihood -1.417051
[ Info: iteration 11, average log likelihood -1.416157
[ Info: iteration 12, average log likelihood -1.415706
[ Info: iteration 13, average log likelihood -1.415519
[ Info: iteration 14, average log likelihood -1.415446
[ Info: iteration 15, average log likelihood -1.415417
[ Info: iteration 16, average log likelihood -1.415405
[ Info: iteration 17, average log likelihood -1.415400
[ Info: iteration 18, average log likelihood -1.415398
[ Info: iteration 19, average log likelihood -1.415396
[ Info: iteration 20, average log likelihood -1.415396
[ Info: iteration 21, average log likelihood -1.415395
[ Info: iteration 22, average log likelihood -1.415395
[ Info: iteration 23, average log likelihood -1.415394
[ Info: iteration 24, average log likelihood -1.415394
[ Info: iteration 25, average log likelihood -1.415394
[ Info: iteration 26, average log likelihood -1.415394
[ Info: iteration 27, average log likelihood -1.415393
[ Info: iteration 28, average log likelihood -1.415393
[ Info: iteration 29, average log likelihood -1.415393
[ Info: iteration 30, average log likelihood -1.415393
[ Info: iteration 31, average log likelihood -1.415393
[ Info: iteration 32, average log likelihood -1.415393
[ Info: iteration 33, average log likelihood -1.415393
[ Info: iteration 34, average log likelihood -1.415393
[ Info: iteration 35, average log likelihood -1.415392
[ Info: iteration 36, average log likelihood -1.415392
[ Info: iteration 37, average log likelihood -1.415392
[ Info: iteration 38, average log likelihood -1.415392
[ Info: iteration 39, average log likelihood -1.415392
[ Info: iteration 40, average log likelihood -1.415392
[ Info: iteration 41, average log likelihood -1.415392
[ Info: iteration 42, average log likelihood -1.415392
[ Info: iteration 43, average log likelihood -1.415392
[ Info: iteration 44, average log likelihood -1.415392
[ Info: iteration 45, average log likelihood -1.415392
[ Info: iteration 46, average log likelihood -1.415392
[ Info: iteration 47, average log likelihood -1.415392
[ Info: iteration 48, average log likelihood -1.415392
[ Info: iteration 49, average log likelihood -1.415392
[ Info: iteration 50, average log likelihood -1.415392
┌ Info: EM with 100000 data points 50 iterations avll -1.415392
└ 952.4 data points per parameter
┌ Info: 1
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4208588956362957
│     -1.4207910237066805
│      ⋮
└     -1.4153918557122411
[ Info: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.415407
[ Info: iteration 2, average log likelihood -1.415336
[ Info: iteration 3, average log likelihood -1.415271
[ Info: iteration 4, average log likelihood -1.415188
[ Info: iteration 5, average log likelihood -1.415082
[ Info: iteration 6, average log likelihood -1.414956
[ Info: iteration 7, average log likelihood -1.414826
[ Info: iteration 8, average log likelihood -1.414710
[ Info: iteration 9, average log likelihood -1.414619
[ Info: iteration 10, average log likelihood -1.414554
[ Info: iteration 11, average log likelihood -1.414508
[ Info: iteration 12, average log likelihood -1.414474
[ Info: iteration 13, average log likelihood -1.414447
[ Info: iteration 14, average log likelihood -1.414426
[ Info: iteration 15, average log likelihood -1.414407
[ Info: iteration 16, average log likelihood -1.414391
[ Info: iteration 17, average log likelihood -1.414376
[ Info: iteration 18, average log likelihood -1.414363
[ Info: iteration 19, average log likelihood -1.414351
[ Info: iteration 20, average log likelihood -1.414339
[ Info: iteration 21, average log likelihood -1.414327
[ Info: iteration 22, average log likelihood -1.414316
[ Info: iteration 23, average log likelihood -1.414305
[ Info: iteration 24, average log likelihood -1.414295
[ Info: iteration 25, average log likelihood -1.414284
[ Info: iteration 26, average log likelihood -1.414275
[ Info: iteration 27, average log likelihood -1.414266
[ Info: iteration 28, average log likelihood -1.414257
[ Info: iteration 29, average log likelihood -1.414249
[ Info: iteration 30, average log likelihood -1.414241
[ Info: iteration 31, average log likelihood -1.414234
[ Info: iteration 32, average log likelihood -1.414228
[ Info: iteration 33, average log likelihood -1.414222
[ Info: iteration 34, average log likelihood -1.414216
[ Info: iteration 35, average log likelihood -1.414211
[ Info: iteration 36, average log likelihood -1.414206
[ Info: iteration 37, average log likelihood -1.414201
[ Info: iteration 38, average log likelihood -1.414197
[ Info: iteration 39, average log likelihood -1.414193
[ Info: iteration 40, average log likelihood -1.414189
[ Info: iteration 41, average log likelihood -1.414185
[ Info: iteration 42, average log likelihood -1.414181
[ Info: iteration 43, average log likelihood -1.414177
[ Info: iteration 44, average log likelihood -1.414174
[ Info: iteration 45, average log likelihood -1.414170
[ Info: iteration 46, average log likelihood -1.414167
[ Info: iteration 47, average log likelihood -1.414163
[ Info: iteration 48, average log likelihood -1.414160
[ Info: iteration 49, average log likelihood -1.414156
[ Info: iteration 50, average log likelihood -1.414153
┌ Info: EM with 100000 data points 50 iterations avll -1.414153
└ 473.9 data points per parameter
┌ Info: 2
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4154068490004856
│     -1.415335790780558
│      ⋮
└     -1.4141530440568684
[ Info: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.414160
[ Info: iteration 2, average log likelihood -1.414105
[ Info: iteration 3, average log likelihood -1.414056
[ Info: iteration 4, average log likelihood -1.413999
[ Info: iteration 5, average log likelihood -1.413930
[ Info: iteration 6, average log likelihood -1.413845
[ Info: iteration 7, average log likelihood -1.413750
[ Info: iteration 8, average log likelihood -1.413650
[ Info: iteration 9, average log likelihood -1.413554
[ Info: iteration 10, average log likelihood -1.413466
[ Info: iteration 11, average log likelihood -1.413389
[ Info: iteration 12, average log likelihood -1.413321
[ Info: iteration 13, average log likelihood -1.413261
[ Info: iteration 14, average log likelihood -1.413208
[ Info: iteration 15, average log likelihood -1.413162
[ Info: iteration 16, average log likelihood -1.413122
[ Info: iteration 17, average log likelihood -1.413087
[ Info: iteration 18, average log likelihood -1.413058
[ Info: iteration 19, average log likelihood -1.413032
[ Info: iteration 20, average log likelihood -1.413009
[ Info: iteration 21, average log likelihood -1.412989
[ Info: iteration 22, average log likelihood -1.412971
[ Info: iteration 23, average log likelihood -1.412954
[ Info: iteration 24, average log likelihood -1.412939
[ Info: iteration 25, average log likelihood -1.412924
[ Info: iteration 26, average log likelihood -1.412910
[ Info: iteration 27, average log likelihood -1.412898
[ Info: iteration 28, average log likelihood -1.412885
[ Info: iteration 29, average log likelihood -1.412874
[ Info: iteration 30, average log likelihood -1.412863
[ Info: iteration 31, average log likelihood -1.412853
[ Info: iteration 32, average log likelihood -1.412843
[ Info: iteration 33, average log likelihood -1.412834
[ Info: iteration 34, average log likelihood -1.412825
[ Info: iteration 35, average log likelihood -1.412817
[ Info: iteration 36, average log likelihood -1.412809
[ Info: iteration 37, average log likelihood -1.412802
[ Info: iteration 38, average log likelihood -1.412795
[ Info: iteration 39, average log likelihood -1.412788
[ Info: iteration 40, average log likelihood -1.412782
[ Info: iteration 41, average log likelihood -1.412776
[ Info: iteration 42, average log likelihood -1.412770
[ Info: iteration 43, average log likelihood -1.412765
[ Info: iteration 44, average log likelihood -1.412759
[ Info: iteration 45, average log likelihood -1.412754
[ Info: iteration 46, average log likelihood -1.412750
[ Info: iteration 47, average log likelihood -1.412745
[ Info: iteration 48, average log likelihood -1.412741
[ Info: iteration 49, average log likelihood -1.412736
[ Info: iteration 50, average log likelihood -1.412732
┌ Info: EM with 100000 data points 50 iterations avll -1.412732
└ 236.4 data points per parameter
┌ Info: 3
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4141600199773179
│     -1.414105136147477
│      ⋮
└     -1.4127320409815913
[ Info: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.412738
[ Info: iteration 2, average log likelihood -1.412688
[ Info: iteration 3, average log likelihood -1.412646
[ Info: iteration 4, average log likelihood -1.412602
[ Info: iteration 5, average log likelihood -1.412550
[ Info: iteration 6, average log likelihood -1.412488
[ Info: iteration 7, average log likelihood -1.412416
[ Info: iteration 8, average log likelihood -1.412334
[ Info: iteration 9, average log likelihood -1.412245
[ Info: iteration 10, average log likelihood -1.412151
[ Info: iteration 11, average log likelihood -1.412056
[ Info: iteration 12, average log likelihood -1.411963
[ Info: iteration 13, average log likelihood -1.411874
[ Info: iteration 14, average log likelihood -1.411790
[ Info: iteration 15, average log likelihood -1.411711
[ Info: iteration 16, average log likelihood -1.411638
[ Info: iteration 17, average log likelihood -1.411572
[ Info: iteration 18, average log likelihood -1.411512
[ Info: iteration 19, average log likelihood -1.411457
[ Info: iteration 20, average log likelihood -1.411408
[ Info: iteration 21, average log likelihood -1.411364
[ Info: iteration 22, average log likelihood -1.411323
[ Info: iteration 23, average log likelihood -1.411286
[ Info: iteration 24, average log likelihood -1.411251
[ Info: iteration 25, average log likelihood -1.411219
[ Info: iteration 26, average log likelihood -1.411188
[ Info: iteration 27, average log likelihood -1.411158
[ Info: iteration 28, average log likelihood -1.411130
[ Info: iteration 29, average log likelihood -1.411103
[ Info: iteration 30, average log likelihood -1.411076
[ Info: iteration 31, average log likelihood -1.411051
[ Info: iteration 32, average log likelihood -1.411027
[ Info: iteration 33, average log likelihood -1.411003
[ Info: iteration 34, average log likelihood -1.410980
[ Info: iteration 35, average log likelihood -1.410958
[ Info: iteration 36, average log likelihood -1.410937
[ Info: iteration 37, average log likelihood -1.410917
[ Info: iteration 38, average log likelihood -1.410897
[ Info: iteration 39, average log likelihood -1.410879
[ Info: iteration 40, average log likelihood -1.410861
[ Info: iteration 41, average log likelihood -1.410844
[ Info: iteration 42, average log likelihood -1.410827
[ Info: iteration 43, average log likelihood -1.410812
[ Info: iteration 44, average log likelihood -1.410797
[ Info: iteration 45, average log likelihood -1.410783
[ Info: iteration 46, average log likelihood -1.410769
[ Info: iteration 47, average log likelihood -1.410756
[ Info: iteration 48, average log likelihood -1.410744
[ Info: iteration 49, average log likelihood -1.410732
[ Info: iteration 50, average log likelihood -1.410721
┌ Info: EM with 100000 data points 50 iterations avll -1.410721
└ 118.1 data points per parameter
┌ Info: 4
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4127375559151183
│     -1.4126878507090503
│      ⋮
└     -1.410721040638653
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.410719
[ Info: iteration 2, average log likelihood -1.410657
[ Info: iteration 3, average log likelihood -1.410597
[ Info: iteration 4, average log likelihood -1.410528
[ Info: iteration 5, average log likelihood -1.410443
[ Info: iteration 6, average log likelihood -1.410337
[ Info: iteration 7, average log likelihood -1.410209
[ Info: iteration 8, average log likelihood -1.410061
[ Info: iteration 9, average log likelihood -1.409902
[ Info: iteration 10, average log likelihood -1.409740
[ Info: iteration 11, average log likelihood -1.409582
[ Info: iteration 12, average log likelihood -1.409436
[ Info: iteration 13, average log likelihood -1.409304
[ Info: iteration 14, average log likelihood -1.409187
[ Info: iteration 15, average log likelihood -1.409085
[ Info: iteration 16, average log likelihood -1.408995
[ Info: iteration 17, average log likelihood -1.408916
[ Info: iteration 18, average log likelihood -1.408847
[ Info: iteration 19, average log likelihood -1.408785
[ Info: iteration 20, average log likelihood -1.408730
[ Info: iteration 21, average log likelihood -1.408680
[ Info: iteration 22, average log likelihood -1.408634
[ Info: iteration 23, average log likelihood -1.408592
[ Info: iteration 24, average log likelihood -1.408553
[ Info: iteration 25, average log likelihood -1.408515
[ Info: iteration 26, average log likelihood -1.408480
[ Info: iteration 27, average log likelihood -1.408445
[ Info: iteration 28, average log likelihood -1.408412
[ Info: iteration 29, average log likelihood -1.408380
[ Info: iteration 30, average log likelihood -1.408349
[ Info: iteration 31, average log likelihood -1.408319
[ Info: iteration 32, average log likelihood -1.408289
[ Info: iteration 33, average log likelihood -1.408261
[ Info: iteration 34, average log likelihood -1.408233
[ Info: iteration 35, average log likelihood -1.408205
[ Info: iteration 36, average log likelihood -1.408179
[ Info: iteration 37, average log likelihood -1.408153
[ Info: iteration 38, average log likelihood -1.408128
[ Info: iteration 39, average log likelihood -1.408104
[ Info: iteration 40, average log likelihood -1.408080
[ Info: iteration 41, average log likelihood -1.408057
[ Info: iteration 42, average log likelihood -1.408035
[ Info: iteration 43, average log likelihood -1.408014
[ Info: iteration 44, average log likelihood -1.407993
[ Info: iteration 45, average log likelihood -1.407973
[ Info: iteration 46, average log likelihood -1.407954
[ Info: iteration 47, average log likelihood -1.407935
[ Info: iteration 48, average log likelihood -1.407917
[ Info: iteration 49, average log likelihood -1.407900
[ Info: iteration 50, average log likelihood -1.407883
┌ Info: EM with 100000 data points 50 iterations avll -1.407883
└ 59.0 data points per parameter
┌ Info: 5
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4107192194864502
│     -1.4106565637833326
│      ⋮
└     -1.4078827392813762
┌ Info: Total log likelihood: 
│   tll =
│    251-element Array{Float64,1}:
│     -1.4208402893437697
│     -1.4208588956362957
│     -1.4207910237066805
│     -1.4207370248692885
│      ⋮
│     -1.4079169092821766
│     -1.4078995052649126
└     -1.4078827392813762
32×26 Array{Float64,2}:
 -0.540841    -0.96104      0.520277   -0.113128   -0.462533   -0.425615     0.432062    -0.724832    -0.137301     0.0145702   0.60856    -0.615179     0.226185    -0.09422     -0.399552    0.0333597    -0.335453   -0.123672      0.0553339  -0.569462    0.0706056   -0.742847   -0.122044    -0.100004    -0.237611     0.375569
 -0.742548     0.431333     0.0948758   0.78586     0.0366044  -0.2391       0.513332    -0.409057    -0.587464    -0.597338   -0.520322   -0.534901    -0.0738634   -0.458862    -0.536647    0.601676     -0.192958   -0.0582202     0.238832   -0.19688     0.32155      0.655985   -0.138137    -0.216277     0.0809978   -0.399665
 -0.212241     0.124378    -0.217469   -0.382131   -0.0719452  -0.0408072    0.320389    -0.467332     0.268445     0.530351   -0.159937   -0.227991     0.129488     0.00848853   0.339758    0.483478      0.183198   -0.225413     -1.36986    -0.499671   -0.177312    -0.470201    0.393372     0.0451843    0.00510962  -0.0440616
 -0.131723    -0.225627     0.724399   -0.636481   -0.234021    1.21882      0.376218     0.159182    -0.0653246    0.293709   -0.0571905  -0.12723      0.352771    -0.624075    -0.481285    0.164081      0.0797845   0.209237     -0.318403   -0.0370985   0.150274    -0.171885    0.371181    -0.382121    -0.0785693   -0.208787
 -0.266903    -0.0283829   -0.204969   -0.293128   -0.111587    0.129077    -0.11635     -0.180547    -0.141275    -0.264873    0.428291   -0.0457662   -0.787453    -0.132713    -0.357573    0.340984     -0.744669   -0.642696      0.112915   -0.828898    0.00568179   0.0139285   0.18608      0.301419     0.179319    -0.086169
  0.373523    -0.139096     0.52302    -0.778155    0.0659667   0.14235      0.530755    -0.0278796   -0.0722189   -0.0388869   0.163027   -0.180032    -0.704072    -0.703893     0.229623    0.26451      -0.336917    0.0945886     0.0778842  -0.69487    -0.349201    -0.14296    -0.431371    -0.261005     0.144997    -0.407469
 -0.421003    -0.370243     0.374876    0.597452    0.377119    0.479249    -0.362147     0.125171     0.161666     0.120952    0.0696982  -0.0415996    0.124192    -0.493465    -0.13253    -0.0600387    -0.112017   -0.221638      0.420319   -0.343499   -0.11179      0.723254   -0.50797     -0.286564    -0.291796     0.252134
  0.222341    -0.0473641   -0.397214   -0.0813712  -0.0655495   0.464545    -0.32475      0.144522     0.816297     0.244619   -0.024064    0.077521     0.197366    -0.417458     0.186625    0.00125185   -0.342276    0.727421      0.22608    -0.124213   -0.100498     0.318516   -0.613282    -0.0589331    0.346        0.270843
  0.00359327  -0.0635587    0.122394    0.168333   -0.757101    0.386546    -0.353473    -0.557754     0.212721    -0.299532   -0.252889    0.0560119   -0.463839     0.162206    -0.485068   -0.481453     -0.33912    -0.163169     -0.0445965   0.111456    0.168398     0.768862    0.362157    -0.127287    -0.253706     0.660199
 -0.182594    -0.0171071    0.347993    0.556343   -0.184181    0.112495     0.383992    -0.082296     0.59116     -0.392315   -0.2164      0.0900742    0.379899     0.204137    -0.160655   -0.300037      0.619531   -0.222904     -0.827512   -0.0975617   0.241424    -0.0552535  -0.0292219   -0.269509    -0.00605459   0.933554
 -0.125212    -0.104239    -0.0207359  -0.152479   -0.349867   -0.1157       0.00126733  -0.0229431    0.0589398   -0.723167   -0.364383    0.169912    -0.0448858    0.541238    -0.067675   -0.161055     -0.205572   -0.000104321   0.395016    0.0370652   0.127709    -0.0577671  -0.416864     0.229837    -0.287817     0.333662
  0.514221     0.0459535   -0.253761    0.140959    0.404273    0.147723    -0.297161     0.0769387    0.0481909    0.259324    0.360845    0.0457008   -0.118119    -0.211647    -0.0973989   0.0581987    -0.0724631  -0.0838482     0.0635257   0.21858    -0.292437     0.352183    0.159603     0.200262     0.0923515    0.0526703
 -0.299928     0.300569    -0.255135   -0.497316    0.228859   -0.339543    -0.286611     0.343749    -0.260746     0.417093    0.159389   -0.139924     0.085251     0.308628     0.217523    0.393005      0.07845     0.153957      0.27857     0.0288635  -0.255341    -0.926981   -0.279199     0.0682822    0.290288    -0.529496
  0.507852     0.251642    -0.292987   -0.500269   -0.0386618  -0.493498     0.370924     0.36205     -0.653759    -0.685408   -0.539084    0.215561     0.126865     0.00715286   0.563132    0.403355      0.161666    0.162998     -0.602549    0.344313    0.43647     -0.748846    0.407105    -0.012938     0.279898    -0.0766163
  0.296547     0.48213     -0.505224    0.471368   -0.0185932  -0.832113    -0.452776     0.0192017    0.116957    -0.080416   -0.506102   -0.0770198    0.368351     0.339475     0.111473   -0.411912      0.577289   -0.387713      0.11569     0.712552    0.706382    -0.250513    0.316689     0.309138    -0.0519586   -0.348125
  0.0587969   -0.197881     0.303351    0.619537   -0.252001   -0.673872     0.401331     0.022631     0.504457     0.282587   -0.0492261  -0.323076    -0.175203    -0.278006     0.93632    -0.341893      0.277478    0.331902      0.199055   -0.0649317   0.50703     -0.0467     -0.00131382  -0.0532205    0.451781    -0.412353
 -0.521658     0.113101    -0.221763    0.364085   -0.054859    0.0802287   -0.00927149  -0.276486    -1.02365     -0.0782958   0.283483    0.519631     0.428417     0.102877    -0.153442   -0.254781      0.601193   -0.331048     -0.238231    0.133446   -0.646743     0.0854321   0.220407    -0.305236     0.0187374   -0.00688996
 -0.0921791    0.00781438   0.220912    0.263294    0.0869271   0.269988    -0.0211538    0.460698    -0.847388    -0.237987    0.129677    0.295475    -0.159254    -0.138242    -0.346832   -0.193866      0.368873   -0.22694       0.946631    0.171788   -0.256341     0.161798    0.296871    -0.193043     0.241759    -0.483366
 -0.269063     0.154058    -0.599197   -0.234192   -0.239348    0.00584648   0.0198977    0.0955767   -0.0683859    0.479008    0.285049   -0.44995      0.354285     0.321891    -0.112446   -0.265988     -0.190364    0.268255     -0.131011   -0.141465   -0.78016      0.760849    0.0627904   -0.218475     0.00795225   0.164607
  0.0694011   -0.458303     0.128532   -0.239658   -0.364211    0.844169    -0.352618    -0.255403    -0.0860042    0.324726    0.742931    0.675789     0.261209     0.0798759    0.0876914  -0.200951      0.381329   -0.37023      -0.377911   -0.0792795  -0.448373    -0.719137    0.375424    -0.0378357   -0.318732     0.554628
 -0.154854    -0.0853931    0.104271    0.121742   -0.19038    -0.194155     0.149927    -0.193401    -0.113728    -0.773723   -0.325728   -0.113271    -0.23932      0.0565773   -0.268252   -0.0550044     0.0522564  -0.352814      0.193888   -0.026786    0.380203     0.0294019  -0.0485568    0.170844    -0.192806     0.162142
  0.443705    -0.0176337    0.206598    0.110233    0.264609   -0.357941     0.119563     0.264037     0.336099    -0.262417    0.363288    0.171271    -0.0529466    0.287395    -0.291028   -0.340689     -0.502124   -0.303623      0.717734    0.414173    0.116737    -0.0403399  -0.0155678    0.208905    -0.141079     0.494884
  0.719485     0.115159     0.496414    0.313697   -0.0210399   0.375317     0.446795    -0.575117     0.0655604   -0.12486    -0.249618    0.359383    -0.119639    -0.125905    -0.412154   -0.0142618     0.416929    0.182859      0.247647    0.557464    0.344332     0.186791    0.359994     0.0311954   -0.0735282   -0.356012
  0.815216     0.856438    -0.0263509   0.0877067   0.184501    0.447621    -0.07212      0.704359     0.26134      0.0451277  -0.466938    0.670145    -0.399545     0.107783     0.0619855   0.183155      0.653918   -0.0330741     0.0405215   0.348627   -0.0747705    0.34588    -0.086891     0.279626    -0.0291815   -0.345966
 -0.0154163   -0.0208942    0.1799     -0.080765    0.0921314   0.302456     0.237997     0.242906    -6.39741e-5  -0.354216   -0.0647562   0.132345     0.23405     -0.146135     0.104733    0.140009      0.400691   -0.0488901    -0.329335   -0.0742289  -0.209916    -0.280851   -0.278602     0.00090244  -0.57456      0.0317361
 -0.126602    -0.110581     0.0495922  -0.0747598  -0.171968   -0.0637886    0.0433749   -0.201833    -0.0844128    0.233632    0.0419902   0.168562     0.138997     0.363854     0.0386428   0.000651257   0.26408    -0.0291364    -0.278897    0.054666   -0.126953    -0.273097    0.340918     0.0153805    0.1752      -0.000672619
  0.447567    -0.178479     0.132552   -0.213584    0.108246    0.0494994   -0.648743     0.218211     0.604362     0.0373574  -0.573065   -0.283094    -0.182341    -0.129168     0.0912985   0.450602     -0.378513    0.159081      0.0109903   0.450584    0.623873    -0.305502   -0.142262     0.447649    -0.205775    -0.0513279
  0.0742168   -0.134203     0.0931246  -0.333843    0.256693    0.180703     0.294325     0.158766     0.205538     0.860916    0.102891   -0.327981    -0.00695523  -0.226999    -0.024424    0.164423     -0.352356    0.00304488   -0.352966   -0.161977    0.207236    -0.114939    0.156921    -0.144745    -0.240042    -0.385967
  0.0100484   -0.0408746   -0.278252    0.0350573   0.182255   -0.363704    -0.260936     0.146651    -0.165821    -0.0811717   0.0321352  -0.0277862    0.0394001   -0.261498     0.381492    0.0839775    -0.151621    0.104808      0.278612   -0.04283     0.146566    -0.250712   -0.289926    -0.0492122    0.271499    -0.214308
  0.144833     0.0465018    0.0337666   0.0132138  -0.0935207   0.25901     -0.126238     0.00469513   0.0718687    0.0250584  -0.0416051   0.00987252  -0.149988    -0.151876    -0.177117   -0.0745785    -0.112842    0.0313706     0.123994   -0.0729529  -0.0935835    0.284851   -0.0528983   -0.0897345    0.0272594    0.0474914
  0.128821     0.56367     -1.17134     0.105539    0.296538   -0.432573    -0.27763     -0.511852     0.0186895   -0.186319   -0.480199    0.0696828    0.175619     0.045602     0.0616816  -0.0158606     0.176551   -0.28555      -0.463288    0.111201   -0.245818     0.0212089  -0.259925     0.123134    -0.628824     0.173757
 -0.0882862    0.0890495   -0.403359    0.387199    0.0112259  -0.359739    -0.333777    -0.201534    -0.0487782   -0.180236   -0.43274     0.307728     0.244192     0.857354     0.0333202   0.231211      0.256202    0.311329      0.202271    0.201426   -0.305593     0.23361    -0.150123     0.317896     0.588101     0.417988[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.407867
[ Info: iteration 2, average log likelihood -1.407851
[ Info: iteration 3, average log likelihood -1.407836
[ Info: iteration 4, average log likelihood -1.407822
[ Info: iteration 5, average log likelihood -1.407808
[ Info: iteration 6, average log likelihood -1.407795
[ Info: iteration 7, average log likelihood -1.407782
[ Info: iteration 8, average log likelihood -1.407770
[ Info: iteration 9, average log likelihood -1.407759
[ Info: iteration 10, average log likelihood -1.407747
┌ Info: EM with 100000 data points 10 iterations avll -1.407747
└ 59.0 data points per parameter
[ Info: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
kind full, method kmeans
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.186696e+05
      1       7.009416e+05      -2.177280e+05 |       32
      2       6.895866e+05      -1.135492e+04 |       32
      3       6.847532e+05      -4.833395e+03 |       32
      4       6.819487e+05      -2.804554e+03 |       32
      5       6.800371e+05      -1.911601e+03 |       32
      6       6.786834e+05      -1.353647e+03 |       32
      7       6.777244e+05      -9.589987e+02 |       32
      8       6.769565e+05      -7.679965e+02 |       32
      9       6.763529e+05      -6.035380e+02 |       32
     10       6.758556e+05      -4.972897e+02 |       32
     11       6.754121e+05      -4.435150e+02 |       32
     12       6.750492e+05      -3.629333e+02 |       32
     13       6.747579e+05      -2.913238e+02 |       32
     14       6.745057e+05      -2.521683e+02 |       32
     15       6.742863e+05      -2.193844e+02 |       32
     16       6.740886e+05      -1.977255e+02 |       32
     17       6.739077e+05      -1.808358e+02 |       32
     18       6.737424e+05      -1.653453e+02 |       32
     19       6.736002e+05      -1.421512e+02 |       32
     20       6.734785e+05      -1.217161e+02 |       32
     21       6.733703e+05      -1.082361e+02 |       32
     22       6.732581e+05      -1.122288e+02 |       32
     23       6.731583e+05      -9.978771e+01 |       32
     24       6.730722e+05      -8.603722e+01 |       32
     25       6.729815e+05      -9.076289e+01 |       32
     26       6.729005e+05      -8.096560e+01 |       32
     27       6.728333e+05      -6.715972e+01 |       32
     28       6.727723e+05      -6.109299e+01 |       32
     29       6.727136e+05      -5.869661e+01 |       32
     30       6.726503e+05      -6.323802e+01 |       32
     31       6.725906e+05      -5.971619e+01 |       32
     32       6.725323e+05      -5.831933e+01 |       32
     33       6.724808e+05      -5.144481e+01 |       32
     34       6.724329e+05      -4.793672e+01 |       32
     35       6.723890e+05      -4.386134e+01 |       32
     36       6.723504e+05      -3.863941e+01 |       32
     37       6.723109e+05      -3.952070e+01 |       32
     38       6.722731e+05      -3.779460e+01 |       32
     39       6.722367e+05      -3.642663e+01 |       32
     40       6.721972e+05      -3.946489e+01 |       32
     41       6.721568e+05      -4.039332e+01 |       32
     42       6.721219e+05      -3.491684e+01 |       32
     43       6.720941e+05      -2.780201e+01 |       32
     44       6.720685e+05      -2.561430e+01 |       32
     45       6.720479e+05      -2.055723e+01 |       32
     46       6.720270e+05      -2.095636e+01 |       32
     47       6.720066e+05      -2.038069e+01 |       32
     48       6.719859e+05      -2.064295e+01 |       32
     49       6.719600e+05      -2.596964e+01 |       32
     50       6.719340e+05      -2.595504e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 671934.0068289924)
┌ Info: K-means with 32000 data points using 50 iterations
└ 37.0 data points per parameter
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.419879
[ Info: iteration 2, average log likelihood -1.414910
[ Info: iteration 3, average log likelihood -1.413620
[ Info: iteration 4, average log likelihood -1.412744
[ Info: iteration 5, average log likelihood -1.411831
[ Info: iteration 6, average log likelihood -1.410892
[ Info: iteration 7, average log likelihood -1.410127
[ Info: iteration 8, average log likelihood -1.409639
[ Info: iteration 9, average log likelihood -1.409355
[ Info: iteration 10, average log likelihood -1.409178
[ Info: iteration 11, average log likelihood -1.409054
[ Info: iteration 12, average log likelihood -1.408956
[ Info: iteration 13, average log likelihood -1.408875
[ Info: iteration 14, average log likelihood -1.408804
[ Info: iteration 15, average log likelihood -1.408740
[ Info: iteration 16, average log likelihood -1.408681
[ Info: iteration 17, average log likelihood -1.408627
[ Info: iteration 18, average log likelihood -1.408576
[ Info: iteration 19, average log likelihood -1.408528
[ Info: iteration 20, average log likelihood -1.408482
[ Info: iteration 21, average log likelihood -1.408439
[ Info: iteration 22, average log likelihood -1.408397
[ Info: iteration 23, average log likelihood -1.408357
[ Info: iteration 24, average log likelihood -1.408319
[ Info: iteration 25, average log likelihood -1.408283
[ Info: iteration 26, average log likelihood -1.408249
[ Info: iteration 27, average log likelihood -1.408216
[ Info: iteration 28, average log likelihood -1.408186
[ Info: iteration 29, average log likelihood -1.408156
[ Info: iteration 30, average log likelihood -1.408129
[ Info: iteration 31, average log likelihood -1.408103
[ Info: iteration 32, average log likelihood -1.408078
[ Info: iteration 33, average log likelihood -1.408055
[ Info: iteration 34, average log likelihood -1.408032
[ Info: iteration 35, average log likelihood -1.408011
[ Info: iteration 36, average log likelihood -1.407991
[ Info: iteration 37, average log likelihood -1.407972
[ Info: iteration 38, average log likelihood -1.407954
[ Info: iteration 39, average log likelihood -1.407937
[ Info: iteration 40, average log likelihood -1.407920
[ Info: iteration 41, average log likelihood -1.407905
[ Info: iteration 42, average log likelihood -1.407889
[ Info: iteration 43, average log likelihood -1.407875
[ Info: iteration 44, average log likelihood -1.407861
[ Info: iteration 45, average log likelihood -1.407848
[ Info: iteration 46, average log likelihood -1.407835
[ Info: iteration 47, average log likelihood -1.407823
[ Info: iteration 48, average log likelihood -1.407811
[ Info: iteration 49, average log likelihood -1.407799
[ Info: iteration 50, average log likelihood -1.407788
┌ Info: EM with 100000 data points 50 iterations avll -1.407788
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.418106   -0.311451      0.529033    -0.384953   -0.0913132   0.491555    0.232009   -0.027008    -0.285345   -0.210997     0.0210277   -0.0605237  -0.54414    -0.227445     -0.472193    0.378339   -0.506816   -0.29888      0.171689   -1.08465     -0.118484     0.0733869  -0.402994     0.228174    -0.306105     0.0417607
 -0.168307   -0.173539     -0.239164     0.35085    -0.185686   -0.227212   -0.173184   -0.196474     0.0233016  -0.0112664   -0.00864237   0.128891    0.218696    0.559858      0.406649   -0.0479733   0.055906    0.282583     0.032221   -0.0321182   -0.39942      0.0174864  -0.204815     0.0975098    0.5027       0.565436
  0.121012   -0.150272      0.31103      0.151843   -0.429701    0.328422    0.145526   -0.451349     0.30957    -0.485826    -0.334684    -0.083076   -0.239881    0.194374     -0.341313   -0.08668     0.23209    -0.256348    -0.294957    0.0893809    0.115223     0.397516    0.370015     0.27921     -0.321074     0.344502
  0.208938   -0.058272     -0.227793     0.753397    0.339999   -0.40955    -0.293346    0.15201      0.259298   -0.226068    -0.153072    -0.0695137   0.141156    0.336834      0.231313   -0.177979    0.283799   -0.148282     0.610956    0.367926     0.151351     0.135032   -0.128317     0.372865    -0.0111862    0.245603
 -0.0966908  -0.440494      0.113423    -0.0877208  -0.441781    0.757077   -0.401068   -0.410668    -0.227816    0.375448     0.756876     0.657573    0.274151    0.0244127    -0.0425192  -0.248896    0.443295   -0.402755    -0.367408   -0.165516    -0.47566     -0.607393    0.361343    -0.249782    -0.341987     0.543622
 -0.429741    0.474796     -0.281924     0.15074     0.295531   -0.402512   -0.328758    0.32005     -0.240736    0.173315    -0.0884257   -0.311128    0.012426    0.015668     -0.105888    0.290884   -0.0675394   0.191738     0.63869     0.122613    -0.00980102  -0.162116   -0.317348     0.0315101    0.322255    -0.618595
  0.510397    0.32765       0.0870884   -0.572835   -0.383584    0.341781    0.230836    0.123551    -0.819548   -0.629263    -0.561015    -0.354769   -0.118872   -0.669101     -0.307587    0.534946    0.163202    0.141055    -0.480034    0.0496651    0.512152    -0.537777    0.294479    -0.307709     0.342539    -0.672515
 -0.0588664   0.0508383    -0.0821074   -0.500978    0.175514    0.114338    0.162775    0.0291226   -0.442618    0.474912     0.396503     0.0111866   0.081366   -0.361758      0.188662    0.351215   -0.107583   -0.00282281  -0.121598   -0.19653     -0.581434    -0.250289   -0.149996    -0.145015     0.265422    -0.402173
  0.789674   -0.0735539     0.499344     0.452892    0.272927    0.288146    0.016152    0.0821046   -0.0717895  -0.283647    -0.346416     0.600887   -0.595598   -0.0631234    -0.241948    0.214786    0.278621   -0.478048     0.652863    0.692519     0.275319    -0.0876425   0.265492     0.0938948    0.0517853   -0.153046
 -0.0113502  -0.0295006     0.357036    -0.40741     0.0353345   0.329333    0.135657    0.0766471   -0.0106571   0.616577     0.145262     0.0719488   0.016917   -0.0425148    -0.201296    0.108273    0.0721521  -0.0274083   -0.289319   -0.0716712    0.0617211   -0.27557     0.3988       0.0340044   -0.111446    -0.52433
  0.347118    0.213831      0.307078    -0.200489    0.0851266  -0.130414    0.328327    0.43994      0.424482   -0.362502     0.258989     0.316378   -0.109401    0.269144     -0.670184   -0.414131   -0.770299   -0.00305486   0.864221    0.218306     0.302792     0.052454   -0.36776      0.0235262    0.0154123    0.273354
  0.55418    -0.55149      -0.336964    -0.2869     -0.578164    0.220954   -0.950037    0.00829483   0.82919    -0.455705    -0.797645    -0.21474    -0.68013     0.629786     -0.191385    0.179764   -0.424763    0.439144     0.665189    0.80887      0.915457    -0.413913   -0.0131014   -0.0271358   -0.611426    -0.0330324
 -0.232481   -0.0833573     0.160496    -0.265437   -0.152544    0.17796     0.362516   -0.377984     0.716693    0.398511    -0.183247    -0.123695    0.321907   -0.022516      0.130925    0.299427    0.176621   -0.0040849   -1.1296     -0.441958     0.209362    -0.431117    0.263364    -0.122862     0.0345597    0.289682
  0.265025   -0.383738     -0.13096     -0.414911    0.16063    -0.173478   -0.0489692   0.0866844   -0.0514883   0.14137      0.227291    -0.153446   -0.567724   -0.325492      0.562711    0.179477   -0.328677   -0.0887898    0.0876026  -0.459479    -0.122236    -0.307926   -0.0741735   -0.083154     0.0950765   -0.3303
  0.306445    0.000617587   0.421036     0.263412   -0.160751   -0.421018    0.51714    -0.0455022    0.484742    0.258226    -0.120336    -0.306023   -0.290204   -0.575045      0.595543   -0.108152    0.0583845   0.384634     0.0999771   0.00358348   0.564475     0.0236981   0.110647    -0.0389702    0.273145    -0.663414
  0.320594    0.528117     -0.432656    -0.767698    0.155841    0.410581   -0.58181     0.0943384   -0.182742    0.00161887   0.413939     0.169077    0.0510156   0.353164     -0.840948    0.278879   -0.46575    -0.0238748    0.298944    0.529663    -0.735375     0.144779    0.471053     0.511193    -0.356348     0.0404678
  0.230532    0.490369     -1.20546      0.277101    0.368887   -0.587526   -0.307993   -0.494659     0.0753633  -0.290482    -0.507236    -0.197981   -0.0432909   0.0453446     0.386272   -0.172497    0.368058   -0.701733    -0.414372    0.166872    -0.0448268   -0.172912   -0.115565     0.161097    -0.624883    -0.00231173
  0.23777     0.362366     -0.144417     0.360092   -0.165841    0.215191   -0.109772   -0.473346     0.0721965   0.0745251   -0.513175     0.561321    0.342475    0.149524     -0.660064   -0.498933    0.0718201   0.35724      0.171052    0.522196     0.214548     0.972074    0.23287      0.00731262  -0.137023     0.177412
 -0.143376   -0.507236      0.615584     0.215393    0.0487211   0.963933    0.0306398   0.190746     0.0419333   0.304351     0.216326    -0.158737    0.245268   -0.657282     -0.363129   -0.295066    0.267448    0.190822     0.600829   -0.0166336   -0.352302     0.452778   -0.161671    -0.479995    -0.0417003   -0.175442
 -0.47082     0.21902      -0.307958     0.249292   -0.315568   -0.136925   -0.158425   -0.314151    -0.812397   -0.0585532   -0.202384     0.545756    0.394514    0.6035       -0.212887   -0.102631    0.841415   -0.0592749    0.0741199   0.397883    -0.409497    -0.0495859   0.294619    -0.0699941    0.264389    -0.148174
  0.0854558   0.0041229     0.0980352    0.0352044   0.205763   -0.200415   -0.200363    0.101462     0.39159    -0.0106905   -0.321055    -0.0735269   0.572374   -0.0860581     0.340933    0.102061    0.0979658   0.342139    -0.129778    0.395602     0.613851    -0.465109   -0.421648     0.270948    -0.16339     -0.027705
  0.199717    0.178322      0.00941971   0.508259    0.517335    0.245566   -0.534519    0.534811     0.0897947   0.190503     0.486441    -0.128472    0.25901    -0.181632     -0.440409   -0.191721    0.0817095  -0.701613    -0.530929    0.599826     0.29068      0.269659    0.608784     0.0499481    0.260433     0.836619
 -0.277146    0.0120065     0.193166     0.28103    -0.693755    0.142614   -0.0895092  -0.555816     0.0563421  -0.607847     0.00341088   0.0352872  -0.478287    0.15888      -0.419501   -0.499969   -0.308758   -0.311231    -0.116731   -0.0148784    0.00742087   0.719282   -0.0212884   -0.250004    -0.147831     0.698371
  0.497717    0.589177     -0.0140077    0.0992708   0.126791    0.47902     0.106997    0.545406     0.190519   -0.254625    -0.335427     0.568128   -0.0753813   0.0666425     0.270145    0.156219    0.900998    0.167925    -0.258553    0.19247     -0.395446     0.0228644  -0.437943    -0.054485    -0.161787    -0.169032
  0.390175    0.438756     -0.468085    -0.536848   -0.106239   -0.950071    0.0872299   0.356201    -0.282203   -0.292825    -0.484535     0.402772    0.0137177   0.655438      0.60176     0.101901    0.0825955  -0.020222    -0.210851    0.216469     0.385772    -0.654724    0.228542     0.476751     0.211306    -0.0722355
 -0.192801    0.064668     -0.628412    -0.332981   -0.139653   -0.115064    0.269093   -0.0238251    0.111612    0.759187     0.114011    -0.594913    0.478379    0.323238     -0.164813   -0.0177062  -0.246476    0.299673    -0.405782   -0.111534    -0.671163     0.389674    0.0860223   -0.245261    -0.12056      0.044441
 -0.0729352  -0.385688      0.0618294   -0.298164    0.203694    0.120897    0.703757    0.321276    -0.385488   -0.822086     0.342181     0.266593    0.620182    0.0465466     0.14421    -0.378167    0.0748841  -0.337382    -0.529571    0.0876584   -0.332271    -0.39115     0.00636186  -0.211536    -0.604122     0.42548
 -0.0782042  -0.132409     -0.0913004    0.0615907   0.113095   -0.189453    0.0134297  -0.0146882   -0.260545   -0.102413    -0.0843013    0.198598    0.208381    0.251917      0.0644175   0.113374    0.278535   -0.216172    -0.146054    0.14515     -0.072467    -0.201121    0.245259     0.0758299    0.0826313    0.0932334
 -0.499443    0.429796      0.0394078    0.726045    0.0191888   0.0544045   0.259304   -0.0209893   -0.933158   -0.656061     0.136524    -0.0589637  -0.0779632  [ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
-0.591278     -0.295153   -0.0538954   0.24512    -0.60287      0.0870562  -0.421834    -0.0838556    0.446441    0.240253    -0.285101     0.0036483   -0.250536
 -0.475695   -0.586137      0.350197     0.0170155  -0.309485   -0.463389    0.316698   -0.534454    -0.106497   -0.228948     0.253377    -0.47463     0.0271806   0.000603289  -0.377476    0.038396   -0.255909   -0.298802     0.161454   -0.394777     0.245665    -0.462988   -0.0601285   -0.0518501   -0.230324     0.31388
  0.108197    0.0530577     0.0026109   -0.0074518  -0.105603    0.154265   -0.0660694   0.0159863    0.0423223  -0.0985043   -0.0425427    0.0181647  -0.170476   -0.163719     -0.0621723  -0.0766486  -0.0791304   0.0259133    0.13293    -0.08926     -0.00669102   0.13145    -0.151588    -0.0909805   -0.00381391   0.023627
  0.184796   -0.0218789    -0.283589    -0.0301564   0.116073    0.332746   -0.475234    0.148931     0.690935    0.0922831   -0.139541    -0.047774   -0.08546    -0.513078      0.0619759   0.239639   -0.620343    0.353032     0.202303   -0.356201    -0.00273606   0.509633   -0.55043      0.0684634    0.148009     0.406769[ Info: iteration 1, average log likelihood -1.407777
[ Info: iteration 2, average log likelihood -1.407767
[ Info: iteration 3, average log likelihood -1.407757
[ Info: iteration 4, average log likelihood -1.407747
[ Info: iteration 5, average log likelihood -1.407738
[ Info: iteration 6, average log likelihood -1.407729
[ Info: iteration 7, average log likelihood -1.407720
[ Info: iteration 8, average log likelihood -1.407711
[ Info: iteration 9, average log likelihood -1.407702
[ Info: iteration 10, average log likelihood -1.407694
┌ Info: EM with 100000 data points 10 iterations avll -1.407694
└ 59.0 data points per parameter
[ Info: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.678561e+05
      1       2.230230e+04      -1.455538e+05 |        2
      2       7.823675e+03      -1.447862e+04 |        0
      3       7.823675e+03       0.000000e+00┌ Info: K-means with 900 data points using 3 iterations
└ 150.0 data points per parameter
[ Info: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
 |        0
K-means converged with 3 iterations (objv = 7823.67549422947)
[ Info: iteration 1, average log likelihood -2.043155
[ Info: iteration 2, average log likelihood -2.043154
[ Info: iteration 3, average log likelihood -2.043154
[ Info: iteration 4, average log likelihood -2.043154
[ Info: iteration 5, average log likelihood -2.043154
[ Info: iteration 6, average log likelihood -2.043154
[ Info: iteration 7, average log likelihood -2.043154
[ Info: iteration 8, average log likelihood -2.043154
[ Info: iteration 9, average log likelihood -2.043154
[ Info: iteration 10, average log likelihood -2.043154
┌ Info: EM with 900 data points 10 iterations avll -2.043154
└ 81.8 data points per parameter
   Testing GaussianMixtures tests passed 
