Julia Version 1.5.1-pre.28
Commit 0d71d8cd8b (2020-08-14 20:35 UTC)
Platform Info:
  OS: Linux (x86_64-linux-gnu)
  CPU: Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-9.0.1 (ORCJIT, skylake-avx512)
Environment:
  JULIA_DEPOT_PATH = ::/usr/local/share/julia
  JULIA_NUM_THREADS = 2

  Resolving package versions...
  Installed CompilerSupportLibraries_jll ─ v0.3.3+0
  Installed CEnum ──────────────────────── v0.4.1
  Installed TranscodingStreams ─────────── v0.9.5
  Installed SpecialFunctions ───────────── v0.10.3
  Installed GPUCompiler ────────────────── v0.5.5
  Installed FileIO ─────────────────────── v1.4.1
  Installed DataStructures ─────────────── v0.17.20
  Installed BinaryProvider ─────────────── v0.5.10
  Installed Zlib_jll ───────────────────── v1.2.11+15
  Installed MacroTools ─────────────────── v0.5.5
  Installed OpenSpecFun_jll ────────────── v0.5.3+3
  Installed Requires ───────────────────── v1.0.1
  Installed LLVM ───────────────────────── v2.0.0
  Installed Knet ───────────────────────── v1.3.9
  Installed CUDA ───────────────────────── v1.2.1
  Installed Reexport ───────────────────── v0.2.0
  Installed CodecZlib ──────────────────── v0.7.0
  Installed TimerOutputs ───────────────── v0.5.6
  Installed JLD2 ───────────────────────── v0.1.14
  Installed OrderedCollections ─────────── v1.3.0
  Installed AbstractFFTs ───────────────── v0.5.0
  Installed GPUArrays ──────────────────── v5.0.0
  Installed NNlib ──────────────────────── v0.7.4
  Installed Adapt ──────────────────────── v2.0.2
  Installed AutoGrad ───────────────────── v1.2.3
  Installed ExprTools ──────────────────── v0.1.1
Updating `~/.julia/environments/v1.5/Project.toml`
  [1902f260] + Knet v1.3.9
Updating `~/.julia/environments/v1.5/Manifest.toml`
  [621f4979] + AbstractFFTs v0.5.0
  [79e6a3ab] + Adapt v2.0.2
  [6710c13c] + AutoGrad v1.2.3
  [b99e7846] + BinaryProvider v0.5.10
  [fa961155] + CEnum v0.4.1
  [052768ef] + CUDA v1.2.1
  [944b1d66] + CodecZlib v0.7.0
  [e66e0078] + CompilerSupportLibraries_jll v0.3.3+0
  [864edb3b] + DataStructures v0.17.20
  [e2ba6199] + ExprTools v0.1.1
  [5789e2e9] + FileIO v1.4.1
  [0c68f7d7] + GPUArrays v5.0.0
  [61eb1bfa] + GPUCompiler v0.5.5
  [033835bb] + JLD2 v0.1.14
  [1902f260] + Knet v1.3.9
  [929cbde3] + LLVM v2.0.0
  [1914dd2f] + MacroTools v0.5.5
  [872c559c] + NNlib v0.7.4
  [efe28fd5] + OpenSpecFun_jll v0.5.3+3
  [bac558e1] + OrderedCollections v1.3.0
  [189a3867] + Reexport v0.2.0
  [ae029012] + Requires v1.0.1
  [276daf66] + SpecialFunctions v0.10.3
  [a759f4b9] + TimerOutputs v0.5.6
  [3bb67fe8] + TranscodingStreams v0.9.5
  [83775a58] + Zlib_jll v1.2.11+15
  [2a0f44e3] + Base64
  [ade2ca70] + Dates
  [8ba89e20] + Distributed
  [b77e0a4c] + InteractiveUtils
  [76f85450] + LibGit2
  [8f399da3] + Libdl
  [37e2e46d] + LinearAlgebra
  [56ddb016] + Logging
  [d6f4376e] + Markdown
  [a63ad114] + Mmap
  [44cfe95a] + Pkg
  [de0858da] + Printf
  [3fa0cd96] + REPL
  [9a3f8284] + Random
  [ea8e919c] + SHA
  [9e88b42a] + Serialization
  [6462fe0b] + Sockets
  [2f01184e] + SparseArrays
  [10745b16] + Statistics
  [8dfed614] + Test
  [cf7118a7] + UUIDs
  [4ec0a83e] + Unicode
    Testing Knet
Status `/tmp/jl_D9RcyL/Project.toml`
  [6710c13c] AutoGrad v1.2.3
  [052768ef] CUDA v1.2.1
  [864edb3b] DataStructures v0.17.20
  [5789e2e9] FileIO v1.4.1
  [033835bb] JLD2 v0.1.14
  [1902f260] Knet v1.3.9
  [872c559c] NNlib v0.7.4
  [276daf66] SpecialFunctions v0.10.3
  [a759f4b9] TimerOutputs v0.5.6
  [8f399da3] Libdl
  [37e2e46d] LinearAlgebra
  [44cfe95a] Pkg
  [de0858da] Printf
  [9a3f8284] Random
  [10745b16] Statistics
  [8dfed614] Test
Status `/tmp/jl_D9RcyL/Manifest.toml`
  [621f4979] AbstractFFTs v0.5.0
  [79e6a3ab] Adapt v2.0.2
  [6710c13c] AutoGrad v1.2.3
  [b99e7846] BinaryProvider v0.5.10
  [fa961155] CEnum v0.4.1
  [052768ef] CUDA v1.2.1
  [944b1d66] CodecZlib v0.7.0
  [e66e0078] CompilerSupportLibraries_jll v0.3.3+0
  [864edb3b] DataStructures v0.17.20
  [e2ba6199] ExprTools v0.1.1
  [5789e2e9] FileIO v1.4.1
  [0c68f7d7] GPUArrays v5.0.0
  [61eb1bfa] GPUCompiler v0.5.5
  [033835bb] JLD2 v0.1.14
  [1902f260] Knet v1.3.9
  [929cbde3] LLVM v2.0.0
  [1914dd2f] MacroTools v0.5.5
  [872c559c] NNlib v0.7.4
  [efe28fd5] OpenSpecFun_jll v0.5.3+3
  [bac558e1] OrderedCollections v1.3.0
  [189a3867] Reexport v0.2.0
  [ae029012] Requires v1.0.1
  [276daf66] SpecialFunctions v0.10.3
  [a759f4b9] TimerOutputs v0.5.6
  [3bb67fe8] TranscodingStreams v0.9.5
  [83775a58] Zlib_jll v1.2.11+15
  [2a0f44e3] Base64
  [ade2ca70] Dates
  [8ba89e20] Distributed
  [b77e0a4c] InteractiveUtils
  [76f85450] LibGit2
  [8f399da3] Libdl
  [37e2e46d] LinearAlgebra
  [56ddb016] Logging
  [d6f4376e] Markdown
  [a63ad114] Mmap
  [44cfe95a] Pkg
  [de0858da] Printf
  [3fa0cd96] REPL
  [9a3f8284] Random
  [ea8e919c] SHA
  [9e88b42a] Serialization
  [6462fe0b] Sockets
  [2f01184e] SparseArrays
  [10745b16] Statistics
  [8dfed614] Test
  [cf7118a7] UUIDs
  [4ec0a83e] Unicode
distributions.jl	  6.500967 seconds (9.64 M allocations: 487.304 MiB, 5.40% gc time)
dropout.jl	┌ Warning: `seed!(seed)` is deprecated, use `CUDA.seed!(seed)` instead.
│   caller = seed!(::Int64) at knetarray.jl:148
└ @ Knet ~/.julia/packages/Knet/exwCE/src/cuarrays/knetarray.jl:148
 24.143532 seconds (8.43 M allocations: 441.280 MiB, 2.03% gc time)
serialize.jl	serialize: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/serialize.jl:5
  Got exception outside of a @test
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] gethandle() at /home/pkgeval/.julia/packages/Knet/exwCE/src/rnn.jl:403
   [14] RNN(::Int64, ::Int64) at /home/pkgeval/.julia/packages/Knet/exwCE/src/rnn.jl:149
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/serialize.jl:6
   [16] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/serialize.jl:6
   [18] include(::String) at ./client.jl:457
   [19] macro expansion at ./timing.jl:174 [inlined]
   [20] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
   [21] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
   [23] include(::String) at ./client.jl:457
   [24] top-level scope at none:6
   [25] eval(::Module, ::Any) at ./boot.jl:331
   [26] exec_options(::Base.JLOptions) at ./client.jl:272
   [27] _start() at ./client.jl:506
  
  4.065771 seconds (3.56 M allocations: 185.095 MiB, 1.61% gc time)
jld.jl	JLD: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/jld.jl:3
  Got exception outside of a @test
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] gethandle() at /home/pkgeval/.julia/packages/Knet/exwCE/src/rnn.jl:403
   [14] RNN(::Int64, ::Int64) at /home/pkgeval/.julia/packages/Knet/exwCE/src/rnn.jl:149
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/jld.jl:7
   [16] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/jld.jl:6
   [18] include(::String) at ./client.jl:457
   [19] macro expansion at ./timing.jl:174 [inlined]
   [20] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
   [21] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
   [23] include(::String) at ./client.jl:457
   [24] top-level scope at none:6
   [25] eval(::Module, ::Any) at ./boot.jl:331
   [26] exec_options(::Base.JLOptions) at ./client.jl:272
   [27] _start() at ./client.jl:506
  
  0.174837 seconds (6.54 k allocations: 405.797 KiB)
gcnode.jl	gcnode: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/gcnode.jl:3
  Got exception outside of a @test
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] gethandle() at /home/pkgeval/.julia/packages/Knet/exwCE/src/rnn.jl:403
   [14] RNN(::Int64, ::Int64) at /home/pkgeval/.julia/packages/Knet/exwCE/src/rnn.jl:149
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/gcnode.jl:8
   [16] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/gcnode.jl:6
   [18] include(::String) at ./client.jl:457
   [19] macro expansion at ./timing.jl:174 [inlined]
   [20] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
   [21] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
   [23] include(::String) at ./client.jl:457
   [24] top-level scope at none:6
   [25] eval(::Module, ::Any) at ./boot.jl:331
   [26] exec_options(::Base.JLOptions) at ./client.jl:272
   [27] _start() at ./client.jl:506
  
  0.149478 seconds (6.10 k allocations: 379.494 KiB)
statistics.jl	 29.776660 seconds (25.70 M allocations: 1.333 GiB, 2.40% gc time)
cuarray.jl	 71.265196 seconds (58.89 M allocations: 2.998 GiB, 4.03% gc time)
bmm.jl	 30.548415 seconds (26.76 M allocations: 1.359 GiB, 2.40% gc time)
linalg.jl	 58.094016 seconds (47.71 M allocations: 2.396 GiB, 2.63% gc time)
batchnorm.jl	gpu-stats: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:54
  Test threw exception
  Expression: abs(mean(batchnorm(kax; training = true))) < 0.001
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] batchnorm4(::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:264
   [14] batchnorm2(::Nothing, ::Nothing, ::KnetArray{Float64,2}; moments::Nothing, training::Bool, o::Base.Iterators.Pairs{Symbol,Knet.BNCache,Tuple{Symbol},NamedTuple{(:cache,),Tuple{Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:503
   [15] #batchnorm2#675 at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:514 [inlined]
   [16] #batchnorm#543 at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:90 [inlined]
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:54
   [18] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:54
   [20] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:40
   [22] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:8
  
gpu-stats: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:55
  Test threw exception
  Expression: isapprox(std2(batchnorm(kax; training = true)), 1.0; rtol = 0.01)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] batchnorm4(::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:264
   [14] batchnorm2(::Nothing, ::Nothing, ::KnetArray{Float64,2}; moments::Nothing, training::Bool, o::Base.Iterators.Pairs{Symbol,Knet.BNCache,Tuple{Symbol},NamedTuple{(:cache,),Tuple{Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:503
   [15] #batchnorm2#675 at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:514 [inlined]
   [16] #batchnorm#543 at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:90 [inlined]
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:55
   [18] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:54
   [20] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:40
   [22] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:8
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] batchnorm4(::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:264
 [14] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [15] #batchnorm4#559 at ./none:0 [inlined]
 [16] batchnorm2(::Nothing, ::Nothing, ::Param{KnetArray{Float64,2}}; moments::Nothing, training::Bool, o::Base.Iterators.Pairs{Symbol,Knet.BNCache,Tuple{Symbol},NamedTuple{(:cache,),Tuple{Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:503
 [17] #batchnorm2#675 at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:514 [inlined]
 [18] #batchnorm#543 at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:90 [inlined]
 [19] (::var"#bn1#115")(::Param{KnetArray{Float64,2}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:14
 [20] gcsum(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [21] gcsum(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [22] (::AutoGrad.var"#203#205"{Tuple{},var"#bn1#115",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [23] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [24] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [25] gradcheck(::var"#bn1#115", ::KnetArray{Float64,2}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [26] gradcheck(::Function, ::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
 [27] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:59
 [28] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:59
 [30] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [31] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:40
 [32] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [33] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:8
 [34] include(::String) at ./client.jl:457
 [35] macro expansion at ./timing.jl:174 [inlined]
 [36] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [37] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [38] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [39] include(::String) at ./client.jl:457
 [40] top-level scope at none:6
 [41] eval(::Module, ::Any) at ./boot.jl:331
 [42] exec_options(::Base.JLOptions) at ./client.jl:272
 [43] _start() at ./client.jl:506
gpu-grads: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:59
  Test threw exception
  Expression: gradcheck(bn1, kax)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#bn1#115", ::KnetArray{Float64,2}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:59
   [6] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:59
   [8] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:40
   [10] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [11] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:8
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] batchnorm4(::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:264
   [14] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [15] #batchnorm4#559 at ./none:0 [inlined]
   [16] batchnorm2(::Nothing, ::Nothing, ::Param{KnetArray{Float64,2}}; moments::Nothing, training::Bool, o::Base.Iterators.Pairs{Symbol,Knet.BNCache,Tuple{Symbol},NamedTuple{(:cache,),Tuple{Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:503
   [17] #batchnorm2#675 at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:514 [inlined]
   [18] #batchnorm#543 at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:90 [inlined]
   [19] (::var"#bn1#115")(::Param{KnetArray{Float64,2}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:14
   [20] gcsum(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [21] gcsum(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [22] (::AutoGrad.var"#203#205"{Tuple{},var"#bn1#115",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [23] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [24] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [25] gradcheck(::var"#bn1#115", ::KnetArray{Float64,2}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [26] gradcheck(::Function, ::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [27] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:59
   [28] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [29] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:59
   [30] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [31] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:40
   [32] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [33] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:8
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] #batchnorm4#572 at ./none:0 [inlined]
 [15] batchnorm2(::AutoGrad.Result{KnetArray{Float64,2}}, ::AutoGrad.Result{KnetArray{Float64,2}}, ::AutoGrad.Result{KnetArray{Float64,2}}; moments::Nothing, training::Bool, o::Base.Iterators.Pairs{Symbol,Knet.BNCache,Tuple{Symbol},NamedTuple{(:cache,),Tuple{Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:503
 [16] batchnorm(::AutoGrad.Result{KnetArray{Float64,2}}, ::Nothing, ::AutoGrad.Result{KnetArray{Float64,1}}; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:90
 [17] (::var"#bn3#114")(::Param{Tuple{KnetArray{Float64,2},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:13
 [18] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,2},KnetArray{Float64,1}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [19] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,2},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [20] (::AutoGrad.var"#203#205"{Tuple{},var"#bn3#114",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [21] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [22] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [23] gradcheck(::var"#bn3#114", ::Tuple{KnetArray{Float64,2},KnetArray{Float64,1}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [24] gradcheck(::Function, ::Tuple{KnetArray{Float64,2},KnetArray{Float64,1}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:60
 [26] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [27] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:59
 [28] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:40
 [30] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [31] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:8
 [32] include(::String) at ./client.jl:457
 [33] macro expansion at ./timing.jl:174 [inlined]
 [34] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [35] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [36] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [37] include(::String) at ./client.jl:457
 [38] top-level scope at none:6
 [39] eval(::Module, ::Any) at ./boot.jl:331
 [40] exec_options(::Base.JLOptions) at ./client.jl:272
 [41] _start() at ./client.jl:506
gpu-grads: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:60
  Test threw exception
  Expression: gradcheck(bn3, (kax, kaw))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#bn3#114", ::Tuple{KnetArray{Float64,2},KnetArray{Float64,1}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::Tuple{KnetArray{Float64,2},KnetArray{Float64,1}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:60
   [6] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:59
   [8] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:40
   [10] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [11] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:8
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] #batchnorm4#572 at ./none:0 [inlined]
   [15] batchnorm2(::AutoGrad.Result{KnetArray{Float64,2}}, ::AutoGrad.Result{KnetArray{Float64,2}}, ::AutoGrad.Result{KnetArray{Float64,2}}; moments::Nothing, training::Bool, o::Base.Iterators.Pairs{Symbol,Knet.BNCache,Tuple{Symbol},NamedTuple{(:cache,),Tuple{Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:503
   [16] batchnorm(::AutoGrad.Result{KnetArray{Float64,2}}, ::Nothing, ::AutoGrad.Result{KnetArray{Float64,1}}; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:90
   [17] (::var"#bn3#114")(::Param{Tuple{KnetArray{Float64,2},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:13
   [18] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,2},KnetArray{Float64,1}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [19] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,2},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [20] (::AutoGrad.var"#203#205"{Tuple{},var"#bn3#114",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [21] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [22] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [23] gradcheck(::var"#bn3#114", ::Tuple{KnetArray{Float64,2},KnetArray{Float64,1}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [24] gradcheck(::Function, ::Tuple{KnetArray{Float64,2},KnetArray{Float64,1}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:60
   [26] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [27] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:59
   [28] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [29] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:40
   [30] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [31] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:8
  
dev-consistency: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:63
  Got exception outside of a @test
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] batchnorm4(::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.BNMoments,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:264
   [14] batchnorm2(::Nothing, ::Nothing, ::KnetArray{Float64,2}; moments::Knet.BNMoments, training::Bool, o::Base.Iterators.Pairs{Symbol,Knet.BNCache,Tuple{Symbol},NamedTuple{(:cache,),Tuple{Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:503
   [15] #batchnorm2#675 at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:514 [inlined]
   [16] #batchnorm#543 at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:90 [inlined]
   [17] batchnorm at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:79 [inlined] (repeats 2 times)
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:67
   [19] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:64
   [21] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:40
   [23] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:8
   [25] include(::String) at ./client.jl:457
   [26] macro expansion at ./timing.jl:174 [inlined]
   [27] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
   [28] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [29] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
   [30] include(::String) at ./client.jl:457
   [31] top-level scope at none:6
   [32] eval(::Module, ::Any) at ./boot.jl:331
   [33] exec_options(::Base.JLOptions) at ./client.jl:272
   [34] _start() at ./client.jl:506
  
test-moments: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:75
  Got exception outside of a @test
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] batchnorm4(::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.BNMoments,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:264
   [14] batchnorm2(::Nothing, ::Nothing, ::KnetArray{Float64,2}; moments::Knet.BNMoments, training::Bool, o::Base.Iterators.Pairs{Symbol,Knet.BNCache,Tuple{Symbol},NamedTuple{(:cache,),Tuple{Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:503
   [15] #batchnorm2#675 at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:514 [inlined]
   [16] #batchnorm#543 at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:90 [inlined]
   [17] batchnorm at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:79 [inlined] (repeats 2 times)
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:84
   [19] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:76
   [21] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:40
   [23] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:8
   [25] include(::String) at ./client.jl:457
   [26] macro expansion at ./timing.jl:174 [inlined]
   [27] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
   [28] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [29] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
   [30] include(::String) at ./client.jl:457
   [31] top-level scope at none:6
   [32] eval(::Module, ::Any) at ./boot.jl:331
   [33] exec_options(::Base.JLOptions) at ./client.jl:272
   [34] _start() at ./client.jl:506
  
training-moments: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:92
  Got exception outside of a @test
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] batchnorm4(::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.BNMoments,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:264
   [14] batchnorm2(::Nothing, ::Nothing, ::KnetArray{Float64,2}; moments::Knet.BNMoments, training::Bool, o::Base.Iterators.Pairs{Symbol,Knet.BNCache,Tuple{Symbol},NamedTuple{(:cache,),Tuple{Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:503
   [15] #batchnorm2#675 at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:514 [inlined]
   [16] #batchnorm#543 at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:90 [inlined]
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:101
   [18] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:93
   [20] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:40
   [22] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:8
   [24] include(::String) at ./client.jl:457
   [25] macro expansion at ./timing.jl:174 [inlined]
   [26] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
   [27] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
   [29] include(::String) at ./client.jl:457
   [30] top-level scope at none:6
   [31] eval(::Module, ::Any) at ./boot.jl:331
   [32] exec_options(::Base.JLOptions) at ./client.jl:272
   [33] _start() at ./client.jl:506
  
gpu-stats: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:54
  Test threw exception
  Expression: abs(mean(batchnorm(kax; training = true))) < 0.001
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] batchnorm4(::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:264
   [14] batchnorm(::KnetArray{Float64,4}, ::Nothing, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:95
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:54
   [16] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:54
   [18] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:40
   [20] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:8
  
gpu-stats: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:55
  Test threw exception
  Expression: isapprox(std2(batchnorm(kax; training = true)), 1.0; rtol = 0.01)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] batchnorm4(::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:264
   [14] batchnorm(::KnetArray{Float64,4}, ::Nothing, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:95
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:55
   [16] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:54
   [18] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:40
   [20] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:8
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] batchnorm4(::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:264
 [14] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [15] #batchnorm4#559 at ./none:0 [inlined]
 [16] batchnorm(::Param{KnetArray{Float64,4}}, ::Nothing, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:95
 [17] (::var"#bn1#115")(::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:14
 [18] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [19] gcsum(::Function, ::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [20] (::AutoGrad.var"#203#205"{Tuple{},var"#bn1#115",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [21] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [22] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [23] gradcheck(::var"#bn1#115", ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [24] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:59
 [26] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [27] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:59
 [28] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:40
 [30] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [31] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:8
 [32] include(::String) at ./client.jl:457
 [33] macro expansion at ./timing.jl:174 [inlined]
 [34] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [35] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [36] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [37] include(::String) at ./client.jl:457
 [38] top-level scope at none:6
 [39] eval(::Module, ::Any) at ./boot.jl:331
 [40] exec_options(::Base.JLOptions) at ./client.jl:272
 [41] _start() at ./client.jl:506
gpu-grads: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:59
  Test threw exception
  Expression: gradcheck(bn1, kax)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#bn1#115", ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:59
   [6] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:59
   [8] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:40
   [10] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [11] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:8
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] batchnorm4(::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:264
   [14] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [15] #batchnorm4#559 at ./none:0 [inlined]
   [16] batchnorm(::Param{KnetArray{Float64,4}}, ::Nothing, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:95
   [17] (::var"#bn1#115")(::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:14
   [18] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [19] gcsum(::Function, ::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [20] (::AutoGrad.var"#203#205"{Tuple{},var"#bn1#115",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [21] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [22] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [23] gradcheck(::var"#bn1#115", ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [24] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:59
   [26] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [27] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:59
   [28] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [29] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:40
   [30] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [31] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:8
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] #batchnorm4#572 at ./none:0 [inlined]
 [15] batchnorm(::AutoGrad.Result{KnetArray{Float64,4}}, ::Nothing, ::AutoGrad.Result{KnetArray{Float64,1}}; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:95
 [16] (::var"#bn3#114")(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:13
 [17] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [18] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [19] (::AutoGrad.var"#203#205"{Tuple{},var"#bn3#114",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [20] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [21] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [22] gradcheck(::var"#bn3#114", ::Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [23] gradcheck(::Function, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:60
 [25] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [26] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:59
 [27] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [28] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:40
 [29] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:8
 [31] include(::String) at ./client.jl:457
 [32] macro expansion at ./timing.jl:174 [inlined]
 [33] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [34] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [35] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [36] include(::String) at ./client.jl:457
 [37] top-level scope at none:6
 [38] eval(::Module, ::Any) at ./boot.jl:331
 [39] exec_options(::Base.JLOptions) at ./client.jl:272
 [40] _start() at ./client.jl:506
gpu-grads: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:60
  Test threw exception
  Expression: gradcheck(bn3, (kax, kaw))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#bn3#114", ::Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:60
   [6] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:59
   [8] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:40
   [10] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [11] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:8
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] #batchnorm4#572 at ./none:0 [inlined]
   [15] batchnorm(::AutoGrad.Result{KnetArray{Float64,4}}, ::Nothing, ::AutoGrad.Result{KnetArray{Float64,1}}; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:95
   [16] (::var"#bn3#114")(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:13
   [17] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [18] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [19] (::AutoGrad.var"#203#205"{Tuple{},var"#bn3#114",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [20] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [21] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [22] gradcheck(::var"#bn3#114", ::Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [23] gradcheck(::Function, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:60
   [25] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [26] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:59
   [27] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:40
   [29] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:8
  
dev-consistency: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:63
  Got exception outside of a @test
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] batchnorm4(::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.BNMoments,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:264
   [14] batchnorm(::KnetArray{Float64,4}, ::Knet.BNMoments, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:95
   [15] batchnorm at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:79 [inlined] (repeats 2 times)
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:67
   [17] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:64
   [19] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:40
   [21] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:8
   [23] include(::String) at ./client.jl:457
   [24] macro expansion at ./timing.jl:174 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
   [26] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
   [28] include(::String) at ./client.jl:457
   [29] top-level scope at none:6
   [30] eval(::Module, ::Any) at ./boot.jl:331
   [31] exec_options(::Base.JLOptions) at ./client.jl:272
   [32] _start() at ./client.jl:506
  
test-moments: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:75
  Got exception outside of a @test
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] batchnorm4(::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.BNMoments,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:264
   [14] batchnorm(::KnetArray{Float64,4}, ::Knet.BNMoments, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:95
   [15] batchnorm at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:79 [inlined] (repeats 2 times)
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:84
   [17] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:76
   [19] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:40
   [21] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:8
   [23] include(::String) at ./client.jl:457
   [24] macro expansion at ./timing.jl:174 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
   [26] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
   [28] include(::String) at ./client.jl:457
   [29] top-level scope at none:6
   [30] eval(::Module, ::Any) at ./boot.jl:331
   [31] exec_options(::Base.JLOptions) at ./client.jl:272
   [32] _start() at ./client.jl:506
  
training-moments: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:92
  Got exception outside of a @test
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] batchnorm4(::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.BNMoments,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:264
   [14] batchnorm(::KnetArray{Float64,4}, ::Knet.BNMoments, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:95
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:101
   [16] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:93
   [18] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:40
   [20] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:8
   [22] include(::String) at ./client.jl:457
   [23] macro expansion at ./timing.jl:174 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
   [25] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
   [27] include(::String) at ./client.jl:457
   [28] top-level scope at none:6
   [29] eval(::Module, ::Any) at ./boot.jl:331
   [30] exec_options(::Base.JLOptions) at ./client.jl:272
   [31] _start() at ./client.jl:506
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] batchnorm4(::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.BNMoments,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:264
 [14] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.BNMoments,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [15] #batchnorm4#559 at ./none:0 [inlined]
 [16] batchnorm(::Param{KnetArray{Float64,4}}, ::Knet.BNMoments, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:95
 [17] (::var"#bn1ts#117")(::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:16
 [18] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [19] gcsum(::Function, ::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [20] (::AutoGrad.var"#203#205"{Tuple{},var"#bn1ts#117",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [21] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [22] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [23] gradcheck(::var"#bn1ts#117", ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [24] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:118
 [26] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [27] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:118
 [28] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:40
 [30] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [31] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:8
 [32] include(::String) at ./client.jl:457
 [33] macro expansion at ./timing.jl:174 [inlined]
 [34] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [35] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [36] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [37] include(::String) at ./client.jl:457
 [38] top-level scope at none:6
 [39] eval(::Module, ::Any) at ./boot.jl:331
 [40] exec_options(::Base.JLOptions) at ./client.jl:272
 [41] _start() at ./client.jl:506
gpu-grads-testing: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:118
  Test threw exception
  Expression: gradcheck(bn1ts, kax)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#bn1ts#117", ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:118
   [6] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:118
   [8] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:40
   [10] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [11] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:8
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] batchnorm4(::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.BNMoments,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:264
   [14] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.BNMoments,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [15] #batchnorm4#559 at ./none:0 [inlined]
   [16] batchnorm(::Param{KnetArray{Float64,4}}, ::Knet.BNMoments, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:95
   [17] (::var"#bn1ts#117")(::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:16
   [18] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [19] gcsum(::Function, ::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [20] (::AutoGrad.var"#203#205"{Tuple{},var"#bn1ts#117",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [21] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [22] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [23] gradcheck(::var"#bn1ts#117", ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [24] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:118
   [26] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [27] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:118
   [28] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [29] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:40
   [30] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [31] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:8
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.BNMoments,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] #batchnorm4#572 at ./none:0 [inlined]
 [15] batchnorm(::AutoGrad.Result{KnetArray{Float64,4}}, ::Knet.BNMoments, ::AutoGrad.Result{KnetArray{Float64,1}}; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:95
 [16] (::var"#bn3ts#116")(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:15
 [17] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [18] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [19] (::AutoGrad.var"#203#205"{Tuple{},var"#bn3ts#116",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [20] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [21] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [22] gradcheck(::var"#bn3ts#116", ::Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [23] gradcheck(::Function, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:119
 [25] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [26] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:118
 [27] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [28] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:40
 [29] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:8
 [31] include(::String) at ./client.jl:457
 [32] macro expansion at ./timing.jl:174 [inlined]
 [33] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [34] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [35] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [36] include(::String) at ./client.jl:457
 [37] top-level scope at none:6
 [38] eval(::Module, ::Any) at ./boot.jl:331
 [39] exec_options(::Base.JLOptions) at ./client.jl:272
 [40] _start() at ./client.jl:506
gpu-grads-testing: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:119
  Test threw exception
  Expression: gradcheck(bn3ts, (kax, kaw))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#bn3ts#116", ::Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:119
   [6] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:118
   [8] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:40
   [10] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [11] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:8
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.BNMoments,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] #batchnorm4#572 at ./none:0 [inlined]
   [15] batchnorm(::AutoGrad.Result{KnetArray{Float64,4}}, ::Knet.BNMoments, ::AutoGrad.Result{KnetArray{Float64,1}}; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:95
   [16] (::var"#bn3ts#116")(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:15
   [17] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [18] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [19] (::AutoGrad.var"#203#205"{Tuple{},var"#bn3ts#116",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [20] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [21] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [22] gradcheck(::var"#bn3ts#116", ::Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [23] gradcheck(::Function, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:119
   [25] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [26] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:118
   [27] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:40
   [29] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:8
  
gpu-stats: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:54
  Test threw exception
  Expression: abs(mean(batchnorm(kax; training = true))) < 0.001
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] batchnorm4(::KnetArray{Float64,5}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:264
   [14] batchnorm(::KnetArray{Float64,5}, ::Nothing, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:95
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:54
   [16] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:54
   [18] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:40
   [20] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:8
  
gpu-stats: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:55
  Test threw exception
  Expression: isapprox(std2(batchnorm(kax; training = true)), 1.0; rtol = 0.01)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] batchnorm4(::KnetArray{Float64,5}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:264
   [14] batchnorm(::KnetArray{Float64,5}, ::Nothing, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:95
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:55
   [16] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:54
   [18] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:40
   [20] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:8
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] batchnorm4(::KnetArray{Float64,5}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:264
 [14] forw(::Function, ::Param{KnetArray{Float64,5}}; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [15] #batchnorm4#559 at ./none:0 [inlined]
 [16] batchnorm(::Param{KnetArray{Float64,5}}, ::Nothing, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:95
 [17] (::var"#bn1#115")(::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:14
 [18] gcsum(::Function, ::Param{KnetArray{Float64,5}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [19] gcsum(::Function, ::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [20] (::AutoGrad.var"#203#205"{Tuple{},var"#bn1#115",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [21] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [22] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [23] gradcheck(::var"#bn1#115", ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [24] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:59
 [26] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [27] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:59
 [28] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:40
 [30] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [31] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:8
 [32] include(::String) at ./client.jl:457
 [33] macro expansion at ./timing.jl:174 [inlined]
 [34] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [35] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [36] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [37] include(::String) at ./client.jl:457
 [38] top-level scope at none:6
 [39] eval(::Module, ::Any) at ./boot.jl:331
 [40] exec_options(::Base.JLOptions) at ./client.jl:272
 [41] _start() at ./client.jl:506
gpu-grads: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:59
  Test threw exception
  Expression: gradcheck(bn1, kax)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#bn1#115", ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:59
   [6] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:59
   [8] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:40
   [10] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [11] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:8
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] batchnorm4(::KnetArray{Float64,5}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:264
   [14] forw(::Function, ::Param{KnetArray{Float64,5}}; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [15] #batchnorm4#559 at ./none:0 [inlined]
   [16] batchnorm(::Param{KnetArray{Float64,5}}, ::Nothing, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:95
   [17] (::var"#bn1#115")(::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:14
   [18] gcsum(::Function, ::Param{KnetArray{Float64,5}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [19] gcsum(::Function, ::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [20] (::AutoGrad.var"#203#205"{Tuple{},var"#bn1#115",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [21] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [22] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [23] gradcheck(::var"#bn1#115", ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [24] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:59
   [26] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [27] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:59
   [28] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [29] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:40
   [30] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [31] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:8
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,5}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,5}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] #batchnorm4#572 at ./none:0 [inlined]
 [15] batchnorm(::AutoGrad.Result{KnetArray{Float64,5}}, ::Nothing, ::AutoGrad.Result{KnetArray{Float64,1}}; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:95
 [16] (::var"#bn3#114")(::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:13
 [17] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [18] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [19] (::AutoGrad.var"#203#205"{Tuple{},var"#bn3#114",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [20] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [21] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [22] gradcheck(::var"#bn3#114", ::Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [23] gradcheck(::Function, ::Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:60
 [25] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [26] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:59
 [27] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [28] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:40
 [29] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:8
 [31] include(::String) at ./client.jl:457
 [32] macro expansion at ./timing.jl:174 [inlined]
 [33] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [34] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [35] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [36] include(::String) at ./client.jl:457
 [37] top-level scope at none:6
 [38] eval(::Module, ::Any) at ./boot.jl:331
 [39] exec_options(::Base.JLOptions) at ./client.jl:272
 [40] _start() at ./client.jl:506
gpu-grads: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:60
  Test threw exception
  Expression: gradcheck(bn3, (kax, kaw))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#bn3#114", ::Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:60
   [6] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:59
   [8] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:40
   [10] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [11] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:8
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,5}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,5}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] #batchnorm4#572 at ./none:0 [inlined]
   [15] batchnorm(::AutoGrad.Result{KnetArray{Float64,5}}, ::Nothing, ::AutoGrad.Result{KnetArray{Float64,1}}; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:95
   [16] (::var"#bn3#114")(::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:13
   [17] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [18] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [19] (::AutoGrad.var"#203#205"{Tuple{},var"#bn3#114",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [20] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [21] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [22] gradcheck(::var"#bn3#114", ::Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [23] gradcheck(::Function, ::Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:60
   [25] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [26] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:59
   [27] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:40
   [29] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:8
  
dev-consistency: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:63
  Got exception outside of a @test
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] batchnorm4(::KnetArray{Float64,5}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.BNMoments,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:264
   [14] batchnorm(::KnetArray{Float64,5}, ::Knet.BNMoments, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:95
   [15] batchnorm at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:79 [inlined] (repeats 2 times)
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:67
   [17] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:64
   [19] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:40
   [21] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:8
   [23] include(::String) at ./client.jl:457
   [24] macro expansion at ./timing.jl:174 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
   [26] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
   [28] include(::String) at ./client.jl:457
   [29] top-level scope at none:6
   [30] eval(::Module, ::Any) at ./boot.jl:331
   [31] exec_options(::Base.JLOptions) at ./client.jl:272
   [32] _start() at ./client.jl:506
  
test-moments: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:75
  Got exception outside of a @test
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] batchnorm4(::KnetArray{Float64,5}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.BNMoments,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:264
   [14] batchnorm(::KnetArray{Float64,5}, ::Knet.BNMoments, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:95
   [15] batchnorm at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:79 [inlined] (repeats 2 times)
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:84
   [17] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:76
   [19] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:40
   [21] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:8
   [23] include(::String) at ./client.jl:457
   [24] macro expansion at ./timing.jl:174 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
   [26] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
   [28] include(::String) at ./client.jl:457
   [29] top-level scope at none:6
   [30] eval(::Module, ::Any) at ./boot.jl:331
   [31] exec_options(::Base.JLOptions) at ./client.jl:272
   [32] _start() at ./client.jl:506
  
training-moments: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:92
  Got exception outside of a @test
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] batchnorm4(::KnetArray{Float64,5}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.BNMoments,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:264
   [14] batchnorm(::KnetArray{Float64,5}, ::Knet.BNMoments, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:95
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:101
   [16] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:93
   [18] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:40
   [20] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:8
   [22] include(::String) at ./client.jl:457
   [23] macro expansion at ./timing.jl:174 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
   [25] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [26] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
   [27] include(::String) at ./client.jl:457
   [28] top-level scope at none:6
   [29] eval(::Module, ::Any) at ./boot.jl:331
   [30] exec_options(::Base.JLOptions) at ./client.jl:272
   [31] _start() at ./client.jl:506
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] batchnorm4(::KnetArray{Float64,5}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.BNMoments,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:264
 [14] forw(::Function, ::Param{KnetArray{Float64,5}}; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.BNMoments,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [15] #batchnorm4#559 at ./none:0 [inlined]
 [16] batchnorm(::Param{KnetArray{Float64,5}}, ::Knet.BNMoments, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:95
 [17] (::var"#bn1ts#117")(::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:16
 [18] gcsum(::Function, ::Param{KnetArray{Float64,5}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [19] gcsum(::Function, ::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [20] (::AutoGrad.var"#203#205"{Tuple{},var"#bn1ts#117",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [21] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [22] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [23] gradcheck(::var"#bn1ts#117", ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [24] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:118
 [26] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [27] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:118
 [28] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:40
 [30] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [31] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:8
 [32] include(::String) at ./client.jl:457
 [33] macro expansion at ./timing.jl:174 [inlined]
 [34] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [35] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [36] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [37] include(::String) at ./client.jl:457
 [38] top-level scope at none:6
 [39] eval(::Module, ::Any) at ./boot.jl:331
 [40] exec_options(::Base.JLOptions) at ./client.jl:272
 [41] _start() at ./client.jl:506
gpu-grads-testing: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:118
  Test threw exception
  Expression: gradcheck(bn1ts, kax)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#bn1ts#117", ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:118
   [6] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:118
   [8] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:40
   [10] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [11] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:8
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] batchnorm4(::KnetArray{Float64,5}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.BNMoments,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:264
   [14] forw(::Function, ::Param{KnetArray{Float64,5}}; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.BNMoments,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [15] #batchnorm4#559 at ./none:0 [inlined]
   [16] batchnorm(::Param{KnetArray{Float64,5}}, ::Knet.BNMoments, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:95
   [17] (::var"#bn1ts#117")(::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:16
   [18] gcsum(::Function, ::Param{KnetArray{Float64,5}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [19] gcsum(::Function, ::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [20] (::AutoGrad.var"#203#205"{Tuple{},var"#bn1ts#117",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [21] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [22] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [23] gradcheck(::var"#bn1ts#117", ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [24] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:118
   [26] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [27] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:118
   [28] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [29] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:40
   [30] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [31] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:8
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,5}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,5}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.BNMoments,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] #batchnorm4#572 at ./none:0 [inlined]
 [15] batchnorm(::AutoGrad.Result{KnetArray{Float64,5}}, ::Knet.BNMoments, ::AutoGrad.Result{KnetArray{Float64,1}}; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:95
 [16] (::var"#bn3ts#116")(::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:15
 [17] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [18] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [19] (::AutoGrad.var"#203#205"{Tuple{},var"#bn3ts#116",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [20] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [21] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [22] gradcheck(::var"#bn3ts#116", ::Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [23] gradcheck(::Function, ::Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:119
 [25] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [26] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:118
 [27] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [28] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:40
 [29] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:8
 [31] include(::String) at ./client.jl:457
 [32] macro expansion at ./timing.jl:174 [inlined]
 [33] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [34] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [35] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [36] include(::String) at ./client.jl:457
 [37] top-level scope at none:6
 [38] eval(::Module, ::Any) at ./boot.jl:331
 [39] exec_options(::Base.JLOptions) at ./client.jl:272
 [40] _start() at ./client.jl:506
gpu-grads-testing: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:119
  Test threw exception
  Expression: gradcheck(bn3ts, (kax, kaw))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#bn3ts#116", ::Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:119
   [6] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:118
   [8] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:40
   [10] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [11] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:8
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,5}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,5}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.BNMoments,Bool,Knet.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] #batchnorm4#572 at ./none:0 [inlined]
   [15] batchnorm(::AutoGrad.Result{KnetArray{Float64,5}}, ::Knet.BNMoments, ::AutoGrad.Result{KnetArray{Float64,1}}; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/batchnorm.jl:95
   [16] (::var"#bn3ts#116")(::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:15
   [17] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [18] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [19] (::AutoGrad.var"#203#205"{Tuple{},var"#bn3ts#116",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [20] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [21] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [22] gradcheck(::var"#bn3ts#116", ::Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [23] gradcheck(::Function, ::Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:119
   [25] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [26] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:118
   [27] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:40
   [29] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/batchnorm.jl:8
  
 57.696723 seconds (44.23 M allocations: 2.157 GiB, 2.48% gc time)
loss.jl	
Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] generic_softmax(::Param{KnetArray{Float64,2}}, ::Int64, ::Function; dims::Function) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:138
 [14] #logp#784 at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:14 [inlined]
 [15] logp(::Param{KnetArray{Float64,2}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:14
 [16] gcsum(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] gcsum(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [18] (::AutoGrad.var"#203#205"{Tuple{},typeof(logp),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [21] gradcheck(::typeof(logp), ::KnetArray{Float64,2}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [22] gradcheck(::Function, ::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:11
 [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
 [26] include(::String) at ./client.jl:457
 [27] macro expansion at ./timing.jl:174 [inlined]
 [28] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [29] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [31] include(::String) at ./client.jl:457
 [32] top-level scope at none:6
 [33] eval(::Module, ::Any) at ./boot.jl:331
 [34] exec_options(::Base.JLOptions) at ./client.jl:272
 [35] _start() at ./client.jl:506
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:11
  Test threw exception
  Expression: gradcheck(f, k)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(logp), ::KnetArray{Float64,2}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:11
   [6] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] generic_softmax(::Param{KnetArray{Float64,2}}, ::Int64, ::Function; dims::Function) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:138
   [14] #logp#784 at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:14 [inlined]
   [15] logp(::Param{KnetArray{Float64,2}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:14
   [16] gcsum(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] gcsum(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [18] (::AutoGrad.var"#203#205"{Tuple{},typeof(logp),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [21] gradcheck(::typeof(logp), ::KnetArray{Float64,2}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [22] gradcheck(::Function, ::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:11
   [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] generic_softmax(::Param{KnetArray{Float64,2}}, ::Int64, ::Function; dims::Int64) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:138
 [14] #logp#784 at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:14 [inlined]
 [15] gcsum(::Function, ::Param{KnetArray{Float64,2}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [16] (::AutoGrad.var"#203#205"{Tuple{Pair{Symbol,Int64}},typeof(logp),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [19] gradcheck(::typeof(logp), ::KnetArray{Float64,2}; kw::Tuple{Pair{Symbol,Int64}}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [20] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:12
 [21] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
 [23] include(::String) at ./client.jl:457
 [24] macro expansion at ./timing.jl:174 [inlined]
 [25] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [26] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [27] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [28] include(::String) at ./client.jl:457
 [29] top-level scope at none:6
 [30] eval(::Module, ::Any) at ./boot.jl:331
 [31] exec_options(::Base.JLOptions) at ./client.jl:272
 [32] _start() at ./client.jl:506
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:12
  Test threw exception
  Expression: gradcheck(f, k, kw = (:dims => 1,))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(logp), ::KnetArray{Float64,2}; kw::Tuple{Pair{Symbol,Int64}}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:12
   [5] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] generic_softmax(::Param{KnetArray{Float64,2}}, ::Int64, ::Function; dims::Int64) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:138
   [14] #logp#784 at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:14 [inlined]
   [15] gcsum(::Function, ::Param{KnetArray{Float64,2}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [16] (::AutoGrad.var"#203#205"{Tuple{Pair{Symbol,Int64}},typeof(logp),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [19] gradcheck(::typeof(logp), ::KnetArray{Float64,2}; kw::Tuple{Pair{Symbol,Int64}}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:12
   [21] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] generic_softmax(::Param{KnetArray{Float64,2}}, ::Int64, ::Function; dims::Int64) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:138
 [14] #logp#784 at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:14 [inlined]
 [15] gcsum(::Function, ::Param{KnetArray{Float64,2}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [16] (::AutoGrad.var"#203#205"{Tuple{Pair{Symbol,Int64}},typeof(logp),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [19] gradcheck(::typeof(logp), ::KnetArray{Float64,2}; kw::Tuple{Pair{Symbol,Int64}}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [20] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:13
 [21] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
 [23] include(::String) at ./client.jl:457
 [24] macro expansion at ./timing.jl:174 [inlined]
 [25] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [26] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [27] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [28] include(::String) at ./client.jl:457
 [29] top-level scope at none:6
 [30] eval(::Module, ::Any) at ./boot.jl:331
 [31] exec_options(::Base.JLOptions) at ./client.jl:272
 [32] _start() at ./client.jl:506
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:13
  Test threw exception
  Expression: gradcheck(f, k, kw = (:dims => 2,))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(logp), ::KnetArray{Float64,2}; kw::Tuple{Pair{Symbol,Int64}}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:13
   [5] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] generic_softmax(::Param{KnetArray{Float64,2}}, ::Int64, ::Function; dims::Int64) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:138
   [14] #logp#784 at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:14 [inlined]
   [15] gcsum(::Function, ::Param{KnetArray{Float64,2}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [16] (::AutoGrad.var"#203#205"{Tuple{Pair{Symbol,Int64}},typeof(logp),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [19] gradcheck(::typeof(logp), ::KnetArray{Float64,2}; kw::Tuple{Pair{Symbol,Int64}}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:13
   [21] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:14
  Test threw exception
  Expression: isapprox(f(a), f(k))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] generic_softmax(::KnetArray{Float64,2}, ::Int64, ::Function; dims::Function) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:138
   [14] #logp#784 at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:14 [inlined]
   [15] logp(::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:14
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:14
   [17] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:15
  Test threw exception
  Expression: isapprox(f(a, dims = 1), f(k, dims = 1))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] generic_softmax(::KnetArray{Float64,2}, ::Int64, ::Function; dims::Int64) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:138
   [14] #logp#784 at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:14 [inlined]
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:15
   [16] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:16
  Test threw exception
  Expression: isapprox(f(a, dims = 2), f(k, dims = 2))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] generic_softmax(::KnetArray{Float64,2}, ::Int64, ::Function; dims::Int64) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:138
   [14] #logp#784 at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:14 [inlined]
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:16
   [16] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] generic_softmax(::Param{KnetArray{Float64,3}}, ::Int64, ::Function; dims::Function) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:138
 [14] #logp#784 at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:14 [inlined]
 [15] logp(::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:14
 [16] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] gcsum(::Function, ::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [18] (::AutoGrad.var"#203#205"{Tuple{},typeof(logp),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [21] gradcheck(::typeof(logp), ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [22] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:30
 [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
 [26] include(::String) at ./client.jl:457
 [27] macro expansion at ./timing.jl:174 [inlined]
 [28] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [29] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [31] include(::String) at ./client.jl:457
 [32] top-level scope at none:6
 [33] eval(::Module, ::Any) at ./boot.jl:331
 [34] exec_options(::Base.JLOptions) at ./client.jl:272
 [35] _start() at ./client.jl:506
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:30
  Test threw exception
  Expression: gradcheck(f, k)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(logp), ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:30
   [6] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] generic_softmax(::Param{KnetArray{Float64,3}}, ::Int64, ::Function; dims::Function) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:138
   [14] #logp#784 at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:14 [inlined]
   [15] logp(::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:14
   [16] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] gcsum(::Function, ::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [18] (::AutoGrad.var"#203#205"{Tuple{},typeof(logp),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [21] gradcheck(::typeof(logp), ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [22] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:30
   [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:31
  Test threw exception
  Expression: isapprox(f(a), f(k))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] generic_softmax(::KnetArray{Float64,3}, ::Int64, ::Function; dims::Function) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:138
   [14] #logp#784 at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:14 [inlined]
   [15] logp(::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:14
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:31
   [17] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] generic_softmax(::Param{KnetArray{Float64,3}}, ::Int64, ::Function; dims::Int64) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:138
 [14] #logp#784 at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:14 [inlined]
 [15] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [16] (::AutoGrad.var"#203#205"{Tuple{Pair{Symbol,Int64}},typeof(logp),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [19] gradcheck(::typeof(logp), ::KnetArray{Float64,3}; kw::Tuple{Pair{Symbol,Int64}}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [20] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:33
 [21] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
 [23] include(::String) at ./client.jl:457
 [24] macro expansion at ./timing.jl:174 [inlined]
 [25] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [26] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [27] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [28] include(::String) at ./client.jl:457
 [29] top-level scope at none:6
 [30] eval(::Module, ::Any) at ./boot.jl:331
 [31] exec_options(::Base.JLOptions) at ./client.jl:272
 [32] _start() at ./client.jl:506
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:33
  Test threw exception
  Expression: gradcheck(f, k, kw = (:dims => d,))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(logp), ::KnetArray{Float64,3}; kw::Tuple{Pair{Symbol,Int64}}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:33
   [5] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] generic_softmax(::Param{KnetArray{Float64,3}}, ::Int64, ::Function; dims::Int64) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:138
   [14] #logp#784 at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:14 [inlined]
   [15] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [16] (::AutoGrad.var"#203#205"{Tuple{Pair{Symbol,Int64}},typeof(logp),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [19] gradcheck(::typeof(logp), ::KnetArray{Float64,3}; kw::Tuple{Pair{Symbol,Int64}}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:33
   [21] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:34
  Test threw exception
  Expression: isapprox(f(a, dims = d), f(k, dims = d))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] generic_softmax(::KnetArray{Float64,3}, ::Int64, ::Function; dims::Int64) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:138
   [14] #logp#784 at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:14 [inlined]
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:34
   [16] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] generic_softmax(::Param{KnetArray{Float64,3}}, ::Int64, ::Function; dims::Int64) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:138
 [14] #logp#784 at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:14 [inlined]
 [15] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [16] (::AutoGrad.var"#203#205"{Tuple{Pair{Symbol,Int64}},typeof(logp),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [19] gradcheck(::typeof(logp), ::KnetArray{Float64,3}; kw::Tuple{Pair{Symbol,Int64}}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [20] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:33
 [21] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
 [23] include(::String) at ./client.jl:457
 [24] macro expansion at ./timing.jl:174 [inlined]
 [25] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [26] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [27] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [28] include(::String) at ./client.jl:457
 [29] top-level scope at none:6
 [30] eval(::Module, ::Any) at ./boot.jl:331
 [31] exec_options(::Base.JLOptions) at ./client.jl:272
 [32] _start() at ./client.jl:506
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:33
  Test threw exception
  Expression: gradcheck(f, k, kw = (:dims => d,))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(logp), ::KnetArray{Float64,3}; kw::Tuple{Pair{Symbol,Int64}}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:33
   [5] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] generic_softmax(::Param{KnetArray{Float64,3}}, ::Int64, ::Function; dims::Int64) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:138
   [14] #logp#784 at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:14 [inlined]
   [15] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [16] (::AutoGrad.var"#203#205"{Tuple{Pair{Symbol,Int64}},typeof(logp),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [19] gradcheck(::typeof(logp), ::KnetArray{Float64,3}; kw::Tuple{Pair{Symbol,Int64}}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:33
   [21] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:34
  Test threw exception
  Expression: isapprox(f(a, dims = d), f(k, dims = d))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] generic_softmax(::KnetArray{Float64,3}, ::Int64, ::Function; dims::Int64) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:138
   [14] #logp#784 at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:14 [inlined]
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:34
   [16] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] generic_softmax(::Param{KnetArray{Float64,3}}, ::Int64, ::Function; dims::Int64) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:138
 [14] #logp#784 at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:14 [inlined]
 [15] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [16] (::AutoGrad.var"#203#205"{Tuple{Pair{Symbol,Int64}},typeof(logp),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [19] gradcheck(::typeof(logp), ::KnetArray{Float64,3}; kw::Tuple{Pair{Symbol,Int64}}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [20] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:33
 [21] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
 [23] include(::String) at ./client.jl:457
 [24] macro expansion at ./timing.jl:174 [inlined]
 [25] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [26] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [27] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [28] include(::String) at ./client.jl:457
 [29] top-level scope at none:6
 [30] eval(::Module, ::Any) at ./boot.jl:331
 [31] exec_options(::Base.JLOptions) at ./client.jl:272
 [32] _start() at ./client.jl:506
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:33
  Test threw exception
  Expression: gradcheck(f, k, kw = (:dims => d,))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(logp), ::KnetArray{Float64,3}; kw::Tuple{Pair{Symbol,Int64}}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:33
   [5] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] generic_softmax(::Param{KnetArray{Float64,3}}, ::Int64, ::Function; dims::Int64) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:138
   [14] #logp#784 at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:14 [inlined]
   [15] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [16] (::AutoGrad.var"#203#205"{Tuple{Pair{Symbol,Int64}},typeof(logp),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [19] gradcheck(::typeof(logp), ::KnetArray{Float64,3}; kw::Tuple{Pair{Symbol,Int64}}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:33
   [21] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:34
  Test threw exception
  Expression: isapprox(f(a, dims = d), f(k, dims = d))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] generic_softmax(::KnetArray{Float64,3}, ::Int64, ::Function; dims::Int64) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:138
   [14] #logp#784 at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:14 [inlined]
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:34
   [16] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:37
  Test threw exception
  Expression: isapprox(f(a, dims = dims), f(k, dims = dims))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] generic_softmax(::KnetArray{Float64,3}, ::Int64, ::Function; dims::Tuple{Int64,Int64}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:138
   [14] #logp#784 at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:14 [inlined]
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:37
   [16] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:37
  Test threw exception
  Expression: isapprox(f(a, dims = dims), f(k, dims = dims))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] generic_softmax(::KnetArray{Float64,3}, ::Int64, ::Function; dims::Tuple{Int64,Int64}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:138
   [14] #logp#784 at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:14 [inlined]
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:37
   [16] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:37
  Test threw exception
  Expression: isapprox(f(a, dims = dims), f(k, dims = dims))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] generic_softmax(::KnetArray{Float64,3}, ::Int64, ::Function; dims::Tuple{Int64,Int64}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:138
   [14] #logp#784 at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:14 [inlined]
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:37
   [16] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:37
  Test threw exception
  Expression: isapprox(f(a, dims = dims), f(k, dims = dims))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] generic_softmax(::KnetArray{Float64,3}, ::Int64, ::Function; dims::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:138
   [14] #logp#784 at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:14 [inlined]
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:37
   [16] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] generic_softmax(::Param{KnetArray{Float64,2}}, ::Int64, ::Function; dims::Int64) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:138
 [14] #logp#784 at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:14 [inlined]
 [15] nll(::Param{KnetArray{Float64,2}}, ::Array{Int64,1}; dims::Int64, average::Bool) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:305
 [16] gcsum(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] (::AutoGrad.var"#203#205"{Tuple{Pair{Symbol,Int64}},typeof(nll),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [20] gradcheck(::typeof(nll), ::KnetArray{Float64,2}, ::Vararg{Any,N} where N; kw::Tuple{Pair{Symbol,Int64}}, args::Int64, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:59
 [22] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
 [24] include(::String) at ./client.jl:457
 [25] macro expansion at ./timing.jl:174 [inlined]
 [26] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [27] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [28] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [29] include(::String) at ./client.jl:457
 [30] top-level scope at none:6
 [31] eval(::Module, ::Any) at ./boot.jl:331
 [32] exec_options(::Base.JLOptions) at ./client.jl:272
 [33] _start() at ./client.jl:506
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:59
  Test threw exception
  Expression: gradcheck(nll, k, indices, kw = (:dims => 1,), args = 1)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(nll), ::KnetArray{Float64,2}, ::Vararg{Any,N} where N; kw::Tuple{Pair{Symbol,Int64}}, args::Int64, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:59
   [5] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] generic_softmax(::Param{KnetArray{Float64,2}}, ::Int64, ::Function; dims::Int64) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:138
   [14] #logp#784 at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:14 [inlined]
   [15] nll(::Param{KnetArray{Float64,2}}, ::Array{Int64,1}; dims::Int64, average::Bool) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:305
   [16] gcsum(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] (::AutoGrad.var"#203#205"{Tuple{Pair{Symbol,Int64}},typeof(nll),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [20] gradcheck(::typeof(nll), ::KnetArray{Float64,2}, ::Vararg{Any,N} where N; kw::Tuple{Pair{Symbol,Int64}}, args::Int64, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:59
   [22] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] generic_softmax(::Param{KnetArray{Float64,2}}, ::Int64, ::Function; dims::Int64) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:138
 [14] #logp#784 at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:14 [inlined]
 [15] nll(::Param{KnetArray{Float64,2}}, ::Array{Int64,1}; dims::Int64, average::Bool) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:305
 [16] gcsum(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] (::AutoGrad.var"#203#205"{Tuple{Pair{Symbol,Int64}},typeof(nll),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [20] gradcheck(::typeof(nll), ::KnetArray{Float64,2}, ::Vararg{Any,N} where N; kw::Tuple{Pair{Symbol,Int64}}, args::Int64, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:60
 [22] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
 [24] include(::String) at ./client.jl:457
 [25] macro expansion at ./timing.jl:174 [inlined]
 [26] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [27] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [28] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [29] include(::String) at ./client.jl:457
 [30] top-level scope at none:6
 [31] eval(::Module, ::Any) at ./boot.jl:331
 [32] exec_options(::Base.JLOptions) at ./client.jl:272
 [33] _start() at ./client.jl:506
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:60
  Test threw exception
  Expression: gradcheck(nll, k, indices, kw = (:dims => 2,), args = 1)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(nll), ::KnetArray{Float64,2}, ::Vararg{Any,N} where N; kw::Tuple{Pair{Symbol,Int64}}, args::Int64, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:60
   [5] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] generic_softmax(::Param{KnetArray{Float64,2}}, ::Int64, ::Function; dims::Int64) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:138
   [14] #logp#784 at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:14 [inlined]
   [15] nll(::Param{KnetArray{Float64,2}}, ::Array{Int64,1}; dims::Int64, average::Bool) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:305
   [16] gcsum(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] (::AutoGrad.var"#203#205"{Tuple{Pair{Symbol,Int64}},typeof(nll),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [20] gradcheck(::typeof(nll), ::KnetArray{Float64,2}, ::Vararg{Any,N} where N; kw::Tuple{Pair{Symbol,Int64}}, args::Int64, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:60
   [22] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:61
  Test threw exception
  Expression: isapprox(nll(k, indices, dims = 1), nll(a, indices, dims = 1))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] generic_softmax(::KnetArray{Float64,2}, ::Int64, ::Function; dims::Int64) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:138
   [14] #logp#784 at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:14 [inlined]
   [15] nll(::KnetArray{Float64,2}, ::Array{Int64,1}; dims::Int64, average::Bool) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:305
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:61
   [17] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:62
  Test threw exception
  Expression: isapprox(nll(k, indices, dims = 2), nll(a, indices, dims = 2))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] generic_softmax(::KnetArray{Float64,2}, ::Int64, ::Function; dims::Int64) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:138
   [14] #logp#784 at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:14 [inlined]
   [15] nll(::KnetArray{Float64,2}, ::Array{Int64,1}; dims::Int64, average::Bool) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:305
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:62
   [17] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:76
  Test threw exception
  Expression: isapprox(_softmax(x, dims = 1), cudnnSoftmaxForward(x, algo = 0))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:76
   [14] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:77
  Test threw exception
  Expression: isapprox(_softmax(x, dims = 1), cudnnSoftmaxForward(x, algo = 1))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:77
   [14] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:78
  Test threw exception
  Expression: isapprox(_logp(x, dims = 1), cudnnSoftmaxForward(x, algo = 2))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:78
   [14] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:79
  Test threw exception
  Expression: isapprox(_softback(x, y1, dy, dims = 1), cudnnSoftmaxBackward(y1, dy, algo = 0))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:79
   [14] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:80
  Test threw exception
  Expression: isapprox(_softback(x, y1, dy, dims = 1), cudnnSoftmaxBackward(y1, dy, algo = 1))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:80
   [14] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:81
  Test threw exception
  Expression: isapprox(_logpback(x, y2, dy, dims = 1), cudnnSoftmaxBackward(y2, dy, algo = 2))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:81
   [14] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::Param{KnetArray{Float64,2}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:algo,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] cudnnSoftmaxForward(::Param{KnetArray{Float64,2}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:algo,),Tuple{Int64}}}) at ./none:0
 [15] (::var"#123#133")() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
 [16] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
 [18] (::AutoGrad.var"#220#225"{Tuple{},var"#123#133",Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [21] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
 [22] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:82
 [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
 [26] include(::String) at ./client.jl:457
 [27] macro expansion at ./timing.jl:174 [inlined]
 [28] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [29] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [31] include(::String) at ./client.jl:457
 [32] top-level scope at none:6
 [33] eval(::Module, ::Any) at ./boot.jl:331
 [34] exec_options(::Base.JLOptions) at ./client.jl:272
 [35] _start() at ./client.jl:506
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:82
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:82 =# @gcheck cudnnSoftmaxForward(Param(x), algo = 0)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:82
   [6] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::Param{KnetArray{Float64,2}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:algo,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] cudnnSoftmaxForward(::Param{KnetArray{Float64,2}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:algo,),Tuple{Int64}}}) at ./none:0
   [15] (::var"#123#133")() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
   [16] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
   [18] (::AutoGrad.var"#220#225"{Tuple{},var"#123#133",Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [21] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [22] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:82
   [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::Param{KnetArray{Float64,2}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:algo,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] cudnnSoftmaxForward(::Param{KnetArray{Float64,2}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:algo,),Tuple{Int64}}}) at ./none:0
 [15] (::var"#124#134")() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
 [16] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
 [18] (::AutoGrad.var"#220#225"{Tuple{},var"#124#134",Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [21] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
 [22] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:83
 [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
 [26] include(::String) at ./client.jl:457
 [27] macro expansion at ./timing.jl:174 [inlined]
 [28] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [29] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [31] include(::String) at ./client.jl:457
 [32] top-level scope at none:6
 [33] eval(::Module, ::Any) at ./boot.jl:331
 [34] exec_options(::Base.JLOptions) at ./client.jl:272
 [35] _start() at ./client.jl:506
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:83
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:83 =# @gcheck cudnnSoftmaxForward(Param(x), algo = 1)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:83
   [6] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::Param{KnetArray{Float64,2}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:algo,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] cudnnSoftmaxForward(::Param{KnetArray{Float64,2}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:algo,),Tuple{Int64}}}) at ./none:0
   [15] (::var"#124#134")() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
   [16] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
   [18] (::AutoGrad.var"#220#225"{Tuple{},var"#124#134",Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [21] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [22] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:83
   [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::Param{KnetArray{Float64,2}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:algo,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] cudnnSoftmaxForward(::Param{KnetArray{Float64,2}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:algo,),Tuple{Int64}}}) at ./none:0
 [15] (::var"#125#135")() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
 [16] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
 [18] (::AutoGrad.var"#220#225"{Tuple{},var"#125#135",Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [21] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
 [22] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:84
 [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
 [26] include(::String) at ./client.jl:457
 [27] macro expansion at ./timing.jl:174 [inlined]
 [28] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [29] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [31] include(::String) at ./client.jl:457
 [32] top-level scope at none:6
 [33] eval(::Module, ::Any) at ./boot.jl:331
 [34] exec_options(::Base.JLOptions) at ./client.jl:272
 [35] _start() at ./client.jl:506
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:84
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:84 =# @gcheck cudnnSoftmaxForward(Param(x), algo = 2)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:84
   [6] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::Param{KnetArray{Float64,2}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:algo,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] cudnnSoftmaxForward(::Param{KnetArray{Float64,2}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:algo,),Tuple{Int64}}}) at ./none:0
   [15] (::var"#125#135")() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
   [16] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
   [18] (::AutoGrad.var"#220#225"{Tuple{},var"#125#135",Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [21] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [22] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:84
   [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Param{KnetArray{Float64,2}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:algo,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] #cudnnSoftmaxBackward#808 at ./none:0 [inlined]
 [15] (::var"#126#136")() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
 [16] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
 [18] (::AutoGrad.var"#220#225"{Tuple{},var"#126#136",Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [21] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
 [22] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:85
 [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
 [26] include(::String) at ./client.jl:457
 [27] macro expansion at ./timing.jl:174 [inlined]
 [28] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [29] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [31] include(::String) at ./client.jl:457
 [32] top-level scope at none:6
 [33] eval(::Module, ::Any) at ./boot.jl:331
 [34] exec_options(::Base.JLOptions) at ./client.jl:272
 [35] _start() at ./client.jl:506
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:85
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:85 =# @gcheck cudnnSoftmaxBackward(Param(y1), Param(dy), algo = 0)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:85
   [6] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Param{KnetArray{Float64,2}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:algo,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] #cudnnSoftmaxBackward#808 at ./none:0 [inlined]
   [15] (::var"#126#136")() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
   [16] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
   [18] (::AutoGrad.var"#220#225"{Tuple{},var"#126#136",Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [21] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [22] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:85
   [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Param{KnetArray{Float64,2}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:algo,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] #cudnnSoftmaxBackward#808 at ./none:0 [inlined]
 [15] (::var"#127#137")() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
 [16] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
 [18] (::AutoGrad.var"#220#225"{Tuple{},var"#127#137",Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [21] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
 [22] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:86
 [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
 [26] include(::String) at ./client.jl:457
 [27] macro expansion at ./timing.jl:174 [inlined]
 [28] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [29] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [31] include(::String) at ./client.jl:457
 [32] top-level scope at none:6
 [33] eval(::Module, ::Any) at ./boot.jl:331
 [34] exec_options(::Base.JLOptions) at ./client.jl:272
 [35] _start() at ./client.jl:506
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:86
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:86 =# @gcheck cudnnSoftmaxBackward(Param(y1), Param(dy), algo = 1)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:86
   [6] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Param{KnetArray{Float64,2}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:algo,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] #cudnnSoftmaxBackward#808 at ./none:0 [inlined]
   [15] (::var"#127#137")() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
   [16] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
   [18] (::AutoGrad.var"#220#225"{Tuple{},var"#127#137",Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [21] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [22] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:86
   [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Param{KnetArray{Float64,2}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:algo,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] #cudnnSoftmaxBackward#808 at ./none:0 [inlined]
 [15] (::var"#128#138")() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
 [16] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
 [18] (::AutoGrad.var"#220#225"{Tuple{},var"#128#138",Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [21] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
 [22] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:87
 [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
 [26] include(::String) at ./client.jl:457
 [27] macro expansion at ./timing.jl:174 [inlined]
 [28] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [29] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [31] include(::String) at ./client.jl:457
 [32] top-level scope at none:6
 [33] eval(::Module, ::Any) at ./boot.jl:331
 [34] exec_options(::Base.JLOptions) at ./client.jl:272
 [35] _start() at ./client.jl:506
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:87
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:87 =# @gcheck cudnnSoftmaxBackward(Param(y2), Param(dy), algo = 2)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:87
   [6] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Param{KnetArray{Float64,2}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:algo,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] #cudnnSoftmaxBackward#808 at ./none:0 [inlined]
   [15] (::var"#128#138")() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
   [16] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
   [18] (::AutoGrad.var"#220#225"{Tuple{},var"#128#138",Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [21] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [22] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:87
   [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:96
  Test threw exception
  Expression: isapprox(f(a, b, c), f(A, B, C))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] generic_softmax(::KnetArray{Float64,2}, ::Int64, ::Function; dims::Int64) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:138
   [14] #logp#784 at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:14 [inlined]
   [15] nll(::KnetArray{Float64,2}, ::Array{Int64,1}; dims::Int64, average::Bool) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:305
   [16] nll at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:304 [inlined]
   [17] (::var"#f#139")(::KnetArray{Float64,2}, ::KnetArray{Float64,2}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:90
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:96
   [19] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] generic_softmax(::AutoGrad.Result{KnetArray{Float64,2}}, ::Int64, ::Function; dims::Int64) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:138
 [14] #logp#784 at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:14 [inlined]
 [15] nll(::AutoGrad.Result{KnetArray{Float64,2}}, ::Array{Int64,1}; dims::Int64, average::Bool) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:305
 [16] nll(::AutoGrad.Result{KnetArray{Float64,2}}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:304
 [17] (::var"#f#139")(::Param{KnetArray{Float64,2}}, ::KnetArray{Float64,2}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:90
 [18] differentiate(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [19] differentiate(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135
 [20] (::AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#f#139",Int64,Bool}})(::KnetArray{Float64,2}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:225
 [21] (::AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#f#139",Int64,Bool}})(::KnetArray{Float64,2}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:221
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:97
 [23] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
 [25] include(::String) at ./client.jl:457
 [26] macro expansion at ./timing.jl:174 [inlined]
 [27] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [28] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [30] include(::String) at ./client.jl:457
 [31] top-level scope at none:6
 [32] eval(::Module, ::Any) at ./boot.jl:331
 [33] exec_options(::Base.JLOptions) at ./client.jl:272
 [34] _start() at ./client.jl:506
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:97
  Test threw exception
  Expression: isapprox(∇f(a, b, c), ∇f(A, B, C))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135
   [3] (::AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#f#139",Int64,Bool}})(::KnetArray{Float64,2}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:225
   [4] (::AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#f#139",Int64,Bool}})(::KnetArray{Float64,2}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:221
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:97
   [6] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] generic_softmax(::AutoGrad.Result{KnetArray{Float64,2}}, ::Int64, ::Function; dims::Int64) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:138
   [14] #logp#784 at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:14 [inlined]
   [15] nll(::AutoGrad.Result{KnetArray{Float64,2}}, ::Array{Int64,1}; dims::Int64, average::Bool) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:305
   [16] nll(::AutoGrad.Result{KnetArray{Float64,2}}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:304
   [17] (::var"#f#139")(::Param{KnetArray{Float64,2}}, ::KnetArray{Float64,2}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:90
   [18] differentiate(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [19] differentiate(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135
   [20] (::AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#f#139",Int64,Bool}})(::KnetArray{Float64,2}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:225
   [21] (::AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#f#139",Int64,Bool}})(::KnetArray{Float64,2}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:221
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:97
   [23] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] generic_softmax(::AutoGrad.Result{KnetArray{Float64,2}}, ::Int64, ::Function; dims::Int64) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:138
 [14] #logp#784 at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:14 [inlined]
 [15] nll(::AutoGrad.Result{KnetArray{Float64,2}}, ::Array{Int64,1}; dims::Int64, average::Bool) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:305
 [16] nll(::AutoGrad.Result{KnetArray{Float64,2}}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:304
 [17] (::var"#f#139")(::AutoGrad.Result{KnetArray{Float64,2}}, ::KnetArray{Float64,2}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:90
 [18] differentiate(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [19] differentiate(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135
 [20] (::AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#f#139",Int64,Bool}})(::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:225
 [21] gradfun at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:221 [inlined]
 [22] (::var"#∇fj#140"{AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#f#139",Int64,Bool}}})(::Param{KnetArray{Float64,2}}, ::KnetArray{Float64,2}, ::Array{Int64,1}, ::Int64) at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:92
 [23] differentiate(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [24] differentiate(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135
 [25] (::AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#∇fj#140"{AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#f#139",Int64,Bool}}},Int64,Bool}})(::KnetArray{Float64,2}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:225
 [26] (::AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#∇fj#140"{AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#f#139",Int64,Bool}}},Int64,Bool}})(::KnetArray{Float64,2}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:221
 [27] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:99
 [28] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
 [30] include(::String) at ./client.jl:457
 [31] macro expansion at ./timing.jl:174 [inlined]
 [32] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [33] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [34] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [35] include(::String) at ./client.jl:457
 [36] top-level scope at none:6
 [37] eval(::Module, ::Any) at ./boot.jl:331
 [38] exec_options(::Base.JLOptions) at ./client.jl:272
 [39] _start() at ./client.jl:506

Stacktrace:
 [1] differentiate(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
 [2] differentiate(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135
 [3] (::AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#f#139",Int64,Bool}})(::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:225
 [4] gradfun at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:221 [inlined]
 [5] (::var"#∇fj#140"{AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#f#139",Int64,Bool}}})(::Param{KnetArray{Float64,2}}, ::KnetArray{Float64,2}, ::Array{Int64,1}, ::Int64) at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:92
 [6] differentiate(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [7] differentiate(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135
 [8] (::AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#∇fj#140"{AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#f#139",Int64,Bool}}},Int64,Bool}})(::KnetArray{Float64,2}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:225
 [9] (::AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#∇fj#140"{AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#f#139",Int64,Bool}}},Int64,Bool}})(::KnetArray{Float64,2}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:221
 [10] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:99
 [11] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [12] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
 [13] include(::String) at ./client.jl:457
 [14] macro expansion at ./timing.jl:174 [inlined]
 [15] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [16] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [17] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [18] include(::String) at ./client.jl:457
 [19] top-level scope at none:6
 [20] eval(::Module, ::Any) at ./boot.jl:331
 [21] exec_options(::Base.JLOptions) at ./client.jl:272
 [22] _start() at ./client.jl:506
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:99
  Test threw exception
  Expression: isapprox(∇∇fj(a, b, c, i), ∇∇fj(A, B, C, i))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135
   [3] (::AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#∇fj#140"{AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#f#139",Int64,Bool}}},Int64,Bool}})(::KnetArray{Float64,2}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:225
   [4] (::AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#∇fj#140"{AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#f#139",Int64,Bool}}},Int64,Bool}})(::KnetArray{Float64,2}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:221
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:99
   [6] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  caused by [exception 2]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135
   [3] (::AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#f#139",Int64,Bool}})(::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:225
   [4] gradfun at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:221 [inlined]
   [5] (::var"#∇fj#140"{AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#f#139",Int64,Bool}}})(::Param{KnetArray{Float64,2}}, ::KnetArray{Float64,2}, ::Array{Int64,1}, ::Int64) at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:92
   [6] differentiate(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [7] differentiate(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135
   [8] (::AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#∇fj#140"{AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#f#139",Int64,Bool}}},Int64,Bool}})(::KnetArray{Float64,2}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:225
   [9] (::AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#∇fj#140"{AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#f#139",Int64,Bool}}},Int64,Bool}})(::KnetArray{Float64,2}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:221
   [10] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:99
   [11] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [12] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] generic_softmax(::AutoGrad.Result{KnetArray{Float64,2}}, ::Int64, ::Function; dims::Int64) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:138
   [14] #logp#784 at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:14 [inlined]
   [15] nll(::AutoGrad.Result{KnetArray{Float64,2}}, ::Array{Int64,1}; dims::Int64, average::Bool) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:305
   [16] nll(::AutoGrad.Result{KnetArray{Float64,2}}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/loss.jl:304
   [17] (::var"#f#139")(::AutoGrad.Result{KnetArray{Float64,2}}, ::KnetArray{Float64,2}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:90
   [18] differentiate(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [19] differentiate(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135
   [20] (::AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#f#139",Int64,Bool}})(::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:225
   [21] gradfun at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:221 [inlined]
   [22] (::var"#∇fj#140"{AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#f#139",Int64,Bool}}})(::Param{KnetArray{Float64,2}}, ::KnetArray{Float64,2}, ::Array{Int64,1}, ::Int64) at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:92
   [23] differentiate(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [24] differentiate(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135
   [25] (::AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#∇fj#140"{AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#f#139",Int64,Bool}}},Int64,Bool}})(::KnetArray{Float64,2}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:225
   [26] (::AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#∇fj#140"{AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#f#139",Int64,Bool}}},Int64,Bool}})(::KnetArray{Float64,2}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:221
   [27] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:99
   [28] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [29] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/loss.jl:4
  
 55.612757 seconds (38.32 M allocations: 1.922 GiB, 2.47% gc time)
karray.jl	 87.004783 seconds (65.97 M allocations: 3.303 GiB, 2.40% gc time)
reduction.jl	126.501892 seconds (113.91 M allocations: 5.448 GiB, 3.01% gc time)
conv.jl	┌ Warning: cpuconv tests commented out until we figure out why they are failing on gitlab-ci
└ @ Main ~/.julia/packages/Knet/exwCE/test/conv.jl:31
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
  Test threw exception
  Expression: isapprox(pool(kx), pool(ax))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] pool(::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:134
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [15] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [17] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] pool(::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:134
 [14] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [15] forw at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:65 [inlined]
 [16] #pool#406 at ./none:0 [inlined]
 [17] pool(::Param{KnetArray{Float64,4}}) at ./none:0
 [18] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [19] gcsum(::Function, ::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [20] (::AutoGrad.var"#203#205"{Tuple{},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [21] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [22] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [23] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [24] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:118
 [26] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [27] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
 [28] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [30] include(::String) at ./client.jl:457
 [31] macro expansion at ./timing.jl:174 [inlined]
 [32] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [33] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [34] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [35] include(::String) at ./client.jl:457
 [36] top-level scope at none:6
 [37] eval(::Module, ::Any) at ./boot.jl:331
 [38] exec_options(::Base.JLOptions) at ./client.jl:272
 [39] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:118
  Test threw exception
  Expression: gradcheck(pool, kx)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:118
   [6] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [8] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] pool(::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:134
   [14] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [15] forw at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:65 [inlined]
   [16] #pool#406 at ./none:0 [inlined]
   [17] pool(::Param{KnetArray{Float64,4}}) at ./none:0
   [18] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [19] gcsum(::Function, ::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [20] (::AutoGrad.var"#203#205"{Tuple{},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [21] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [22] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [23] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [24] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:118
   [26] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [27] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [28] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [29] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:119
  Test threw exception
  Expression: isapprox(unpool(kx), unpool(ax))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] unpool(::KnetArray{Float64,4}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:238
   [14] unpool(::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:236
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:119
   [16] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [18] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] unpool(::KnetArray{Float64,4}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:238
 [14] unpool(::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:236
 [15] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [16] forw at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:65 [inlined]
 [17] #unpool#440 at ./none:0 [inlined]
 [18] unpool(::Param{KnetArray{Float64,4}}) at ./none:0
 [19] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [20] gcsum(::Function, ::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [21] (::AutoGrad.var"#203#205"{Tuple{},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [22] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [23] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [24] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [25] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
 [26] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:120
 [27] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [28] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
 [29] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [31] include(::String) at ./client.jl:457
 [32] macro expansion at ./timing.jl:174 [inlined]
 [33] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [34] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [35] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [36] include(::String) at ./client.jl:457
 [37] top-level scope at none:6
 [38] eval(::Module, ::Any) at ./boot.jl:331
 [39] exec_options(::Base.JLOptions) at ./client.jl:272
 [40] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:120
  Test threw exception
  Expression: gradcheck(unpool, kx)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:120
   [6] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [8] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] unpool(::KnetArray{Float64,4}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:238
   [14] unpool(::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:236
   [15] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [16] forw at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:65 [inlined]
   [17] #unpool#440 at ./none:0 [inlined]
   [18] unpool(::Param{KnetArray{Float64,4}}) at ./none:0
   [19] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [20] gcsum(::Function, ::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [21] (::AutoGrad.var"#203#205"{Tuple{},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [22] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [23] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [24] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [25] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [26] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:120
   [27] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [29] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:121
  Test threw exception
  Expression: isapprox(conv4(kw, kx), conv4(aw, ax))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] conv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:38
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:121
   [15] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [17] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] conv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:38
 [14] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [15] forw at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:65 [inlined]
 [16] #conv4#344 at ./none:0 [inlined]
 [17] conv4(::AutoGrad.Result{KnetArray{Float64,4}}, ::AutoGrad.Result{KnetArray{Float64,4}}) at ./none:0
 [18] (::var"#conv41#185"{var"#conv41#183#186"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [19] (::var"#conv41#185"{var"#conv41#183#186"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [20] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [21] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [22] (::AutoGrad.var"#203#205"{Tuple{},var"#conv41#185"{var"#conv41#183#186"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [23] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [24] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [25] gradcheck(::var"#conv41#185"{var"#conv41#183#186"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [26] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:122
 [27] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [28] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
 [29] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [31] include(::String) at ./client.jl:457
 [32] macro expansion at ./timing.jl:174 [inlined]
 [33] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [34] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [35] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [36] include(::String) at ./client.jl:457
 [37] top-level scope at none:6
 [38] eval(::Module, ::Any) at ./boot.jl:331
 [39] exec_options(::Base.JLOptions) at ./client.jl:272
 [40] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:122
  Test threw exception
  Expression: gradcheck(conv41, (kw, kx); rtol = TOL)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#conv41#185"{var"#conv41#183#186"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:122
   [5] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [7] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] conv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:38
   [14] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [15] forw at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:65 [inlined]
   [16] #conv4#344 at ./none:0 [inlined]
   [17] conv4(::AutoGrad.Result{KnetArray{Float64,4}}, ::AutoGrad.Result{KnetArray{Float64,4}}) at ./none:0
   [18] (::var"#conv41#185"{var"#conv41#183#186"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
   [19] (::var"#conv41#185"{var"#conv41#183#186"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
   [20] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [21] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [22] (::AutoGrad.var"#203#205"{Tuple{},var"#conv41#185"{var"#conv41#183#186"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [23] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [24] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [25] gradcheck(::var"#conv41#185"{var"#conv41#183#186"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [26] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:122
   [27] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [29] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:123
  Test threw exception
  Expression: isapprox(deconv4(kd, kx), deconv4(ad, ax))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] conv4x at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:51 [inlined]
   [14] deconv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:283
   [15] deconv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:282
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:123
   [17] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [19] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] conv4x at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:51 [inlined]
 [14] deconv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:283
 [15] deconv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:282
 [16] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [17] forw at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:65 [inlined]
 [18] #deconv4#447 at ./none:0 [inlined]
 [19] deconv4(::AutoGrad.Result{KnetArray{Float64,4}}, ::AutoGrad.Result{KnetArray{Float64,4}}) at ./none:0
 [20] (::var"#deconv41#187"{var"#deconv41#184#188"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:8
 [21] (::var"#deconv41#187"{var"#deconv41#184#188"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:8
 [22] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [23] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [24] (::AutoGrad.var"#203#205"{Tuple{},var"#deconv41#187"{var"#deconv41#184#188"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [25] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [26] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [27] gradcheck(::var"#deconv41#187"{var"#deconv41#184#188"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [28] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:124
 [29] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
 [31] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [33] include(::String) at ./client.jl:457
 [34] macro expansion at ./timing.jl:174 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [36] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [38] include(::String) at ./client.jl:457
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:272
 [42] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:124
  Test threw exception
  Expression: gradcheck(deconv41, (kd, kx); rtol = TOL)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#deconv41#187"{var"#deconv41#184#188"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:124
   [5] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [7] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] conv4x at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:51 [inlined]
   [14] deconv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:283
   [15] deconv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:282
   [16] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [17] forw at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:65 [inlined]
   [18] #deconv4#447 at ./none:0 [inlined]
   [19] deconv4(::AutoGrad.Result{KnetArray{Float64,4}}, ::AutoGrad.Result{KnetArray{Float64,4}}) at ./none:0
   [20] (::var"#deconv41#187"{var"#deconv41#184#188"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:8
   [21] (::var"#deconv41#187"{var"#deconv41#184#188"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:8
   [22] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [23] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [24] (::AutoGrad.var"#203#205"{Tuple{},var"#deconv41#187"{var"#deconv41#184#188"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [25] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [26] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [27] gradcheck(::var"#deconv41#187"{var"#deconv41#184#188"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:124
   [29] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [31] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:127
  Test threw exception
  Expression: isapprox(pool(kx32), pool(ax32))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] pool(::KnetArray{Float32,4}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:134
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:127
   [15] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [17] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] pool(::KnetArray{Float32,4}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:134
 [14] forw(::Function, ::Param{KnetArray{Float32,4}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [15] forw at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:65 [inlined]
 [16] #pool#406 at ./none:0 [inlined]
 [17] pool(::Param{KnetArray{Float32,4}}) at ./none:0
 [18] gcsum(::Function, ::Param{KnetArray{Float32,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [19] gcsum(::Function, ::Param{KnetArray{Float32,4}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [20] (::AutoGrad.var"#203#205"{Tuple{},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [21] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [22] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [23] gradcheck(::typeof(pool), ::KnetArray{Float32,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [24] gradcheck(::Function, ::KnetArray{Float32,4}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:128
 [26] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [27] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
 [28] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [30] include(::String) at ./client.jl:457
 [31] macro expansion at ./timing.jl:174 [inlined]
 [32] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [33] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [34] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [35] include(::String) at ./client.jl:457
 [36] top-level scope at none:6
 [37] eval(::Module, ::Any) at ./boot.jl:331
 [38] exec_options(::Base.JLOptions) at ./client.jl:272
 [39] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:128
  Test threw exception
  Expression: gradcheck(pool, kx32)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(pool), ::KnetArray{Float32,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float32,4}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:128
   [6] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [8] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] pool(::KnetArray{Float32,4}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:134
   [14] forw(::Function, ::Param{KnetArray{Float32,4}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [15] forw at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:65 [inlined]
   [16] #pool#406 at ./none:0 [inlined]
   [17] pool(::Param{KnetArray{Float32,4}}) at ./none:0
   [18] gcsum(::Function, ::Param{KnetArray{Float32,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [19] gcsum(::Function, ::Param{KnetArray{Float32,4}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [20] (::AutoGrad.var"#203#205"{Tuple{},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [21] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [22] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [23] gradcheck(::typeof(pool), ::KnetArray{Float32,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [24] gradcheck(::Function, ::KnetArray{Float32,4}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:128
   [26] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [27] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [28] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [29] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:129
  Test threw exception
  Expression: isapprox(unpool(kx32), unpool(ax32))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] unpool(::KnetArray{Float32,4}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:238
   [14] unpool(::KnetArray{Float32,4}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:236
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:129
   [16] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [18] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] unpool(::KnetArray{Float32,4}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:238
 [14] unpool(::KnetArray{Float32,4}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:236
 [15] forw(::Function, ::Param{KnetArray{Float32,4}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [16] forw at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:65 [inlined]
 [17] #unpool#440 at ./none:0 [inlined]
 [18] unpool(::Param{KnetArray{Float32,4}}) at ./none:0
 [19] gcsum(::Function, ::Param{KnetArray{Float32,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [20] gcsum(::Function, ::Param{KnetArray{Float32,4}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [21] (::AutoGrad.var"#203#205"{Tuple{},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [22] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [23] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [24] gradcheck(::typeof(unpool), ::KnetArray{Float32,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [25] gradcheck(::Function, ::KnetArray{Float32,4}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
 [26] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:130
 [27] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [28] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
 [29] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [31] include(::String) at ./client.jl:457
 [32] macro expansion at ./timing.jl:174 [inlined]
 [33] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [34] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [35] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [36] include(::String) at ./client.jl:457
 [37] top-level scope at none:6
 [38] eval(::Module, ::Any) at ./boot.jl:331
 [39] exec_options(::Base.JLOptions) at ./client.jl:272
 [40] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:130
  Test threw exception
  Expression: gradcheck(unpool, kx32)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(unpool), ::KnetArray{Float32,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float32,4}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:130
   [6] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [8] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] unpool(::KnetArray{Float32,4}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:238
   [14] unpool(::KnetArray{Float32,4}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:236
   [15] forw(::Function, ::Param{KnetArray{Float32,4}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [16] forw at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:65 [inlined]
   [17] #unpool#440 at ./none:0 [inlined]
   [18] unpool(::Param{KnetArray{Float32,4}}) at ./none:0
   [19] gcsum(::Function, ::Param{KnetArray{Float32,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [20] gcsum(::Function, ::Param{KnetArray{Float32,4}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [21] (::AutoGrad.var"#203#205"{Tuple{},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [22] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [23] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [24] gradcheck(::typeof(unpool), ::KnetArray{Float32,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [25] gradcheck(::Function, ::KnetArray{Float32,4}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [26] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:130
   [27] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [29] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:131
  Test threw exception
  Expression: isapprox(conv4(kw32, kx32), conv4(aw32, ax32))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] conv4(::KnetArray{Float32,4}, ::KnetArray{Float32,4}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:38
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:131
   [15] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [17] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] conv4(::KnetArray{Float32,4}, ::KnetArray{Float32,4}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:38
 [14] forw(::Function, ::AutoGrad.Result{KnetArray{Float32,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float32,4}},N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [15] forw at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:65 [inlined]
 [16] #conv4#344 at ./none:0 [inlined]
 [17] conv4(::AutoGrad.Result{KnetArray{Float32,4}}, ::AutoGrad.Result{KnetArray{Float32,4}}) at ./none:0
 [18] (::var"#conv41#185"{var"#conv41#183#186"})(::Param{Tuple{KnetArray{Float32,4},KnetArray{Float32,4}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [19] (::var"#conv41#185"{var"#conv41#183#186"})(::Param{Tuple{KnetArray{Float32,4},KnetArray{Float32,4}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [20] gcsum(::Function, ::Param{Tuple{KnetArray{Float32,4},KnetArray{Float32,4}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [21] gcsum(::Function, ::Param{Tuple{KnetArray{Float32,4},KnetArray{Float32,4}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [22] (::AutoGrad.var"#203#205"{Tuple{},var"#conv41#185"{var"#conv41#183#186"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [23] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [24] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [25] gradcheck(::var"#conv41#185"{var"#conv41#183#186"}, ::Tuple{KnetArray{Float32,4},KnetArray{Float32,4}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [26] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:132
 [27] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [28] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
 [29] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [31] include(::String) at ./client.jl:457
 [32] macro expansion at ./timing.jl:174 [inlined]
 [33] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [34] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [35] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [36] include(::String) at ./client.jl:457
 [37] top-level scope at none:6
 [38] eval(::Module, ::Any) at ./boot.jl:331
 [39] exec_options(::Base.JLOptions) at ./client.jl:272
 [40] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:132
  Test threw exception
  Expression: gradcheck(conv41, (kw32, kx32); rtol = TOL)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#conv41#185"{var"#conv41#183#186"}, ::Tuple{KnetArray{Float32,4},KnetArray{Float32,4}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:132
   [5] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [7] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] conv4(::KnetArray{Float32,4}, ::KnetArray{Float32,4}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:38
   [14] forw(::Function, ::AutoGrad.Result{KnetArray{Float32,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float32,4}},N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [15] forw at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:65 [inlined]
   [16] #conv4#344 at ./none:0 [inlined]
   [17] conv4(::AutoGrad.Result{KnetArray{Float32,4}}, ::AutoGrad.Result{KnetArray{Float32,4}}) at ./none:0
   [18] (::var"#conv41#185"{var"#conv41#183#186"})(::Param{Tuple{KnetArray{Float32,4},KnetArray{Float32,4}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
   [19] (::var"#conv41#185"{var"#conv41#183#186"})(::Param{Tuple{KnetArray{Float32,4},KnetArray{Float32,4}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
   [20] gcsum(::Function, ::Param{Tuple{KnetArray{Float32,4},KnetArray{Float32,4}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [21] gcsum(::Function, ::Param{Tuple{KnetArray{Float32,4},KnetArray{Float32,4}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [22] (::AutoGrad.var"#203#205"{Tuple{},var"#conv41#185"{var"#conv41#183#186"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [23] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [24] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [25] gradcheck(::var"#conv41#185"{var"#conv41#183#186"}, ::Tuple{KnetArray{Float32,4},KnetArray{Float32,4}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [26] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:132
   [27] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [29] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:133
  Test threw exception
  Expression: isapprox(deconv4(kd32, kx32), deconv4(ad32, ax32))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] conv4x at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:51 [inlined]
   [14] deconv4(::KnetArray{Float32,4}, ::KnetArray{Float32,4}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:283
   [15] deconv4(::KnetArray{Float32,4}, ::KnetArray{Float32,4}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:282
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:133
   [17] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [19] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] conv4x at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:51 [inlined]
 [14] deconv4(::KnetArray{Float32,4}, ::KnetArray{Float32,4}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:283
 [15] deconv4(::KnetArray{Float32,4}, ::KnetArray{Float32,4}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:282
 [16] forw(::Function, ::AutoGrad.Result{KnetArray{Float32,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float32,4}},N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [17] forw at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:65 [inlined]
 [18] #deconv4#447 at ./none:0 [inlined]
 [19] deconv4(::AutoGrad.Result{KnetArray{Float32,4}}, ::AutoGrad.Result{KnetArray{Float32,4}}) at ./none:0
 [20] (::var"#deconv41#187"{var"#deconv41#184#188"})(::Param{Tuple{KnetArray{Float32,4},KnetArray{Float32,4}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:8
 [21] (::var"#deconv41#187"{var"#deconv41#184#188"})(::Param{Tuple{KnetArray{Float32,4},KnetArray{Float32,4}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:8
 [22] gcsum(::Function, ::Param{Tuple{KnetArray{Float32,4},KnetArray{Float32,4}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [23] gcsum(::Function, ::Param{Tuple{KnetArray{Float32,4},KnetArray{Float32,4}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [24] (::AutoGrad.var"#203#205"{Tuple{},var"#deconv41#187"{var"#deconv41#184#188"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [25] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [26] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [27] gradcheck(::var"#deconv41#187"{var"#deconv41#184#188"}, ::Tuple{KnetArray{Float32,4},KnetArray{Float32,4}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [28] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:134
 [29] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
 [31] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [33] include(::String) at ./client.jl:457
 [34] macro expansion at ./timing.jl:174 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [36] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [38] include(::String) at ./client.jl:457
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:272
 [42] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:134
  Test threw exception
  Expression: gradcheck(deconv41, (kd32, kx32); rtol = TOL)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#deconv41#187"{var"#deconv41#184#188"}, ::Tuple{KnetArray{Float32,4},KnetArray{Float32,4}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:134
   [5] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [7] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] conv4x at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:51 [inlined]
   [14] deconv4(::KnetArray{Float32,4}, ::KnetArray{Float32,4}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:283
   [15] deconv4(::KnetArray{Float32,4}, ::KnetArray{Float32,4}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:282
   [16] forw(::Function, ::AutoGrad.Result{KnetArray{Float32,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float32,4}},N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [17] forw at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:65 [inlined]
   [18] #deconv4#447 at ./none:0 [inlined]
   [19] deconv4(::AutoGrad.Result{KnetArray{Float32,4}}, ::AutoGrad.Result{KnetArray{Float32,4}}) at ./none:0
   [20] (::var"#deconv41#187"{var"#deconv41#184#188"})(::Param{Tuple{KnetArray{Float32,4},KnetArray{Float32,4}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:8
   [21] (::var"#deconv41#187"{var"#deconv41#184#188"})(::Param{Tuple{KnetArray{Float32,4},KnetArray{Float32,4}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:8
   [22] gcsum(::Function, ::Param{Tuple{KnetArray{Float32,4},KnetArray{Float32,4}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [23] gcsum(::Function, ::Param{Tuple{KnetArray{Float32,4},KnetArray{Float32,4}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [24] (::AutoGrad.var"#203#205"{Tuple{},var"#deconv41#187"{var"#deconv41#184#188"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [25] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [26] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [27] gradcheck(::var"#deconv41#187"{var"#deconv41#184#188"}, ::Tuple{KnetArray{Float32,4},KnetArray{Float32,4}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:134
   [29] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [31] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:137
  Test threw exception
  Expression: isapprox(pool(kx5), pool(ax5))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] pool(::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:134
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:137
   [15] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [17] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] pool(::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:134
 [14] forw(::Function, ::Param{KnetArray{Float64,5}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [15] forw at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:65 [inlined]
 [16] #pool#406 at ./none:0 [inlined]
 [17] pool(::Param{KnetArray{Float64,5}}) at ./none:0
 [18] gcsum(::Function, ::Param{KnetArray{Float64,5}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [19] gcsum(::Function, ::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [20] (::AutoGrad.var"#203#205"{Tuple{},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [21] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [22] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [23] gradcheck(::typeof(pool), ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [24] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:138
 [26] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [27] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
 [28] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [30] include(::String) at ./client.jl:457
 [31] macro expansion at ./timing.jl:174 [inlined]
 [32] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [33] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [34] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [35] include(::String) at ./client.jl:457
 [36] top-level scope at none:6
 [37] eval(::Module, ::Any) at ./boot.jl:331
 [38] exec_options(::Base.JLOptions) at ./client.jl:272
 [39] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:138
  Test threw exception
  Expression: gradcheck(pool, kx5)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(pool), ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:138
   [6] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [8] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] pool(::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:134
   [14] forw(::Function, ::Param{KnetArray{Float64,5}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [15] forw at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:65 [inlined]
   [16] #pool#406 at ./none:0 [inlined]
   [17] pool(::Param{KnetArray{Float64,5}}) at ./none:0
   [18] gcsum(::Function, ::Param{KnetArray{Float64,5}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [19] gcsum(::Function, ::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [20] (::AutoGrad.var"#203#205"{Tuple{},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [21] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [22] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [23] gradcheck(::typeof(pool), ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [24] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:138
   [26] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [27] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [28] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [29] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:139
  Test threw exception
  Expression: isapprox(unpool(kx5), unpool(ax5))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] unpool(::KnetArray{Float64,5}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:238
   [14] unpool(::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:236
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:139
   [16] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [18] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] unpool(::KnetArray{Float64,5}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:238
 [14] unpool(::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:236
 [15] forw(::Function, ::Param{KnetArray{Float64,5}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [16] forw at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:65 [inlined]
 [17] #unpool#440 at ./none:0 [inlined]
 [18] unpool(::Param{KnetArray{Float64,5}}) at ./none:0
 [19] gcsum(::Function, ::Param{KnetArray{Float64,5}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [20] gcsum(::Function, ::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [21] (::AutoGrad.var"#203#205"{Tuple{},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [22] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [23] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [24] gradcheck(::typeof(unpool), ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [25] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
 [26] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:140
 [27] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [28] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
 [29] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [31] include(::String) at ./client.jl:457
 [32] macro expansion at ./timing.jl:174 [inlined]
 [33] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [34] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [35] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [36] include(::String) at ./client.jl:457
 [37] top-level scope at none:6
 [38] eval(::Module, ::Any) at ./boot.jl:331
 [39] exec_options(::Base.JLOptions) at ./client.jl:272
 [40] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:140
  Test threw exception
  Expression: gradcheck(unpool, kx5)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(unpool), ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:140
   [6] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [8] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] unpool(::KnetArray{Float64,5}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:238
   [14] unpool(::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:236
   [15] forw(::Function, ::Param{KnetArray{Float64,5}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [16] forw at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:65 [inlined]
   [17] #unpool#440 at ./none:0 [inlined]
   [18] unpool(::Param{KnetArray{Float64,5}}) at ./none:0
   [19] gcsum(::Function, ::Param{KnetArray{Float64,5}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [20] gcsum(::Function, ::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [21] (::AutoGrad.var"#203#205"{Tuple{},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [22] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [23] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [24] gradcheck(::typeof(unpool), ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [25] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [26] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:140
   [27] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [29] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:141
  Test threw exception
  Expression: isapprox(conv4(kw5, kx5), conv4(aw5, ax5))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] conv4(::KnetArray{Float64,5}, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:38
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:141
   [15] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [17] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] conv4(::KnetArray{Float64,5}, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:38
 [14] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,5}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,5}},N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [15] forw at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:65 [inlined]
 [16] #conv4#344 at ./none:0 [inlined]
 [17] conv4(::AutoGrad.Result{KnetArray{Float64,5}}, ::AutoGrad.Result{KnetArray{Float64,5}}) at ./none:0
 [18] (::var"#conv41#185"{var"#conv41#183#186"})(::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,5}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [19] (::var"#conv41#185"{var"#conv41#183#186"})(::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,5}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [20] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,5}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [21] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,5}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [22] (::AutoGrad.var"#203#205"{Tuple{},var"#conv41#185"{var"#conv41#183#186"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [23] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [24] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [25] gradcheck(::var"#conv41#185"{var"#conv41#183#186"}, ::Tuple{KnetArray{Float64,5},KnetArray{Float64,5}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [26] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:142
 [27] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [28] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
 [29] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [31] include(::String) at ./client.jl:457
 [32] macro expansion at ./timing.jl:174 [inlined]
 [33] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [34] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [35] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [36] include(::String) at ./client.jl:457
 [37] top-level scope at none:6
 [38] eval(::Module, ::Any) at ./boot.jl:331
 [39] exec_options(::Base.JLOptions) at ./client.jl:272
 [40] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:142
  Test threw exception
  Expression: gradcheck(conv41, (kw5, kx5); rtol = TOL)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#conv41#185"{var"#conv41#183#186"}, ::Tuple{KnetArray{Float64,5},KnetArray{Float64,5}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:142
   [5] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [7] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] conv4(::KnetArray{Float64,5}, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:38
   [14] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,5}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,5}},N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [15] forw at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:65 [inlined]
   [16] #conv4#344 at ./none:0 [inlined]
   [17] conv4(::AutoGrad.Result{KnetArray{Float64,5}}, ::AutoGrad.Result{KnetArray{Float64,5}}) at ./none:0
   [18] (::var"#conv41#185"{var"#conv41#183#186"})(::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,5}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
   [19] (::var"#conv41#185"{var"#conv41#183#186"})(::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,5}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
   [20] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,5}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [21] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,5}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [22] (::AutoGrad.var"#203#205"{Tuple{},var"#conv41#185"{var"#conv41#183#186"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [23] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [24] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [25] gradcheck(::var"#conv41#185"{var"#conv41#183#186"}, ::Tuple{KnetArray{Float64,5},KnetArray{Float64,5}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [26] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:142
   [27] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [29] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:143
  Test threw exception
  Expression: isapprox(deconv4(kw5, kx5), deconv4(aw5, ax5))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] conv4x at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:51 [inlined]
   [14] deconv4(::KnetArray{Float64,5}, ::KnetArray{Float64,5}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:283
   [15] deconv4(::KnetArray{Float64,5}, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:282
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:143
   [17] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [19] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] conv4x at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:51 [inlined]
 [14] deconv4(::KnetArray{Float64,5}, ::KnetArray{Float64,5}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:283
 [15] deconv4(::KnetArray{Float64,5}, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:282
 [16] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,5}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,5}},N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [17] forw at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:65 [inlined]
 [18] #deconv4#447 at ./none:0 [inlined]
 [19] deconv4(::AutoGrad.Result{KnetArray{Float64,5}}, ::AutoGrad.Result{KnetArray{Float64,5}}) at ./none:0
 [20] (::var"#deconv41#187"{var"#deconv41#184#188"})(::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,5}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:8
 [21] (::var"#deconv41#187"{var"#deconv41#184#188"})(::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,5}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:8
 [22] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,5}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [23] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,5}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [24] (::AutoGrad.var"#203#205"{Tuple{},var"#deconv41#187"{var"#deconv41#184#188"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [25] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [26] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [27] gradcheck(::var"#deconv41#187"{var"#deconv41#184#188"}, ::Tuple{KnetArray{Float64,5},KnetArray{Float64,5}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [28] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:144
 [29] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
 [31] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [33] include(::String) at ./client.jl:457
 [34] macro expansion at ./timing.jl:174 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [36] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [38] include(::String) at ./client.jl:457
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:272
 [42] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:144
  Test threw exception
  Expression: gradcheck(deconv41, (kw5, kx5); rtol = TOL)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#deconv41#187"{var"#deconv41#184#188"}, ::Tuple{KnetArray{Float64,5},KnetArray{Float64,5}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:144
   [5] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [7] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] conv4x at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:51 [inlined]
   [14] deconv4(::KnetArray{Float64,5}, ::KnetArray{Float64,5}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:283
   [15] deconv4(::KnetArray{Float64,5}, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:282
   [16] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,5}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,5}},N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [17] forw at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:65 [inlined]
   [18] #deconv4#447 at ./none:0 [inlined]
   [19] deconv4(::AutoGrad.Result{KnetArray{Float64,5}}, ::AutoGrad.Result{KnetArray{Float64,5}}) at ./none:0
   [20] (::var"#deconv41#187"{var"#deconv41#184#188"})(::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,5}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:8
   [21] (::var"#deconv41#187"{var"#deconv41#184#188"})(::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,5}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:8
   [22] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,5}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [23] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,5}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [24] (::AutoGrad.var"#203#205"{Tuple{},var"#deconv41#187"{var"#deconv41#184#188"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [25] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [26] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [27] gradcheck(::var"#deconv41#187"{var"#deconv41#184#188"}, ::Tuple{KnetArray{Float64,5},KnetArray{Float64,5}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:144
   [29] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [31] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:147
  Test threw exception
  Expression: isapprox(pool(kx; window = 3), pool(ax; window = 3))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:147
   [14] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [16] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:window,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] pool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:window,),Tuple{Int64}}}) at ./none:0
 [15] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:window,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [16] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [19] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [20] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:148
 [21] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
 [23] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [25] include(::String) at ./client.jl:457
 [26] macro expansion at ./timing.jl:174 [inlined]
 [27] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [28] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [30] include(::String) at ./client.jl:457
 [31] top-level scope at none:6
 [32] eval(::Module, ::Any) at ./boot.jl:331
 [33] exec_options(::Base.JLOptions) at ./client.jl:272
 [34] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:148
  Test threw exception
  Expression: gradcheck(pool, kx; kw = [(:window, 3)])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:148
   [5] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [7] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:window,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] pool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:window,),Tuple{Int64}}}) at ./none:0
   [15] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:window,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [16] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [19] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:148
   [21] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [23] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:149
  Test threw exception
  Expression: isapprox(unpool(kx; window = 3), unpool(ax; window = 3))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] unpool(::KnetArray{Float64,4}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:238
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:149
   [15] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [17] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] unpool(::KnetArray{Float64,4}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:238
 [14] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:window,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [15] unpool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:window,),Tuple{Int64}}}) at ./none:0
 [16] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:window,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [20] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:150
 [22] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
 [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [26] include(::String) at ./client.jl:457
 [27] macro expansion at ./timing.jl:174 [inlined]
 [28] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [29] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [31] include(::String) at ./client.jl:457
 [32] top-level scope at none:6
 [33] eval(::Module, ::Any) at ./boot.jl:331
 [34] exec_options(::Base.JLOptions) at ./client.jl:272
 [35] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:150
  Test threw exception
  Expression: gradcheck(unpool, kx; kw = [(:window, 3)])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:150
   [5] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [7] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] unpool(::KnetArray{Float64,4}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:238
   [14] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:window,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [15] unpool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:window,),Tuple{Int64}}}) at ./none:0
   [16] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:window,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [20] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:150
   [22] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:151
  Test threw exception
  Expression: isapprox(pool(kx; window = (3, 3)), pool(ax; window = (3, 3)))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:151
   [14] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [16] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:window,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] pool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:window,),Tuple{Tuple{Int64,Int64}}}}) at ./none:0
 [15] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:window,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [16] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Tuple{Int64,Int64}},1},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [19] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [20] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:152
 [21] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
 [23] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [25] include(::String) at ./client.jl:457
 [26] macro expansion at ./timing.jl:174 [inlined]
 [27] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [28] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [30] include(::String) at ./client.jl:457
 [31] top-level scope at none:6
 [32] eval(::Module, ::Any) at ./boot.jl:331
 [33] exec_options(::Base.JLOptions) at ./client.jl:272
 [34] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:152
  Test threw exception
  Expression: gradcheck(pool, kx; kw = [(:window, (3, 3))])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:152
   [5] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [7] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:window,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] pool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:window,),Tuple{Tuple{Int64,Int64}}}}) at ./none:0
   [15] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:window,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [16] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Tuple{Int64,Int64}},1},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [19] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:152
   [21] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [23] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:153
  Test threw exception
  Expression: isapprox(unpool(kx; window = (3, 3)), unpool(ax; window = (3, 3)))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] unpool(::KnetArray{Float64,4}; window::Tuple{Int64,Int64}, alpha::Int64, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:238
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:153
   [15] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [17] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] unpool(::KnetArray{Float64,4}; window::Tuple{Int64,Int64}, alpha::Int64, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:238
 [14] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:window,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [15] unpool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:window,),Tuple{Tuple{Int64,Int64}}}}) at ./none:0
 [16] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:window,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Tuple{Int64,Int64}},1},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [20] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:154
 [22] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
 [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [26] include(::String) at ./client.jl:457
 [27] macro expansion at ./timing.jl:174 [inlined]
 [28] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [29] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [31] include(::String) at ./client.jl:457
 [32] top-level scope at none:6
 [33] eval(::Module, ::Any) at ./boot.jl:331
 [34] exec_options(::Base.JLOptions) at ./client.jl:272
 [35] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:154
  Test threw exception
  Expression: gradcheck(unpool, kx; kw = [(:window, (3, 3))])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:154
   [5] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [7] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] unpool(::KnetArray{Float64,4}; window::Tuple{Int64,Int64}, alpha::Int64, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:238
   [14] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:window,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [15] unpool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:window,),Tuple{Tuple{Int64,Int64}}}}) at ./none:0
   [16] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:window,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Tuple{Int64,Int64}},1},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [20] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:154
   [22] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:157
  Test threw exception
  Expression: isapprox(pool(kx; padding = 1), pool(ax; padding = 1))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:157
   [14] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [16] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] pool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at ./none:0
 [15] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [16] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [19] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [20] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:158
 [21] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
 [23] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [25] include(::String) at ./client.jl:457
 [26] macro expansion at ./timing.jl:174 [inlined]
 [27] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [28] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [30] include(::String) at ./client.jl:457
 [31] top-level scope at none:6
 [32] eval(::Module, ::Any) at ./boot.jl:331
 [33] exec_options(::Base.JLOptions) at ./client.jl:272
 [34] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:158
  Test threw exception
  Expression: gradcheck(pool, kx; kw = [(:padding, 1)])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:158
   [5] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [7] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] pool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at ./none:0
   [15] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [16] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [19] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:158
   [21] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [23] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:159
  Test threw exception
  Expression: isapprox(unpool(kx; padding = 1), unpool(ax; padding = 1))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] unpool(::KnetArray{Float64,4}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:238
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:159
   [15] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [17] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] unpool(::KnetArray{Float64,4}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:238
 [14] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [15] unpool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at ./none:0
 [16] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [20] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:160
 [22] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
 [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [26] include(::String) at ./client.jl:457
 [27] macro expansion at ./timing.jl:174 [inlined]
 [28] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [29] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [31] include(::String) at ./client.jl:457
 [32] top-level scope at none:6
 [33] eval(::Module, ::Any) at ./boot.jl:331
 [34] exec_options(::Base.JLOptions) at ./client.jl:272
 [35] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:160
  Test threw exception
  Expression: gradcheck(unpool, kx; kw = [(:padding, 1)])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:160
   [5] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [7] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] unpool(::KnetArray{Float64,4}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:238
   [14] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [15] unpool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at ./none:0
   [16] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [20] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:160
   [22] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:161
  Test threw exception
  Expression: isapprox(conv4(kw, kx; padding = 1), conv4(aw, ax; padding = 1))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:161
   [14] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [16] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] #conv4#344 at ./none:0 [inlined]
 [15] (::var"#conv41#185"{var"#conv41#183#186"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [16] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},var"#conv41#185"{var"#conv41#183#186"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [20] gradcheck(::var"#conv41#185"{var"#conv41#183#186"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:162
 [22] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
 [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [26] include(::String) at ./client.jl:457
 [27] macro expansion at ./timing.jl:174 [inlined]
 [28] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [29] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [31] include(::String) at ./client.jl:457
 [32] top-level scope at none:6
 [33] eval(::Module, ::Any) at ./boot.jl:331
 [34] exec_options(::Base.JLOptions) at ./client.jl:272
 [35] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:162
  Test threw exception
  Expression: gradcheck(conv41, (kw, kx); rtol = TOL, kw = [(:padding, 1)])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#conv41#185"{var"#conv41#183#186"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:162
   [5] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [7] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] #conv4#344 at ./none:0 [inlined]
   [15] (::var"#conv41#185"{var"#conv41#183#186"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
   [16] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},var"#conv41#185"{var"#conv41#183#186"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [20] gradcheck(::var"#conv41#185"{var"#conv41#183#186"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:162
   [22] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:163
  Test threw exception
  Expression: isapprox(deconv4(kd, kx; padding = 1), deconv4(ad, ax; padding = 1))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] deconv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:283
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:163
   [15] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [17] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] deconv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:283
 [14] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [15] deconv4(::AutoGrad.Result{KnetArray{Float64,4}}, ::AutoGrad.Result{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at ./none:0
 [16] (::var"#deconv41#187"{var"#deconv41#184#188"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:8
 [17] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [18] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},var"#deconv41#187"{var"#deconv41#184#188"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [21] gradcheck(::var"#deconv41#187"{var"#deconv41#184#188"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:164
 [23] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
 [25] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [26] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [27] include(::String) at ./client.jl:457
 [28] macro expansion at ./timing.jl:174 [inlined]
 [29] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [30] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [31] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [32] include(::String) at ./client.jl:457
 [33] top-level scope at none:6
 [34] eval(::Module, ::Any) at ./boot.jl:331
 [35] exec_options(::Base.JLOptions) at ./client.jl:272
 [36] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:164
  Test threw exception
  Expression: gradcheck(deconv41, (kd, kx); rtol = TOL, kw = [(:padding, 1)])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#deconv41#187"{var"#deconv41#184#188"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:164
   [5] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [7] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] deconv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:283
   [14] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [15] deconv4(::AutoGrad.Result{KnetArray{Float64,4}}, ::AutoGrad.Result{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at ./none:0
   [16] (::var"#deconv41#187"{var"#deconv41#184#188"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:8
   [17] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [18] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},var"#deconv41#187"{var"#deconv41#184#188"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [21] gradcheck(::var"#deconv41#187"{var"#deconv41#184#188"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:164
   [23] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [25] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [26] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:166
  Test threw exception
  Expression: isapprox(pool(kx; padding = (1, 1)), pool(ax; padding = (1, 1)))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:166
   [14] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [16] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] pool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at ./none:0
 [15] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [16] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Tuple{Int64,Int64}},1},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [19] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [20] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:167
 [21] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
 [23] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [25] include(::String) at ./client.jl:457
 [26] macro expansion at ./timing.jl:174 [inlined]
 [27] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [28] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [30] include(::String) at ./client.jl:457
 [31] top-level scope at none:6
 [32] eval(::Module, ::Any) at ./boot.jl:331
 [33] exec_options(::Base.JLOptions) at ./client.jl:272
 [34] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:167
  Test threw exception
  Expression: gradcheck(pool, kx; kw = [(:padding, (1, 1))])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:167
   [5] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [7] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] pool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at ./none:0
   [15] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [16] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Tuple{Int64,Int64}},1},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [19] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:167
   [21] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [23] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:168
  Test threw exception
  Expression: isapprox(unpool(kx; padding = (1, 1)), unpool(ax; padding = (1, 1)))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] unpool(::KnetArray{Float64,4}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:238
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:168
   [15] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [17] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] unpool(::KnetArray{Float64,4}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:238
 [14] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [15] unpool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at ./none:0
 [16] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Tuple{Int64,Int64}},1},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [20] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:169
 [22] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
 [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [26] include(::String) at ./client.jl:457
 [27] macro expansion at ./timing.jl:174 [inlined]
 [28] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [29] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [31] include(::String) at ./client.jl:457
 [32] top-level scope at none:6
 [33] eval(::Module, ::Any) at ./boot.jl:331
 [34] exec_options(::Base.JLOptions) at ./client.jl:272
 [35] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:169
  Test threw exception
  Expression: gradcheck(unpool, kx; kw = [(:padding, (1, 1))])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:169
   [5] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [7] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] unpool(::KnetArray{Float64,4}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:238
   [14] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [15] unpool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at ./none:0
   [16] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Tuple{Int64,Int64}},1},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [20] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:169
   [22] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:170
  Test threw exception
  Expression: isapprox(conv4(kw, kx; padding = (1, 1)), conv4(aw, ax; padding = (1, 1)))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:170
   [14] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [16] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] #conv4#344 at ./none:0 [inlined]
 [15] (::var"#conv41#185"{var"#conv41#183#186"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [16] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Tuple{Int64,Int64}},1},var"#conv41#185"{var"#conv41#183#186"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [20] gradcheck(::var"#conv41#185"{var"#conv41#183#186"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:171
 [22] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
 [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [26] include(::String) at ./client.jl:457
 [27] macro expansion at ./timing.jl:174 [inlined]
 [28] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [29] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [31] include(::String) at ./client.jl:457
 [32] top-level scope at none:6
 [33] eval(::Module, ::Any) at ./boot.jl:331
 [34] exec_options(::Base.JLOptions) at ./client.jl:272
 [35] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:171
  Test threw exception
  Expression: gradcheck(conv41, (kw, kx); rtol = TOL, kw = [(:padding, (1, 1))])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#conv41#185"{var"#conv41#183#186"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:171
   [5] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [7] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] #conv4#344 at ./none:0 [inlined]
   [15] (::var"#conv41#185"{var"#conv41#183#186"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
   [16] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Tuple{Int64,Int64}},1},var"#conv41#185"{var"#conv41#183#186"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [20] gradcheck(::var"#conv41#185"{var"#conv41#183#186"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:171
   [22] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:172
  Test threw exception
  Expression: isapprox(deconv4(kd, kx; padding = (1, 1)), deconv4(ad, ax; padding = (1, 1)))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] deconv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:283
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:172
   [15] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [17] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] deconv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:283
 [14] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [15] deconv4(::AutoGrad.Result{KnetArray{Float64,4}}, ::AutoGrad.Result{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at ./none:0
 [16] (::var"#deconv41#187"{var"#deconv41#184#188"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:8
 [17] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [18] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Tuple{Int64,Int64}},1},var"#deconv41#187"{var"#deconv41#184#188"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [21] gradcheck(::var"#deconv41#187"{var"#deconv41#184#188"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:173
 [23] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
 [25] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [26] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [27] include(::String) at ./client.jl:457
 [28] macro expansion at ./timing.jl:174 [inlined]
 [29] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [30] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [31] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [32] include(::String) at ./client.jl:457
 [33] top-level scope at none:6
 [34] eval(::Module, ::Any) at ./boot.jl:331
 [35] exec_options(::Base.JLOptions) at ./client.jl:272
 [36] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:173
  Test threw exception
  Expression: gradcheck(deconv41, (kd, kx); rtol = TOL, kw = [(:padding, (1, 1))])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#deconv41#187"{var"#deconv41#184#188"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:173
   [5] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [7] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] deconv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:283
   [14] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [15] deconv4(::AutoGrad.Result{KnetArray{Float64,4}}, ::AutoGrad.Result{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at ./none:0
   [16] (::var"#deconv41#187"{var"#deconv41#184#188"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:8
   [17] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [18] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Tuple{Int64,Int64}},1},var"#deconv41#187"{var"#deconv41#184#188"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [21] gradcheck(::var"#deconv41#187"{var"#deconv41#184#188"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:173
   [23] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [25] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [26] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:176
  Test threw exception
  Expression: isapprox(pool(kx; stride = 3), pool(ax; stride = 3))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:176
   [14] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [16] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] pool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at ./none:0
 [15] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [16] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [19] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [20] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:177
 [21] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
 [23] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [25] include(::String) at ./client.jl:457
 [26] macro expansion at ./timing.jl:174 [inlined]
 [27] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [28] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [30] include(::String) at ./client.jl:457
 [31] top-level scope at none:6
 [32] eval(::Module, ::Any) at ./boot.jl:331
 [33] exec_options(::Base.JLOptions) at ./client.jl:272
 [34] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:177
  Test threw exception
  Expression: gradcheck(pool, kx; kw = [(:stride, 3)])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:177
   [5] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [7] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] pool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at ./none:0
   [15] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [16] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [19] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:177
   [21] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [23] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:178
  Test threw exception
  Expression: isapprox(unpool(kx; stride = 3), unpool(ax; stride = 3))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] unpool(::KnetArray{Float64,4}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:238
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:178
   [15] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [17] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] unpool(::KnetArray{Float64,4}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:238
 [14] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [15] unpool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at ./none:0
 [16] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [20] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:179
 [22] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
 [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [26] include(::String) at ./client.jl:457
 [27] macro expansion at ./timing.jl:174 [inlined]
 [28] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [29] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [31] include(::String) at ./client.jl:457
 [32] top-level scope at none:6
 [33] eval(::Module, ::Any) at ./boot.jl:331
 [34] exec_options(::Base.JLOptions) at ./client.jl:272
 [35] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:179
  Test threw exception
  Expression: gradcheck(unpool, kx; kw = [(:stride, 3)])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:179
   [5] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [7] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] unpool(::KnetArray{Float64,4}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:238
   [14] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [15] unpool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at ./none:0
   [16] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [20] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:179
   [22] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:180
  Test threw exception
  Expression: isapprox(conv4(kw, kx; stride = 3), conv4(aw, ax; stride = 3))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:180
   [14] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [16] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] #conv4#344 at ./none:0 [inlined]
 [15] (::var"#conv41#185"{var"#conv41#183#186"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [16] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},var"#conv41#185"{var"#conv41#183#186"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [20] gradcheck(::var"#conv41#185"{var"#conv41#183#186"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:181
 [22] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
 [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [26] include(::String) at ./client.jl:457
 [27] macro expansion at ./timing.jl:174 [inlined]
 [28] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [29] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [31] include(::String) at ./client.jl:457
 [32] top-level scope at none:6
 [33] eval(::Module, ::Any) at ./boot.jl:331
 [34] exec_options(::Base.JLOptions) at ./client.jl:272
 [35] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:181
  Test threw exception
  Expression: gradcheck(conv41, (kw, kx); rtol = TOL, kw = [(:stride, 3)])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#conv41#185"{var"#conv41#183#186"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:181
   [5] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [7] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] #conv4#344 at ./none:0 [inlined]
   [15] (::var"#conv41#185"{var"#conv41#183#186"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
   [16] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},var"#conv41#185"{var"#conv41#183#186"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [20] gradcheck(::var"#conv41#185"{var"#conv41#183#186"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:181
   [22] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:182
  Test threw exception
  Expression: isapprox(deconv4(kd, kx; stride = 3), deconv4(ad, ax; stride = 3); rtol = 1.0e-6)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] deconv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:283
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:182
   [15] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [17] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] deconv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:283
 [14] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [15] deconv4(::AutoGrad.Result{KnetArray{Float64,4}}, ::AutoGrad.Result{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at ./none:0
 [16] (::var"#deconv41#187"{var"#deconv41#184#188"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:8
 [17] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [18] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},var"#deconv41#187"{var"#deconv41#184#188"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [21] gradcheck(::var"#deconv41#187"{var"#deconv41#184#188"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:183
 [23] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
 [25] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [26] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [27] include(::String) at ./client.jl:457
 [28] macro expansion at ./timing.jl:174 [inlined]
 [29] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [30] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [31] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [32] include(::String) at ./client.jl:457
 [33] top-level scope at none:6
 [34] eval(::Module, ::Any) at ./boot.jl:331
 [35] exec_options(::Base.JLOptions) at ./client.jl:272
 [36] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:183
  Test threw exception
  Expression: gradcheck(deconv41, (kd, kx); rtol = TOL, kw = [(:stride, 3)])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#deconv41#187"{var"#deconv41#184#188"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:183
   [5] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [7] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] deconv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:283
   [14] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [15] deconv4(::AutoGrad.Result{KnetArray{Float64,4}}, ::AutoGrad.Result{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at ./none:0
   [16] (::var"#deconv41#187"{var"#deconv41#184#188"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:8
   [17] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [18] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},var"#deconv41#187"{var"#deconv41#184#188"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [21] gradcheck(::var"#deconv41#187"{var"#deconv41#184#188"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:183
   [23] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [25] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [26] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:185
  Test threw exception
  Expression: isapprox(pool(kx; stride = (3, 3)), pool(ax; stride = (3, 3)))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:185
   [14] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [16] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] pool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at ./none:0
 [15] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [16] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Tuple{Int64,Int64}},1},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [19] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [20] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:186
 [21] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
 [23] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [25] include(::String) at ./client.jl:457
 [26] macro expansion at ./timing.jl:174 [inlined]
 [27] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [28] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [30] include(::String) at ./client.jl:457
 [31] top-level scope at none:6
 [32] eval(::Module, ::Any) at ./boot.jl:331
 [33] exec_options(::Base.JLOptions) at ./client.jl:272
 [34] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:186
  Test threw exception
  Expression: gradcheck(pool, kx; kw = [(:stride, (3, 3))])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:186
   [5] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [7] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] pool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at ./none:0
   [15] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [16] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Tuple{Int64,Int64}},1},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [19] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:186
   [21] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [23] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:187
  Test threw exception
  Expression: isapprox(unpool(kx; stride = (3, 3)), unpool(ax; stride = (3, 3)))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] unpool(::KnetArray{Float64,4}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:238
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:187
   [15] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [17] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] unpool(::KnetArray{Float64,4}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:238
 [14] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [15] unpool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at ./none:0
 [16] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Tuple{Int64,Int64}},1},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [20] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:188
 [22] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
 [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [26] include(::String) at ./client.jl:457
 [27] macro expansion at ./timing.jl:174 [inlined]
 [28] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [29] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [31] include(::String) at ./client.jl:457
 [32] top-level scope at none:6
 [33] eval(::Module, ::Any) at ./boot.jl:331
 [34] exec_options(::Base.JLOptions) at ./client.jl:272
 [35] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:188
  Test threw exception
  Expression: gradcheck(unpool, kx; kw = [(:stride, (3, 3))])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:188
   [5] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [7] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] unpool(::KnetArray{Float64,4}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:238
   [14] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [15] unpool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at ./none:0
   [16] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Tuple{Int64,Int64}},1},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [20] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:188
   [22] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:189
  Test threw exception
  Expression: isapprox(conv4(kw, kx; stride = (3, 3)), conv4(aw, ax; stride = (3, 3)))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:189
   [14] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [16] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] #conv4#344 at ./none:0 [inlined]
 [15] (::var"#conv41#185"{var"#conv41#183#186"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [16] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Tuple{Int64,Int64}},1},var"#conv41#185"{var"#conv41#183#186"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [20] gradcheck(::var"#conv41#185"{var"#conv41#183#186"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:190
 [22] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
 [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [26] include(::String) at ./client.jl:457
 [27] macro expansion at ./timing.jl:174 [inlined]
 [28] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [29] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [31] include(::String) at ./client.jl:457
 [32] top-level scope at none:6
 [33] eval(::Module, ::Any) at ./boot.jl:331
 [34] exec_options(::Base.JLOptions) at ./client.jl:272
 [35] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:190
  Test threw exception
  Expression: gradcheck(conv41, (kw, kx); rtol = TOL, kw = [(:stride, (3, 3))])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#conv41#185"{var"#conv41#183#186"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:190
   [5] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [7] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] #conv4#344 at ./none:0 [inlined]
   [15] (::var"#conv41#185"{var"#conv41#183#186"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
   [16] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Tuple{Int64,Int64}},1},var"#conv41#185"{var"#conv41#183#186"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [20] gradcheck(::var"#conv41#185"{var"#conv41#183#186"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:190
   [22] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:191
  Test threw exception
  Expression: isapprox(deconv4(kd, kx; stride = (3, 3)), deconv4(ad, ax; stride = (3, 3)); rtol = 1.0e-6)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] deconv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:283
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:191
   [15] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [17] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] deconv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:283
 [14] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [15] deconv4(::AutoGrad.Result{KnetArray{Float64,4}}, ::AutoGrad.Result{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at ./none:0
 [16] (::var"#deconv41#187"{var"#deconv41#184#188"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:8
 [17] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [18] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Tuple{Int64,Int64}},1},var"#deconv41#187"{var"#deconv41#184#188"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [21] gradcheck(::var"#deconv41#187"{var"#deconv41#184#188"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:192
 [23] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
 [25] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [26] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [27] include(::String) at ./client.jl:457
 [28] macro expansion at ./timing.jl:174 [inlined]
 [29] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [30] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [31] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [32] include(::String) at ./client.jl:457
 [33] top-level scope at none:6
 [34] eval(::Module, ::Any) at ./boot.jl:331
 [35] exec_options(::Base.JLOptions) at ./client.jl:272
 [36] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:192
  Test threw exception
  Expression: gradcheck(deconv41, (kd, kx); rtol = TOL, kw = [(:stride, (3, 3))])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#deconv41#187"{var"#deconv41#184#188"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:192
   [5] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [7] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] deconv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:283
   [14] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [15] deconv4(::AutoGrad.Result{KnetArray{Float64,4}}, ::AutoGrad.Result{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at ./none:0
   [16] (::var"#deconv41#187"{var"#deconv41#184#188"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:8
   [17] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [18] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Tuple{Int64,Int64}},1},var"#deconv41#187"{var"#deconv41#184#188"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [21] gradcheck(::var"#deconv41#187"{var"#deconv41#184#188"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:192
   [23] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [25] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [26] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:195
  Test threw exception
  Expression: isapprox(pool(kx; mode = 1, padding = 1), pool(ax; mode = 1, padding = 1))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:195
   [14] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [16] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] pool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at ./none:0
 [15] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [16] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [19] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [20] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:196
 [21] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
 [23] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [25] include(::String) at ./client.jl:457
 [26] macro expansion at ./timing.jl:174 [inlined]
 [27] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [28] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [30] include(::String) at ./client.jl:457
 [31] top-level scope at none:6
 [32] eval(::Module, ::Any) at ./boot.jl:331
 [33] exec_options(::Base.JLOptions) at ./client.jl:272
 [34] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:196
  Test threw exception
  Expression: gradcheck(pool, kx; kw = [(:mode, 1), (:padding, 1)])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:196
   [5] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [7] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] pool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at ./none:0
   [15] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [16] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [19] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:196
   [21] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [23] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:197
  Test threw exception
  Expression: isapprox(unpool(kx; mode = 1, padding = 1), unpool(ax; mode = 1, padding = 1))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] unpool(::KnetArray{Float64,4}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:238
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:197
   [15] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [17] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] unpool(::KnetArray{Float64,4}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:238
 [14] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [15] unpool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at ./none:0
 [16] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [20] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:198
 [22] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
 [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [26] include(::String) at ./client.jl:457
 [27] macro expansion at ./timing.jl:174 [inlined]
 [28] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [29] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [31] include(::String) at ./client.jl:457
 [32] top-level scope at none:6
 [33] eval(::Module, ::Any) at ./boot.jl:331
 [34] exec_options(::Base.JLOptions) at ./client.jl:272
 [35] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:198
  Test threw exception
  Expression: gradcheck(unpool, kx; kw = [(:mode, 1), (:padding, 1)])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:198
   [5] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [7] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] unpool(::KnetArray{Float64,4}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:238
   [14] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [15] unpool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at ./none:0
   [16] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [20] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:198
   [22] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:199
  Test threw exception
  Expression: isapprox(conv4(kw, kx; mode = 1, padding = 1), conv4(aw, ax; mode = 1, padding = 1))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:199
   [14] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [16] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] #conv4#344 at ./none:0 [inlined]
 [15] (::var"#conv41#185"{var"#conv41#183#186"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [16] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},var"#conv41#185"{var"#conv41#183#186"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [20] gradcheck(::var"#conv41#185"{var"#conv41#183#186"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:200
 [22] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
 [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [26] include(::String) at ./client.jl:457
 [27] macro expansion at ./timing.jl:174 [inlined]
 [28] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [29] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [31] include(::String) at ./client.jl:457
 [32] top-level scope at none:6
 [33] eval(::Module, ::Any) at ./boot.jl:331
 [34] exec_options(::Base.JLOptions) at ./client.jl:272
 [35] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:200
  Test threw exception
  Expression: gradcheck(conv41, (kw, kx); rtol = TOL, kw = [(:mode, 1), (:padding, 1)])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#conv41#185"{var"#conv41#183#186"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:200
   [5] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [7] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] #conv4#344 at ./none:0 [inlined]
   [15] (::var"#conv41#185"{var"#conv41#183#186"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
   [16] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},var"#conv41#185"{var"#conv41#183#186"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [20] gradcheck(::var"#conv41#185"{var"#conv41#183#186"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:200
   [22] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:201
  Test threw exception
  Expression: isapprox(deconv4(kd, kx; mode = 1, padding = 1), deconv4(ad, ax; mode = 1, padding = 1))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] deconv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:283
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:201
   [15] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [17] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] deconv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:283
 [14] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [15] deconv4(::AutoGrad.Result{KnetArray{Float64,4}}, ::AutoGrad.Result{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at ./none:0
 [16] (::var"#deconv41#187"{var"#deconv41#184#188"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:8
 [17] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [18] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},var"#deconv41#187"{var"#deconv41#184#188"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [21] gradcheck(::var"#deconv41#187"{var"#deconv41#184#188"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:202
 [23] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
 [25] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [26] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [27] include(::String) at ./client.jl:457
 [28] macro expansion at ./timing.jl:174 [inlined]
 [29] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [30] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [31] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [32] include(::String) at ./client.jl:457
 [33] top-level scope at none:6
 [34] eval(::Module, ::Any) at ./boot.jl:331
 [35] exec_options(::Base.JLOptions) at ./client.jl:272
 [36] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:202
  Test threw exception
  Expression: gradcheck(deconv41, (kd, kx); rtol = TOL, kw = [(:mode, 1), (:padding, 1)])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#deconv41#187"{var"#deconv41#184#188"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:202
   [5] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [7] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] deconv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:283
   [14] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [15] deconv4(::AutoGrad.Result{KnetArray{Float64,4}}, ::AutoGrad.Result{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at ./none:0
   [16] (::var"#deconv41#187"{var"#deconv41#184#188"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:8
   [17] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [18] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},var"#deconv41#187"{var"#deconv41#184#188"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [21] gradcheck(::var"#deconv41#187"{var"#deconv41#184#188"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:202
   [23] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [25] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [26] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] pool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at ./none:0
 [15] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [16] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [19] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [20] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:206
 [21] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
 [23] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [25] include(::String) at ./client.jl:457
 [26] macro expansion at ./timing.jl:174 [inlined]
 [27] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [28] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [30] include(::String) at ./client.jl:457
 [31] top-level scope at none:6
 [32] eval(::Module, ::Any) at ./boot.jl:331
 [33] exec_options(::Base.JLOptions) at ./client.jl:272
 [34] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:206
  Test threw exception
  Expression: gradcheck(pool, kx; kw = [(:mode, 2), (:padding, 1)])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:206
   [5] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [7] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] pool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at ./none:0
   [15] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [16] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [19] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:206
   [21] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [23] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] unpool(::KnetArray{Float64,4}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:238
 [14] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [15] unpool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at ./none:0
 [16] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [20] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:208
 [22] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
 [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [26] include(::String) at ./client.jl:457
 [27] macro expansion at ./timing.jl:174 [inlined]
 [28] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [29] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [31] include(::String) at ./client.jl:457
 [32] top-level scope at none:6
 [33] eval(::Module, ::Any) at ./boot.jl:331
 [34] exec_options(::Base.JLOptions) at ./client.jl:272
 [35] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:208
  Test threw exception
  Expression: gradcheck(unpool, kx; kw = [(:mode, 2), (:padding, 1)])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:208
   [5] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [7] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] unpool(::KnetArray{Float64,4}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:238
   [14] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [15] unpool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at ./none:0
   [16] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [20] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:208
   [22] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:211
  Test threw exception
  Expression: isapprox(pool(kx; alpha = 2), pool(ax; alpha = 2))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:211
   [14] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [16] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] pool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at ./none:0
 [15] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [16] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [19] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [20] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:212
 [21] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
 [23] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [25] include(::String) at ./client.jl:457
 [26] macro expansion at ./timing.jl:174 [inlined]
 [27] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [28] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [30] include(::String) at ./client.jl:457
 [31] top-level scope at none:6
 [32] eval(::Module, ::Any) at ./boot.jl:331
 [33] exec_options(::Base.JLOptions) at ./client.jl:272
 [34] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:212
  Test threw exception
  Expression: gradcheck(pool, kx; kw = [(:alpha, 2)])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:212
   [5] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [7] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] pool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at ./none:0
   [15] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [16] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [19] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:212
   [21] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [23] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:213
  Test threw exception
  Expression: isapprox(unpool(kx; alpha = 2), unpool(ax; alpha = 2))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] unpool(::KnetArray{Float64,4}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:238
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:213
   [15] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [17] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] unpool(::KnetArray{Float64,4}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:238
 [14] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [15] unpool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at ./none:0
 [16] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [20] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:214
 [22] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
 [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [26] include(::String) at ./client.jl:457
 [27] macro expansion at ./timing.jl:174 [inlined]
 [28] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [29] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [31] include(::String) at ./client.jl:457
 [32] top-level scope at none:6
 [33] eval(::Module, ::Any) at ./boot.jl:331
 [34] exec_options(::Base.JLOptions) at ./client.jl:272
 [35] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:214
  Test threw exception
  Expression: gradcheck(unpool, kx; kw = [(:alpha, 2)])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:214
   [5] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [7] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] unpool(::KnetArray{Float64,4}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:238
   [14] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [15] unpool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at ./none:0
   [16] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [20] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:214
   [22] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:215
  Test threw exception
  Expression: isapprox(pool(kx; alpha = 2, mode = 1, padding = 1), pool(ax; alpha = 2, mode = 1, padding = 1))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:215
   [14] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [16] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:alpha, :mode, :padding),Tuple{Int64,Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] pool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:alpha, :mode, :padding),Tuple{Int64,Int64,Int64}}}) at ./none:0
 [15] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:alpha, :mode, :padding),Tuple{Int64,Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [16] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [19] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [20] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:216
 [21] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
 [23] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [25] include(::String) at ./client.jl:457
 [26] macro expansion at ./timing.jl:174 [inlined]
 [27] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [28] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [30] include(::String) at ./client.jl:457
 [31] top-level scope at none:6
 [32] eval(::Module, ::Any) at ./boot.jl:331
 [33] exec_options(::Base.JLOptions) at ./client.jl:272
 [34] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:216
  Test threw exception
  Expression: gradcheck(pool, kx; kw = [(:alpha, 2), (:mode, 1), (:padding, 1)])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:216
   [5] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [7] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:alpha, :mode, :padding),Tuple{Int64,Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] pool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:alpha, :mode, :padding),Tuple{Int64,Int64,Int64}}}) at ./none:0
   [15] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:alpha, :mode, :padding),Tuple{Int64,Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [16] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [19] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:216
   [21] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [23] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:217
  Test threw exception
  Expression: isapprox(unpool(kx; alpha = 2, mode = 1, padding = 1), unpool(ax; alpha = 2, mode = 1, padding = 1))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] unpool(::KnetArray{Float64,4}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:238
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:217
   [15] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [17] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] unpool(::KnetArray{Float64,4}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:238
 [14] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:alpha, :mode, :padding),Tuple{Int64,Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [15] unpool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:alpha, :mode, :padding),Tuple{Int64,Int64,Int64}}}) at ./none:0
 [16] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:alpha, :mode, :padding),Tuple{Int64,Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [20] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:218
 [22] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
 [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [26] include(::String) at ./client.jl:457
 [27] macro expansion at ./timing.jl:174 [inlined]
 [28] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [29] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [31] include(::String) at ./client.jl:457
 [32] top-level scope at none:6
 [33] eval(::Module, ::Any) at ./boot.jl:331
 [34] exec_options(::Base.JLOptions) at ./client.jl:272
 [35] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:218
  Test threw exception
  Expression: gradcheck(unpool, kx; kw = [(:alpha, 2), (:mode, 1), (:padding, 1)])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:218
   [5] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [7] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] unpool(::KnetArray{Float64,4}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:238
   [14] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:alpha, :mode, :padding),Tuple{Int64,Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [15] unpool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:alpha, :mode, :padding),Tuple{Int64,Int64,Int64}}}) at ./none:0
   [16] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:alpha, :mode, :padding),Tuple{Int64,Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [20] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:218
   [22] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:alpha, :mode, :padding),Tuple{Int64,Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] pool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:alpha, :mode, :padding),Tuple{Int64,Int64,Int64}}}) at ./none:0
 [15] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:alpha, :mode, :padding),Tuple{Int64,Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [16] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [19] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [20] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:221
 [21] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
 [23] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [25] include(::String) at ./client.jl:457
 [26] macro expansion at ./timing.jl:174 [inlined]
 [27] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [28] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [30] include(::String) at ./client.jl:457
 [31] top-level scope at none:6
 [32] eval(::Module, ::Any) at ./boot.jl:331
 [33] exec_options(::Base.JLOptions) at ./client.jl:272
 [34] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:221
  Test threw exception
  Expression: gradcheck(pool, kx; kw = [(:alpha, 2), (:mode, 2), (:padding, 1)])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:221
   [5] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [7] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:alpha, :mode, :padding),Tuple{Int64,Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] pool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:alpha, :mode, :padding),Tuple{Int64,Int64,Int64}}}) at ./none:0
   [15] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:alpha, :mode, :padding),Tuple{Int64,Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [16] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [19] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:221
   [21] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [23] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] unpool(::KnetArray{Float64,4}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:238
 [14] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:alpha, :mode, :padding),Tuple{Int64,Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [15] unpool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:alpha, :mode, :padding),Tuple{Int64,Int64,Int64}}}) at ./none:0
 [16] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:alpha, :mode, :padding),Tuple{Int64,Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [20] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:223
 [22] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
 [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [26] include(::String) at ./client.jl:457
 [27] macro expansion at ./timing.jl:174 [inlined]
 [28] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [29] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [31] include(::String) at ./client.jl:457
 [32] top-level scope at none:6
 [33] eval(::Module, ::Any) at ./boot.jl:331
 [34] exec_options(::Base.JLOptions) at ./client.jl:272
 [35] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:223
  Test threw exception
  Expression: gradcheck(unpool, kx; kw = [(:alpha, 2), (:mode, 2), (:padding, 1)])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:223
   [5] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [7] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] unpool(::KnetArray{Float64,4}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:238
   [14] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:alpha, :mode, :padding),Tuple{Int64,Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [15] unpool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:alpha, :mode, :padding),Tuple{Int64,Int64,Int64}}}) at ./none:0
   [16] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:alpha, :mode, :padding),Tuple{Int64,Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [20] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:223
   [22] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:225
  Test threw exception
  Expression: isapprox(conv4(kw, kx; alpha = 2), conv4(aw, ax; alpha = 2))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:225
   [14] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [16] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] #conv4#344 at ./none:0 [inlined]
 [15] (::var"#conv41#185"{var"#conv41#183#186"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [16] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},var"#conv41#185"{var"#conv41#183#186"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [20] gradcheck(::var"#conv41#185"{var"#conv41#183#186"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:226
 [22] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
 [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [26] include(::String) at ./client.jl:457
 [27] macro expansion at ./timing.jl:174 [inlined]
 [28] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [29] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [31] include(::String) at ./client.jl:457
 [32] top-level scope at none:6
 [33] eval(::Module, ::Any) at ./boot.jl:331
 [34] exec_options(::Base.JLOptions) at ./client.jl:272
 [35] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:226
  Test threw exception
  Expression: gradcheck(conv41, (kw, kx); rtol = TOL, kw = [(:alpha, 2)])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#conv41#185"{var"#conv41#183#186"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:226
   [5] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [7] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] #conv4#344 at ./none:0 [inlined]
   [15] (::var"#conv41#185"{var"#conv41#183#186"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
   [16] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},var"#conv41#185"{var"#conv41#183#186"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [20] gradcheck(::var"#conv41#185"{var"#conv41#183#186"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:226
   [22] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:227
  Test threw exception
  Expression: isapprox(deconv4(kd, kx; alpha = 2), deconv4(ad, ax; alpha = 2))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] deconv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:283
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:227
   [15] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [17] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
 [13] deconv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:283
 [14] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [15] deconv4(::AutoGrad.Result{KnetArray{Float64,4}}, ::AutoGrad.Result{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at ./none:0
 [16] (::var"#deconv41#187"{var"#deconv41#184#188"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:8
 [17] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [18] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},var"#deconv41#187"{var"#deconv41#184#188"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [21] gradcheck(::var"#deconv41#187"{var"#deconv41#184#188"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:228
 [23] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
 [25] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [26] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
 [27] include(::String) at ./client.jl:457
 [28] macro expansion at ./timing.jl:174 [inlined]
 [29] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
 [30] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [31] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
 [32] include(::String) at ./client.jl:457
 [33] top-level scope at none:6
 [34] eval(::Module, ::Any) at ./boot.jl:331
 [35] exec_options(::Base.JLOptions) at ./client.jl:272
 [36] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:228
  Test threw exception
  Expression: gradcheck(deconv41, (kd, kx); rtol = TOL, kw = [(:alpha, 2)])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#deconv41#187"{var"#deconv41#184#188"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:228
   [5] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [7] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] deconv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/src/conv.jl:283
   [14] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [15] deconv4(::AutoGrad.Result{KnetArray{Float64,4}}, ::AutoGrad.Result{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at ./none:0
   [16] (::var"#deconv41#187"{var"#deconv41#184#188"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:8
   [17] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [18] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},var"#deconv41#187"{var"#deconv41#184#188"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [21] gradcheck(::var"#deconv41#187"{var"#deconv41#184#188"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:228
   [23] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:117
   [25] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [26] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/conv.jl:7
  
 84.540531 seconds (63.37 M allocations: 3.091 GiB, 3.41% gc time)
rnn.jl	rnn: Error During Test at /home/pkgeval/.julia/packages/Knet/exwCE/test/rnn.jl:7
  Got exception outside of a @test
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CUDA.CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] gethandle() at /home/pkgeval/.julia/packages/Knet/exwCE/src/rnn.jl:403
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/rnn.jl:19
   [15] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/rnn.jl:9
   [17] include(::String) at ./client.jl:457
   [18] macro expansion at ./timing.jl:174 [inlined]
   [19] macro expansion at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:3 [inlined]
   [20] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:12
   [22] include(::String) at ./client.jl:457
   [23] top-level scope at none:6
   [24] eval(::Module, ::Any) at ./boot.jl:331
   [25] exec_options(::Base.JLOptions) at ./client.jl:272
   [26] _start() at ./client.jl:506
  
  2.459982 seconds (302.49 k allocations: 16.686 MiB, 20.72% gc time)
binary.jl	283.306648 seconds (200.80 M allocations: 9.843 GiB, 2.62% gc time)
unary.jl	253.930094 seconds (157.04 M allocations: 8.020 GiB, 2.32% gc time)
update.jl	┌ Warning: optimizers is deprecated, use sgd, adam etc. instead.
└ @ Knet ~/.julia/packages/Knet/exwCE/src/update.jl:598
471.555808 seconds (133.72 M allocations: 5.741 GiB, 2.96% gc time)
Test Summary:           | Pass  Error  Total
Knet                    | 6888    154   7042
  distributions         |    4             4
  dropout               |    3             3
  serialize             |           1      1
  JLD                   |           1      1
  gcnode                |           1      1
  statistics            |   68            68
  cuarray               |   99            99
  bmm                   |   39            39
  linalg                |  200           200
  batchnorm             |   28     25     53
    {Float64, 2}        |    8      7     15
      cpu-stat          |    2             2
      cpu-grads         |    2             2
      gpu-stats         |           2      2
      gpu-grads         |           2      2
      dev-consistency   |           1      1
      test-moments      |    2      1      3
      training-moments  |    2      1      3
    {Float64, 4}        |   10      9     19
      cpu-stat          |    2             2
      cpu-grads         |    2             2
      gpu-stats         |           2      2
      gpu-grads         |           2      2
      dev-consistency   |           1      1
      test-moments      |    2      1      3
      training-moments  |    2      1      3
      cpu-grads-testing |    2             2
      gpu-grads-testing |           2      2
    {Float64, 5}        |   10      9     19
      cpu-stat          |    2             2
      cpu-grads         |    2             2
      gpu-stats         |           2      2
      gpu-grads         |           2      2
      dev-consistency   |           1      1
      test-moments      |    2      1      3
      training-moments  |    2      1      3
      cpu-grads-testing |    2             2
      gpu-grads-testing |           2      2
  loss                  |   74     37    111
  karray                |  328           328
  reduction             | 1620          1620
  conv                  |    1     88     89
    gpuconv             |    1     88     89
  rnn                   |           1      1
  binary                | 3782          3782
  unary                 |  620           620
  update!               |   20            20
  optimizers            |    2             2
ERROR: LoadError: Some tests did not pass: 6888 passed, 0 failed, 154 errored, 0 broken.
in expression starting at /home/pkgeval/.julia/packages/Knet/exwCE/test/runtests.jl:6
ERROR: Package Knet errored during testing
Stacktrace:
 [1] pkgerror(::String) at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/Types.jl:52
 [2] test(::Pkg.Types.Context, ::Array{Pkg.Types.PackageSpec,1}; coverage::Bool, julia_args::Cmd, test_args::Cmd, test_fn::Nothing) at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/Operations.jl:1566
 [3] test(::Pkg.Types.Context, ::Array{Pkg.Types.PackageSpec,1}; coverage::Bool, test_fn::Nothing, julia_args::Cmd, test_args::Cmd, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:328
 [4] test(::Pkg.Types.Context, ::Array{Pkg.Types.PackageSpec,1}) at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:315
 [5] #test#61 at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:67 [inlined]
 [6] test at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:67 [inlined]
 [7] #test#60 at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:66 [inlined]
 [8] test at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:66 [inlined]
 [9] test(::String; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:65
 [10] test(::String) at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:65
 [11] top-level scope at none:16
