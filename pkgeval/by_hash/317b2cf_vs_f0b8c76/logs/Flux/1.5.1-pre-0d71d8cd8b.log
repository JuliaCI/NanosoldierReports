Julia Version 1.5.1-pre.28
Commit 0d71d8cd8b (2020-08-14 20:35 UTC)
Platform Info:
  OS: Linux (x86_64-linux-gnu)
  CPU: Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-9.0.1 (ORCJIT, skylake-avx512)
Environment:
  JULIA_DEPOT_PATH = ::/usr/local/share/julia
  JULIA_NUM_THREADS = 2

  Resolving package versions...
  Installed CompilerSupportLibraries_jll ─ v0.3.3+0
  Installed TranscodingStreams ─────────── v0.9.5
  Installed SpecialFunctions ───────────── v0.10.3
  Installed AbstractTrees ──────────────── v0.3.3
  Installed OffsetArrays ───────────────── v1.1.2
  Installed StatsBase ──────────────────── v0.33.0
  Installed ForwardDiff ────────────────── v0.10.12
  Installed Flux ───────────────────────── v0.11.1
  Installed MuladdMacro ────────────────── v0.2.2
  Installed CEnum ──────────────────────── v0.4.1
  Installed SortingAlgorithms ──────────── v0.3.1
  Installed SLEEFPirates ───────────────── v0.5.5
  Installed GPUCompiler ────────────────── v0.5.5
  Installed StaticArrays ───────────────── v0.12.4
  Installed IRTools ────────────────────── v0.4.0
  Installed DocStringExtensions ────────── v0.8.2
  Installed DataStructures ─────────────── v0.17.20
  Installed BinaryProvider ─────────────── v0.5.10
  Installed UnPack ─────────────────────── v1.0.2
  Installed OpenSpecFun_jll ────────────── v0.5.3+3
  Installed MacroTools ─────────────────── v0.5.5
  Installed ChainRules ─────────────────── v0.7.12
  Installed Zlib_jll ───────────────────── v1.2.11+15
  Installed ChainRulesCore ─────────────── v0.9.5
  Installed SIMDPirates ────────────────── v0.8.24
  Installed LLVM ───────────────────────── v2.0.0
  Installed Requires ───────────────────── v1.0.1
  Installed Reexport ───────────────────── v0.2.0
  Installed ArrayLayouts ───────────────── v0.3.8
  Installed CodecZlib ──────────────────── v0.7.0
  Installed VectorizationBase ──────────── v0.12.32
  Installed Missings ───────────────────── v0.4.3
  Installed DiffResults ────────────────── v1.0.2
  Installed FixedPointNumbers ──────────── v0.8.4
  Installed CommonSubexpressions ───────── v0.3.0
  Installed LoopVectorization ──────────── v0.8.24
  Installed CUDA ───────────────────────── v1.2.1
  Installed ZygoteRules ────────────────── v0.2.0
  Installed NaNMath ────────────────────── v0.3.4
  Installed TimerOutputs ───────────────── v0.5.6
  Installed FillArrays ─────────────────── v0.8.14
  Installed ZipFile ────────────────────── v0.9.2
  Installed DiffRules ──────────────────── v1.0.1
  Installed NNlib ──────────────────────── v0.7.4
  Installed GPUArrays ──────────────────── v5.0.0
  Installed OrderedCollections ─────────── v1.3.0
  Installed Adapt ──────────────────────── v2.0.2
  Installed Juno ───────────────────────── v0.8.3
  Installed CpuId ──────────────────────── v0.2.2
  Installed AbstractFFTs ───────────────── v0.5.0
  Installed DataAPI ────────────────────── v1.3.0
  Installed Zygote ─────────────────────── v0.5.4
  Installed Media ──────────────────────── v0.5.0
  Installed ColorTypes ─────────────────── v0.10.8
  Installed Functors ───────────────────── v0.1.0
  Installed ExprTools ──────────────────── v0.1.1
  Installed Colors ─────────────────────── v0.12.3
Updating `~/.julia/environments/v1.5/Project.toml`
  [587475ba] + Flux v0.11.1
Updating `~/.julia/environments/v1.5/Manifest.toml`
  [621f4979] + AbstractFFTs v0.5.0
  [1520ce14] + AbstractTrees v0.3.3
  [79e6a3ab] + Adapt v2.0.2
  [4c555306] + ArrayLayouts v0.3.8
  [b99e7846] + BinaryProvider v0.5.10
  [fa961155] + CEnum v0.4.1
  [052768ef] + CUDA v1.2.1
  [082447d4] + ChainRules v0.7.12
  [d360d2e6] + ChainRulesCore v0.9.5
  [944b1d66] + CodecZlib v0.7.0
  [3da002f7] + ColorTypes v0.10.8
  [5ae59095] + Colors v0.12.3
  [bbf7d656] + CommonSubexpressions v0.3.0
  [e66e0078] + CompilerSupportLibraries_jll v0.3.3+0
  [adafc99b] + CpuId v0.2.2
  [9a962f9c] + DataAPI v1.3.0
  [864edb3b] + DataStructures v0.17.20
  [163ba53b] + DiffResults v1.0.2
  [b552c78f] + DiffRules v1.0.1
  [ffbed154] + DocStringExtensions v0.8.2
  [e2ba6199] + ExprTools v0.1.1
  [1a297f60] + FillArrays v0.8.14
  [53c48c17] + FixedPointNumbers v0.8.4
  [587475ba] + Flux v0.11.1
  [f6369f11] + ForwardDiff v0.10.12
  [d9f16b24] + Functors v0.1.0
  [0c68f7d7] + GPUArrays v5.0.0
  [61eb1bfa] + GPUCompiler v0.5.5
  [7869d1d1] + IRTools v0.4.0
  [e5e0dc1b] + Juno v0.8.3
  [929cbde3] + LLVM v2.0.0
  [bdcacae8] + LoopVectorization v0.8.24
  [1914dd2f] + MacroTools v0.5.5
  [e89f7d12] + Media v0.5.0
  [e1d29d7a] + Missings v0.4.3
  [46d2c3a1] + MuladdMacro v0.2.2
  [872c559c] + NNlib v0.7.4
  [77ba4419] + NaNMath v0.3.4
  [6fe1bfb0] + OffsetArrays v1.1.2
  [efe28fd5] + OpenSpecFun_jll v0.5.3+3
  [bac558e1] + OrderedCollections v1.3.0
  [189a3867] + Reexport v0.2.0
  [ae029012] + Requires v1.0.1
  [21efa798] + SIMDPirates v0.8.24
  [476501e8] + SLEEFPirates v0.5.5
  [a2af1166] + SortingAlgorithms v0.3.1
  [276daf66] + SpecialFunctions v0.10.3
  [90137ffa] + StaticArrays v0.12.4
  [2913bbd2] + StatsBase v0.33.0
  [a759f4b9] + TimerOutputs v0.5.6
  [3bb67fe8] + TranscodingStreams v0.9.5
  [3a884ed6] + UnPack v1.0.2
  [3d5dd08c] + VectorizationBase v0.12.32
  [a5390f91] + ZipFile v0.9.2
  [83775a58] + Zlib_jll v1.2.11+15
  [e88e6eb3] + Zygote v0.5.4
  [700de1a5] + ZygoteRules v0.2.0
  [2a0f44e3] + Base64
  [ade2ca70] + Dates
  [8bb1440f] + DelimitedFiles
  [8ba89e20] + Distributed
  [9fa8497b] + Future
  [b77e0a4c] + InteractiveUtils
  [76f85450] + LibGit2
  [8f399da3] + Libdl
  [37e2e46d] + LinearAlgebra
  [56ddb016] + Logging
  [d6f4376e] + Markdown
  [a63ad114] + Mmap
  [44cfe95a] + Pkg
  [de0858da] + Printf
  [9abbd945] + Profile
  [3fa0cd96] + REPL
  [9a3f8284] + Random
  [ea8e919c] + SHA
  [9e88b42a] + Serialization
  [6462fe0b] + Sockets
  [2f01184e] + SparseArrays
  [10745b16] + Statistics
  [8dfed614] + Test
  [cf7118a7] + UUIDs
  [4ec0a83e] + Unicode
   Building SLEEFPirates → `~/.julia/packages/SLEEFPirates/jGsib/deps/build.log`
    Testing Flux
Status `/tmp/jl_zV4461/Project.toml`
  [1520ce14] AbstractTrees v0.3.3
  [79e6a3ab] Adapt v2.0.2
  [052768ef] CUDA v1.2.1
  [944b1d66] CodecZlib v0.7.0
  [5ae59095] Colors v0.12.3
  [e30172f5] Documenter v0.25.1
  [587475ba] Flux v0.11.1
  [d9f16b24] Functors v0.1.0
  [c8e1da08] IterTools v1.3.0
  [e5e0dc1b] Juno v0.8.3
  [1914dd2f] MacroTools v0.5.5
  [872c559c] NNlib v0.7.4
  [189a3867] Reexport v0.2.0
  [2913bbd2] StatsBase v0.33.0
  [a5390f91] ZipFile v0.9.2
  [e88e6eb3] Zygote v0.5.4
  [8bb1440f] DelimitedFiles
  [37e2e46d] LinearAlgebra
  [44cfe95a] Pkg
  [de0858da] Printf
  [9a3f8284] Random
  [ea8e919c] SHA
  [10745b16] Statistics
  [8dfed614] Test
Status `/tmp/jl_zV4461/Manifest.toml`
  [621f4979] AbstractFFTs v0.5.0
  [1520ce14] AbstractTrees v0.3.3
  [79e6a3ab] Adapt v2.0.2
  [4c555306] ArrayLayouts v0.3.8
  [b99e7846] BinaryProvider v0.5.10
  [fa961155] CEnum v0.4.1
  [052768ef] CUDA v1.2.1
  [082447d4] ChainRules v0.7.12
  [d360d2e6] ChainRulesCore v0.9.5
  [944b1d66] CodecZlib v0.7.0
  [3da002f7] ColorTypes v0.10.8
  [5ae59095] Colors v0.12.3
  [bbf7d656] CommonSubexpressions v0.3.0
  [e66e0078] CompilerSupportLibraries_jll v0.3.3+0
  [adafc99b] CpuId v0.2.2
  [9a962f9c] DataAPI v1.3.0
  [864edb3b] DataStructures v0.17.20
  [163ba53b] DiffResults v1.0.2
  [b552c78f] DiffRules v1.0.1
  [ffbed154] DocStringExtensions v0.8.2
  [e30172f5] Documenter v0.25.1
  [e2ba6199] ExprTools v0.1.1
  [1a297f60] FillArrays v0.8.14
  [53c48c17] FixedPointNumbers v0.8.4
  [587475ba] Flux v0.11.1
  [f6369f11] ForwardDiff v0.10.12
  [d9f16b24] Functors v0.1.0
  [0c68f7d7] GPUArrays v5.0.0
  [61eb1bfa] GPUCompiler v0.5.5
  [7869d1d1] IRTools v0.4.0
  [c8e1da08] IterTools v1.3.0
  [682c06a0] JSON v0.21.0
  [e5e0dc1b] Juno v0.8.3
  [929cbde3] LLVM v2.0.0
  [bdcacae8] LoopVectorization v0.8.24
  [1914dd2f] MacroTools v0.5.5
  [e89f7d12] Media v0.5.0
  [e1d29d7a] Missings v0.4.3
  [46d2c3a1] MuladdMacro v0.2.2
  [872c559c] NNlib v0.7.4
  [77ba4419] NaNMath v0.3.4
  [6fe1bfb0] OffsetArrays v1.1.2
  [efe28fd5] OpenSpecFun_jll v0.5.3+3
  [bac558e1] OrderedCollections v1.3.0
  [69de0a69] Parsers v1.0.10
  [189a3867] Reexport v0.2.0
  [ae029012] Requires v1.0.1
  [21efa798] SIMDPirates v0.8.24
  [476501e8] SLEEFPirates v0.5.5
  [a2af1166] SortingAlgorithms v0.3.1
  [276daf66] SpecialFunctions v0.10.3
  [90137ffa] StaticArrays v0.12.4
  [2913bbd2] StatsBase v0.33.0
  [a759f4b9] TimerOutputs v0.5.6
  [3bb67fe8] TranscodingStreams v0.9.5
  [3a884ed6] UnPack v1.0.2
  [3d5dd08c] VectorizationBase v0.12.32
  [a5390f91] ZipFile v0.9.2
  [83775a58] Zlib_jll v1.2.11+15
  [e88e6eb3] Zygote v0.5.4
  [700de1a5] ZygoteRules v0.2.0
  [2a0f44e3] Base64
  [ade2ca70] Dates
  [8bb1440f] DelimitedFiles
  [8ba89e20] Distributed
  [9fa8497b] Future
  [b77e0a4c] InteractiveUtils
  [76f85450] LibGit2
  [8f399da3] Libdl
  [37e2e46d] LinearAlgebra
  [56ddb016] Logging
  [d6f4376e] Markdown
  [a63ad114] Mmap
  [44cfe95a] Pkg
  [de0858da] Printf
  [9abbd945] Profile
  [3fa0cd96] REPL
  [9a3f8284] Random
  [ea8e919c] SHA
  [9e88b42a] Serialization
  [6462fe0b] Sockets
  [2f01184e] SparseArrays
  [10745b16] Statistics
  [8dfed614] Test
  [cf7118a7] UUIDs
  [4ec0a83e] Unicode
┌ Warning: CUDA.jl found cuda, but did not find libcudnn. Some functionality will not be available.
└ @ Flux ~/.julia/packages/Flux/05b38/src/Flux.jl:56
Test Summary: | Pass  Total
Utils         |   60     60
Test Summary: | Pass  Total
Onehot        |    9      9
Test Summary: | Pass  Total
Optimise      |   24     24
[ Info: Downloading CMUDict dataset
[ Info: Downloading MNIST dataset
[ Info: Downloading MNIST dataset
[ Info: Downloading MNIST dataset
[ Info: Downloading MNIST dataset
[ Info: Downloading Fashion-MNIST dataset
[ Info: Downloading Fashion-MNIST dataset
[ Info: Downloading Fashion-MNIST dataset
[ Info: Downloading Fashion-MNIST dataset
[ Info: Downloading sentiment treebank dataset
[ Info: Downloading iris dataset.
[ Info: Downloading the Boston housing Dataset
Test Summary: | Pass  Total
Data          |   53     53
Test Summary: | Pass  Total
Losses        |   88     88
┌ Warning: Slow fallback implementation invoked for conv!  You probably don't want this; check your datatypes.
│   yT = Float64
│   T1 = Float32
│   T2 = Float64
└ @ NNlib ~/.julia/packages/NNlib/PI8Xh/src/conv.jl:206
┌ Warning: Slow fallback implementation invoked for conv!  You probably don't want this; check your datatypes.
│   yT = Float64
│   T1 = Float32
│   T2 = Float64
└ @ NNlib ~/.julia/packages/NNlib/PI8Xh/src/conv.jl:206
┌ Warning: Slow fallback implementation invoked for conv!  You probably don't want this; check your datatypes.
│   yT = Float64
│   T1 = Float32
│   T2 = Float64
└ @ NNlib ~/.julia/packages/NNlib/PI8Xh/src/conv.jl:206
┌ Warning: Slow fallback implementation invoked for conv!  You probably don't want this; check your datatypes.
│   yT = Float64
│   T1 = Float32
│   T2 = Float64
└ @ NNlib ~/.julia/packages/NNlib/PI8Xh/src/conv.jl:206
┌ Warning: Slow fallback implementation invoked for conv!  You probably don't want this; check your datatypes.
│   yT = Float64
│   T1 = Float32
│   T2 = Float64
└ @ NNlib ~/.julia/packages/NNlib/PI8Xh/src/conv.jl:206
Test Summary: | Pass  Total
Layers        |  218    218
[ Info: Testing GPU Support
CUDA: Error During Test at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/cuda.jl:7
  Got exception outside of a @test
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn() at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73
   [4] (::CUDA.CUDNN.var"#19207#cache_fptr!#6")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:32 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/error.jl:28 [inlined]
   [9] cudnnGetProperty(::CUDA.libraryPropertyType, ::Base.RefValue{Int32}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:93
   [10] cudnnGetProperty(::CUDA.libraryPropertyType) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:16
   [11] version() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:20
   [12] conv!(::CuArray{Float32,4}, ::CuArray{Float32,4}, ::CuArray{Float32,4}, ::DenseConvDims{2,(2, 2),3,4,(1, 1),(0, 0, 0, 0),(1, 1),false}; alpha::Int64, algo::Int64) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/nnlib.jl:56
   [13] conv! at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/nnlib.jl:56 [inlined]
   [14] conv(::CuArray{Float32,4}, ::CuArray{Float32,4}, ::DenseConvDims{2,(2, 2),3,4,(1, 1),(0, 0, 0, 0),(1, 1),false}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/NNlib/PI8Xh/src/conv.jl:91
   [15] conv(::CuArray{Float32,4}, ::CuArray{Float32,4}, ::DenseConvDims{2,(2, 2),3,4,(1, 1),(0, 0, 0, 0),(1, 1),false}) at /home/pkgeval/.julia/packages/NNlib/PI8Xh/src/conv.jl:89
   [16] (::Conv{2,4,typeof(identity),CuArray{Float32,4},CuArray{Float32,1}})(::CuArray{Float32,4}) at /home/pkgeval/.julia/packages/Flux/05b38/src/layers/conv.jl:147
   [17] top-level scope at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/cuda.jl:31
   [18] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [19] top-level scope at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/cuda.jl:8
   [20] include(::String) at ./client.jl:457
   [21] top-level scope at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/runtests.jl:6
   [22] include(::String) at ./client.jl:457
   [23] top-level scope at /home/pkgeval/.julia/packages/Flux/05b38/test/runtests.jl:38
   [24] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [25] top-level scope at /home/pkgeval/.julia/packages/Flux/05b38/test/runtests.jl:37
   [26] include(::String) at ./client.jl:457
   [27] top-level scope at none:6
   [28] eval(::Module, ::Any) at ./boot.jl:331
   [29] exec_options(::Base.JLOptions) at ./client.jl:272
   [30] _start() at ./client.jl:506
  
Conv GPU grad test: Error During Test at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:33
  Test threw exception
  Expression: gradient((()->begin
                sum(l(xs))
            end), ps) isa Flux.Zygote.Grads
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn() at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73
   [4] (::CUDA.CUDNN.var"#19207#cache_fptr!#6")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:32 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/error.jl:28 [inlined]
   [9] cudnnGetProperty(::CUDA.libraryPropertyType, ::Base.RefValue{Int32}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:93
   [10] cudnnGetProperty(::CUDA.libraryPropertyType) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:16
   [11] version() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:20
   [12] conv!(::CuArray{Float32,4}, ::CuArray{Float32,4}, ::CuArray{Float32,4}, ::DenseConvDims{2,(2, 2),1,3,(1, 1),(0, 0, 0, 0),(1, 1),false}; alpha::Int64, algo::Int64) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/nnlib.jl:56
   [13] conv! at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/nnlib.jl:56 [inlined]
   [14] conv(::CuArray{Float32,4}, ::CuArray{Float32,4}, ::DenseConvDims{2,(2, 2),1,3,(1, 1),(0, 0, 0, 0),(1, 1),false}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/NNlib/PI8Xh/src/conv.jl:91
   [15] conv(::CuArray{Float32,4}, ::CuArray{Float32,4}, ::DenseConvDims{2,(2, 2),1,3,(1, 1),(0, 0, 0, 0),(1, 1),false}) at /home/pkgeval/.julia/packages/NNlib/PI8Xh/src/conv.jl:89
   [16] #adjoint#1238 at /home/pkgeval/.julia/packages/Zygote/seGHk/src/lib/nnlib.jl:38 [inlined]
   [17] adjoint at ./none:0 [inlined]
   [18] _pullback at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:47 [inlined]
   [19] Conv at /home/pkgeval/.julia/packages/Flux/05b38/src/layers/conv.jl:147 [inlined]
   [20] _pullback(::Zygote.Context, ::Conv{2,4,typeof(identity),CuArray{Float32,4},CuArray{Float32,1}}, ::CuArray{Float32,4}) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface2.jl:0
   [21] #134 at ./none:0 [inlined]
   [22] _pullback(::Zygote.Context, ::var"#134#138"{Conv{2,4,typeof(identity),CuArray{Float32,4},CuArray{Float32,1}}}) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface2.jl:0
   [23] pullback(::Function, ::Params) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface.jl:172
   [24] gradient(::Function, ::Params) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface.jl:53
   [25] macro expansion at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:33 [inlined]
   [26] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [27] macro expansion at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:26 [inlined]
   [28] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [29] gradtest(::String, ::Array{UnionAll,1}, ::Array{Float32,4}, ::Tuple{Int64,Int64}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:24
  
Conv GPU grad test: Error During Test at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:25
  Got exception outside of a @test
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn() at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73
   [4] (::CUDA.CUDNN.var"#19207#cache_fptr!#6")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:32 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/error.jl:28 [inlined]
   [9] cudnnGetProperty(::CUDA.libraryPropertyType, ::Base.RefValue{Int32}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:93
   [10] cudnnGetProperty(::CUDA.libraryPropertyType) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:16
   [11] version() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:20
   [12] conv!(::CuArray{Float32,4}, ::CuArray{Float32,4}, ::CuArray{Float32,4}, ::DenseConvDims{2,(2, 2),1,3,(1, 1),(0, 0, 0, 0),(1, 1),false}; alpha::Int64, algo::Int64) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/nnlib.jl:56
   [13] conv! at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/nnlib.jl:56 [inlined]
   [14] conv(::CuArray{Float32,4}, ::CuArray{Float32,4}, ::DenseConvDims{2,(2, 2),1,3,(1, 1),(0, 0, 0, 0),(1, 1),false}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/NNlib/PI8Xh/src/conv.jl:91
   [15] conv(::CuArray{Float32,4}, ::CuArray{Float32,4}, ::DenseConvDims{2,(2, 2),1,3,(1, 1),(0, 0, 0, 0),(1, 1),false}) at /home/pkgeval/.julia/packages/NNlib/PI8Xh/src/conv.jl:89
   [16] #adjoint#1238 at /home/pkgeval/.julia/packages/Zygote/seGHk/src/lib/nnlib.jl:38 [inlined]
   [17] adjoint at ./none:0 [inlined]
   [18] _pullback at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:47 [inlined]
   [19] Conv at /home/pkgeval/.julia/packages/Flux/05b38/src/layers/conv.jl:147 [inlined]
   [20] _pullback(::Zygote.Context, ::Conv{2,4,typeof(identity),CuArray{Float32,4},CuArray{Float32,1}}, ::CuArray{Float32,4}) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface2.jl:0
   [21] #135 at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:34 [inlined]
   [22] _pullback(::Zygote.Context, ::var"#135#139"{Conv{2,4,typeof(identity),CuArray{Float32,4},CuArray{Float32,1}}}) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface2.jl:0
   [23] pullback(::Function, ::Params) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface.jl:172
   [24] gradient(::Function, ::Params) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface.jl:53
   [25] macro expansion at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:34 [inlined]
   [26] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [27] macro expansion at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:26 [inlined]
   [28] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [29] gradtest(::String, ::Array{UnionAll,1}, ::Array{Float32,4}, ::Tuple{Int64,Int64}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:24
   [30] top-level scope at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:50
   [31] include(::String) at ./client.jl:457
   [32] top-level scope at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/runtests.jl:8
   [33] include(::String) at ./client.jl:457
   [34] top-level scope at /home/pkgeval/.julia/packages/Flux/05b38/test/runtests.jl:38
   [35] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [36] top-level scope at /home/pkgeval/.julia/packages/Flux/05b38/test/runtests.jl:37
   [37] include(::String) at ./client.jl:457
   [38] top-level scope at none:6
   [39] eval(::Module, ::Any) at ./boot.jl:331
   [40] exec_options(::Base.JLOptions) at ./client.jl:272
   [41] _start() at ./client.jl:506
  
ConvTranspose GPU grad test: Error During Test at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:33
  Test threw exception
  Expression: gradient((()->begin
                sum(l(xs))
            end), ps) isa Flux.Zygote.Grads
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn() at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73
   [4] (::CUDA.CUDNN.var"#19207#cache_fptr!#6")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:32 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/error.jl:28 [inlined]
   [9] cudnnGetProperty(::CUDA.libraryPropertyType, ::Base.RefValue{Int32}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:93
   [10] cudnnGetProperty(::CUDA.libraryPropertyType) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:16
   [11] version() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:20
   [12] ∇conv_data!(::CuArray{Float32,4}, ::CuArray{Float32,4}, ::CuArray{Float32,4}, ::DenseConvDims{2,(2, 2),3,1,(1, 1),(0, 0, 0, 0),(1, 1),false}; alpha::Int64, algo::Int64) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/nnlib.jl:75
   [13] ∇conv_data! at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/nnlib.jl:75 [inlined]
   [14] #∇conv_data#43 at /home/pkgeval/.julia/packages/NNlib/PI8Xh/src/conv.jl:103 [inlined]
   [15] ∇conv_data(::CuArray{Float32,4}, ::CuArray{Float32,4}, ::DenseConvDims{2,(2, 2),3,1,(1, 1),(0, 0, 0, 0),(1, 1),false}) at /home/pkgeval/.julia/packages/NNlib/PI8Xh/src/conv.jl:101
   [16] #adjoint#1243 at /home/pkgeval/.julia/packages/Zygote/seGHk/src/lib/nnlib.jl:49 [inlined]
   [17] adjoint at ./none:0 [inlined]
   [18] _pullback at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:47 [inlined]
   [19] ConvTranspose at /home/pkgeval/.julia/packages/Flux/05b38/src/layers/conv.jl:273 [inlined]
   [20] _pullback(::Zygote.Context, ::ConvTranspose{2,4,typeof(identity),CuArray{Float32,4},CuArray{Float32,1}}, ::CuArray{Float32,4}) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface2.jl:0
   [21] #134 at ./none:0 [inlined]
   [22] _pullback(::Zygote.Context, ::var"#134#138"{ConvTranspose{2,4,typeof(identity),CuArray{Float32,4},CuArray{Float32,1}}}) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface2.jl:0
   [23] pullback(::Function, ::Params) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface.jl:172
   [24] gradient(::Function, ::Params) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface.jl:53
   [25] macro expansion at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:33 [inlined]
   [26] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [27] macro expansion at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:26 [inlined]
   [28] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [29] gradtest(::String, ::Array{UnionAll,1}, ::Array{Float32,4}, ::Tuple{Int64,Int64}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:24
  
ConvTranspose GPU grad test: Error During Test at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:25
  Got exception outside of a @test
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn() at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73
   [4] (::CUDA.CUDNN.var"#19207#cache_fptr!#6")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:32 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/error.jl:28 [inlined]
   [9] cudnnGetProperty(::CUDA.libraryPropertyType, ::Base.RefValue{Int32}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:93
   [10] cudnnGetProperty(::CUDA.libraryPropertyType) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:16
   [11] version() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:20
   [12] ∇conv_data!(::CuArray{Float32,4}, ::CuArray{Float32,4}, ::CuArray{Float32,4}, ::DenseConvDims{2,(2, 2),3,1,(1, 1),(0, 0, 0, 0),(1, 1),false}; alpha::Int64, algo::Int64) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/nnlib.jl:75
   [13] ∇conv_data! at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/nnlib.jl:75 [inlined]
   [14] #∇conv_data#43 at /home/pkgeval/.julia/packages/NNlib/PI8Xh/src/conv.jl:103 [inlined]
   [15] ∇conv_data(::CuArray{Float32,4}, ::CuArray{Float32,4}, ::DenseConvDims{2,(2, 2),3,1,(1, 1),(0, 0, 0, 0),(1, 1),false}) at /home/pkgeval/.julia/packages/NNlib/PI8Xh/src/conv.jl:101
   [16] #adjoint#1243 at /home/pkgeval/.julia/packages/Zygote/seGHk/src/lib/nnlib.jl:49 [inlined]
   [17] adjoint at ./none:0 [inlined]
   [18] _pullback at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:47 [inlined]
   [19] ConvTranspose at /home/pkgeval/.julia/packages/Flux/05b38/src/layers/conv.jl:273 [inlined]
   [20] _pullback(::Zygote.Context, ::ConvTranspose{2,4,typeof(identity),CuArray{Float32,4},CuArray{Float32,1}}, ::CuArray{Float32,4}) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface2.jl:0
   [21] #135 at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:34 [inlined]
   [22] _pullback(::Zygote.Context, ::var"#135#139"{ConvTranspose{2,4,typeof(identity),CuArray{Float32,4},CuArray{Float32,1}}}) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface2.jl:0
   [23] pullback(::Function, ::Params) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface.jl:172
   [24] gradient(::Function, ::Params) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface.jl:53
   [25] macro expansion at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:34 [inlined]
   [26] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [27] macro expansion at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:26 [inlined]
   [28] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [29] gradtest(::String, ::Array{UnionAll,1}, ::Array{Float32,4}, ::Tuple{Int64,Int64}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:24
   [30] top-level scope at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:50
   [31] include(::String) at ./client.jl:457
   [32] top-level scope at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/runtests.jl:8
   [33] include(::String) at ./client.jl:457
   [34] top-level scope at /home/pkgeval/.julia/packages/Flux/05b38/test/runtests.jl:38
   [35] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [36] top-level scope at /home/pkgeval/.julia/packages/Flux/05b38/test/runtests.jl:37
   [37] include(::String) at ./client.jl:457
   [38] top-level scope at none:6
   [39] eval(::Module, ::Any) at ./boot.jl:331
   [40] exec_options(::Base.JLOptions) at ./client.jl:272
   [41] _start() at ./client.jl:506
  
CrossCor GPU grad test: Error During Test at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:33
  Test threw exception
  Expression: gradient((()->begin
                sum(l(xs))
            end), ps) isa Flux.Zygote.Grads
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn() at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73
   [4] (::CUDA.CUDNN.var"#19207#cache_fptr!#6")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:32 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/error.jl:28 [inlined]
   [9] cudnnGetProperty(::CUDA.libraryPropertyType, ::Base.RefValue{Int32}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:93
   [10] cudnnGetProperty(::CUDA.libraryPropertyType) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:16
   [11] version() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:20
   [12] conv!(::CuArray{Float32,4}, ::CuArray{Float32,4}, ::CuArray{Float32,4}, ::DenseConvDims{2,(2, 2),1,3,(1, 1),(0, 0, 0, 0),(1, 1),true}; alpha::Int64, algo::Int64) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/nnlib.jl:56
   [13] conv! at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/nnlib.jl:56 [inlined]
   [14] conv(::CuArray{Float32,4}, ::CuArray{Float32,4}, ::DenseConvDims{2,(2, 2),1,3,(1, 1),(0, 0, 0, 0),(1, 1),true}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/NNlib/PI8Xh/src/conv.jl:91
   [15] conv(::CuArray{Float32,4}, ::CuArray{Float32,4}, ::DenseConvDims{2,(2, 2),1,3,(1, 1),(0, 0, 0, 0),(1, 1),true}) at /home/pkgeval/.julia/packages/NNlib/PI8Xh/src/conv.jl:89
   [16] #adjoint#1238 at /home/pkgeval/.julia/packages/Zygote/seGHk/src/lib/nnlib.jl:38 [inlined]
   [17] adjoint at ./none:0 [inlined]
   [18] _pullback at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:47 [inlined]
   [19] crosscor at /home/pkgeval/.julia/packages/Flux/05b38/src/layers/conv.jl:496 [inlined]
   [20] _pullback(::Zygote.Context, ::typeof(Flux.crosscor), ::CuArray{Float32,4}, ::CuArray{Float32,4}, ::DenseConvDims{2,(2, 2),1,3,(1, 1),(0, 0, 0, 0),(1, 1),false}) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface2.jl:0
   [21] CrossCor at /home/pkgeval/.julia/packages/Flux/05b38/src/layers/conv.jl:504 [inlined]
   [22] _pullback(::Zygote.Context, ::CrossCor{2,4,typeof(identity),CuArray{Float32,4},CuArray{Float32,1}}, ::CuArray{Float32,4}) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface2.jl:0
   [23] #134 at ./none:0 [inlined]
   [24] _pullback(::Zygote.Context, ::var"#134#138"{CrossCor{2,4,typeof(identity),CuArray{Float32,4},CuArray{Float32,1}}}) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface2.jl:0
   [25] pullback(::Function, ::Params) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface.jl:172
   [26] gradient(::Function, ::Params) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface.jl:53
   [27] macro expansion at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:33 [inlined]
   [28] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [29] macro expansion at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:26 [inlined]
   [30] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [31] gradtest(::String, ::Array{UnionAll,1}, ::Array{Float32,4}, ::Tuple{Int64,Int64}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:24
  
CrossCor GPU grad test: Error During Test at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:25
  Got exception outside of a @test
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn() at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73
   [4] (::CUDA.CUDNN.var"#19207#cache_fptr!#6")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:32 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/error.jl:28 [inlined]
   [9] cudnnGetProperty(::CUDA.libraryPropertyType, ::Base.RefValue{Int32}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:93
   [10] cudnnGetProperty(::CUDA.libraryPropertyType) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:16
   [11] version() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:20
   [12] conv!(::CuArray{Float32,4}, ::CuArray{Float32,4}, ::CuArray{Float32,4}, ::DenseConvDims{2,(2, 2),1,3,(1, 1),(0, 0, 0, 0),(1, 1),true}; alpha::Int64, algo::Int64) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/nnlib.jl:56
   [13] conv! at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/nnlib.jl:56 [inlined]
   [14] conv(::CuArray{Float32,4}, ::CuArray{Float32,4}, ::DenseConvDims{2,(2, 2),1,3,(1, 1),(0, 0, 0, 0),(1, 1),true}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/NNlib/PI8Xh/src/conv.jl:91
   [15] conv(::CuArray{Float32,4}, ::CuArray{Float32,4}, ::DenseConvDims{2,(2, 2),1,3,(1, 1),(0, 0, 0, 0),(1, 1),true}) at /home/pkgeval/.julia/packages/NNlib/PI8Xh/src/conv.jl:89
   [16] #adjoint#1238 at /home/pkgeval/.julia/packages/Zygote/seGHk/src/lib/nnlib.jl:38 [inlined]
   [17] adjoint at ./none:0 [inlined]
   [18] _pullback at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:47 [inlined]
   [19] crosscor at /home/pkgeval/.julia/packages/Flux/05b38/src/layers/conv.jl:496 [inlined]
   [20] _pullback(::Zygote.Context, ::typeof(Flux.crosscor), ::CuArray{Float32,4}, ::CuArray{Float32,4}, ::DenseConvDims{2,(2, 2),1,3,(1, 1),(0, 0, 0, 0),(1, 1),false}) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface2.jl:0
   [21] CrossCor at /home/pkgeval/.julia/packages/Flux/05b38/src/layers/conv.jl:504 [inlined]
   [22] _pullback(::Zygote.Context, ::CrossCor{2,4,typeof(identity),CuArray{Float32,4},CuArray{Float32,1}}, ::CuArray{Float32,4}) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface2.jl:0
   [23] #135 at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:34 [inlined]
   [24] _pullback(::Zygote.Context, ::var"#135#139"{CrossCor{2,4,typeof(identity),CuArray{Float32,4},CuArray{Float32,1}}}) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface2.jl:0
   [25] pullback(::Function, ::Params) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface.jl:172
   [26] gradient(::Function, ::Params) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface.jl:53
   [27] macro expansion at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:34 [inlined]
   [28] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [29] macro expansion at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:26 [inlined]
   [30] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [31] gradtest(::String, ::Array{UnionAll,1}, ::Array{Float32,4}, ::Tuple{Int64,Int64}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:24
   [32] top-level scope at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:50
   [33] include(::String) at ./client.jl:457
   [34] top-level scope at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/runtests.jl:8
   [35] include(::String) at ./client.jl:457
   [36] top-level scope at /home/pkgeval/.julia/packages/Flux/05b38/test/runtests.jl:38
   [37] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [38] top-level scope at /home/pkgeval/.julia/packages/Flux/05b38/test/runtests.jl:37
   [39] include(::String) at ./client.jl:457
   [40] top-level scope at none:6
   [41] eval(::Module, ::Any) at ./boot.jl:331
   [42] exec_options(::Base.JLOptions) at ./client.jl:272
   [43] _start() at ./client.jl:506
  
MaxPool GPU grad test: Error During Test at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:33
  Test threw exception
  Expression: gradient((()->begin
                sum(l(xs))
            end), ps) isa Flux.Zygote.Grads
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] cudnnPoolingForward(::CuArray{Float32,4}, ::CuArray{Float32,4}, ::PoolDims{2,(2, 2),(2, 2),(0, 0, 0, 0),(1, 1)}; alpha::Int64, mode::Int64) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/pooling.jl:38
   [14] maxpool! at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/nnlib.jl:86 [inlined]
   [15] maxpool(::CuArray{Float32,4}, ::PoolDims{2,(2, 2),(2, 2),(0, 0, 0, 0),(1, 1)}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/NNlib/PI8Xh/src/pooling.jl:115
   [16] maxpool(::CuArray{Float32,4}, ::PoolDims{2,(2, 2),(2, 2),(0, 0, 0, 0),(1, 1)}) at /home/pkgeval/.julia/packages/NNlib/PI8Xh/src/pooling.jl:113
   [17] #adjoint#1258 at /home/pkgeval/.julia/packages/Zygote/seGHk/src/lib/nnlib.jl:83 [inlined]
   [18] adjoint at ./none:0 [inlined]
   [19] _pullback at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:47 [inlined]
   [20] MaxPool at /home/pkgeval/.julia/packages/Flux/05b38/src/layers/conv.jl:643 [inlined]
   [21] _pullback(::Zygote.Context, ::MaxPool{2,4}, ::CuArray{Float32,4}) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface2.jl:0
   [22] #134 at ./none:0 [inlined]
   [23] _pullback(::Zygote.Context, ::var"#134#138"{MaxPool{2,4}}) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface2.jl:0
   [24] pullback(::Function, ::Params) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface.jl:172
   [25] gradient(::Function, ::Params) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface.jl:53
   [26] macro expansion at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:33 [inlined]
   [27] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [28] macro expansion at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:26 [inlined]
   [29] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [30] gradtest(::String, ::Array{UnionAll,1}, ::Array{Float32,4}, ::Tuple{Int64,Int64}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:24
  
MaxPool GPU grad test: Error During Test at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:25
  Got exception outside of a @test
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] cudnnPoolingForward(::CuArray{Float32,4}, ::CuArray{Float32,4}, ::PoolDims{2,(2, 2),(2, 2),(0, 0, 0, 0),(1, 1)}; alpha::Int64, mode::Int64) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/pooling.jl:38
   [14] maxpool! at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/nnlib.jl:86 [inlined]
   [15] maxpool(::CuArray{Float32,4}, ::PoolDims{2,(2, 2),(2, 2),(0, 0, 0, 0),(1, 1)}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/NNlib/PI8Xh/src/pooling.jl:115
   [16] maxpool(::CuArray{Float32,4}, ::PoolDims{2,(2, 2),(2, 2),(0, 0, 0, 0),(1, 1)}) at /home/pkgeval/.julia/packages/NNlib/PI8Xh/src/pooling.jl:113
   [17] #adjoint#1258 at /home/pkgeval/.julia/packages/Zygote/seGHk/src/lib/nnlib.jl:83 [inlined]
   [18] adjoint at ./none:0 [inlined]
   [19] _pullback at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:47 [inlined]
   [20] MaxPool at /home/pkgeval/.julia/packages/Flux/05b38/src/layers/conv.jl:643 [inlined]
   [21] _pullback(::Zygote.Context, ::MaxPool{2,4}, ::CuArray{Float32,4}) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface2.jl:0
   [22] #135 at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:34 [inlined]
   [23] _pullback(::Zygote.Context, ::var"#135#139"{MaxPool{2,4}}) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface2.jl:0
   [24] pullback(::Function, ::Params) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface.jl:172
   [25] gradient(::Function, ::Params) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface.jl:53
   [26] macro expansion at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:34 [inlined]
   [27] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [28] macro expansion at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:26 [inlined]
   [29] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [30] gradtest(::String, ::Array{UnionAll,1}, ::Array{Float32,4}, ::Tuple{Int64,Int64}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:24
   [31] top-level scope at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:53
   [32] include(::String) at ./client.jl:457
   [33] top-level scope at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/runtests.jl:8
   [34] include(::String) at ./client.jl:457
   [35] top-level scope at /home/pkgeval/.julia/packages/Flux/05b38/test/runtests.jl:38
   [36] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [37] top-level scope at /home/pkgeval/.julia/packages/Flux/05b38/test/runtests.jl:37
   [38] include(::String) at ./client.jl:457
   [39] top-level scope at none:6
   [40] eval(::Module, ::Any) at ./boot.jl:331
   [41] exec_options(::Base.JLOptions) at ./client.jl:272
   [42] _start() at ./client.jl:506
  
MeanPool GPU grad test: Error During Test at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:33
  Test threw exception
  Expression: gradient((()->begin
                sum(l(xs))
            end), ps) isa Flux.Zygote.Grads
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] cudnnPoolingForward(::CuArray{Float32,4}, ::CuArray{Float32,4}, ::PoolDims{2,(2, 2),(2, 2),(0, 0, 0, 0),(1, 1)}; alpha::Int64, mode::Int64) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/pooling.jl:38
   [14] meanpool! at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/nnlib.jl:93 [inlined]
   [15] meanpool(::CuArray{Float32,4}, ::PoolDims{2,(2, 2),(2, 2),(0, 0, 0, 0),(1, 1)}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/NNlib/PI8Xh/src/pooling.jl:115
   [16] meanpool(::CuArray{Float32,4}, ::PoolDims{2,(2, 2),(2, 2),(0, 0, 0, 0),(1, 1)}) at /home/pkgeval/.julia/packages/NNlib/PI8Xh/src/pooling.jl:113
   [17] #adjoint#1263 at /home/pkgeval/.julia/packages/Zygote/seGHk/src/lib/nnlib.jl:88 [inlined]
   [18] adjoint at ./none:0 [inlined]
   [19] _pullback at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:47 [inlined]
   [20] MeanPool at /home/pkgeval/.julia/packages/Flux/05b38/src/layers/conv.jl:673 [inlined]
   [21] _pullback(::Zygote.Context, ::MeanPool{2,4}, ::CuArray{Float32,4}) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface2.jl:0
   [22] #134 at ./none:0 [inlined]
   [23] _pullback(::Zygote.Context, ::var"#134#138"{MeanPool{2,4}}) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface2.jl:0
   [24] pullback(::Function, ::Params) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface.jl:172
   [25] gradient(::Function, ::Params) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface.jl:53
   [26] macro expansion at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:33 [inlined]
   [27] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [28] macro expansion at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:26 [inlined]
   [29] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [30] gradtest(::String, ::Array{UnionAll,1}, ::Array{Float32,4}, ::Tuple{Int64,Int64}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:24
  
MeanPool GPU grad test: Error During Test at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:25
  Got exception outside of a @test
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] cudnnPoolingForward(::CuArray{Float32,4}, ::CuArray{Float32,4}, ::PoolDims{2,(2, 2),(2, 2),(0, 0, 0, 0),(1, 1)}; alpha::Int64, mode::Int64) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/pooling.jl:38
   [14] meanpool! at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/nnlib.jl:93 [inlined]
   [15] meanpool(::CuArray{Float32,4}, ::PoolDims{2,(2, 2),(2, 2),(0, 0, 0, 0),(1, 1)}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/NNlib/PI8Xh/src/pooling.jl:115
   [16] meanpool(::CuArray{Float32,4}, ::PoolDims{2,(2, 2),(2, 2),(0, 0, 0, 0),(1, 1)}) at /home/pkgeval/.julia/packages/NNlib/PI8Xh/src/pooling.jl:113
   [17] #adjoint#1263 at /home/pkgeval/.julia/packages/Zygote/seGHk/src/lib/nnlib.jl:88 [inlined]
   [18] adjoint at ./none:0 [inlined]
   [19] _pullback at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:47 [inlined]
   [20] MeanPool at /home/pkgeval/.julia/packages/Flux/05b38/src/layers/conv.jl:673 [inlined]
   [21] _pullback(::Zygote.Context, ::MeanPool{2,4}, ::CuArray{Float32,4}) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface2.jl:0
   [22] #135 at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:34 [inlined]
   [23] _pullback(::Zygote.Context, ::var"#135#139"{MeanPool{2,4}}) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface2.jl:0
   [24] pullback(::Function, ::Params) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface.jl:172
   [25] gradient(::Function, ::Params) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface.jl:53
   [26] macro expansion at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:34 [inlined]
   [27] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [28] macro expansion at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:26 [inlined]
   [29] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [30] gradtest(::String, ::Array{UnionAll,1}, ::Array{Float32,4}, ::Tuple{Int64,Int64}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:24
   [31] top-level scope at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:53
   [32] include(::String) at ./client.jl:457
   [33] top-level scope at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/runtests.jl:8
   [34] include(::String) at ./client.jl:457
   [35] top-level scope at /home/pkgeval/.julia/packages/Flux/05b38/test/runtests.jl:38
   [36] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [37] top-level scope at /home/pkgeval/.julia/packages/Flux/05b38/test/runtests.jl:37
   [38] include(::String) at ./client.jl:457
   [39] top-level scope at none:6
   [40] eval(::Module, ::Any) at ./boot.jl:331
   [41] exec_options(::Base.JLOptions) at ./client.jl:272
   [42] _start() at ./client.jl:506
  
AdaptiveMaxPool GPU grad test: Error During Test at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:33
  Test threw exception
  Expression: gradient((()->begin
                sum(l(xs))
            end), ps) isa Flux.Zygote.Grads
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] cudnnPoolingForward(::CuArray{Float32,4}, ::CuArray{Float32,4}, ::PoolDims{2,(4, 4),(4, 4),(0, 0, 0, 0),(1, 1)}; alpha::Int64, mode::Int64) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/pooling.jl:38
   [14] maxpool! at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/nnlib.jl:86 [inlined]
   [15] maxpool(::CuArray{Float32,4}, ::PoolDims{2,(4, 4),(4, 4),(0, 0, 0, 0),(1, 1)}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/NNlib/PI8Xh/src/pooling.jl:115
   [16] maxpool(::CuArray{Float32,4}, ::PoolDims{2,(4, 4),(4, 4),(0, 0, 0, 0),(1, 1)}) at /home/pkgeval/.julia/packages/NNlib/PI8Xh/src/pooling.jl:113
   [17] #adjoint#1258 at /home/pkgeval/.julia/packages/Zygote/seGHk/src/lib/nnlib.jl:83 [inlined]
   [18] adjoint at ./none:0 [inlined]
   [19] _pullback at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:47 [inlined]
   [20] AdaptiveMaxPool at /home/pkgeval/.julia/packages/Flux/05b38/src/layers/conv.jl:540 [inlined]
   [21] _pullback(::Zygote.Context, ::AdaptiveMaxPool{4,2}, ::CuArray{Float32,4}) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface2.jl:0
   [22] #134 at ./none:0 [inlined]
   [23] _pullback(::Zygote.Context, ::var"#134#138"{AdaptiveMaxPool{4,2}}) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface2.jl:0
   [24] pullback(::Function, ::Params) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface.jl:172
   [25] gradient(::Function, ::Params) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface.jl:53
   [26] macro expansion at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:33 [inlined]
   [27] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [28] macro expansion at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:26 [inlined]
   [29] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [30] gradtest(::String, ::Array{UnionAll,1}, ::Array{Float32,4}, ::Tuple{Int64,Int64}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:24
  
AdaptiveMaxPool GPU grad test: Error During Test at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:25
  Got exception outside of a @test
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] cudnnPoolingForward(::CuArray{Float32,4}, ::CuArray{Float32,4}, ::PoolDims{2,(4, 4),(4, 4),(0, 0, 0, 0),(1, 1)}; alpha::Int64, mode::Int64) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/pooling.jl:38
   [14] maxpool! at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/nnlib.jl:86 [inlined]
   [15] maxpool(::CuArray{Float32,4}, ::PoolDims{2,(4, 4),(4, 4),(0, 0, 0, 0),(1, 1)}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/NNlib/PI8Xh/src/pooling.jl:115
   [16] maxpool(::CuArray{Float32,4}, ::PoolDims{2,(4, 4),(4, 4),(0, 0, 0, 0),(1, 1)}) at /home/pkgeval/.julia/packages/NNlib/PI8Xh/src/pooling.jl:113
   [17] #adjoint#1258 at /home/pkgeval/.julia/packages/Zygote/seGHk/src/lib/nnlib.jl:83 [inlined]
   [18] adjoint at ./none:0 [inlined]
   [19] _pullback at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:47 [inlined]
   [20] AdaptiveMaxPool at /home/pkgeval/.julia/packages/Flux/05b38/src/layers/conv.jl:540 [inlined]
   [21] _pullback(::Zygote.Context, ::AdaptiveMaxPool{4,2}, ::CuArray{Float32,4}) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface2.jl:0
   [22] #135 at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:34 [inlined]
   [23] _pullback(::Zygote.Context, ::var"#135#139"{AdaptiveMaxPool{4,2}}) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface2.jl:0
   [24] pullback(::Function, ::Params) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface.jl:172
   [25] gradient(::Function, ::Params) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface.jl:53
   [26] macro expansion at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:34 [inlined]
   [27] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [28] macro expansion at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:26 [inlined]
   [29] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [30] gradtest(::String, ::Array{UnionAll,1}, ::Array{Float32,4}, ::Tuple{Int64,Int64}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:24
   [31] top-level scope at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:56
   [32] include(::String) at ./client.jl:457
   [33] top-level scope at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/runtests.jl:8
   [34] include(::String) at ./client.jl:457
   [35] top-level scope at /home/pkgeval/.julia/packages/Flux/05b38/test/runtests.jl:38
   [36] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [37] top-level scope at /home/pkgeval/.julia/packages/Flux/05b38/test/runtests.jl:37
   [38] include(::String) at ./client.jl:457
   [39] top-level scope at none:6
   [40] eval(::Module, ::Any) at ./boot.jl:331
   [41] exec_options(::Base.JLOptions) at ./client.jl:272
   [42] _start() at ./client.jl:506
  
AdaptiveMeanPool GPU grad test: Error During Test at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:33
  Test threw exception
  Expression: gradient((()->begin
                sum(l(xs))
            end), ps) isa Flux.Zygote.Grads
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] cudnnPoolingForward(::CuArray{Float32,4}, ::CuArray{Float32,4}, ::PoolDims{2,(4, 4),(4, 4),(0, 0, 0, 0),(1, 1)}; alpha::Int64, mode::Int64) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/pooling.jl:38
   [14] meanpool! at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/nnlib.jl:93 [inlined]
   [15] meanpool(::CuArray{Float32,4}, ::PoolDims{2,(4, 4),(4, 4),(0, 0, 0, 0),(1, 1)}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/NNlib/PI8Xh/src/pooling.jl:115
   [16] meanpool(::CuArray{Float32,4}, ::PoolDims{2,(4, 4),(4, 4),(0, 0, 0, 0),(1, 1)}) at /home/pkgeval/.julia/packages/NNlib/PI8Xh/src/pooling.jl:113
   [17] #adjoint#1263 at /home/pkgeval/.julia/packages/Zygote/seGHk/src/lib/nnlib.jl:88 [inlined]
   [18] adjoint at ./none:0 [inlined]
   [19] _pullback at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:47 [inlined]
   [20] AdaptiveMeanPool at /home/pkgeval/.julia/packages/Flux/05b38/src/layers/conv.jl:564 [inlined]
   [21] _pullback(::Zygote.Context, ::AdaptiveMeanPool{4,2}, ::CuArray{Float32,4}) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface2.jl:0
   [22] #134 at ./none:0 [inlined]
   [23] _pullback(::Zygote.Context, ::var"#134#138"{AdaptiveMeanPool{4,2}}) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface2.jl:0
   [24] pullback(::Function, ::Params) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface.jl:172
   [25] gradient(::Function, ::Params) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface.jl:53
   [26] macro expansion at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:33 [inlined]
   [27] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [28] macro expansion at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:26 [inlined]
   [29] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [30] gradtest(::String, ::Array{UnionAll,1}, ::Array{Float32,4}, ::Tuple{Int64,Int64}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:24
  
AdaptiveMeanPool GPU grad test: Error During Test at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:25
  Got exception outside of a @test
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19219#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/CUDNN.jl:43
   [13] cudnnPoolingForward(::CuArray{Float32,4}, ::CuArray{Float32,4}, ::PoolDims{2,(4, 4),(4, 4),(0, 0, 0, 0),(1, 1)}; alpha::Int64, mode::Int64) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/pooling.jl:38
   [14] meanpool! at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/nnlib.jl:93 [inlined]
   [15] meanpool(::CuArray{Float32,4}, ::PoolDims{2,(4, 4),(4, 4),(0, 0, 0, 0),(1, 1)}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/NNlib/PI8Xh/src/pooling.jl:115
   [16] meanpool(::CuArray{Float32,4}, ::PoolDims{2,(4, 4),(4, 4),(0, 0, 0, 0),(1, 1)}) at /home/pkgeval/.julia/packages/NNlib/PI8Xh/src/pooling.jl:113
   [17] #adjoint#1263 at /home/pkgeval/.julia/packages/Zygote/seGHk/src/lib/nnlib.jl:88 [inlined]
   [18] adjoint at ./none:0 [inlined]
   [19] _pullback at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:47 [inlined]
   [20] AdaptiveMeanPool at /home/pkgeval/.julia/packages/Flux/05b38/src/layers/conv.jl:564 [inlined]
   [21] _pullback(::Zygote.Context, ::AdaptiveMeanPool{4,2}, ::CuArray{Float32,4}) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface2.jl:0
   [22] #135 at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:34 [inlined]
   [23] _pullback(::Zygote.Context, ::var"#135#139"{AdaptiveMeanPool{4,2}}) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface2.jl:0
   [24] pullback(::Function, ::Params) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface.jl:172
   [25] gradient(::Function, ::Params) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface.jl:53
   [26] macro expansion at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:34 [inlined]
   [27] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [28] macro expansion at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:26 [inlined]
   [29] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [30] gradtest(::String, ::Array{UnionAll,1}, ::Array{Float32,4}, ::Tuple{Int64,Int64}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:24
   [31] top-level scope at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:56
   [32] include(::String) at ./client.jl:457
   [33] top-level scope at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/runtests.jl:8
   [34] include(::String) at ./client.jl:457
   [35] top-level scope at /home/pkgeval/.julia/packages/Flux/05b38/test/runtests.jl:38
   [36] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [37] top-level scope at /home/pkgeval/.julia/packages/Flux/05b38/test/runtests.jl:37
   [38] include(::String) at ./client.jl:457
   [39] top-level scope at none:6
   [40] eval(::Module, ::Any) at ./boot.jl:331
   [41] exec_options(::Base.JLOptions) at ./client.jl:272
   [42] _start() at ./client.jl:506
  
BatchNorm GPU grad test: Error During Test at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:33
  Test threw exception
  Expression: gradient((()->begin
                sum(l(xs))
            end), ps) isa Flux.Zygote.Grads
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19252#cache_fptr!#16")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:67 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/error.jl:28 [inlined]
   [9] cudnnCreateTensorDescriptor(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:93
   [10] CUDA.CUDNN.TensorDesc(::Type{T} where T, ::NTuple{4,Int64}, ::NTuple{4,Int64}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/tensor.jl:15
   [11] TensorDesc at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/tensor.jl:22 [inlined]
   [12] cudnnBNForward!(::CuArray{Float32,4}, ::CuArray{Float32,1}, ::CuArray{Float32,1}, ::CuArray{Float32,4}, ::CuArray{Float32,1}, ::CuArray{Float32,1}, ::Float32; cache::Nothing, alpha::Int64, beta::Int64, eps::Float32, training::Bool) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/batchnorm.jl:41
   [13] #batchnorm#478 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/batchnorm.jl:26 [inlined]
   [14] #adjoint#17 at /home/pkgeval/.julia/packages/Flux/05b38/src/cuda/cudnn.jl:6 [inlined]
   [15] _pullback at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:53 [inlined]
   [16] BatchNorm at /home/pkgeval/.julia/packages/Flux/05b38/src/cuda/cudnn.jl:3 [inlined] (repeats 2 times)
   [17] _pullback(::Zygote.Context, ::BatchNorm{typeof(identity),CuArray{Float32,1},CuArray{Float32,1},Float32}, ::CuArray{Float32,4}) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface2.jl:0
   [18] #134 at ./none:0 [inlined]
   [19] _pullback(::Zygote.Context, ::var"#134#138"{BatchNorm{typeof(identity),CuArray{Float32,1},CuArray{Float32,1},Float32}}) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface2.jl:0
   [20] pullback(::Function, ::Params) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface.jl:172
   [21] gradient(::Function, ::Params) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface.jl:53
   [22] macro expansion at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:33 [inlined]
   [23] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:26 [inlined]
   [25] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [26] gradtest(::String, ::Array{UnionAll,1}, ::Array{Float32,4}, ::Int64) at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:24
  
BatchNorm GPU grad test: Error During Test at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:25
  Got exception outside of a @test
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/7vLVC/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19252#cache_fptr!#16")() at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:39 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/libcudnn.jl:67 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/src/pool.jl:338 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/error.jl:28 [inlined]
   [9] cudnnCreateTensorDescriptor(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/utils/call.jl:93
   [10] CUDA.CUDNN.TensorDesc(::Type{T} where T, ::NTuple{4,Int64}, ::NTuple{4,Int64}) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/tensor.jl:15
   [11] TensorDesc at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/tensor.jl:22 [inlined]
   [12] cudnnBNForward!(::CuArray{Float32,4}, ::CuArray{Float32,1}, ::CuArray{Float32,1}, ::CuArray{Float32,4}, ::CuArray{Float32,1}, ::CuArray{Float32,1}, ::Float32; cache::Nothing, alpha::Int64, beta::Int64, eps::Float32, training::Bool) at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/batchnorm.jl:41
   [13] #batchnorm#478 at /home/pkgeval/.julia/packages/CUDA/7vLVC/lib/cudnn/batchnorm.jl:26 [inlined]
   [14] #adjoint#17 at /home/pkgeval/.julia/packages/Flux/05b38/src/cuda/cudnn.jl:6 [inlined]
   [15] _pullback at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:53 [inlined]
   [16] BatchNorm at /home/pkgeval/.julia/packages/Flux/05b38/src/cuda/cudnn.jl:3 [inlined] (repeats 2 times)
   [17] _pullback(::Zygote.Context, ::BatchNorm{typeof(identity),CuArray{Float32,1},CuArray{Float32,1},Float32}, ::CuArray{Float32,4}) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface2.jl:0
   [18] #135 at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:34 [inlined]
   [19] _pullback(::Zygote.Context, ::var"#135#139"{BatchNorm{typeof(identity),CuArray{Float32,1},CuArray{Float32,1},Float32}}) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface2.jl:0
   [20] pullback(::Function, ::Params) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface.jl:172
   [21] gradient(::Function, ::Params) at /home/pkgeval/.julia/packages/Zygote/seGHk/src/compiler/interface.jl:53
   [22] macro expansion at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:34 [inlined]
   [23] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [24] macro expansion at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:26 [inlined]
   [25] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [26] gradtest(::String, ::Array{UnionAll,1}, ::Array{Float32,4}, ::Int64) at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:24
   [27] top-level scope at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/layers.jl:62
   [28] include(::String) at ./client.jl:457
   [29] top-level scope at /home/pkgeval/.julia/packages/Flux/05b38/test/cuda/runtests.jl:8
   [30] include(::String) at ./client.jl:457
   [31] top-level scope at /home/pkgeval/.julia/packages/Flux/05b38/test/runtests.jl:38
   [32] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [33] top-level scope at /home/pkgeval/.julia/packages/Flux/05b38/test/runtests.jl:37
   [34] include(::String) at ./client.jl:457
   [35] top-level scope at none:6
   [36] eval(::Module, ::Any) at ./boot.jl:331
   [37] exec_options(::Base.JLOptions) at ./client.jl:272
   [38] _start() at ./client.jl:506
  
┌ Warning: CUDNN unavailable, not testing GPU DNN support
└ @ Main ~/.julia/packages/Flux/05b38/test/cuda/runtests.jl:15
Test Summary:                      | Pass  Error  Broken  Total
CUDA                               |   65     17       4     86
  CUDA                             |    7      1              8
  onecold gpu                      |    2                     2
  restructure gpu                  |    1                     1
  GPU functors                     |    2                     2
  Losses                           |   47                    47
  Basic GPU Movement               |    2                     2
  Conv GPU grad tests              |           6       1      7
    Conv GPU grad test             |           2              2
    ConvTranspose GPU grad test    |           2              2
    CrossCor GPU grad test         |           2              2
    DepthwiseConv GPU grad test    |                   1      1
  Pooling GPU grad tests           |           4              4
    MaxPool GPU grad test          |           2              2
    MeanPool GPU grad test         |           2              2
  AdaptivePooling GPU grad tests   |           4              4
    AdaptiveMaxPool GPU grad test  |           2              2
    AdaptiveMeanPool GPU grad test |           2              2
  Dropout GPU grad tests           |    1              1      2
  Normalising GPU grad tests       |    2      2              4
    LayerNorm GPU grad test        |    2                     2
    BatchNorm GPU grad test        |           2              2
  InstanceNorm GPU grad tests      |                   1      1
  GroupNorm GPU grad tests         |                   1      1
  Stateless GPU grad tests         |    1                     1
ERROR: LoadError: Some tests did not pass: 65 passed, 0 failed, 17 errored, 4 broken.
in expression starting at /home/pkgeval/.julia/packages/Flux/05b38/test/runtests.jl:36
ERROR: Package Flux errored during testing
Stacktrace:
 [1] pkgerror(::String) at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/Types.jl:52
 [2] test(::Pkg.Types.Context, ::Array{Pkg.Types.PackageSpec,1}; coverage::Bool, julia_args::Cmd, test_args::Cmd, test_fn::Nothing) at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/Operations.jl:1566
 [3] test(::Pkg.Types.Context, ::Array{Pkg.Types.PackageSpec,1}; coverage::Bool, test_fn::Nothing, julia_args::Cmd, test_args::Cmd, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:328
 [4] test(::Pkg.Types.Context, ::Array{Pkg.Types.PackageSpec,1}) at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:315
 [5] #test#61 at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:67 [inlined]
 [6] test at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:67 [inlined]
 [7] #test#60 at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:66 [inlined]
 [8] test at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:66 [inlined]
 [9] test(::String; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:65
 [10] test(::String) at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:65
 [11] top-level scope at none:16
