Julia Version 1.5.0-DEV.264
Commit 7d47855e1f (2020-02-10 22:29 UTC)
Platform Info:
  OS: Linux (x86_64-linux-gnu)
  CPU: Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-9.0.1 (ORCJIT, skylake)
Environment:
  JULIA_DEPOT_PATH = ::/usr/local/share/julia
  JULIA_NUM_THREADS = 2

  Resolving package versions...
  Installed Missings ─────────── v0.4.3
  Installed URIParser ────────── v0.4.0
  Installed LegacyStrings ────── v0.4.1
  Installed GaussianMixtures ─── v0.3.0
  Installed Arpack_jll ───────── v3.5.0+2
  Installed Compat ───────────── v2.2.0
  Installed SpecialFunctions ─── v0.9.0
  Installed DataStructures ───── v0.17.9
  Installed SortingAlgorithms ── v0.3.1
  Installed Distances ────────── v0.8.2
  Installed Rmath ────────────── v0.6.0
  Installed OrderedCollections ─ v1.1.0
  Installed StatsFuns ────────── v0.9.3
  Installed StaticArrays ─────── v0.12.1
  Installed StatsBase ────────── v0.32.0
  Installed FileIO ───────────── v1.2.2
  Installed Clustering ───────── v0.13.3
  Installed Distributions ────── v0.22.4
  Installed Blosc ────────────── v0.5.1
  Installed BinaryProvider ───── v0.5.8
  Installed NearestNeighbors ─── v0.4.4
  Installed FillArrays ───────── v0.8.4
  Installed ScikitLearnBase ──── v0.5.0
  Installed DataAPI ──────────── v1.1.0
  Installed CMakeWrapper ─────── v0.2.3
  Installed CMake ────────────── v1.1.2
  Installed Arpack ───────────── v0.4.0
  Installed OpenBLAS_jll ─────── v0.3.7+5
  Installed PDMats ───────────── v0.9.11
  Installed JLD ──────────────── v0.9.2
  Installed BinDeps ──────────── v1.0.0
  Installed Parameters ───────── v0.12.0
  Installed OpenSpecFun_jll ──── v0.5.3+1
  Installed HDF5 ─────────────── v0.12.5
  Installed QuadGK ───────────── v2.3.1
#=#=#                                                                         ###############                                                           21.8%######################################################################## 100.0%
#=#=#                                                                                                                                                    0.3%##                                                                         3.4%#####                                                                      8.0%##########                                                                14.1%################                                                          22.3%######################                                                    31.3%##########################                                                36.9%#####################################                                     52.1%##################################################                        69.8%####################################################################      95.1%######################################################################## 100.0%
#=#=#                                                                         ##############################                                            42.5%######################################################################## 100.0%
   Updating `~/.julia/environments/v1.5/Project.toml`
  [cc18c42c] + GaussianMixtures v0.3.0
   Updating `~/.julia/environments/v1.5/Manifest.toml`
  [7d9fca2a] + Arpack v0.4.0
  [68821587] + Arpack_jll v3.5.0+2
  [9e28174c] + BinDeps v1.0.0
  [b99e7846] + BinaryProvider v0.5.8
  [a74b3585] + Blosc v0.5.1
  [631607c0] + CMake v1.1.2
  [d5fb7624] + CMakeWrapper v0.2.3
  [aaaa29a8] + Clustering v0.13.3
  [34da2185] + Compat v2.2.0
  [9a962f9c] + DataAPI v1.1.0
  [864edb3b] + DataStructures v0.17.9
  [b4f34e82] + Distances v0.8.2
  [31c24e10] + Distributions v0.22.4
  [5789e2e9] + FileIO v1.2.2
  [1a297f60] + FillArrays v0.8.4
  [cc18c42c] + GaussianMixtures v0.3.0
  [f67ccb44] + HDF5 v0.12.5
  [4138dd39] + JLD v0.9.2
  [1b4a561d] + LegacyStrings v0.4.1
  [e1d29d7a] + Missings v0.4.3
  [b8a86587] + NearestNeighbors v0.4.4
  [4536629a] + OpenBLAS_jll v0.3.7+5
  [efe28fd5] + OpenSpecFun_jll v0.5.3+1
  [bac558e1] + OrderedCollections v1.1.0
  [90014a1f] + PDMats v0.9.11
  [d96e819e] + Parameters v0.12.0
  [1fd47b50] + QuadGK v2.3.1
  [79098fc4] + Rmath v0.6.0
  [6e75b9c4] + ScikitLearnBase v0.5.0
  [a2af1166] + SortingAlgorithms v0.3.1
  [276daf66] + SpecialFunctions v0.9.0
  [90137ffa] + StaticArrays v0.12.1
  [2913bbd2] + StatsBase v0.32.0
  [4c63d2b9] + StatsFuns v0.9.3
  [30578b45] + URIParser v0.4.0
  [2a0f44e3] + Base64 
  [ade2ca70] + Dates 
  [8bb1440f] + DelimitedFiles 
  [8ba89e20] + Distributed 
  [b77e0a4c] + InteractiveUtils 
  [76f85450] + LibGit2 
  [8f399da3] + Libdl 
  [37e2e46d] + LinearAlgebra 
  [56ddb016] + Logging 
  [d6f4376e] + Markdown 
  [a63ad114] + Mmap 
  [44cfe95a] + Pkg 
  [de0858da] + Printf 
  [3fa0cd96] + REPL 
  [9a3f8284] + Random 
  [ea8e919c] + SHA 
  [9e88b42a] + Serialization 
  [1a1011a3] + SharedArrays 
  [6462fe0b] + Sockets 
  [2f01184e] + SparseArrays 
  [10745b16] + Statistics 
  [4607b0f0] + SuiteSparse 
  [8dfed614] + Test 
  [cf7118a7] + UUIDs 
  [4ec0a83e] + Unicode 
   Building Rmath → `~/.julia/packages/Rmath/BoBag/deps/build.log`
   Building CMake → `~/.julia/packages/CMake/nSK2r/deps/build.log`
   Building Blosc → `~/.julia/packages/Blosc/lzFr0/deps/build.log`
   Building HDF5 ─→ `~/.julia/packages/HDF5/Zh9on/deps/build.log`
    Testing GaussianMixtures
Status `/tmp/jl_30O8wL/Manifest.toml`
  [7d9fca2a] Arpack v0.4.0
  [68821587] Arpack_jll v3.5.0+2
  [9e28174c] BinDeps v1.0.0
  [b99e7846] BinaryProvider v0.5.8
  [a74b3585] Blosc v0.5.1
  [631607c0] CMake v1.1.2
  [d5fb7624] CMakeWrapper v0.2.3
  [aaaa29a8] Clustering v0.13.3
  [34da2185] Compat v2.2.0
  [9a962f9c] DataAPI v1.1.0
  [864edb3b] DataStructures v0.17.9
  [b4f34e82] Distances v0.8.2
  [31c24e10] Distributions v0.22.4
  [5789e2e9] FileIO v1.2.2
  [1a297f60] FillArrays v0.8.4
  [cc18c42c] GaussianMixtures v0.3.0
  [f67ccb44] HDF5 v0.12.5
  [4138dd39] JLD v0.9.2
  [1b4a561d] LegacyStrings v0.4.1
  [e1d29d7a] Missings v0.4.3
  [b8a86587] NearestNeighbors v0.4.4
  [4536629a] OpenBLAS_jll v0.3.7+5
  [efe28fd5] OpenSpecFun_jll v0.5.3+1
  [bac558e1] OrderedCollections v1.1.0
  [90014a1f] PDMats v0.9.11
  [d96e819e] Parameters v0.12.0
  [1fd47b50] QuadGK v2.3.1
  [79098fc4] Rmath v0.6.0
  [6e75b9c4] ScikitLearnBase v0.5.0
  [a2af1166] SortingAlgorithms v0.3.1
  [276daf66] SpecialFunctions v0.9.0
  [90137ffa] StaticArrays v0.12.1
  [2913bbd2] StatsBase v0.32.0
  [4c63d2b9] StatsFuns v0.9.3
  [30578b45] URIParser v0.4.0
  [2a0f44e3] Base64 
  [ade2ca70] Dates 
  [8bb1440f] DelimitedFiles 
  [8ba89e20] Distributed 
  [b77e0a4c] InteractiveUtils 
  [76f85450] LibGit2 
  [8f399da3] Libdl 
  [37e2e46d] LinearAlgebra 
  [56ddb016] Logging 
  [d6f4376e] Markdown 
  [a63ad114] Mmap 
  [44cfe95a] Pkg 
  [de0858da] Printf 
  [3fa0cd96] REPL 
  [9a3f8284] Random 
  [ea8e919c] SHA 
  [9e88b42a] Serialization 
  [1a1011a3] SharedArrays 
  [6462fe0b] Sockets 
  [2f01184e] SparseArrays 
  [10745b16] Statistics 
  [4607b0f0] SuiteSparse 
  [8dfed614] Test 
  [cf7118a7] UUIDs 
  [4ec0a83e] Unicode 
[ Info: Testing Data
(100000, -4.080352458993212e6, [78102.91497071854, 21897.08502928146], [16837.757529367085 7969.148886677265 -9117.786796524548; -16989.623625950666 -7896.095281648339 8986.08804250975], [[70364.19592338579 -4198.7609787301735 -1119.727192617109; -4198.7609787301735 86200.2540867639 -6431.93589778906; -1119.7271926171088 -6431.935897789059 77804.76073867729], [30389.841307460443 4725.252632876214 1020.9679692747644; 4725.252632876215 14627.52568814089 6620.333834372283; 1020.9679692747644 6620.3338343722835 22827.155290866034]])
┌ Warning: rmprocs: process 1 not removed
└ @ Distributed /workspace/srcdir/usr/share/julia/stdlib/v1.5/Distributed/src/cluster.jl:1030
[ Info: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.890618e+03
      1       1.182146e+03      -7.084725e+02 |        8
      2       9.637378e+02      -2.184078e+02 |        7
      3       8.790024e+02      -8.473538e+01 |        0
      4       8.790024e+02       0.000000e+00 |        0
K-means converged with 4 iterations (objv = 879.0024300735827)
┌ Info: K-means with 272 data points using 4 iterations
└ 11.3 data points per parameter
[ Info: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
┌ Info: EM with 272 data points 0 iterations avll -2.076890
└ 5.8 data points per parameter
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:221
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:221
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:221
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:221
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:631 [inlined]
└ @ Core ./broadcast.jl:631
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:230
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:230
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:631 [inlined]
└ @ Core ./broadcast.jl:631
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:631 [inlined]
└ @ Core ./broadcast.jl:631
[ Info: iteration 1, lowerbound -3.843560
[ Info: iteration 2, lowerbound -3.720064
[ Info: iteration 3, lowerbound -3.577930
[ Info: iteration 4, lowerbound -3.402516
[ Info: iteration 5, lowerbound -3.216222
[ Info: iteration 6, lowerbound -3.046305
[ Info: dropping number of Gaussions to 6
[ Info: iteration 7, lowerbound -2.897575
[ Info: iteration 8, lowerbound -2.773845
[ Info: iteration 9, lowerbound -2.694538
[ Info: dropping number of Gaussions to 4
[ Info: iteration 10, lowerbound -2.627188
[ Info: dropping number of Gaussions to 3
[ Info: iteration 11, lowerbound -2.548494
[ Info: iteration 12, lowerbound -2.476928
[ Info: iteration 13, lowerbound -2.418519
[ Info: iteration 14, lowerbound -2.373427
[ Info: iteration 15, lowerbound -2.340008
[ Info: iteration 16, lowerbound -2.317311
[ Info: iteration 17, lowerbound -2.307632
[ Info: dropping number of Gaussions to 2
[ Info: iteration 18, lowerbound -2.303000
[ Info: iteration 19, lowerbound -2.299263
[ Info: iteration 20, lowerbound -2.299257
[ Info: iteration 21, lowerbound -2.299255
[ Info: iteration 22, lowerbound -2.299254
[ Info: iteration 23, lowerbound -2.299253
[ Info: iteration 24, lowerbound -2.299253
[ Info: iteration 25, lowerbound -2.299253
[ Info: iteration 26, lowerbound -2.299253
[ Info: iteration 27, lowerbound -2.299253
[ Info: iteration 28, lowerbound -2.299253
[ Info: iteration 29, lowerbound -2.299253
[ Info: iteration 30, lowerbound -2.299253
[ Info: iteration 31, lowerbound -2.299253
[ Info: iteration 32, lowerbound -2.299253
[ Info: iteration 33, lowerbound -2.299253
[ Info: iteration 34, lowerbound -2.299253
[ Info: iteration 35, lowerbound -2.299253
[ Info: iteration 36, lowerbound -2.299253
[ Info: iteration 37, lowerbound -2.299253
[ Info: iteration 38, lowerbound -2.299253
[ Info: iteration 39, lowerbound -2.299253
[ Info: iteration 40, lowerbound -2.299253
[ Info: iteration 41, lowerbound -2.299253
[ Info: iteration 42, lowerbound -2.299253
[ Info: iteration 43, lowerbound -2.299253
[ Info: iteration 44, lowerbound -2.299253
[ Info: iteration 45, lowerbound -2.299253
[ Info: iteration 46, lowerbound -2.299253
[ Info: iteration 47, lowerbound -2.299253
[ Info: iteration 48, lowerbound -2.299253
[ Info: iteration 49, lowerbound -2.299253
[ Info: iteration 50, lowerbound -2.299253
[ Info: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
History[Tue Feb 11 15:03:05 2020: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
, Tue Feb 11 15:03:13 2020: K-means with 272 data points using 4 iterations
11.3 data points per parameter
, Tue Feb 11 15:03:15 2020: EM with 272 data points 0 iterations avll -2.076890
5.8 data points per parameter
, Tue Feb 11 15:03:17 2020: GMM converted to Variational GMM
, Tue Feb 11 15:03:26 2020: iteration 1, lowerbound -3.843560
, Tue Feb 11 15:03:26 2020: iteration 2, lowerbound -3.720064
, Tue Feb 11 15:03:26 2020: iteration 3, lowerbound -3.577930
, Tue Feb 11 15:03:26 2020: iteration 4, lowerbound -3.402516
, Tue Feb 11 15:03:26 2020: iteration 5, lowerbound -3.216222
, Tue Feb 11 15:03:26 2020: iteration 6, lowerbound -3.046305
, Tue Feb 11 15:03:26 2020: dropping number of Gaussions to 6
, Tue Feb 11 15:03:26 2020: iteration 7, lowerbound -2.897575
, Tue Feb 11 15:03:26 2020: iteration 8, lowerbound -2.773845
, Tue Feb 11 15:03:26 2020: iteration 9, lowerbound -2.694538
, Tue Feb 11 15:03:26 2020: dropping number of Gaussions to 4
, Tue Feb 11 15:03:26 2020: iteration 10, lowerbound -2.627188
, Tue Feb 11 15:03:26 2020: dropping number of Gaussions to 3
, Tue Feb 11 15:03:26 2020: iteration 11, lowerbound -2.548494
, Tue Feb 11 15:03:26 2020: iteration 12, lowerbound -2.476928
, Tue Feb 11 15:03:26 2020: iteration 13, lowerbound -2.418519
, Tue Feb 11 15:03:26 2020: iteration 14, lowerbound -2.373427
, Tue Feb 11 15:03:26 2020: iteration 15, lowerbound -2.340008
, Tue Feb 11 15:03:26 2020: iteration 16, lowerbound -2.317311
, Tue Feb 11 15:03:26 2020: iteration 17, lowerbound -2.307632
, Tue Feb 11 15:03:26 2020: dropping number of Gaussions to 2
, Tue Feb 11 15:03:26 2020: iteration 18, lowerbound -2.303000
, Tue Feb 11 15:03:26 2020: iteration 19, lowerbound -2.299263
, Tue Feb 11 15:03:26 2020: iteration 20, lowerbound -2.299257
, Tue Feb 11 15:03:26 2020: iteration 21, lowerbound -2.299255
, Tue Feb 11 15:03:26 2020: iteration 22, lowerbound -2.299254
, Tue Feb 11 15:03:26 2020: iteration 23, lowerbound -2.299253
, Tue Feb 11 15:03:26 2020: iteration 24, lowerbound -2.299253
, Tue Feb 11 15:03:26 2020: iteration 25, lowerbound -2.299253
, Tue Feb 11 15:03:26 2020: iteration 26, lowerbound -2.299253
, Tue Feb 11 15:03:26 2020: iteration 27, lowerbound -2.299253
, Tue Feb 11 15:03:26 2020: iteration 28, lowerbound -2.299253
, Tue Feb 11 15:03:26 2020: iteration 29, lowerbound -2.299253
, Tue Feb 11 15:03:26 2020: iteration 30, lowerbound -2.299253
, Tue Feb 11 15:03:26 2020: iteration 31, lowerbound -2.299253
, Tue Feb 11 15:03:26 2020: iteration 32, lowerbound -2.299253
, Tue Feb 11 15:03:26 2020: iteration 33, lowerbound -2.299253
, Tue Feb 11 15:03:26 2020: iteration 34, lowerbound -2.299253
, Tue Feb 11 15:03:26 2020: iteration 35, lowerbound -2.299253
, Tue Feb 11 15:03:26 2020: iteration 36, lowerbound -2.299253
, Tue Feb 11 15:03:26 2020: iteration 37, lowerbound -2.299253
, Tue Feb 11 15:03:26 2020: iteration 38, lowerbound -2.299253
, Tue Feb 11 15:03:26 2020: iteration 39, lowerbound -2.299253
, Tue Feb 11 15:03:26 2020: iteration 40, lowerbound -2.299253
, Tue Feb 11 15:03:26 2020: iteration 41, lowerbound -2.299253
, Tue Feb 11 15:03:26 2020: iteration 42, lowerbound -2.299253
, Tue Feb 11 15:03:26 2020: iteration 43, lowerbound -2.299253
, Tue Feb 11 15:03:26 2020: iteration 44, lowerbound -2.299253
, Tue Feb 11 15:03:26 2020: iteration 45, lowerbound -2.299253
, Tue Feb 11 15:03:26 2020: iteration 46, lowerbound -2.299253
, Tue Feb 11 15:03:26 2020: iteration 47, lowerbound -2.299253
, Tue Feb 11 15:03:26 2020: iteration 48, lowerbound -2.299253
, Tue Feb 11 15:03:26 2020: iteration 49, lowerbound -2.299253
, Tue Feb 11 15:03:26 2020: iteration 50, lowerbound -2.299253
, Tue Feb 11 15:03:26 2020: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [178.04509222601396, 95.95490777398604]
β = [178.04509222601396, 95.95490777398604]
m = [4.250300733269909 79.28686694436183; 2.00022925777537 53.851987172461286]
ν = [180.04509222601396, 97.95490777398604]
W = LinearAlgebra.UpperTriangular{Float64,Array{Float64,2}}[[0.18404155547484555 -0.007644049042327318; 0.0 0.00858170516633351], [0.3758763611948448 -0.008953123827345956; 0.0 0.012748664777409255]]
┌ Warning: Assignment to `x` in soft scope is ambiguous because a global variable by the same name exists: `x` will be treated as a new local. Disambiguate by using `local x` to suppress this warning or `global x` to assign to the existing global variable.
└ @ ~/.julia/packages/GaussianMixtures/RGtTJ/test/train.jl:7
┌ Warning: Assignment to `p` in soft scope is ambiguous because a global variable by the same name exists: `p` will be treated as a new local. Disambiguate by using `local p` to suppress this warning or `global p` to assign to the existing global variable.
└ @ ~/.julia/packages/GaussianMixtures/RGtTJ/test/train.jl:17
Kind: diag, size256
nx: 100000 sum(zeroth order stats): 100000.0
avll from stats: -1.0045051499501898
avll from llpg:  -1.0045051499501907
avll direct:     -1.0045051499501907
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 100000.0
avll from stats: -1.0115756571349879
avll from llpg:  -1.0115756571349879
avll direct:     -1.0115756571349879
sum posterior: 100000.0
┌ Warning: Assignment to `x` in soft scope is ambiguous because a global variable by the same name exists: `x` will be treated as a new local. Disambiguate by using `local x` to suppress this warning or `global x` to assign to the existing global variable.
└ @ ~/.julia/packages/GaussianMixtures/RGtTJ/test/train.jl:26
32×26 Array{Float64,2}:
 -0.151989    -0.0358869   -0.012844     0.175086      0.329215    -0.218015    -0.16453     0.0800168    0.103667     0.258565     0.0826837    0.0221571    0.0650582   -0.115671    -0.102885      0.000126468   0.16932     -0.0303733   -0.0267292   -0.0476232    0.0474219    0.0685238   -0.0397367   -0.0955885     0.00819662   -0.0649831
  0.131276    -0.0887747    0.036259     0.00898905   -0.0335432    0.103669     0.0248249   0.199428    -0.0249621   -0.147338    -0.162864    -0.227851    -0.1243      -0.0769621   -0.0250532     0.0497682     0.067771     0.115263     0.00723971   0.105983     0.0880621    0.111318    -0.0596135   -0.0200182    -0.0901999    -0.134351
  0.00563576  -0.0418284   -0.162166     0.0465089     0.180973     0.0173608    0.013871   -0.0124763   -0.00872297  -0.0279919   -0.00578366  -0.0668606   -0.109396     0.0625429    0.182147      0.105629      0.0518968    0.0605691    0.167479    -0.0459111   -0.0989112    0.0304707   -0.00456925   0.0259483     0.135989      0.187244
  0.109676     0.162447    -0.0324107   -0.0769106     0.100295     0.0664044    0.0106568  -0.0268721    0.125534    -0.0974481   -0.0445123   -0.0514092    0.0657661    0.0342371   -0.0989792     0.113523      0.0363453    0.127336    -0.0286305    0.0333689    0.0600537   -0.120203    -0.0917843   -0.0707407     0.0319033     0.0561734
  0.0934674    0.00299143   0.0821604   -0.0299338     0.0476183   -0.111581    -0.0145233   0.203245    -0.0779552   -0.00821918   0.179575    -0.0104127    0.220836     0.11516      0.146491      0.0167536    -0.0375747    0.0691097   -0.0639022   -0.114055    -0.0245878   -0.172081    -0.233747     0.0200632    -0.0374604     0.190033
 -0.0222994    0.166368     0.0713189    0.0782614    -0.187508     0.0747062   -0.0166365   0.0875468    0.0755286    0.0314704   -0.019222     0.120588     0.00117632   0.179296     0.182243      0.0769951     0.0707      -0.0565813    0.0439112    0.131831     0.115955     0.116252    -0.218743     0.0591981    -0.0857473    -0.111109
 -0.0223446    0.161349    -0.0436577    0.072019      0.106609     0.127717    -0.0571561  -0.10745      0.206209     0.0288576   -0.11515      0.106488    -0.0961889   -0.148709    -0.101186      0.120886      0.0222206   -0.194348     0.0629257    0.0445475    0.0814764    0.151815     0.0543393    0.140724     -0.0370444    -0.0279906
 -0.065882     0.130397    -0.0659143    0.204623     -0.0681833    0.0811237    0.107143   -0.0384123    0.133202    -0.112018    -0.0781782   -0.0372152    0.037687     0.0198882    0.133175      0.0637462     0.232175    -0.138761    -0.0662922    0.0518411   -0.0951696   -0.0430498   -0.0410702   -0.0534802    -0.0800911     0.0487562
 -0.0209941    0.0699967   -0.0705461    0.0250993    -0.02156      0.180022     0.0356045  -0.0460491   -0.0467806   -0.153148    -0.219071     0.241616    -0.158158    -0.284652     0.00301132    0.0109776     0.0711503   -0.0688419   -0.0791983   -0.204454     0.143141     0.162634    -0.0843695    0.0224209    -0.0302028     0.0471632
 -0.078105    -0.111534     0.0422314    0.157795     -0.0147981   -0.0637887    0.0707626  -0.202278    -0.0525507   -0.0747066   -0.0083544   -0.0942776    0.0974954   -0.135463     0.0980769     0.0453861     0.0510582   -0.0140554   -0.0895951    0.10093      0.0292296   -0.0946403    0.162477     0.0941392     0.104277      0.0973347
 -0.0393814   -0.0902105   -0.0161822   -0.000766449   0.102016    -0.0179734   -0.0266739  -0.0429742   -0.245211    -0.0896139   -0.0788427   -0.0278909   -0.13751      0.094134    -0.0253697    -0.11059       0.0677679   -0.05505      0.152214     0.0178061   -0.100884     0.0643108   -0.0761987    0.00133116    0.0402523    -0.092747
  0.0499792    0.0695576   -0.0749688    0.10566       0.0308835   -0.00883458   0.0291373   0.0636283   -0.110883     0.0429424    0.019759     0.065038     0.0454348   -0.00625474   0.101354     -0.0292101     0.141675    -0.00141107   0.0402876    0.242658     0.0302666    0.0192727   -0.125794     0.000282687   0.0221471     0.0901499
  0.00944305   0.0957431   -0.144342    -0.0677199     0.0597617    0.0265959    0.0185608   0.0119909   -0.174408    -0.0375165    0.0809107   -0.0212327   -0.0994093    0.0750749   -0.0297348    -0.0233523    -0.042375    -0.0889032    0.104028     0.0466454   -0.0695169   -0.0952677   -0.206822     0.083543     -0.000481502   0.0145109
  0.120266     0.0228372   -0.0121901    0.056376      0.313253     0.0923369    0.0873144   0.0743641   -0.0198655    0.0390024   -0.147502     0.141366     0.0821363    0.110911    -0.000624334  -0.014489     -0.00760376   0.109218    -0.0818035   -0.0596864   -0.0551522   -0.117169     0.0247348    0.192009      0.143035      0.120422
  0.106476     0.155917     0.0413786   -0.0865545    -0.213761    -0.0650216   -0.0488976   0.164682     0.142287     0.0308972    0.050922     0.0209382    0.192944     0.0622496    0.0462356    -0.0810213    -0.187179     0.0264654    0.0348379    0.116472    -0.0508954   -0.0282419    0.0936284   -0.0972433    -0.104307      0.0710654
  0.0518621   -0.0904353   -0.0472494    5.34746e-5   -0.0152002    0.114759    -0.0195302   0.0390361    0.0903501   -0.0608664    0.175378    -0.0874753    0.0238628   -0.020261     0.112563      0.0646231     0.0246682    0.0473864    0.0661285   -0.0377987   -0.132723     0.111188    -0.0117742    0.0796082     0.107284     -0.0383595
  0.0675907    0.00896777   0.0283927    0.0496834     0.111513    -0.126096    -0.0402939  -0.117307    -0.117087    -0.130867    -0.0801135    0.0685997   -0.0296036   -0.123796    -0.0646107     0.173464     -0.16395     -0.181205    -0.0614564   -0.0208948   -0.0399729   -0.0323917   -0.0660202    0.104871     -0.120702      0.000760131
 -0.0940162    0.182154     0.123289    -0.091791      0.0244339   -0.00324992   0.0692107   0.0965412    0.0401844   -0.0783426   -0.140329     0.0126279    0.0794006   -0.0487034    0.0563859    -0.152118      0.0813041   -0.0403494    0.065282    -0.003738    -0.0925023   -0.156055     0.00965323   0.0332333     0.146815     -0.197438
 -0.0183618    0.00316243   0.00614727   0.0877766     0.102772     0.100795     0.0123583  -0.150668     0.202595    -0.0275056   -0.0292543   -0.23333      0.00745762  -0.191737    -0.112466      0.173604     -0.0400997    0.0524717    0.115863     0.0707165   -0.143227     0.0907679   -0.0200177   -0.0915735     0.0715043    -0.0199471
  0.0400536    0.066356     0.0934306   -0.0447329     0.151538     0.00329167  -0.0249571   0.0831575    0.00667553   0.0501788   -0.0250641    0.00990019  -0.0503695   -0.0304399    0.0979317     0.0520697     0.138592    -0.111102     0.029358    -0.142099    -0.199576     0.00606837  -0.0626559   -0.0362319     0.09097       0.0329213
 -0.0930447   -0.079257    -0.0506811   -0.127787     -0.0352261    0.029687     0.0146297   0.0413956   -0.0287376   -0.0570995    0.157808    -0.0868327    0.00373101   0.0134142    0.0612277     0.035306      0.0848734    0.175404     0.140679    -0.00902709  -0.00415815  -0.104102    -0.00323956   0.148878      0.157071     -0.11305
 -0.0731152   -0.0639672    0.13428      0.0752192    -0.0788997   -0.042459    -0.08666    -0.00931832   0.0122098    0.058149     0.202855    -0.134645     0.301146    -0.072177    -0.000200154   0.0690653    -0.19876      0.0197898    0.173529     0.103222     0.0224738    0.0667405   -8.64819e-5   0.0860672    -0.028174     -0.101479
  0.121203     0.0315187   -0.0619104    0.137919     -0.061222    -0.0126557   -0.0578922   0.0446348   -0.177371    -0.0568858   -0.0127432   -0.155097     0.070698    -0.26158     -0.0929193     0.00631437   -0.119805     0.00658524   0.168115    -0.104521     0.0989091    0.125854     0.0116932    0.0345302    -0.111448      0.160685
 -0.00873107   0.119738     0.0885602    0.0715366     0.0551827    0.0243154   -0.022925    0.155651     0.111141     0.171996    -0.163529    -0.00983557   0.01867     -0.0668436    0.101385      0.0504239     0.0535092   -0.0465756    0.075615    -0.0859989    0.0313823   -0.0272162   -0.0317018    0.0656032     0.103276      0.190824
 -0.104828    -0.0650797    0.0462389   -0.00802391    0.00655836   0.0599509    0.0793417   0.163058    -0.102811     0.0973625   -0.0280466    0.0209587    0.184503     0.0921074   -0.0420379     0.08109      -0.0298788    0.200404    -0.0947729    0.0407402   -0.0179888   -0.0937214    0.0998434   -0.0547934    -0.014508     -0.0662015
  0.0836774    0.143225    -0.0712778    0.0444464     0.0303312    0.032363    -0.0345374  -0.0530473   -0.0115842   -0.149392     0.0122638   -0.0761446    0.0240159   -0.0634963    0.0171064     0.0868784     0.0563595   -0.166406     0.21366     -0.0746708   -0.0282367    0.0408539    0.100788     0.020891     -0.0460677    -0.136336
 -0.0477777   -0.0274913   -0.166114     0.0999262     0.126738    -0.190913    -0.127899   -0.0231192    0.0351135    0.0875722    0.128442    -0.0350798    0.0230681   -0.134012    -0.1062        0.0365263     0.130961     0.138507     0.0901276   -0.0785389   -0.0239261    0.0419343   -0.0840505   -0.19516       0.0834171     0.0206356
  0.0514604    0.016156     0.0371459   -0.0430456     0.207061    -0.115899     0.147538   -0.1043       0.189792     0.0826502   -0.110968    -0.0320993    0.0608906   -0.143037    -0.176219      0.0361514     0.0249167    0.0419638   -0.112131     0.00387792   0.194403     0.00123821   0.0737975   -0.089433      0.0767217     0.061607
 -0.270189    -0.123682    -0.00793405  -0.0624178    -0.102707    -0.0762434   -0.0995938   0.0082174    0.13535      0.134525    -0.128995     0.0840527   -0.17816     -0.014613    -0.142105     -0.0289919    -0.00134827   0.106403    -0.0191569    0.124071    -0.0266983    0.0476621    0.0740395    0.0531576     0.0179934     0.260482
 -0.00429095   0.0455293    0.0769567   -0.0205412     0.113665    -0.0480856   -0.044087    0.036481     0.0934445   -0.0556608    0.0922112   -0.0163829    0.0634322   -0.0510503    0.169436     -0.084696     -0.0418497   -0.106863     0.113817     0.109654    -0.182409     0.116852     0.0792444    0.13076       0.149502      0.0131666
 -0.0143296   -0.193322    -0.0955708    0.0796523    -0.158525    -0.0133419    0.0758074  -0.0164359    0.0106269    0.0868971   -0.0402546    0.00012757   0.107036    -0.225396    -0.198339     -0.0325327     0.0666171    0.116062    -0.0287362   -0.0373563    0.00614002   0.0153375   -0.0338551    0.107473     -0.0633515    -0.179309
 -0.103898     0.0528934    0.00199415   0.0273263     0.0151105   -0.0332741    0.0255051   0.183994     0.160194    -0.0792046    0.185483     0.111753    -0.0356303    0.0568788   -0.0403463     0.00264101   -0.120903    -0.0810342   -0.199871     0.0051889    0.106177    -0.128573     0.0710145    0.117373     -0.101934      0.0064027kind diag, method split
┌ Info: 0: avll = 
└   tll[1] = -1.4198816459825179
[ Info: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.419974
[ Info: iteration 2, average log likelihood -1.419873
[ Info: iteration 3, average log likelihood -1.419006
[ Info: iteration 4, average log likelihood -1.410821
[ Info: iteration 5, average log likelihood -1.393873
[ Info: iteration 6, average log likelihood -1.386064
[ Info: iteration 7, average log likelihood -1.383945
[ Info: iteration 8, average log likelihood -1.383159
[ Info: iteration 9, average log likelihood -1.382750
[ Info: iteration 10, average log likelihood -1.382464
[ Info: iteration 11, average log likelihood -1.382229
[ Info: iteration 12, average log likelihood -1.382011
[ Info: iteration 13, average log likelihood -1.381793
[ Info: iteration 14, average log likelihood -1.381570
[ Info: iteration 15, average log likelihood -1.381337
[ Info: iteration 16, average log likelihood -1.381095
[ Info: iteration 17, average log likelihood -1.380872
[ Info: iteration 18, average log likelihood -1.380675
[ Info: iteration 19, average log likelihood -1.380493
[ Info: iteration 20, average log likelihood -1.380319
[ Info: iteration 21, average log likelihood -1.380147
[ Info: iteration 22, average log likelihood -1.379969
[ Info: iteration 23, average log likelihood -1.379777
[ Info: iteration 24, average log likelihood -1.379565
[ Info: iteration 25, average log likelihood -1.379328
[ Info: iteration 26, average log likelihood -1.379051
[ Info: iteration 27, average log likelihood -1.378698
[ Info: iteration 28, average log likelihood -1.378294
[ Info: iteration 29, average log likelihood -1.377951
[ Info: iteration 30, average log likelihood -1.377733
[ Info: iteration 31, average log likelihood -1.377600
[ Info: iteration 32, average log likelihood -1.377516
[ Info: iteration 33, average log likelihood -1.377461
[ Info: iteration 34, average log likelihood -1.377422
[ Info: iteration 35, average log likelihood -1.377393
[ Info: iteration 36, average log likelihood -1.377371
[ Info: iteration 37, average log likelihood -1.377353
[ Info: iteration 38, average log likelihood -1.377339
[ Info: iteration 39, average log likelihood -1.377327
[ Info: iteration 40, average log likelihood -1.377315
[ Info: iteration 41, average log likelihood -1.377303
[ Info: iteration 42, average log likelihood -1.377292
[ Info: iteration 43, average log likelihood -1.377281
[ Info: iteration 44, average log likelihood -1.377271
[ Info: iteration 45, average log likelihood -1.377263
[ Info: iteration 46, average log likelihood -1.377255
[ Info: iteration 47, average log likelihood -1.377249
[ Info: iteration 48, average log likelihood -1.377245
[ Info: iteration 49, average log likelihood -1.377241
[ Info: iteration 50, average log likelihood -1.377237
┌ Info: EM with 100000 data points 50 iterations avll -1.377237
└ 952.4 data points per parameter
┌ Info: 1
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4199740277979669
│     -1.4198730183851942
│      ⋮
└     -1.3772373503635509
[ Info: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.377365
[ Info: iteration 2, average log likelihood -1.377210
[ Info: iteration 3, average log likelihood -1.376358
[ Info: iteration 4, average log likelihood -1.368167
[ Info: iteration 5, average log likelihood -1.348132
[ Info: iteration 6, average log likelihood -1.338053
[ Info: iteration 7, average log likelihood -1.334814
[ Info: iteration 8, average log likelihood -1.333346
[ Info: iteration 9, average log likelihood -1.332525
[ Info: iteration 10, average log likelihood -1.331899
[ Info: iteration 11, average log likelihood -1.331196
[ Info: iteration 12, average log likelihood -1.330223
[ Info: iteration 13, average log likelihood -1.328985
[ Info: iteration 14, average log likelihood -1.328217
[ Info: iteration 15, average log likelihood -1.327901
[ Info: iteration 16, average log likelihood -1.327717
[ Info: iteration 17, average log likelihood -1.327567
[ Info: iteration 18, average log likelihood -1.327430
[ Info: iteration 19, average log likelihood -1.327307
[ Info: iteration 20, average log likelihood -1.327200
[ Info: iteration 21, average log likelihood -1.327115
[ Info: iteration 22, average log likelihood -1.327046
[ Info: iteration 23, average log likelihood -1.326988
[ Info: iteration 24, average log likelihood -1.326934
[ Info: iteration 25, average log likelihood -1.326880
[ Info: iteration 26, average log likelihood -1.326823
[ Info: iteration 27, average log likelihood -1.326761
[ Info: iteration 28, average log likelihood -1.326694
[ Info: iteration 29, average log likelihood -1.326623
[ Info: iteration 30, average log likelihood -1.326553
[ Info: iteration 31, average log likelihood -1.326482
[ Info: iteration 32, average log likelihood -1.326400
[ Info: iteration 33, average log likelihood -1.326303
[ Info: iteration 34, average log likelihood -1.326182
[ Info: iteration 35, average log likelihood -1.326036
[ Info: iteration 36, average log likelihood -1.325852
[ Info: iteration 37, average log likelihood -1.325615
[ Info: iteration 38, average log likelihood -1.325346
[ Info: iteration 39, average log likelihood -1.325116
[ Info: iteration 40, average log likelihood -1.324962
[ Info: iteration 41, average log likelihood -1.324867
[ Info: iteration 42, average log likelihood -1.324815
[ Info: iteration 43, average log likelihood -1.324786
[ Info: iteration 44, average log likelihood -1.324772
[ Info: iteration 45, average log likelihood -1.324765
[ Info: iteration 46, average log likelihood -1.324761
[ Info: iteration 47, average log likelihood -1.324759
[ Info: iteration 48, average log likelihood -1.324757
[ Info: iteration 49, average log likelihood -1.324756
[ Info: iteration 50, average log likelihood -1.324755
┌ Info: EM with 100000 data points 50 iterations avll -1.324755
└ 473.9 data points per parameter
┌ Info: 2
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.3773645648268775
│     -1.3772102211572048
│      ⋮
└     -1.32475530789357
[ Info: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.324936
[ Info: iteration 2, average log likelihood -1.324784
[ Info: iteration 3, average log likelihood -1.324514
[ Info: iteration 4, average log likelihood -1.321689
[ Info: iteration 5, average log likelihood -1.307127
[ Info: iteration 6, average log likelihood -1.286849
[ Info: iteration 7, average log likelihood -1.277714
[ Info: iteration 8, average log likelihood -1.273940
[ Info: iteration 9, average log likelihood -1.271967
[ Info: iteration 10, average log likelihood -1.270581
[ Info: iteration 11, average log likelihood -1.269359
[ Info: iteration 12, average log likelihood -1.268208
[ Info: iteration 13, average log likelihood -1.267202
[ Info: iteration 14, average log likelihood -1.266483
[ Info: iteration 15, average log likelihood -1.265999
[ Info: iteration 16, average log likelihood -1.265488
[ Info: iteration 17, average log likelihood -1.264862
[ Info: iteration 18, average log likelihood -1.264324
[ Info: iteration 19, average log likelihood -1.263999
[ Info: iteration 20, average log likelihood -1.263736
[ Info: iteration 21, average log likelihood -1.263458
[ Info: iteration 22, average log likelihood -1.263163
[ Info: iteration 23, average log likelihood -1.262902
[ Info: iteration 24, average log likelihood -1.262732
[ Info: iteration 25, average log likelihood -1.262636
[ Info: iteration 26, average log likelihood -1.262585
[ Info: iteration 27, average log likelihood -1.262559
[ Info: iteration 28, average log likelihood -1.262545
[ Info: iteration 29, average log likelihood -1.262537
[ Info: iteration 30, average log likelihood -1.262533
[ Info: iteration 31, average log likelihood -1.262530
[ Info: iteration 32, average log likelihood -1.262528
[ Info: iteration 33, average log likelihood -1.262527
[ Info: iteration 34, average log likelihood -1.262526
[ Info: iteration 35, average log likelihood -1.262526
[ Info: iteration 36, average log likelihood -1.262525
[ Info: iteration 37, average log likelihood -1.262525
[ Info: iteration 38, average log likelihood -1.262525
[ Info: iteration 39, average log likelihood -1.262525
[ Info: iteration 40, average log likelihood -1.262525
[ Info: iteration 41, average log likelihood -1.262525
[ Info: iteration 42, average log likelihood -1.262525
[ Info: iteration 43, average log likelihood -1.262525
[ Info: iteration 44, average log likelihood -1.262525
[ Info: iteration 45, average log likelihood -1.262525
[ Info: iteration 46, average log likelihood -1.262525
[ Info: iteration 47, average log likelihood -1.262525
[ Info: iteration 48, average log likelihood -1.262525
[ Info: iteration 49, average log likelihood -1.262525
[ Info: iteration 50, average log likelihood -1.262525
┌ Info: EM with 100000 data points 50 iterations avll -1.262525
└ 236.4 data points per parameter
┌ Info: 3
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.3249359022862697
│     -1.3247839288067782
│      ⋮
└     -1.2625248570493859
[ Info: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.262788
[ Info: iteration 2, average log likelihood -1.262518
[ Info: iteration 3, average log likelihood -1.261585
[ Info: iteration 4, average log likelihood -1.251801
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     3
│     4
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.216832
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     3
│     4
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.190489
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.185950
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     3
│     4
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.167448
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     4
│     7
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.165499
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     3
│     4
│     6
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.179637
[ Info: iteration 11, average log likelihood -1.188947
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     3
│     4
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 12, average log likelihood -1.166459
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     4
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 13, average log likelihood -1.175198
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     3
│     4
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 14, average log likelihood -1.162737
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 15, average log likelihood -1.175544
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     3
│     4
│     6
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 16, average log likelihood -1.167391
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 17, average log likelihood -1.173486
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     3
│     4
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 18, average log likelihood -1.162336
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      4
│      7
│     12
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 19, average log likelihood -1.173114
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     3
│     4
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 20, average log likelihood -1.185417
[ Info: iteration 21, average log likelihood -1.170868
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│     3
│     4
│     6
│     7
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 22, average log likelihood -1.155647
[ Info: iteration 23, average log likelihood -1.190336
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     3
│     4
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 24, average log likelihood -1.175285
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     4
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 25, average log likelihood -1.172891
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     3
│     4
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 26, average log likelihood -1.171067
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 27, average log likelihood -1.165240
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     3
│     4
│     6
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 28, average log likelihood -1.167198
[ Info: iteration 29, average log likelihood -1.184326
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     3
│     4
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 30, average log likelihood -1.161945
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     4
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 31, average log likelihood -1.172964
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     3
│     4
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 32, average log likelihood -1.175085
[ Info: iteration 33, average log likelihood -1.176256
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     3
│     4
│     6
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 34, average log likelihood -1.158463
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     7
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 35, average log likelihood -1.171186
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     3
│     4
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 36, average log likelihood -1.176789
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     4
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 37, average log likelihood -1.182219
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     3
│     4
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 38, average log likelihood -1.169177
[ Info: iteration 39, average log likelihood -1.173905
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     3
│     4
│     6
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 40, average log likelihood -1.156873
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 41, average log likelihood -1.183597
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     3
│     4
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 42, average log likelihood -1.170826
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     4
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 43, average log likelihood -1.168481
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     3
│     4
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 44, average log likelihood -1.168764
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 45, average log likelihood -1.177580
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     3
│     4
│     6
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 46, average log likelihood -1.167749
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 47, average log likelihood -1.174786
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     3
│     4
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 48, average log likelihood -1.165465
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     4
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 49, average log likelihood -1.178932
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     3
│     4
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 50, average log likelihood -1.173743
┌ Info: EM with 100000 data points 50 iterations avll -1.173743
└ 118.1 data points per parameter
┌ Info: 4
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.2627883503294413
│     -1.2625184510807568
│      ⋮
└     -1.1737433582961698
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.167363
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      5
│      6
│      7
│      8
│      ⋮
│     14
│     15
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 2, average log likelihood -1.154872
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      7
│      8
│     13
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 3, average log likelihood -1.165306
┌ Warning: Variances had to be floored 
│   ind =
│    12-element Array{Int64,1}:
│      1
│      5
│      6
│      7
│      ⋮
│     15
│     16
│     18
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.137983
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     18
│     19
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.108726
┌ Warning: Variances had to be floored 
│   ind =
│    14-element Array{Int64,1}:
│      1
│      5
│      6
│      7
│      ⋮
│     24
│     27
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.086859
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      1
│      5
│      6
│      7
│      ⋮
│     18
│     19
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.077601
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     15
│     18
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.067741
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      5
│      6
│      7
│      8
│      ⋮
│     24
│     30
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.060566
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      1
│      5
│      6
│      7
│      ⋮
│     14
│     15
│     18
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.087403
┌ Warning: Variances had to be floored 
│   ind =
│    16-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     24
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 11, average log likelihood -1.059885
┌ Warning: Variances had to be floored 
│   ind =
│    12-element Array{Int64,1}:
│      1
│      5
│      6
│      8
│      ⋮
│     18
│     25
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 12, average log likelihood -1.075314
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      1
│      5
│      6
│      8
│      ⋮
│     19
│     24
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 13, average log likelihood -1.076781
┌ Warning: Variances had to be floored 
│   ind =
│    14-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     19
│     22
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 14, average log likelihood -1.074022
┌ Warning: Variances had to be floored 
│   ind =
│    12-element Array{Int64,1}:
│      1
│      5
│      6
│      7
│      ⋮
│     18
│     24
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 15, average log likelihood -1.082545
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      1
│      5
│      6
│      8
│      ⋮
│     19
│     27
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 16, average log likelihood -1.068848
┌ Warning: Variances had to be floored 
│   ind =
│    14-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     18
│     24
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 17, average log likelihood -1.071527
┌ Warning: Variances had to be floored 
│   ind =
│    14-element Array{Int64,1}:
│      1
│      5
│      6
│      8
│      ⋮
│     26
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 18, average log likelihood -1.073916
┌ Warning: Variances had to be floored 
│   ind =
│    14-element Array{Int64,1}:
│      1
│      5
│      6
│      7
│      ⋮
│     19
│     22
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 19, average log likelihood -1.078605
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     16
│     18
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 20, average log likelihood -1.081746
┌ Warning: Variances had to be floored 
│   ind =
│    18-element Array{Int64,1}:
│      1
│      5
│      6
│      7
│      ⋮
│     30
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 21, average log likelihood -1.058340
┌ Warning: Variances had to be floored 
│   ind =
│    12-element Array{Int64,1}:
│      1
│      5
│      6
│      7
│      ⋮
│     16
│     18
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 22, average log likelihood -1.102958
┌ Warning: Variances had to be floored 
│   ind =
│    14-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     18
│     19
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 23, average log likelihood -1.071212
┌ Warning: Variances had to be floored 
│   ind =
│    18-element Array{Int64,1}:
│      1
│      5
│      6
│      7
│      ⋮
│     30
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 24, average log likelihood -1.062249
┌ Warning: Variances had to be floored 
│   ind =
│    12-element Array{Int64,1}:
│      1
│      5
│      6
│      7
│      ⋮
│     18
│     19
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 25, average log likelihood -1.103374
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     16
│     18
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 26, average log likelihood -1.079823
┌ Warning: Variances had to be floored 
│   ind =
│    18-element Array{Int64,1}:
│      1
│      5
│      6
│      7
│      ⋮
│     30
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 27, average log likelihood -1.059270
┌ Warning: Variances had to be floored 
│   ind =
│    12-element Array{Int64,1}:
│      1
│      5
│      6
│      7
│      ⋮
│     16
│     18
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 28, average log likelihood -1.100708
┌ Warning: Variances had to be floored 
│   ind =
│    15-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     19
│     22
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 29, average log likelihood -1.069849
┌ Warning: Variances had to be floored 
│   ind =
│    16-element Array{Int64,1}:
│      1
│      5
│      6
│      7
│      ⋮
│     30
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 30, average log likelihood -1.074360
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      1
│      5
│      6
│      7
│      ⋮
│     18
│     19
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 31, average log likelihood -1.093660
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     16
│     18
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 32, average log likelihood -1.080774
┌ Warning: Variances had to be floored 
│   ind =
│    18-element Array{Int64,1}:
│      1
│      5
│      6
│      7
│      ⋮
│     30
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 33, average log likelihood -1.056296
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      1
│      5
│      6
│      7
│      ⋮
│     18
│     19
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 34, average log likelihood -1.099317
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     18
│     19
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 35, average log likelihood -1.081685
┌ Warning: Variances had to be floored 
│   ind =
│    17-element Array{Int64,1}:
│      1
│      5
│      6
│      7
│      ⋮
│     30
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 36, average log likelihood -1.064617
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      1
│      5
│      6
│      7
│      ⋮
│     18
│     19
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 37, average log likelihood -1.095439
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     16
│     18
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 38, average log likelihood -1.078231
┌ Warning: Variances had to be floored 
│   ind =
│    19-element Array{Int64,1}:
│      1
│      5
│      6
│      7
│      ⋮
│     30
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 39, average log likelihood -1.054755
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      1
│      5
│      6
│      7
│      ⋮
│     16
│     18
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 40, average log likelihood -1.110926
┌ Warning: Variances had to be floored 
│   ind =
│    14-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     18
│     19
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 41, average log likelihood -1.073008
┌ Warning: Variances had to be floored 
│   ind =
│    17-element Array{Int64,1}:
│      1
│      5
│      6
│      7
│      ⋮
│     30
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 42, average log likelihood -1.066893
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      1
│      5
│      6
│      7
│      ⋮
│     18
│     19
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 43, average log likelihood -1.093146
┌ Warning: Variances had to be floored 
│   ind =
│    14-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     18
│     19
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 44, average log likelihood -1.076736
┌ Warning: Variances had to be floored 
│   ind =
│    17-element Array{Int64,1}:
│      1
│      5
│      6
│      7
│      ⋮
│     30
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 45, average log likelihood -1.066740
┌ Warning: Variances had to be floored 
│   ind =
│    12-element Array{Int64,1}:
│      1
│      5
│      6
│      7
│      ⋮
│     16
│     18
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 46, average log likelihood -1.101110
┌ Warning: Variances had to be floored 
│   ind =
│    14-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     18
│     19
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 47, average log likelihood -1.073887
┌ Warning: Variances had to be floored 
│   ind =
│    17-element Array{Int64,1}:
│      1
│      5
│      6
│      7
│      ⋮
│     30
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 48, average log likelihood -1.063914
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      1
│      5
│      6
│      7
│      ⋮
│     19
│     22
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 49, average log likelihood -1.091743
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     16
│     18
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 50, average log likelihood -1.083335
┌ Info: EM with 100000 data points 50 iterations avll -1.083335
└ 59.0 data points per parameter
┌ Info: 5
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.1673629444659637
│     -1.1548718543496141
│      ⋮
└     -1.0833349820936298
┌ Info: Total log likelihood: 
│   tll =
│    251-element Array{Float64,1}:
│     -1.4198816459825179
│     -1.4199740277979669
│     -1.4198730183851942
│     -1.4190059797973047
│      ⋮
│     -1.0639138346162202
│     -1.0917434343278276
└     -1.0833349820936298
32×26 Array{Float64,2}:
 -0.0341393   -0.0270274   -0.165322      0.0918003    0.126269    -0.191105    -0.0945338   -0.0172853    0.0452939    0.0898825    0.128373    -0.0359745    0.0294146   -0.130722    -0.108902      0.04017      0.134657      0.138435    0.0981232   -0.057299    -0.0255371   -0.00419057  -0.0713019   -0.191633     0.0828964    0.0191857
 -0.284937    -0.125999    -0.00692224   -0.0994336   -0.102984    -0.0807779   -0.0938763    0.00855134   0.136311     0.134937    -0.12263      0.0880033   -0.182913    -0.0157013   -0.154547     -0.035082    -0.000276202   0.100538   -0.00055036   0.128154    -0.0255511    0.0617027    0.0900621    0.0852247    0.017313     0.273067
  0.102169     0.0876163   -0.0638939     0.0666565   -0.00605165   0.0128568   -0.0700158    9.67416e-5  -0.113937    -0.108864     0.00665802  -0.117685     0.0462721   -0.172884    -0.0512169     0.045393    -0.0510671    -0.087194    0.188139    -0.103027     0.035578     0.0594638    0.0715228    0.0261377   -0.0783918   -0.0226775
  0.106698    -0.00651088  -0.0251855     0.0317574   -0.00256761   0.0442026    0.0270388    0.126543    -0.0730534   -0.0507567   -0.0918875   -0.079148    -0.0422195   -0.0363195    0.0362502     0.0162782    0.104379      0.0469308   0.0227012    0.178189     0.0573797    0.108036    -0.0962773    0.00727551  -0.0308018   -0.0107335
 -0.0532045   -0.111506     0.040286      0.136953     0.00571808  -0.0524543   -0.260262    -0.246038    -0.714724    -0.0748831    1.03539     -0.0708961    0.10056     -0.164488     0.296973      0.0935078    0.0570336    -0.415718   -0.0969553    0.0463478    0.0320596   -0.0774818    0.166548     0.0488675    0.0945308    0.213474
 -0.0593186   -0.113395     0.0321838     0.1713      -0.0329163   -0.0795565    0.177388    -0.177603    -0.184926    -0.074537    -1.58733     -0.135173     0.114943    -0.136444     0.220822      0.0881653    0.0136985    -0.231001   -0.0838915    0.170433     0.0217728   -0.0972473    0.163966     0.154812     0.0930354    0.258492
 -0.127101    -0.122197     0.0206835     0.227904    -0.0250458   -0.0620874    0.245816    -0.175303     0.812904    -0.0748199    0.194351    -0.0831756    0.0269395   -0.119817    -0.248278      0.0444302    0.11089       0.544801   -0.0969446    0.128781     0.0213506   -0.094633     0.160795     0.0754773    0.0868332   -0.29768
 -0.042319     0.160095    -0.0492478     0.0742299    0.09765      0.136772    -0.0478553   -0.102781     0.217166     0.0284623   -0.111997     0.113294    -0.111814    -0.128424    -0.104006      0.1354       0.0496297    -0.195561    0.0801885    0.0467916    0.0594702    0.168207     0.0553901    0.168297    -0.0356607   -0.0424359
 -0.107156    -1.39012      0.128587      0.0710837   -0.0954733   -0.0937783    0.136289     0.0126144    0.0183209   -0.025415     0.243689    -0.127537     0.333228    -0.0591041   -0.0225971     0.072639    -0.179594      0.0128071   0.161288     0.113        0.0240222   -0.0166949   -0.057411     0.066697    -0.0248655   -0.102647
 -0.0781502    1.22107      0.143074      0.0948814   -0.0455667   -0.0449308   -0.266131    -0.0484507    0.00768846   0.13388      0.211753    -0.14237      0.262897    -0.0855507    0.0184314     0.0785615   -0.216684      0.0704727   0.164595     0.083198     0.0139987    0.138416     0.0529694    0.0843987   -0.0322803   -0.098679
 -0.0181349   -0.105942     0.00466289   -0.0466676   -0.173627    -0.0423952   -0.603038     0.181815     0.151998     0.11135      0.0488045    0.0538096    0.222702     0.18078      0.205342      0.0410559   -0.15488       0.0927858   0.1226       0.145782    -0.164152    -0.0199284    0.0935679    0.0556187   -0.0885967    0.112013
  0.213644     0.306895     0.0559397    -0.0884325   -0.218904    -0.0857796    0.49737      0.127489     0.13101     -0.0349688    0.0500309    0.0438142    0.165693    -0.0529244   -0.0617747    -0.186888    -0.162184      0.0460381  -0.152372     0.0970533    0.0155668   -0.0271424    0.094426    -0.274276    -0.101147     0.0358267
 -0.088236    -0.370292     0.122393     -0.118883     0.0246581    0.0826476   -0.0610236    0.0933578   -0.512112    -0.0780889   -0.123258    -0.0250487   -0.05135     -0.0500002    0.0219502    -0.14586      0.0620816    -0.0335467   0.182095     0.00706065  -0.178283    -0.325576     0.00449406  -0.351266     0.171515    -0.139149
 -0.127886     0.621204     0.121814     -0.0384373    0.0249998   -0.0660313    0.108666     0.0826583    0.664409    -0.0776867   -0.121018     0.0781581    0.151749     0.0312209    0.0764016    -0.145796     0.095459     -0.0290491  -0.124862     0.00361692   0.00326964  -0.107923    -0.00369784   0.438115     0.112021    -0.250552
 -0.00676161   0.00775604   0.0686761    -0.0208656    0.109818    -0.0605227   -0.00322926   0.117136     0.0908912   -0.0756525   -0.0922876   -0.18481      0.0500018   -0.00475083   0.16006      -0.232112    -0.0420533    -0.102342    0.115366     0.166387    -0.183524    -0.0143515    0.0625179    0.0909868    0.200015     0.0194883
  0.00504348   0.321926     0.0679502    -0.0203588    0.123407     0.00942497  -0.221553    -0.0119965    0.0912852   -0.00178045   0.580538     0.362304     0.0893819   -0.146787     0.179742      0.264694    -0.0417127    -0.100126    0.118018     0.187036    -0.146504     0.433932     0.0993757    0.24922      0.0985063    0.0177507
 -0.0411599   -0.101061    -0.0322246    -0.0206574    0.102109    -0.0226762   -0.0319138   -0.0336832   -0.247912    -0.142266    -0.0856242   -0.0292807   -0.150617     0.0993575   -0.0340021    -0.140707     0.077384     -0.0508321   0.129703     0.035624    -0.118176     0.0757541   -0.0825165    0.00221785   0.0147575   -0.0789204
  0.0387488    0.0686688    0.113013     -0.0551496    0.15995      0.0027403   -0.0366168    0.0757118   -0.00453235   0.0464682   -0.0196409    0.0103324   -0.0797391   -0.0227541    0.0916493     0.0390426    0.13881      -0.113381    0.0301544   -0.1096      -0.216506     0.00467737  -0.0560988   -0.0386408    0.0643247    0.024335
  0.0130667    0.0269981    0.000543564   0.0466806    0.16219     -0.118304    -0.0306296   -0.11772     -0.124728    -0.146915    -0.0797034    0.0671196   -0.0510301   -0.119332    -0.0767789     0.189362    -0.157364     -0.176453   -0.0598217    0.0117185   -0.0345514   -0.0258276   -0.0676441    0.102081    -0.123027     0.0144005
 -0.0765052    0.114795    -0.0528831     0.204243    -0.07386      0.0504553    0.0936014   -0.0273555    0.130299    -0.0935908   -0.0779074   -0.0367372   -0.0157781    0.0449797    0.138099      0.0638979    0.230477     -0.151639   -0.0657511    0.084558    -0.0915743   -0.0393035   -0.0332626   -0.0512887   -0.0792603    0.0406519
  0.00455637  -0.00236464  -0.172134     -0.00251244   0.167912     0.0165515    0.0169185   -0.0187026   -0.120108    -0.0106674    0.045449    -0.0864402   -0.11723      0.0753858    0.0830821     0.03951      0.0124427    -0.0125176   0.179418    -0.0167275   -0.0916895   -0.0387559   -0.0957955    0.0412102    0.107896     0.10927
 -0.0471406    0.149453     0.0531781     0.104192    -0.164557     0.0689438   -0.0134432    0.142539     0.0464719    0.0310835   -0.00632061   0.118484    -0.0104509    0.206263     0.185013      0.0710572    0.0851111    -0.0374213   0.0589621    0.135101     0.10588      0.0910915   -0.176932     0.0698021   -0.0836817   -0.110636
 -0.0246191   -0.177934    -0.0942625     0.053006    -0.150418    -0.0123325    0.071323    -0.0224594    0.0151886    0.0597664   -0.0530677    0.00758346   0.0925318   -0.226468    -0.214387     -0.0156631    0.0665133     0.128344   -0.0549956   -0.0404002   -0.0131173    0.0130251   -0.0348011    0.100611    -0.0787579   -0.160596
  0.110645     0.0514186    0.0685204    -0.0148453    0.101818    -0.10875     -0.0347336    0.200203    -0.0836448    0.00863342   0.176679    -0.00946047   0.141271     0.109811     0.145488      0.0341625   -0.0357577     0.046654   -0.062446    -0.111306    -0.0251335   -0.170301    -0.237785     0.0106417   -0.0341281    0.124179
 -0.145159    -0.0326408   -0.0121585     0.153278     0.316505    -0.207192    -0.148972     0.0623077    0.107901     0.250183     0.0951521    0.0121633    0.0599791   -0.140766    -0.0989686     0.00915964   0.158699     -0.0204259  -0.0180585   -0.0470307    0.0410974    0.0523669   -0.0607306   -0.095327    -0.00168951  -0.0623633
  0.0313809   -0.048068    -0.021757      0.0366963    0.0114866    0.123128    -0.00765181  -0.0627375    0.144086    -0.0220522    0.0728662   -0.159893     0.00900643  -0.103449     0.000680448   0.115962     0.00729971    0.0359405   0.0930736    0.00575649  -0.12352      0.0960151   -0.0196631   -0.00881611   0.0946316   -0.01414
  0.011684    -0.0194947    0.042625      0.0152746    0.158859     0.080734     0.0784027    0.125874    -0.0408405    0.067551    -0.0708643    0.0624735    0.128814     0.100927    -0.0123326     0.0320738   -0.00543099    0.174422   -0.0869494   -0.00709791  -0.0499114   -0.108952     0.0494746    0.0528335    0.0522783    0.0234161
  0.0942091    0.161723    -0.0859209    -0.0744608    0.0969595    0.0524585    0.00146281  -0.0219254    0.126985    -0.0933007   -0.0227342   -0.0456564    0.0784725    0.0388243   -0.0410738     0.120519     0.0391949     0.127761   -0.0143865   -0.00936223   0.0743025   -0.11768     -0.120977    -0.10077      0.0761451    0.0726137
 -0.0932683   -0.032382    -0.0189596    -0.0631166   -0.0123548   -0.0475359    0.0253128    0.117388     0.0630778   -0.0678964    0.170785    -0.00175943  -0.0113123    0.0419273   -0.0159468     0.0420745   -0.0107001     0.0738306  -0.0256519    0.00581576   0.041046    -0.117586     0.0404559    0.133057     0.04654     -0.0413201
 -0.0223964    0.0661367   -0.0624816     0.0416062   -0.0179891    0.178634     0.0397693   -0.044239    -0.0351959   -0.151166    -0.209826     0.210247    -0.164457    -0.284711     0.00098247    0.0241934    0.0371386    -0.0716847  -0.0823684   -0.189097     0.144666     0.133192    -0.0790867    0.0227241   -0.0284442   -0.0116349
  0.0534196    0.00611124   0.0321697    -0.0439664    0.197752    -0.0770579    0.14882     -0.116855     0.189698     0.0839432   -0.107526    -0.0277628    0.0615282   -0.142821    -0.180578      0.0379538    0.0491953     0.0431793  -0.126677     0.0278263    0.184884    -0.0106855    0.0721986   -0.0960242    0.0728233    0.0638088
 -0.0153197    0.120501     0.126588      0.0854375    0.0496046   -0.00318133  -0.0171219    0.152768     0.0959758    0.165481    -0.161284    -0.00730256   0.015626    -0.0650602    0.0874867     0.0483331    0.0415563    -0.0400497   0.111425    -0.107784     0.0273115   -0.0230335   -0.034043     0.0660434    0.115949     0.209724[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
┌ Warning: Variances had to be floored 
│   ind =
│    18-element Array{Int64,1}:
│      1
│      5
│      6
│      7
│      ⋮
│     30
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 1, average log likelihood -1.062310
┌ Warning: Variances had to be floored 
│   ind =
│    19-element Array{Int64,1}:
│      1
│      5
│      6
│      7
│      ⋮
│     30
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 2, average log likelihood -1.055063
┌ Warning: Variances had to be floored 
│   ind =
│    19-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     30
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 3, average log likelihood -1.049734
┌ Warning: Variances had to be floored 
│   ind =
│    20-element Array{Int64,1}:
│      1
│      5
│      6
│      7
│      ⋮
│     30
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.054765
┌ Warning: Variances had to be floored 
│   ind =
│    17-element Array{Int64,1}:
│      1
│      5
│      6
│      7
│      ⋮
│     30
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.058562
┌ Warning: Variances had to be floored 
│   ind =
│    20-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     30
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.046590
┌ Warning: Variances had to be floored 
│   ind =
│    18-element Array{Int64,1}:
│      1
│      5
│      6
│      7
│      ⋮
│     30
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.055866
┌ Warning: Variances had to be floored 
│   ind =
│    20-element Array{Int64,1}:
│      1
│      5
│      6
│      7
│      ⋮
│     30
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.051205
┌ Warning: Variances had to be floored 
│   ind =
│    19-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     30
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.055631
┌ Warning: Variances had to be floored 
│   ind =
│    18-element Array{Int64,1}:
│      1
│      5
│      6
│      7
│      ⋮
│     30
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.058154
┌ Info: EM with 100000 data points 10 iterations avll -1.058154
└ 59.0 data points per parameter
kind diag, method kmeans
[ Info: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.389960e+05
      1       6.971368e+05      -1.418592e+05 |       32
      2       6.638338e+05      -3.330297e+04 |       32
      3       6.480392e+05      -1.579460e+04 |       32
      4       6.382949e+05      -9.744353e+03 |       32
      5       6.326353e+05      -5.659587e+03 |       32
      6       6.274417e+05      -5.193608e+03 |       32
      7       6.234759e+05      -3.965776e+03 |       32
      8       6.213569e+05      -2.118980e+03 |       32
      9       6.204585e+05      -8.984508e+02 |       32
     10       6.200200e+05      -4.385214e+02 |       32
     11       6.197889e+05      -2.310994e+02 |       32
     12       6.196305e+05      -1.583688e+02 |       32
     13       6.195156e+05      -1.148654e+02 |       32
     14       6.194163e+05      -9.930759e+01 |       32
     15       6.193323e+05      -8.401661e+01 |       32
     16       6.192649e+05      -6.741120e+01 |       32
     17       6.191990e+05      -6.589394e+01 |       32
     18       6.190711e+05      -1.278953e+02 |       32
     19       6.186444e+05      -4.266477e+02 |       32
     20       6.171758e+05      -1.468600e+03 |       32
     21       6.150569e+05      -2.118927e+03 |       32
     22       6.143655e+05      -6.913894e+02 |       32
     23       6.142647e+05      -1.008137e+02 |       31
     24       6.142035e+05      -6.118372e+01 |       31
     25       6.141232e+05      -8.034325e+01 |       32
     26       6.140291e+05      -9.412612e+01 |       31
     27       6.139290e+05      -1.000886e+02 |       32
     28       6.138014e+05      -1.276130e+02 |       32
     29       6.136623e+05      -1.391009e+02 |       32
     30       6.135123e+05      -1.499198e+02 |       31
     31       6.133921e+05      -1.202361e+02 |       32
     32       6.133030e+05      -8.910619e+01 |       32
     33       6.132332e+05      -6.984487e+01 |       32
     34       6.131683e+05      -6.480878e+01 |       32
     35       6.130994e+05      -6.894015e+01 |       31
     36       6.130276e+05      -7.184788e+01 |       32
     37       6.129695e+05      -5.801780e+01 |       31
     38       6.129285e+05      -4.107164e+01 |       31
     39       6.129082e+05      -2.030522e+01 |       30
     40       6.128950e+05      -1.311036e+01 |       29
     41       6.128875e+05      -7.591073e+00 |       24
     42       6.128820e+05      -5.452683e+00 |       25
     43       6.128762e+05      -5.791337e+00 |       23
     44       6.128701e+05      -6.143713e+00 |       23
     45       6.128646e+05      -5.453741e+00 |       25
     46       6.128594e+05      -5.217618e+00 |       25
     47       6.128555e+05      -3.862099e+00 |       25
     48       6.128521e+05      -3.476023e+00 |       24
     49       6.128491e+05      -2.960208e+00 |       23
     50       6.128470e+05      -2.058171e+00 |       18
K-means terminated without convergence after 50 iterations (objv = 612847.0425531764)
┌ Info: K-means with 32000 data points using 50 iterations
└ 37.0 data points per parameter
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.321168
[ Info: iteration 2, average log likelihood -1.287312
[ Info: iteration 3, average log likelihood -1.250120
[ Info: iteration 4, average log likelihood -1.208663
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      1
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.160827
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     4
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.130001
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      2
│     11
│     18
│     19
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.081296
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     17
│     20
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.106415
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      1
│     21
│     24
│     26
│     27
│     29
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.066013
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.133007
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      2
│      4
│     11
│     18
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 11, average log likelihood -1.079664
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      1
│     20
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 12, average log likelihood -1.094353
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│     10
│     17
│     24
│     26
│     27
│     29
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 13, average log likelihood -1.067667
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      2
│      7
│     21
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 14, average log likelihood -1.106586
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      4
│     11
│     18
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 15, average log likelihood -1.104590
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      1
│     20
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 16, average log likelihood -1.105984
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     14
│     26
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 17, average log likelihood -1.085189
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      2
│      7
│     11
│     17
│     19
│     21
│     24
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 18, average log likelihood -1.055908
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      4
│     18
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 19, average log likelihood -1.123232
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      1
│     10
│     20
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 20, average log likelihood -1.106387
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     22
│     26
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 21, average log likelihood -1.082325
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      2
│      7
│     11
│     14
│     19
│     21
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 22, average log likelihood -1.060705
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      4
│     17
│     18
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 23, average log likelihood -1.104344
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      1
│     20
│     26
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 24, average log likelihood -1.091592
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     10
│     29
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 25, average log likelihood -1.101986
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      2
│     11
│     14
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 26, average log likelihood -1.084467
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      4
│      7
│     17
│     18
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 27, average log likelihood -1.090131
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      1
│     20
│     21
│     26
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 28, average log likelihood -1.085734
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     10
│     19
│     24
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 29, average log likelihood -1.109828
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      2
│     11
│     22
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 30, average log likelihood -1.085155
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      4
│      7
│     17
│     18
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 31, average log likelihood -1.090081
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      1
│     14
│     20
│     26
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 32, average log likelihood -1.087027
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     10
│     19
│     21
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 33, average log likelihood -1.110822
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      2
│     11
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 34, average log likelihood -1.099626
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      4
│      7
│     17
│     18
│     24
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 35, average log likelihood -1.078023
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      1
│     20
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 36, average log likelihood -1.108975
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     10
│     19
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 37, average log likelihood -1.098369
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      2
│     11
│     14
│     21
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 38, average log likelihood -1.072361
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      4
│      7
│     17
│     18
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 39, average log likelihood -1.095702
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      1
│     20
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 40, average log likelihood -1.104958
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     19
│     22
│     29
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 41, average log likelihood -1.093952
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      2
│     10
│     11
│     14
│     21
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 42, average log likelihood -1.075478
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      4
│      7
│     17
│     18
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 43, average log likelihood -1.083286
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      1
│     15
│     20
│     26
│     27
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 44, average log likelihood -1.067327
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     19
│     22
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 45, average log likelihood -1.122643
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      2
│     10
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 46, average log likelihood -1.080840
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      4
│      7
│     11
│     15
│     17
│     18
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 47, average log likelihood -1.044826
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      1
│     20
│     21
│     26
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 48, average log likelihood -1.095338
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     19
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 49, average log likelihood -1.118659
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      2
│     14
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 50, average log likelihood -1.078683
┌ Info: EM with 100000 data points 50 iterations avll -1.078683
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
  0.0135315    0.0292947    0.00556753   0.0570431   0.150888    -0.122731    -0.0370616   -0.105746   -0.117283   -0.156491     -0.0838803    0.066887    -0.051556    -0.119814   -0.0776296     0.184596    -0.165575    -0.186398   -0.0674738   0.0129765   -0.0235521   -0.0254999   -0.0611029    0.0957793   -0.139476    0.0256707
 -0.0386545    0.159116    -0.0483161    0.0733197   0.0966914    0.134985    -0.047702    -0.105757    0.215957    0.028063     -0.112812     0.115145    -0.10922     -0.128521   -0.104188      0.133836     0.0476824   -0.196307    0.082075    0.0482005    0.0594051    0.167716     0.0555336    0.166984    -0.0356115  -0.0422323
  0.0976621   -0.0104085   -0.0264201    0.030842   -0.0036216    0.0413891    0.0269334    0.120372   -0.075381   -0.050156     -0.0924189   -0.0789358   -0.0400473   -0.0378497   0.0357432     0.0219493    0.105013     0.0458071   0.0185453   0.178479     0.0561301    0.101778    -0.0924919    0.0133817   -0.0292933  -0.0112808
 -0.0805545   -0.0637069   -0.046974    -0.133019   -0.0414048   -0.0050252    0.0273072    0.0484972  -0.0262826  -0.0505676     0.151681    -0.0859123    0.0131311    0.0046608   0.041967      0.0387505    0.0835667    0.22581     0.128011   -0.0117644    0.00190494  -0.103995     0.0175446    0.143876     0.202805   -0.0891672
 -0.0216374    0.0119873    0.0102679    0.0998518   0.0781829    0.108882     0.00454302  -0.149418    0.220069   -0.0290368    -0.0355131   -0.22554     -0.00337408  -0.206465   -0.113057      0.169186     0.0122709    0.0471263   0.125211    0.0564157   -0.138866     0.0775438   -0.0431856   -0.0826879    0.0496681  -0.00883943
 -0.0391482    0.0567827   -0.0381929    0.105269    0.181931    -0.0993914   -0.12286      0.0215414   0.0341084   0.064071      0.0541092   -0.0255561    0.0431816   -0.107976   -0.0513575     0.0426512    0.100795    -0.0983823   0.0906156  -0.065778     0.0175002    0.045689     0.029829    -0.0419421   -0.0288772  -0.102444
 -0.0407594    0.166629     0.0741419    0.117989   -0.17604      0.0748553   -0.0160402    0.139218    0.0646063   0.0236414    -0.0192294    0.111873     0.0025639    0.207906    0.183221      0.0711826    0.0879795   -0.0463598   0.0466584   0.138319     0.130487     0.113135    -0.18028      0.0688632   -0.115759   -0.10959
  0.09487      0.159745    -0.0773059   -0.0749597   0.0961561    0.0512179    0.00223949  -0.0118182   0.123771   -0.0900851    -0.0362427   -0.044923     0.0780279    0.0344293  -0.0521221     0.122247     0.0435558    0.120553   -0.0128201  -0.0128348    0.0744071   -0.118362    -0.110467    -0.102956     0.0803988   0.0731677
 -0.0402767   -0.0946842   -0.0281559   -0.0216515   0.102059    -0.022507    -0.0317324   -0.0290834  -0.24524    -0.139272     -0.0856768   -0.0283072   -0.146521     0.0971084  -0.0317149    -0.138809     0.0760752   -0.0519028   0.1329      0.0354347   -0.120089     0.0714334   -0.0815573    0.00455757   0.019832   -0.0757897
  0.00385382   0.141835     0.059608    -0.0167624   0.0984911   -0.00116849   0.138559     0.0539474   0.131703   -0.0332107     0.220425     0.00284691   0.0605286   -0.123441    0.0634823     0.0601452   -0.0408963   -0.0663667   0.0776479  -0.0742436   -0.0979873    0.0858759    0.108147     0.0201409    0.152826    0.0246174
 -0.264698    -0.124249    -0.00650011  -0.103908   -0.102191    -0.0845814   -0.0948571    0.0124089   0.135438    0.130234     -0.11727      0.0864332   -0.171641    -0.0158641  -0.148898     -0.034986    -0.00309972   0.103133   -0.0114268   0.125909    -0.0241866    0.0559248    0.0892781    0.0754993    0.0173462   0.2639
 -0.0185806   -0.176166    -0.0932311    0.0478174  -0.146458    -0.0131138    0.0697699   -0.0204269   0.0158543   0.0609682    -0.0524044    0.00713631   0.0912677   -0.226604   -0.210954     -0.0127833    0.0649349    0.127337   -0.0549547  -0.0408589   -0.0132625    0.0110522   -0.0345393    0.0993327   -0.0788216  -0.158605
 -0.0786278    0.11631     -0.0544375    0.211186   -0.0770517    0.05043      0.0939717   -0.0237091   0.131284   -0.0960118    -0.0779225   -0.0370876   -0.0170136    0.046668    0.140744      0.0635135    0.230324    -0.152584   -0.0660363   0.0882184   -0.0905697   -0.0403485   -0.0304502   -0.0513142   -0.0784801   0.0433825
 -0.0215723    0.0602814   -0.0717943    0.0431054  -0.0168341    0.181229     0.0399587   -0.0466763  -0.0430321  -0.154236     -0.24039      0.199485    -0.173942    -0.299088   -0.00377061    0.0217362    0.0595017   -0.0686168  -0.0783543  -0.203654     0.141872     0.14712     -0.08614      0.018827    -0.0305918  -0.0223103
 -0.0248563   -0.032087    -0.163399     0.0993299   0.122291    -0.178959    -0.0872983   -0.0108318   0.047484    0.0811822     0.128414    -0.0372301    0.0275502   -0.128202   -0.103792      0.0381978    0.127255     0.137551    0.0927187  -0.0601549   -0.0200915    0.00985567  -0.0603736   -0.188553     0.0784619   0.0137511
  0.0436296    0.0400836    0.0740372   -0.0488389   0.187445    -0.0393194    0.0495169   -0.0120565   0.0872989   0.0678232    -0.0642645   -0.00373936  -0.00878181  -0.0823958  -0.0345369     0.0374555    0.102902    -0.0451704  -0.0450304  -0.0508003   -0.0246163    0.00512644  -0.00125531  -0.0628523    0.0596964   0.0416232
  0.171328     0.020364    -0.00165093   0.0555056   0.303318     0.0907868    0.0851544    0.0832785  -0.0211812   0.0385862    -0.143203     0.107365     0.0479606    0.114206   -0.000204254  -0.017192     0.0267753    0.152613   -0.0770096  -0.0588924   -0.105272    -0.137873     0.0345204    0.18448      0.136999    0.135164
 -0.0958037   -0.0538144    0.0789252   -0.0111262   0.0597861    0.0769318    0.0764196    0.162669   -0.0631019   0.095529     -0.0243827    0.0355128    0.198673     0.0936739  -0.028277      0.0714731   -0.0388276    0.200396   -0.0932852   0.0353649   -0.0124719   -0.0972916    0.0655246   -0.0476555   -0.0124761  -0.0522998
  0.0963874    0.118244     0.0342235   -0.0357484  -0.181231    -0.0768502    0.0375926    0.142903    0.133841    0.0344306     0.0309825    0.0333423    0.170515     0.0432823   0.0729529    -0.0702518   -0.168856     0.0798486  -0.028416    0.104403    -0.0463439   -0.0264018    0.0908428   -0.121309    -0.0860346   0.0876531
  0.0553266   -0.110706    -0.0454907   -0.0129877  -0.0138382    0.114346    -0.0188519    0.0225349   0.0986882   0.00346995    0.201371    -0.0857999    0.0258358   -0.0170532   0.0901535     0.0693499    0.0121329    0.0336995   0.0596916  -0.0504477   -0.116366     0.132126    -0.005607     0.0509559    0.181641   -0.0215206
 -0.0364818   -0.110557     0.0293515    0.0715081   0.0575924   -0.0421499    0.0474042   -0.241027   -0.0101275  -0.0558221    -0.112695    -0.0893545    0.0507891   -0.112004    0.142538      0.0881956   -0.0080577   -0.059747    0.0337471   0.10229     -0.0667216   -0.0393975    0.108345     0.120283     0.142119    0.0188365
 -0.013521     0.0650576    0.064066     0.0738354   0.0511818   -0.0295849   -0.0684479    0.0790585   0.0902203   0.141421     -0.167682    -0.0330362    0.00400832  -0.0692419   0.137891      0.0474593    0.102521    -0.0433675   0.107526   -0.0360691    0.0238041    0.0125568    0.0172004    0.0642239    0.214959    0.0959701
  0.00643227   0.053326    -0.0496088    0.0160343   0.167345     0.0337696    0.00186813   0.0192395  -0.173604    0.0416539     0.00369814  -0.0678037   -0.0485115    0.0684238   0.0342833    -0.0364185    0.0728919   -0.0913612   0.0736931   0.00678846   0.371838    -0.113293    -0.174172     0.0941755   -0.246313    0.0452859
  0.118        0.0198802   -0.0563936    0.0843781  -0.0453316   -0.00777409  -0.0650636    0.0424732  -0.18369    -0.0562145    -0.00236591  -0.159201     0.0707785   -0.269496   -0.100663      0.00576884  -0.129002    -0.0125493   0.163372   -0.119095     0.0930326    0.0746631    0.0287484    0.0323901   -0.111926    0.0945765
  0.00498154  -0.0601859   -0.163824     0.0469963   0.186016     0.0182052    0.00275955  -0.0336425  -0.0323848  -0.0196529    -0.00690305  -0.0958652   -0.113467     0.0625287   0.197454      0.0998856    0.0412862    0.0616617   0.178212   -0.0469686   -0.106039     0.0271653   -0.00683307   0.00612957   0.141947    0.142137
  0.107787     0.0579779    0.0717135   -0.0097569   0.108589    -0.107596    -0.0372243    0.1922     -0.0863448   0.0128453     0.170891    -0.0151458    0.160099     0.106182    0.144471      0.0347078   -0.0326885    0.0407269  -0.0660417  -0.108723    -0.0226309   -0.18411     -0.228499     0.0101602   -0.0308069   0.144404
 -0.0451847    0.0586639    0.0595856    0.0670077   0.0403717   -0.0492438   -0.103871     0.0387204   0.0131805  -0.0580209     0.0856006   -0.0395728    0.0905267   -0.0851067   0.143039     -0.0611904    0.0052362   -0.114592    0.0100082   0.195517    -0.0898021    0.0538143    0.110688     0.13404      0.112545    0.0696038
 -0.0270215    0.108004    -0.168786    -0.0877146   0.00708539   0.020061     0.0356735    0.0218717  -0.172224   -0.000854478   0.140545     0.00677832  -0.115393     0.138195   -0.0486862    -0.0077097   -0.0487061   -0.0913095   0.187401    0.0817747   -0.259502    -0.0810045   -0.22339      0.0759365    0.139637    0.0191087
  0.00044391   0.119622     0.18507      0.0838724   0.0581701    0.018608     0.020619     0.150699    0.126564    0.152582     -0.157475    -0.0121911    0.017963    -0.079077    0.0508709     0.0491066    0.0861047   -0.0414173   0.112834   -0.129477     0.0396921   -0.0443305   -0.0271241    0.0282382    0.15196     0.225125
 -0.105334     0.130442     0.123041    -0.072373    0.0246098    0.00283585   0.0314975    0.0809721   0.0811701  -0.0773251    -0.123781     0.0277846    0.0561228   -0.0109535   0.0490796    -0.148986     0.0792647   -0.0304501   0.0231215   0.0100055   -0.0820282   -0.207607     0.00281182   0.0530696    0.140793   -0.191925
 -0.0875982   -0.00814915   0.136633     0.0783676  -0.0671345   -0.0684156   -0.0672022   -0.0200854   0.0157712   0.0604161     0.226663    -0.130453     0.294047    -0.0738839  -0.000920012   0.075917    -0.207509     0.038142    0.155652    0.0965592    0.0214323    0.0636774    0.00273433   0.0796017   -0.028713   -0.100237
 -0.0904505    0.0167559    0.0104089    0.0186463   0.014953    -0.0740683    0.023801     0.178774    0.145521   -0.0794819     0.179451     0.101304    -0.0333678    0.0641438  -0.0561822     0.0289991   -0.121785    -0.0878096  -0.19676     0.0166787    0.0887909   -0.127491     0.0629137    0.114229    -0.107573    0.0200062[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      4
│      7
│     11
│     15
│     18
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 1, average log likelihood -1.059439
┌ Warning: Variances had to be floored 
│   ind =
│    12-element Array{Int64,1}:
│      1
│      4
│      7
│     11
│      ⋮
│     22
│     26
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 2, average log likelihood -1.012569
┌ Warning: Variances had to be floored 
│   ind =
│    12-element Array{Int64,1}:
│      1
│      2
│      4
│      7
│      ⋮
│     20
│     22
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 3, average log likelihood -1.014904
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      4
│      7
│     10
│     11
│     15
│     18
│     26
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.029561
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      1
│      4
│      7
│     11
│      ⋮
│     22
│     27
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.011841
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      1
│      2
│      4
│      7
│      ⋮
│     22
│     26
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.009601
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      4
│      7
│     11
│     15
│     18
│     19
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.046553
┌ Warning: Variances had to be floored 
│   ind =
│    14-element Array{Int64,1}:
│      1
│      4
│      7
│     11
│      ⋮
│     27
│     29
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -0.993107
┌ Warning: Variances had to be floored 
│   ind =
│    14-element Array{Int64,1}:
│      1
│      2
│      4
│      7
│      ⋮
│     20
│     21
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.015681
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      4
│      7
│     11
│     15
│     18
│     22
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.039240
┌ Info: EM with 100000 data points 10 iterations avll -1.039240
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
  0.152041    -0.0151021  -0.012294    -0.141993     0.0483206    0.0419122     0.0351501   -0.130371    -0.0833371   -0.109478    -0.209854    -0.104763     0.0529371    -0.113997     0.087342     0.0168191    -0.191881     0.0299985    0.0207893    -0.0271984    -0.0801311    0.018166     0.164548     -0.0266793     0.0640917    -0.0480809
 -0.00957475   0.124279    0.0947672    0.0571331    0.129099    -0.135503     -0.0438426    0.195339     0.0159583   -0.041483     0.0923437    0.0498289    0.0505325    -0.0728307    0.159823     0.0208373     0.0535146   -0.0616043    0.0113022     0.022218     -0.13785      0.0655581    0.220852     -0.000970483   0.103659      0.0790645
  0.0159639   -0.0399758   0.139796     0.0793345   -0.0689911   -0.0117145     0.0581521   -0.0550702   -0.0931626   -0.0414816   -0.171368    -0.0730277    0.0180181    -0.0207152   -0.0277207   -0.109224      0.0731151   -0.0254865   -0.149953      0.165209     -0.0477516    0.00767627  -0.110667     -0.139987     -0.00795293   -0.125043
 -0.117447     0.0433484   0.203428     0.00324748  -0.133692     0.199607     -0.114333    -0.00786815   0.09757     -0.291883    -0.177722     0.13094      0.235578      0.116796    -0.106444     0.000909415   0.174466     0.051924    -0.11115      -0.021784      0.00783324  -0.213663     0.0620983    -0.111683      0.0341848    -0.0711778
 -0.172072    -0.0410609   0.152504     0.0529822    0.0790282   -0.191401     -0.049262    -0.035769     0.0769142    0.0252706    0.235603    -0.0545131    0.0625222     0.00326398   0.057798    -0.119375     -0.125876    -0.0194722    0.00508408    0.0853936    -0.165789    -0.0438253    0.0493636    -0.040601      0.0320667     0.00379028
 -0.0436291    0.0540507   0.0948024   -0.10742      0.0381876    0.0985379     0.0946009    0.0534601   -0.147253     0.1464       0.00848844   0.0666528    0.03842      -0.15117     -0.128841     0.0284525     0.01899     -0.13519      0.0019554    -0.0819678    -0.00118598   0.0381023   -0.0132112    -0.0212843    -0.0120847     0.00862887
 -0.0296708    0.0636911  -0.106721    -0.2008      -0.0398881   -0.105768     -0.0101788   -0.0652376    0.183944    -0.165317     0.152539     0.00457736   0.107468      0.113013     0.0952217    0.0056135     0.0523213    0.044985    -0.196764      0.123629      0.154        0.116145    -0.0948932     0.0704165    -0.0432739     0.153513
 -0.0058058   -0.151935    0.103906     0.0792914   -0.0318182   -0.0161692    -0.026805    -0.196801     0.0672291   -0.0832396   -0.0557437    0.0473298    0.0958396    -0.0587356   -0.00174295   0.0222797     0.080004    -0.147292     0.130893     -0.0463816    -0.064483     0.169773    -0.0484293    -0.121334     -0.0528996    -0.0487501
  0.237025     0.164836    0.125458     0.178191    -0.0980655   -0.000944826   0.0175091   -0.0637557    0.138206    -0.102465     0.0722102   -0.0568034    0.000444596  -0.0212013   -0.0736082    0.0236634     0.00868579   0.138633     0.0688976     0.156733     -0.133448    -0.162767    -0.0279409     0.211271     -0.0616435     0.127575
  0.0968158    0.0894577   0.0741148    0.164001    -0.159764     0.04676       0.0723299    0.133057     0.048725    -0.0339822    0.011676     0.0895627    0.0507725    -0.181824    -0.237728     0.0940499    -0.0155019   -0.078514    -0.0126499    -0.0860012    -0.0925427   -0.0753139    0.164167      0.0629444     0.148961     -0.149007
  0.0494701   -0.190778   -0.0674159    0.0959358   -0.0407042   -0.0560927     0.0550212   -0.0637794   -0.0477263   -0.143167     0.083961    -0.0841361    0.152725      0.143821    -0.00928808  -0.100732      0.0243147   -0.192074     0.0143792    -0.0781721     0.0129933   -0.0425419    0.00427042   -0.0532255    -0.136193     -0.176978
 -0.0312973   -0.155081    0.0805729    0.0143719    0.132935     0.0526514    -0.0732941   -0.189346    -0.100179    -0.0461043   -0.136606     0.154817     0.03133       0.156434    -0.117238    -0.0528908     0.0707479    0.116753    -0.0755962     0.0541901    -0.0317678    0.0488096   -0.0023083    -0.0757697     0.13322      -0.0827177
 -0.0440418    0.0291054  -0.00595974   0.168925     0.0316789   -0.0466669     0.0708854    0.0692955   -0.00511277  -0.025684    -0.0164563    0.148144    -0.0148024     0.0982169   -0.0895797   -0.0548581     0.0493063    0.0237753   -0.0804602    -0.101543      0.00236663   0.232662     0.239208      0.120801     -0.236929      0.084429
 -0.137768    -0.0490222  -0.238347     0.0272836    0.0465931   -0.0327121    -0.036144    -0.186894     0.0207943    0.118268     0.02073     -0.201951     0.0934996    -0.0233995   -0.0491025    0.0917319    -0.14576     -0.0807882    0.0716547     0.0172589     0.0505215    0.136435    -0.224961      0.271922     -0.127934     -0.017111
 -0.0646298    0.0185405  -0.0441637    0.0453208   -0.049423     0.134488     -0.0755545   -0.14394     -0.0106121    0.158552    -0.12092     -0.0522002    0.0448736    -0.107694    -0.0113149    0.0315092     0.0266566    0.11212      0.0366988    -0.0648759    -0.207577     0.198501    -0.0801927    -0.0221595     0.0991462    -0.0240233
 -0.108577     0.0572581   0.160466    -0.0991928    0.143078     0.168323     -0.00190457   0.079434    -0.0926168   -0.0301273    0.0690435    0.0611743    0.00443204    0.0880905    0.103415    -0.0128969     0.0877751    0.132336     0.0821931     0.130106      0.0165423    0.0040024    0.0615173    -0.0546798    -0.0717593     0.0144243
 -0.125424     0.0296407   0.00575143   0.148424    -0.0499129    0.0322549     0.0212481   -0.1961      -0.10716     -0.105413    -0.0312379    0.0832179    0.0192677    -0.150361     0.0801542   -0.0253293     0.0348303   -0.0185022   -0.000994715  -0.164701     -0.13594     -0.322328    -0.012959      0.0143589    -0.0634489     0.0560772
  0.0311575   -0.0237528   0.00620346   0.167258     0.0858071   -0.132521      0.175637    -0.183765    -0.271652     0.201346    -0.214944     0.0709752    0.220572     -0.0561279    0.0631418    0.016987     -0.0102787   -0.0558544    0.115474      0.00234026   -0.0265155   -0.10556     -0.078172      0.0340451    -0.0929509    -0.0322241
  0.0562701   -0.109377   -0.202412     0.0575618   -0.00274062  -0.0205209    -0.185948     0.195861     0.0773866   -0.153639     0.0495404    0.221498     0.126603      0.0332929    0.0302366   -0.0225455     0.0564407    0.00732395   0.00382954    0.125483      0.193298     0.0401796    0.298893     -0.0990662    -0.0225246    -0.116434
 -0.0777381   -0.0324751   0.148537    -0.0133584    0.0490299    0.0802323    -0.132971    -0.19468      0.127305     0.258212     0.148381     0.0401823    0.115993     -0.0719504   -0.103398     0.0929042     0.0299008    0.0845502   -0.0288542    -0.0280784     0.0880111    0.0315022    0.0797852    -0.0515043     0.0647419    -0.0613376
  0.28247      0.0197151   0.0263712   -0.312402    -0.222394     0.0442948    -0.118084    -0.012978    -0.0651876    0.123819    -0.0167107   -0.057988     0.0701063    -0.190407    -0.0343602    0.00693729   -0.00152835   0.119191     0.163902     -0.116105     -0.0139235   -0.119234    -0.068555     -0.0245943     0.0401317     0.0893173
 -0.0695229    0.0661195   0.0431481    0.170122     0.0157578    0.0373815    -0.147906     0.0854865   -0.0156704    0.244825     0.0571931    0.00197799   0.15508       0.0453888    0.0361514    0.0804115    -0.132239    -0.115403    -0.0728659     0.0753882     0.00634808   0.2413      -0.0647949    -0.0600431    -0.0891012     0.0385947
  0.169889    -0.11365    -0.0784935   -0.0800902   -0.0567053    0.0446132     0.0491005    0.0375104    0.0899888   -0.0826462   -0.0143028   -0.131401     0.0382706    -0.0468287   -0.158482    -0.0204444     0.118372    -0.136571     0.0603749    -0.0681238     0.115002     0.148642     0.210088     -0.0397048    -0.00377386   -0.0487368
 -0.180004     0.0946595  -0.203927     0.150281    -0.00704526   0.0975119    -0.0456643   -0.0216609   -0.108134     0.0113597    0.0725464   -0.0990779   -0.0984501     0.0101426    0.0583134   -0.0269849     0.0391269    0.013715     0.0317164     0.0592473     0.042499     0.0342045    0.0315091     0.047625      0.0204008     0.0306455
 -0.020493    -0.233609    0.0194485   -0.0159574   -0.0888238    0.19626       0.142581    -0.0366823    0.0306628   -0.184447     0.0367255   -0.0323966   -0.0122563    -0.0491598   -0.0388338   -0.164292     -0.115515    -0.0504456   -0.045252      0.0490565    -0.0681234    0.103642    -0.161099     -0.0801322    -0.0740097     0.0771042
 -0.0454543    0.144509   -0.0231442    0.0947355    0.165483    -0.000293897   0.109842     0.0187367    0.113203    -0.0505999    0.173286    -0.0526889    0.0389795    -0.179683    -0.0330161   -0.0799293     0.0162912   -0.0649722   -0.0285941     0.000562608  -0.0514959    0.0527579   -0.0172923    -0.158862     -0.0878        0.115108
 -0.150982    -0.0593501  -0.00591631  -8.20059e-5   0.122764     0.0340044     0.024569    -0.128327    -0.0395151    0.0359717   -0.00327194   0.103112    -0.0542388     0.016154     0.14118      0.0672981     0.120017     0.148742    -0.0673144     0.0326129    -0.216347    -0.014373     0.101792     -0.0679626    -0.00307396    0.0119859
 -0.105364     0.0956984  -0.207489    -0.192139    -0.0248536    0.193301      0.042984    -0.0136182   -0.0230207   -0.0143388    0.0599909   -0.0230556    0.100933     -0.128325     0.0240684    0.0202566     0.112115    -0.119652     0.0811172    -0.0319966     0.0677245   -0.0199692   -0.000656899   0.087354      0.0672373    -0.0489694
  0.0456787    0.0191353  -0.0838932   -0.0603724    0.0122034    0.236952     -0.117179     0.150635    -0.121876    -0.0103085    0.189578    -0.00319233  -0.0314325     0.00517452  -0.238393     0.107761      0.125693    -0.0531244   -0.217717     -0.0758397     0.0361047   -0.205238    -0.161415     -0.0995937     0.010224      0.0201759
 -0.08864      0.0551904   0.0249959    0.115794    -0.187641    -0.042914     -0.0904207   -0.0866867    0.138834     0.240526     0.153611     0.0422789   -0.0508768     0.0668634   -0.0103512    0.0516206    -0.0542829   -0.0632436    0.0972606     0.0709815    -0.0272023   -0.0272411   -0.0787563    -0.109334      0.109799     -0.0202269
  0.0170892   -0.165937    0.00381592  -0.0643934   -0.0154864   -0.0455742     0.0214939   -0.132341     0.0891468    0.0390164    0.0240584   -0.0159969    0.123519     -0.0941957   -0.0459025   -0.00292469    0.0810247   -0.00297747  -0.0638254    -0.0571585     0.0821665    0.161718     0.039853      0.0353431    -0.186029      0.0392876
  0.15641      0.0862702   0.00491158  -0.0440564    0.0739921    0.0622706     0.111682    -0.0681449    0.0504602   -0.00910943   0.0567737    0.121473     0.0663311    -0.0306957   -0.0214026   -0.0624249    -0.048914     0.120652    -0.063928      0.0536558    -0.218295    -0.0693813    0.170354     -0.119445      0.000729178  -0.119718kind full, method split
┌ Info: 0: avll = 
└   tll[1] = -1.4200481384686912
[ Info: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.420067
[ Info: iteration 2, average log likelihood -1.420009
[ Info: iteration 3, average log likelihood -1.419963
[ Info: iteration 4, average log likelihood -1.419906
[ Info: iteration 5, average log likelihood -1.419834
[ Info: iteration 6, average log likelihood -1.419748
[ Info: iteration 7, average log likelihood -1.419652
[ Info: iteration 8, average log likelihood -1.419553
[ Info: iteration 9, average log likelihood -1.419458
[ Info: iteration 10, average log likelihood -1.419366
[ Info: iteration 11, average log likelihood -1.419258
[ Info: iteration 12, average log likelihood -1.419094
[ Info: iteration 13, average log likelihood -1.418807
[ Info: iteration 14, average log likelihood -1.418304
[ Info: iteration 15, average log likelihood -1.417529
[ Info: iteration 16, average log likelihood -1.416588
[ Info: iteration 17, average log likelihood -1.415761
[ Info: iteration 18, average log likelihood -1.415232
[ Info: iteration 19, average log likelihood -1.414961
[ Info: iteration 20, average log likelihood -1.414838
[ Info: iteration 21, average log likelihood -1.414784
[ Info: iteration 22, average log likelihood -1.414760
[ Info: iteration 23, average log likelihood -1.414750
[ Info: iteration 24, average log likelihood -1.414745
[ Info: iteration 25, average log likelihood -1.414743
[ Info: iteration 26, average log likelihood -1.414742
[ Info: iteration 27, average log likelihood -1.414742
[ Info: iteration 28, average log likelihood -1.414741
[ Info: iteration 29, average log likelihood -1.414741
[ Info: iteration 30, average log likelihood -1.414741
[ Info: iteration 31, average log likelihood -1.414741
[ Info: iteration 32, average log likelihood -1.414741
[ Info: iteration 33, average log likelihood -1.414741
[ Info: iteration 34, average log likelihood -1.414741
[ Info: iteration 35, average log likelihood -1.414741
[ Info: iteration 36, average log likelihood -1.414741
[ Info: iteration 37, average log likelihood -1.414741
[ Info: iteration 38, average log likelihood -1.414741
[ Info: iteration 39, average log likelihood -1.414741
[ Info: iteration 40, average log likelihood -1.414741
[ Info: iteration 41, average log likelihood -1.414741
[ Info: iteration 42, average log likelihood -1.414741
[ Info: iteration 43, average log likelihood -1.414741
[ Info: iteration 44, average log likelihood -1.414741
[ Info: iteration 45, average log likelihood -1.414741
[ Info: iteration 46, average log likelihood -1.414741
[ Info: iteration 47, average log likelihood -1.414741
[ Info: iteration 48, average log likelihood -1.414741
[ Info: iteration 49, average log likelihood -1.414741
[ Info: iteration 50, average log likelihood -1.414741
┌ Info: EM with 100000 data points 50 iterations avll -1.414741
└ 952.4 data points per parameter
┌ Info: 1
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4200665997002226
│     -1.4200087660607505
│      ⋮
└     -1.4147406238391744
[ Info: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.414755
[ Info: iteration 2, average log likelihood -1.414700
[ Info: iteration 3, average log likelihood -1.414649
[ Info: iteration 4, average log likelihood -1.414585
[ Info: iteration 5, average log likelihood -1.414502
[ Info: iteration 6, average log likelihood -1.414399
[ Info: iteration 7, average log likelihood -1.414283
[ Info: iteration 8, average log likelihood -1.414165
[ Info: iteration 9, average log likelihood -1.414058
[ Info: iteration 10, average log likelihood -1.413969
[ Info: iteration 11, average log likelihood -1.413901
[ Info: iteration 12, average log likelihood -1.413851
[ Info: iteration 13, average log likelihood -1.413816
[ Info: iteration 14, average log likelihood -1.413791
[ Info: iteration 15, average log likelihood -1.413773
[ Info: iteration 16, average log likelihood -1.413759
[ Info: iteration 17, average log likelihood -1.413748
[ Info: iteration 18, average log likelihood -1.413739
[ Info: iteration 19, average log likelihood -1.413730
[ Info: iteration 20, average log likelihood -1.413723
[ Info: iteration 21, average log likelihood -1.413716
[ Info: iteration 22, average log likelihood -1.413709
[ Info: iteration 23, average log likelihood -1.413702
[ Info: iteration 24, average log likelihood -1.413696
[ Info: iteration 25, average log likelihood -1.413690
[ Info: iteration 26, average log likelihood -1.413684
[ Info: iteration 27, average log likelihood -1.413677
[ Info: iteration 28, average log likelihood -1.413672
[ Info: iteration 29, average log likelihood -1.413666
[ Info: iteration 30, average log likelihood -1.413660
[ Info: iteration 31, average log likelihood -1.413654
[ Info: iteration 32, average log likelihood -1.413648
[ Info: iteration 33, average log likelihood -1.413642
[ Info: iteration 34, average log likelihood -1.413637
[ Info: iteration 35, average log likelihood -1.413631
[ Info: iteration 36, average log likelihood -1.413625
[ Info: iteration 37, average log likelihood -1.413620
[ Info: iteration 38, average log likelihood -1.413614
[ Info: iteration 39, average log likelihood -1.413609
[ Info: iteration 40, average log likelihood -1.413603
[ Info: iteration 41, average log likelihood -1.413598
[ Info: iteration 42, average log likelihood -1.413593
[ Info: iteration 43, average log likelihood -1.413588
[ Info: iteration 44, average log likelihood -1.413583
[ Info: iteration 45, average log likelihood -1.413579
[ Info: iteration 46, average log likelihood -1.413574
[ Info: iteration 47, average log likelihood -1.413570
[ Info: iteration 48, average log likelihood -1.413566
[ Info: iteration 49, average log likelihood -1.413562
[ Info: iteration 50, average log likelihood -1.413558
┌ Info: EM with 100000 data points 50 iterations avll -1.413558
└ 473.9 data points per parameter
┌ Info: 2
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4147551127311093
│     -1.4146995956176753
│      ⋮
└     -1.4135579232820463
[ Info: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.413565
[ Info: iteration 2, average log likelihood -1.413520
[ Info: iteration 3, average log likelihood -1.413482
[ Info: iteration 4, average log likelihood -1.413440
[ Info: iteration 5, average log likelihood -1.413389
[ Info: iteration 6, average log likelihood -1.413328
[ Info: iteration 7, average log likelihood -1.413256
[ Info: iteration 8, average log likelihood -1.413174
[ Info: iteration 9, average log likelihood -1.413084
[ Info: iteration 10, average log likelihood -1.412991
[ Info: iteration 11, average log likelihood -1.412900
[ Info: iteration 12, average log likelihood -1.412814
[ Info: iteration 13, average log likelihood -1.412735
[ Info: iteration 14, average log likelihood -1.412664
[ Info: iteration 15, average log likelihood -1.412600
[ Info: iteration 16, average log likelihood -1.412543
[ Info: iteration 17, average log likelihood -1.412492
[ Info: iteration 18, average log likelihood -1.412446
[ Info: iteration 19, average log likelihood -1.412405
[ Info: iteration 20, average log likelihood -1.412370
[ Info: iteration 21, average log likelihood -1.412338
[ Info: iteration 22, average log likelihood -1.412310
[ Info: iteration 23, average log likelihood -1.412286
[ Info: iteration 24, average log likelihood -1.412265
[ Info: iteration 25, average log likelihood -1.412246
[ Info: iteration 26, average log likelihood -1.412230
[ Info: iteration 27, average log likelihood -1.412216
[ Info: iteration 28, average log likelihood -1.412204
[ Info: iteration 29, average log likelihood -1.412192
[ Info: iteration 30, average log likelihood -1.412183
[ Info: iteration 31, average log likelihood -1.412174
[ Info: iteration 32, average log likelihood -1.412166
[ Info: iteration 33, average log likelihood -1.412158
[ Info: iteration 34, average log likelihood -1.412152
[ Info: iteration 35, average log likelihood -1.412146
[ Info: iteration 36, average log likelihood -1.412140
[ Info: iteration 37, average log likelihood -1.412135
[ Info: iteration 38, average log likelihood -1.412131
[ Info: iteration 39, average log likelihood -1.412126
[ Info: iteration 40, average log likelihood -1.412122
[ Info: iteration 41, average log likelihood -1.412118
[ Info: iteration 42, average log likelihood -1.412115
[ Info: iteration 43, average log likelihood -1.412111
[ Info: iteration 44, average log likelihood -1.412108
[ Info: iteration 45, average log likelihood -1.412105
[ Info: iteration 46, average log likelihood -1.412102
[ Info: iteration 47, average log likelihood -1.412100
[ Info: iteration 48, average log likelihood -1.412097
[ Info: iteration 49, average log likelihood -1.412094
[ Info: iteration 50, average log likelihood -1.412092
┌ Info: EM with 100000 data points 50 iterations avll -1.412092
└ 236.4 data points per parameter
┌ Info: 3
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.413564822461802
│     -1.4135198965868654
│      ⋮
└     -1.4120916502802783
[ Info: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.412097
[ Info: iteration 2, average log likelihood -1.412047
[ Info: iteration 3, average log likelihood -1.412001
[ Info: iteration 4, average log likelihood -1.411948
[ Info: iteration 5, average log likelihood -1.411884
[ Info: iteration 6, average log likelihood -1.411806
[ Info: iteration 7, average log likelihood -1.411713
[ Info: iteration 8, average log likelihood -1.411605
[ Info: iteration 9, average log likelihood -1.411486
[ Info: iteration 10, average log likelihood -1.411362
[ Info: iteration 11, average log likelihood -1.411239
[ Info: iteration 12, average log likelihood -1.411122
[ Info: iteration 13, average log likelihood -1.411015
[ Info: iteration 14, average log likelihood -1.410919
[ Info: iteration 15, average log likelihood -1.410834
[ Info: iteration 16, average log likelihood -1.410762
[ Info: iteration 17, average log likelihood -1.410699
[ Info: iteration 18, average log likelihood -1.410644
[ Info: iteration 19, average log likelihood -1.410596
[ Info: iteration 20, average log likelihood -1.410553
[ Info: iteration 21, average log likelihood -1.410514
[ Info: iteration 22, average log likelihood -1.410479
[ Info: iteration 23, average log likelihood -1.410446
[ Info: iteration 24, average log likelihood -1.410416
[ Info: iteration 25, average log likelihood -1.410388
[ Info: iteration 26, average log likelihood -1.410361
[ Info: iteration 27, average log likelihood -1.410337
[ Info: iteration 28, average log likelihood -1.410313
[ Info: iteration 29, average log likelihood -1.410291
[ Info: iteration 30, average log likelihood -1.410270
[ Info: iteration 31, average log likelihood -1.410250
[ Info: iteration 32, average log likelihood -1.410231
[ Info: iteration 33, average log likelihood -1.410213
[ Info: iteration 34, average log likelihood -1.410196
[ Info: iteration 35, average log likelihood -1.410180
[ Info: iteration 36, average log likelihood -1.410165
[ Info: iteration 37, average log likelihood -1.410150
[ Info: iteration 38, average log likelihood -1.410136
[ Info: iteration 39, average log likelihood -1.410122
[ Info: iteration 40, average log likelihood -1.410109
[ Info: iteration 41, average log likelihood -1.410096
[ Info: iteration 42, average log likelihood -1.410084
[ Info: iteration 43, average log likelihood -1.410073
[ Info: iteration 44, average log likelihood -1.410062
[ Info: iteration 45, average log likelihood -1.410051
[ Info: iteration 46, average log likelihood -1.410041
[ Info: iteration 47, average log likelihood -1.410031
[ Info: iteration 48, average log likelihood -1.410021
[ Info: iteration 49, average log likelihood -1.410012
[ Info: iteration 50, average log likelihood -1.410003
┌ Info: EM with 100000 data points 50 iterations avll -1.410003
└ 118.1 data points per parameter
┌ Info: 4
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.41209669677439
│     -1.4120466211299483
│      ⋮
└     -1.4100032888564753
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.410003
[ Info: iteration 2, average log likelihood -1.409937
[ Info: iteration 3, average log likelihood -1.409873
[ Info: iteration 4, average log likelihood -1.409798
[ Info: iteration 5, average log likelihood -1.409705
[ Info: iteration 6, average log likelihood -1.409589
[ Info: iteration 7, average log likelihood -1.409449
[ Info: iteration 8, average log likelihood -1.409289
[ Info: iteration 9, average log likelihood -1.409117
[ Info: iteration 10, average log likelihood -1.408945
[ Info: iteration 11, average log likelihood -1.408780
[ Info: iteration 12, average log likelihood -1.408628
[ Info: iteration 13, average log likelihood -1.408492
[ Info: iteration 14, average log likelihood -1.408373
[ Info: iteration 15, average log likelihood -1.408268
[ Info: iteration 16, average log likelihood -1.408176
[ Info: iteration 17, average log likelihood -1.408095
[ Info: iteration 18, average log likelihood -1.408023
[ Info: iteration 19, average log likelihood -1.407959
[ Info: iteration 20, average log likelihood -1.407901
[ Info: iteration 21, average log likelihood -1.407849
[ Info: iteration 22, average log likelihood -1.407800
[ Info: iteration 23, average log likelihood -1.407756
[ Info: iteration 24, average log likelihood -1.407715
[ Info: iteration 25, average log likelihood -1.407677
[ Info: iteration 26, average log likelihood -1.407642
[ Info: iteration 27, average log likelihood -1.407609
[ Info: iteration 28, average log likelihood -1.407578
[ Info: iteration 29, average log likelihood -1.407550
[ Info: iteration 30, average log likelihood -1.407522
[ Info: iteration 31, average log likelihood -1.407497
[ Info: iteration 32, average log likelihood -1.407473
[ Info: iteration 33, average log likelihood -1.407450
[ Info: iteration 34, average log likelihood -1.407429
[ Info: iteration 35, average log likelihood -1.407408
[ Info: iteration 36, average log likelihood -1.407389
[ Info: iteration 37, average log likelihood -1.407370
[ Info: iteration 38, average log likelihood -1.407353
[ Info: iteration 39, average log likelihood -1.407336
[ Info: iteration 40, average log likelihood -1.407319
[ Info: iteration 41, average log likelihood -1.407303
[ Info: iteration 42, average log likelihood -1.407288
[ Info: iteration 43, average log likelihood -1.407273
[ Info: iteration 44, average log likelihood -1.407259
[ Info: iteration 45, average log likelihood -1.407244
[ Info: iteration 46, average log likelihood -1.407230
[ Info: iteration 47, average log likelihood -1.407216
[ Info: iteration 48, average log likelihood -1.407202
[ Info: iteration 49, average log likelihood -1.407189
[ Info: iteration 50, average log likelihood -1.407175
┌ Info: EM with 100000 data points 50 iterations avll -1.407175
└ 59.0 data points per parameter
┌ Info: 5
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4100026241391002
│     -1.4099370080125915
│      ⋮
└     -1.4071748075649384
┌ Info: Total log likelihood: 
│   tll =
│    251-element Array{Float64,1}:
│     -1.4200481384686912
│     -1.4200665997002226
│     -1.4200087660607505
│     -1.4199626958747738
│      ⋮
│     -1.4072022697136652
│     -1.4071885066431178
└     -1.4071748075649384
32×26 Array{Float64,2}:
 -0.833775   -0.462484     0.179464    -0.335922    -0.0355948    0.571606   -0.558267     0.255861    0.380703    -0.558503    -0.20476      0.366067    0.0438115   0.598914   -0.0556066   -0.156972    0.163281     0.515565    -0.688807   -0.0919273  -0.102686    -0.242588   -0.0025484  -0.0567091  -0.304455    -0.16234
  0.223959   -0.551585    -0.148029    -0.185597    -1.30545      0.422838    0.126598     0.213798    0.306899    -0.452786    -0.32767      0.252628   -0.0116128  -0.536514   -0.174032    -0.276973    0.0510501    0.473699     0.182201   -0.490918   -0.0923627   -0.333475    0.125021    0.176504   -0.253294    -0.195869
  0.0448128   0.054728     0.113267    -0.379912     0.235425     0.309451    0.344735     0.17628     0.447922    -0.201386     0.0180321   -0.0647367   0.0636598   0.489144    0.130615     0.0814784   0.0867827    0.143457    -0.131784   -0.0789108  -0.184164     0.317965   -0.0629974   0.117459   -0.0237904   -0.0818362
 -0.182936   -0.332957    -0.127789    -0.084782    -0.61071     -0.131131   -0.059586    -0.168899    0.227203     0.716193     0.0594372   -0.0929134   0.0883892  -0.843422   -0.303098     0.0519625  -0.229906     0.215906     0.214023    0.182458    0.0382254    0.108178    0.0421966   0.23205     0.137315    -0.0195266
  0.101701    0.375629     0.51637      0.739157     0.398361     0.173575   -0.19004     -0.11319    -0.00327678  -0.509514    -0.69805      0.558353   -0.381327   -0.392674   -0.169833     0.119498    0.214915     0.101002    -0.042235   -0.1815     -0.123881     0.25267     0.0273738  -0.725298   -0.290714    -0.25491
 -0.263891    0.506037    -0.535067     0.98957     -0.298308     0.344316    0.510447     0.253055   -0.287966    -0.306672     0.233545    -0.331302   -0.200433    0.499616    0.548722     0.089629    0.487634     0.00844254   0.150634   -0.319197    0.334322    -0.32667     0.0108736  -0.672278   -0.783222     0.00448241
 -0.165452    0.326097     0.338064     0.382542     0.0661899    0.174667    0.120838     0.054565   -0.176812    -0.384084    -0.438567     0.623363   -0.512941    0.0871364   0.452867     0.532161   -0.207255     0.430656     0.491416   -0.329521   -0.122762    -0.153098   -0.393684    0.468547    0.305708    -0.340773
  0.0399576  -0.064584     0.030805    -0.264126    -0.723893    -0.0295562   0.0868904    0.431335   -0.183891    -0.0941234   -0.308332     0.603752    0.566479    0.0632202  -0.0265705   -0.101756   -0.675093     0.0550882    0.953742    0.308084   -0.239466    -0.0285783   0.0437489   0.401092    0.563371     0.1725
  0.430921    0.324807    -0.420854     0.00816963   0.348919     0.0182275  -0.165204     0.16282    -0.543145    -0.388915    -0.085123    -0.0468213   0.101278   -0.527463    0.648174     0.110256   -0.291953    -0.865736    -0.858069   -0.345738   -0.518546    -0.112015    0.266067    0.164945    0.109551     0.0943386
  0.493337    0.592945     0.0854883   -0.285918     1.0277       0.178288    0.207305    -0.369965   -0.331463    -0.159355    -0.0442928    0.374217   -0.0413002   0.126549    0.0976273    0.241851    0.13366     -0.667875     0.278998    0.332121   -0.25608      0.347602    0.173436    0.0975113   0.144839     0.151798
 -0.0784952   0.299964    -0.0779426   -0.136461     0.490426    -0.373446   -0.324029    -0.19386     0.701014     0.127894     0.0865267   -0.317964   -0.331682    0.655545    0.192825     0.387754   -0.111919    -0.504623    -0.433173    0.708652    0.229043     0.696375   -0.279534   -0.162577   -0.224628     0.205625
 -0.253369   -0.00461677  -0.427472     0.0382455   -0.147546    -0.150854    0.293129    -0.400175    0.565465    -0.329098     0.0592287    0.0175889   0.0598728   0.22778    -0.37224     -0.335222   -0.433513     0.511919     0.318668    0.05884     0.535362     0.740932   -0.227827   -0.0595577  -0.601219     0.281954
 -0.342856   -0.308625     0.130131    -0.216849     0.085508    -0.481627   -0.922084     0.0775229   0.00417968   0.0201707    0.00659802  -0.108869   -0.710718   -0.13032    -0.334109    -0.350602    0.0528898   -0.100861    -0.148543    0.18739    -0.668065     0.283949    0.233454   -0.0437995   0.304587     0.0643023
  0.0716477  -0.0838324   -0.260494    -0.154163     0.205335    -0.320365   -0.224851    -0.133675    0.0617862    0.458789     0.456598    -0.389441    0.778766   -0.264426   -0.304428    -0.678727   -0.0109064   -0.284392    -0.403098    0.455664   -0.00807767   0.116662    0.253747   -0.518842   -0.277307     0.534669
  0.237104   -0.0459558   -0.993086     0.0652037    0.165952    -0.291487   -0.1072      -0.248797   -0.933966    -0.00995057   0.653656    -0.0486606   0.0770475   0.265961   -0.149492    -0.31483    -0.414297    -0.286866     0.206374   -0.300784    0.411171     0.0122131  -0.109318   -0.0845621  -0.318123     0.0444535
  0.168686   -0.124332     0.600315     0.237023     0.0667595   -0.116497   -0.349388    -0.088488   -0.622817    -0.158708     0.518884     0.0592356   0.556691   -0.0103327   0.25454     -0.260773    0.670466    -0.19233     -0.0397889   0.132651    0.0220876   -1.10309     0.372179    0.0249296   0.123491     0.263673
 -0.184799   -0.454489     0.182871     0.125695    -0.274901    -0.148253   -0.123118     0.0237168  -0.685046    -0.042336     0.274159     0.205441    0.41148     0.0783123  -0.129048    -0.51405     0.176094     0.126763     0.0721359  -0.421081    0.23957     -0.431451   -0.353295    0.273743   -0.221286    -0.0424348
  0.28749    -0.0342725    0.42485      0.361134    -0.333945    -0.391123    0.00290427   0.0708678  -0.15334     -0.217154     0.0239362    0.129911    0.241828   -0.136702    0.131722    -0.17342     0.228878    -0.198379     0.225337    0.0745796  -0.0453891   -0.489803    0.259342    0.253024    0.0880864    0.379009
 -0.0107254  -0.0296598   -0.115033     0.216776     0.00188644   0.0165433  -0.0594189   -0.106143   -0.0515903   -0.0140084   -0.0155196   -0.131279    0.0155471  -0.0724771   0.0313011   -0.0756449   0.0145074   -0.0498481   -0.115161   -0.0944934   0.00262178   0.0551602   0.0949188  -0.473633   -0.0269523   -0.0995592
 -0.0386955   0.109886    -0.00991017  -0.176624     0.0269992    0.10749     0.140973     0.0061846   0.0722719    0.0475134   -0.00462232   0.101045    0.0509578   0.0508402  -0.0497224    0.0760439  -0.0708614   -0.0229986    0.0727947   0.0647986  -0.0759296    0.125834   -0.141359    0.324092   -0.0206619    0.0422072
 -0.288237   -0.105977    -0.66589     -0.789572    -0.164795     0.274583   -0.214744    -0.085779   -0.098366     0.303372    -0.00849991   0.0501776  -0.0879675  -0.0313897  -0.474775     0.370719   -0.139822    -0.281485    -0.0693932   0.0180527  -0.11517      0.527654    0.152679    0.368871   -0.00122316  -0.339693
 -0.31161    -0.0983389   -0.237039     0.394595    -0.217557    -0.229074   -0.272396    -0.274585   -0.417669     0.363634    -0.396753     0.307776   -0.42055    -0.242439   -0.861622    -0.144112   -0.153892    -0.167362     0.123968    0.26175     0.647719     0.316484   -0.135291   -0.0596378  -0.25433     -0.372465
  0.0337436   0.229046    -0.418624     0.304605    -0.40218     -0.0286105  -0.275224    -0.678086   -0.591337     0.305788     0.249304    -0.290649    0.386681   -0.195304    0.271564     0.641508   -0.181405     0.0851797    0.329957   -0.137667    0.369623    -0.20002     0.122191   -0.140043    0.509621    -0.0264147
 -0.338657   -0.158263     0.00618107   0.268288    -0.265274     0.455298    0.164295     0.422078    0.289933     0.348699     0.382173    -0.282103    0.445908    0.0691112   0.17418      0.392567   -0.524999     0.531454     0.125623   -0.106404    0.188795    -0.370721    0.0880328  -0.10881     0.33958     -0.578914
  0.108171    0.219013     0.564281    -0.606828     0.323838     0.4009      0.396338     0.0576359   0.702914    -0.140175    -0.452473    -0.384637    0.0305174  -0.0485841   0.0959957   -0.622126    0.276809    -0.183284    -0.0867286  -0.0911471  -0.332715     0.275865   -0.772325   -0.0420627  -0.241495    -0.043421
  0.233972   -0.279944     0.0134766   -0.591404    -0.154497    -0.0351176   0.369969     0.01231     0.564757     0.0506799    0.543503    -0.520059    0.515051    0.246105    0.0952067   -0.131512    0.403367    -0.293473    -0.251177   -0.0241005  -0.533963    -0.180664   -0.187539    0.496358   -0.170744     0.385218
  0.409731   -0.169047     0.691234     0.673719     0.245907    -0.0316016   0.907113     0.0429439   0.227959    -0.161003     0.0985813    0.27518     0.370602   -0.0876031   0.143904    -0.348455    0.110133     0.118568    -0.0822741  -0.196667    0.031229    -0.512376   -0.335642    0.0376055  -0.0888798    0.656273
 -0.265719    0.979235     0.535785     0.911538     0.392148    -0.382765    0.553383     0.0635416  -0.0649358    0.290499    -0.107423    -0.458612    0.0197204  -0.0809755  -0.00451584   0.375864   -0.00753705  -0.0457886   -0.0707076  -0.338307   -0.211396     0.208557   -0.236061   -0.31244     0.529941     0.0379164
  0.219553    0.0543755    0.459741     0.108761     0.307634     0.206483   -0.260162    -0.298626    0.280397    -0.902755    -0.341707    -0.430992   -0.115437    0.537144    0.585992    -0.134594    0.261447     0.626546     0.487391   -0.183642    0.0502721    0.313068    1.2679     -0.796657    0.401916     0.18523
  0.233852    0.155679     0.40194      0.0660236    0.766863     0.36895     0.0996129    0.456976   -0.171055    -0.380547     0.00201197   0.676362   -0.0646568   0.649472   -0.0445608    0.171976    0.252507     0.186574    -0.138364    0.33702     0.125624    -0.0715543   0.246471   -0.218013   -0.0501708   -0.309589
 -0.144157   -0.0870352    0.246093     0.169349     0.319162     0.41404    -0.0651725   -0.408905   -0.0832656    0.660506    -0.0917291   -0.528351   -0.101368   -0.86608     0.0920241    0.542748    0.614749    -0.134115    -0.723584   -0.23562    -0.0841857   -0.0458234  -0.167533   -0.390896   -0.245689    -0.532954
  0.108037   -0.31557      0.116252    -0.284168    -0.184606     0.093701   -0.104647    -0.154438    0.442632     0.227677     0.102349    -0.0900858   0.0494059  -0.576323    0.116005     0.268694    0.446006     0.234351    -0.14956     0.478348   -0.313776    -0.105208    0.72322    -0.1507     -0.045908    -0.00252415[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.407161
[ Info: iteration 2, average log likelihood -1.407148
[ Info: iteration 3, average log likelihood -1.407134
[ Info: iteration 4, average log likelihood -1.407120
[ Info: iteration 5, average log likelihood -1.407107
[ Info: iteration 6, average log likelihood -1.407093
[ Info: iteration 7, average log likelihood -1.407080
[ Info: iteration 8, average log likelihood -1.407066
[ Info: iteration 9, average log likelihood -1.407053
[ Info: iteration 10, average log likelihood -1.407039
┌ Info: EM with 100000 data points 10 iterations avll -1.407039
└ 59.0 data points per parameter
kind full, method kmeans
[ Info: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.931978e+05
      1       7.005664e+05      -1.926314e+05 |       32
      2       6.873738e+05      -1.319258e+04 |       32
      3       6.822982e+05      -5.075598e+03 |       32
      4       6.796065e+05      -2.691751e+03 |       32
      5       6.778283e+05      -1.778133e+03 |       32
      6       6.765333e+05      -1.295000e+03 |       32
      7       6.755717e+05      -9.615923e+02 |       32
      8       6.748265e+05      -7.452895e+02 |       32
      9       6.742209e+05      -6.055148e+02 |       32
     10       6.737087e+05      -5.121941e+02 |       32
     11       6.732911e+05      -4.176545e+02 |       32
     12       6.729115e+05      -3.795895e+02 |       32
     13       6.725980e+05      -3.134680e+02 |       32
     14       6.723443e+05      -2.537682e+02 |       32
     15       6.721237e+05      -2.205625e+02 |       32
     16       6.719139e+05      -2.097684e+02 |       32
     17       6.717384e+05      -1.755498e+02 |       32
     18       6.715865e+05      -1.518891e+02 |       32
     19       6.714418e+05      -1.446905e+02 |       32
     20       6.713057e+05      -1.361244e+02 |       32
     21       6.711837e+05      -1.219905e+02 |       32
     22       6.710754e+05      -1.083121e+02 |       32
     23       6.709785e+05      -9.686710e+01 |       32
     24       6.708863e+05      -9.219890e+01 |       32
     25       6.708014e+05      -8.488281e+01 |       32
     26       6.707205e+05      -8.096157e+01 |       32
     27       6.706413e+05      -7.920955e+01 |       32
     28       6.705636e+05      -7.767059e+01 |       32
     29       6.704918e+05      -7.176762e+01 |       32
     30       6.704226e+05      -6.924807e+01 |       32
     31       6.703556e+05      -6.692425e+01 |       32
     32       6.702973e+05      -5.837280e+01 |       32
     33       6.702380e+05      -5.928786e+01 |       32
     34       6.701718e+05      -6.623585e+01 |       32
     35       6.701054e+05      -6.637626e+01 |       32
     36       6.700493e+05      -5.604442e+01 |       32
     37       6.699934e+05      -5.590917e+01 |       32
     38       6.699358e+05      -5.766366e+01 |       32
     39       6.698880e+05      -4.773311e+01 |       32
     40       6.698510e+05      -3.706229e+01 |       32
     41       6.698178e+05      -3.320202e+01 |       32
     42       6.697875e+05      -3.023952e+01 |       32
     43       6.697525e+05      -3.500641e+01 |       32
     44       6.697188e+05      -3.375366e+01 |       32
     45       6.696902e+05      -2.859811e+01 |       32
     46       6.696628e+05      -2.736941e+01 |       32
     47       6.696352e+05      -2.759165e+01 |       32
     48       6.696103e+05      -2.489519e+01 |       32
     49       6.695866e+05      -2.371892e+01 |       32
     50       6.695629e+05      -2.368405e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 669562.903140753)
┌ Info: K-means with 32000 data points using 50 iterations
└ 37.0 data points per parameter
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.418860
[ Info: iteration 2, average log likelihood -1.413850
[ Info: iteration 3, average log likelihood -1.412550
[ Info: iteration 4, average log likelihood -1.411639
[ Info: iteration 5, average log likelihood -1.410698
[ Info: iteration 6, average log likelihood -1.409785
[ Info: iteration 7, average log likelihood -1.409090
[ Info: iteration 8, average log likelihood -1.408659
[ Info: iteration 9, average log likelihood -1.408403
[ Info: iteration 10, average log likelihood -1.408233
[ Info: iteration 11, average log likelihood -1.408104
[ Info: iteration 12, average log likelihood -1.407998
[ Info: iteration 13, average log likelihood -1.407905
[ Info: iteration 14, average log likelihood -1.407821
[ Info: iteration 15, average log likelihood -1.407746
[ Info: iteration 16, average log likelihood -1.407677
[ Info: iteration 17, average log likelihood -1.407614
[ Info: iteration 18, average log likelihood -1.407557
[ Info: iteration 19, average log likelihood -1.407506
[ Info: iteration 20, average log likelihood -1.407459
[ Info: iteration 21, average log likelihood -1.407416
[ Info: iteration 22, average log likelihood -1.407376
[ Info: iteration 23, average log likelihood -1.407340
[ Info: iteration 24, average log likelihood -1.407307
[ Info: iteration 25, average log likelihood -1.407276
[ Info: iteration 26, average log likelihood -1.407247
[ Info: iteration 27, average log likelihood -1.407220
[ Info: iteration 28, average log likelihood -1.407195
[ Info: iteration 29, average log likelihood -1.407172
[ Info: iteration 30, average log likelihood -1.407150
[ Info: iteration 31, average log likelihood -1.407129
[ Info: iteration 32, average log likelihood -1.407109
[ Info: iteration 33, average log likelihood -1.407090
[ Info: iteration 34, average log likelihood -1.407072
[ Info: iteration 35, average log likelihood -1.407055
[ Info: iteration 36, average log likelihood -1.407038
[ Info: iteration 37, average log likelihood -1.407023
[ Info: iteration 38, average log likelihood -1.407007
[ Info: iteration 39, average log likelihood -1.406993
[ Info: iteration 40, average log likelihood -1.406979
[ Info: iteration 41, average log likelihood -1.406965
[ Info: iteration 42, average log likelihood -1.406952
[ Info: iteration 43, average log likelihood -1.406940
[ Info: iteration 44, average log likelihood -1.406927
[ Info: iteration 45, average log likelihood -1.406916
[ Info: iteration 46, average log likelihood -1.406904
[ Info: iteration 47, average log likelihood -1.406893
[ Info: iteration 48, average log likelihood -1.406882
[ Info: iteration 49, average log likelihood -1.406872
[ Info: iteration 50, average log likelihood -1.406862
┌ Info: EM with 100000 data points 50 iterations avll -1.406862
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.102833   -0.195024    0.346577   -0.038648    -0.362124   -0.204698     0.0458095   0.32331    -0.571208   -0.162761    0.264203     0.495193    0.492931    0.0849372  -0.124178   -0.491728    0.266752    0.0341563    0.486657      0.0486848    0.0205363  -0.80104     -0.034743     0.502591   -0.0645263   0.333432
 -0.0130751   0.0710249   0.0924731   0.182757     0.0242203  -0.158872     0.0935918   0.0284916   0.0769347  -0.14792    -0.0824071    0.0713308   0.0386139   0.176327    0.06584    -0.168564   -0.0557071   0.00500959   0.0331402     0.0565479    0.0754028   0.0766948    0.0421793   -0.070083   -0.047272    0.184959
 -0.260746   -0.460338    0.136484    0.18657     -0.37089     0.267303     0.541741   -0.176613    0.52379     0.166325   -0.170759     0.0838783  -0.096945   -0.443297   -0.549369   -0.260198    0.11658     0.637727     0.0982028    -0.446887     0.413321    0.553074    -0.198199     0.130213   -0.351061   -0.0669387
  0.0207042  -0.746514   -0.16246    -0.390587    -0.806731    0.468853    -0.394902    0.159651    0.0708793  -0.721866   -0.21144      0.244814    0.0626562   0.0228213  -0.052571   -0.48796     0.0533637   0.328645    -0.115274     -0.388695    -0.191305   -0.607516     0.00687323   0.091154   -0.381413   -0.17185
 -0.216075    0.0962175  -0.0286761   0.653414     0.0390583  -0.30147     -0.411449   -0.33797    -0.44732     0.295016   -0.633134     0.53672    -0.672054   -0.135119   -0.897055   -0.189905   -0.11292    -0.190376     0.0520409     0.575494     0.811698    0.451338    -0.0566676   -0.248237   -0.330818   -0.245771
 -0.1145      0.259206   -0.183651   -0.0748574    0.335527   -0.122053    -0.173462   -0.405542    0.902421    0.126292    0.00511727  -0.43441    -0.387421    0.376696    0.192299    0.74338    -0.231832   -0.108407    -0.420655      0.595866     0.345349    0.706787    -0.169286    -0.0126647  -0.488317    0.0646101
 -0.143716   -0.515953    0.977659   -0.575553     0.300651   -0.121086     0.0233687   0.438199    0.91932    -0.170844   -0.111903    -0.221533   -0.139088    0.0905516  -0.165789   -0.640435    0.155542    0.0134246   -0.529238     -0.0627775   -0.611537    0.10818      0.0924992    0.333993    0.16206     0.0317426
 -0.0507094   0.226531   -0.261969    1.0383      -0.203417    0.127105     0.405566   -0.0678279  -0.431748   -0.0114925   0.382576    -0.348108    0.0385449   0.246883    0.275006   -0.0745018   0.321145   -0.0580162    0.0650924    -0.441175     0.75212    -0.581616    -0.20812     -0.268249   -0.455914    0.0347533
 -0.19633    -0.297434    0.374327    0.136801    -0.559928    0.903239     0.0895501   0.722213    0.527105    0.283361   -0.214787     0.427927    0.718245   -0.207717   -0.0774159   0.744691   -0.308019    0.174741     0.398148      0.63437     -0.0287082  -0.217973     0.286609    -0.0740787   0.305336   -0.407201
  0.0646359  -0.0649441   0.022296   -0.00212849  -0.185564    0.102612     0.162884   -0.0257682   0.0831657  -0.0445932  -0.0719759    0.0879481   0.0587792  -0.160435   -0.0386981  -0.0440159   0.0337568   0.156592     0.16012      -0.064293    -0.0313491   0.0640499   -0.0254565    0.0824506  -0.0108317  -0.00394387
 -0.0291844  -0.0767619   0.266688   -0.0629649    0.0377859   0.251827    -0.0123323  -0.098593   -0.0152732   0.0832726   0.0811476   -0.201438    0.164208   -0.226799    0.0872624   0.119676    0.272458    0.103822    -0.273045     -0.123889    -0.185579   -0.219047     0.0817828   -0.0902025   0.0100445  -0.225348
  0.0823122  -0.107437   -0.778062   -0.251426    -0.398268   -0.410803     0.314259   -0.212255   -0.262711   -0.226603    0.325519     0.0751249   0.396197    0.254027   -0.281423   -0.535036   -0.74102     0.0503135    0.64997       0.110537     0.478678    0.462817    -0.219838    -0.0312655  -0.323234    0.0775519
  0.119563    0.458705    0.222385   -0.440886     0.144409    0.360441     0.53949    -0.239142    0.729366   -0.18288    -0.118301    -0.580622    0.106067    0.0806765   0.202092   -0.269162    0.161676   -0.241633     0.000905788  -0.0562927   -0.360922    0.195158    -0.781765    -0.097559   -0.355239    0.246959
  0.418575    0.666178    0.032408   -0.306171     0.935663    0.295971     0.272453   -0.383955   -0.279147   -0.144134   -0.144446     0.332642   -0.133109    0.139871    0.215707    0.467378    0.152302   -0.534743     0.211786      0.34167     -0.274455    0.273638     0.379267     0.0570164   0.184243    0.145134
 -0.301027    0.0257165   0.0140794  -0.260002    -0.320799   -0.216711    -0.27735     0.504935   -0.0633733   0.258513    0.157014     0.243787   -0.270346   -0.387281   -0.128918    0.369579   -0.772132    0.55999      0.464198      0.17673     -0.533695   -0.235622     0.304088     0.494805    0.475763   -0.0933437
  0.0113877  -0.359869    0.122463    0.111745    -0.0837375  -0.212667    -0.44473     0.0268312   0.230077    0.254434    0.238681    -0.27646    -0.617858   -0.595856   -0.0947803  -0.0460647   0.331937   -0.155834     0.113452      0.477744    -0.584816   -0.0331304    0.318938    -0.305447    0.353432    0.0798548
  0.426688    0.19635    -0.31107     0.169284     0.118296   -0.0817257    0.072023    0.135054   -0.426571   -0.338935   -0.0566209    0.0133109   0.130426   -0.787192    0.43975     0.0331225  -0.252696   -0.829005    -0.626594     -0.429056    -0.533752   -0.097281     0.26425      0.242167    0.118084    0.175486
  0.140896   -0.0443695  -0.156481    0.067593     0.719787   -0.44248     -0.727418   -0.219674   -0.514515    0.34508     0.717013    -0.292823    0.271465    0.190984    0.109715    0.0531066  -0.118674   -0.72727     -0.115814      0.245961    -0.0547806  -0.141518    -0.092358    -0.18339     0.288787    0.0360269
 -0.0918935   0.448739   -0.130561    0.287192     0.367252    0.337399    -0.457257   -0.0898818  -0.209364   -0.624456   -0.175797    -0.259968   -0.108329    0.325452    0.461512   -0.150397    0.0430015   0.391297     0.293958     -0.184362    -0.0147807   0.539879     0.630861    -1.32296     0.212115   -0.243111
 -0.370375    0.902387    0.560764    0.872128     0.347821   -0.142107     0.518719    0.0800194  -0.0045219   0.401427   -0.238192    -0.444247    0.095464   -0.376659    0.0458143   0.548101    0.105013    0.0923348   -0.281739     -0.32683     -0.174615    0.0355717   -0.310192    -0.359676    0.463667   -0.149643
 -0.1925      0.134025   -0.371251   -0.115854    -0.0382137  -0.785194    -0.67389    -0.467357   -0.294161   -0.0401298  -0.0189385   -0.310036   -0.53473     0.0536995  -0.234092   -0.553565    0.239985   -0.143889    -0.454746     -0.189807    -0.546128    0.369939    -0.0753874   -0.360589   -0.091724    0.38996
  0.126602    0.154552   -0.333498   -0.227903     0.291382    0.781469     0.549286    0.688568    0.0304739  -0.202358    0.539844    -0.0834603   0.364776    0.823293    0.455537    0.359391   -0.126423    0.655451    -0.342189     -0.447963     0.0600725  -0.164003     0.236703     0.0169335   0.103977   -0.367698
 -0.0553381   0.233647    0.338175    0.338976     0.124899    0.229591     0.237306   -0.0573097  -0.284503   -0.25277    -0.478162     0.631193   -0.150988    0.40412     0.274888    0.396707   -0.382623    0.159453     0.541119     -0.468449     0.0504321   0.00541699  -0.618698     0.557005    0.335661   -0.293787
 -0.0403547  -0.053994   -0.624233    0.0287662   -0.147166    0.121684    -0.0301487  -0.100334   -0.252735    0.318939    0.127407     0.0462037   0.0464084  -0.213827   -0.227215    0.187146   -0.237571   -0.215422     0.149827      0.0272069    0.120042    0.139081    -0.0885756   -0.0327841  -0.0733039  -0.198034
  0.164862    0.0998819  -0.0859562  -0.615858     0.711406    0.0324719   -0.110031    0.414418    0.0960192  -0.147171    0.0738129    0.441345    0.0662831   0.389515   -0.241089   -0.354937    0.204464   -0.55891     -0.293927      0.517332    -0.0930167   0.598999    -0.35439      0.298684   -0.293412    0.191175
 -0.259542   -0.16718    -0.0285507  -0.688877    -0.135322   -0.00750103  -0.599339   -0.0513329   0.324806    0.0755411  -0.238568    -0.0445004  -0.0586576   0.258245   -0.389611   -0.010738   -0.0826751   0.08066      0.217345      0.212023    -0.150988    0.785877     0.37682      0.155143    0.496986   -0.253288
  0.578801    0.199401    1.15233     0.591888     0.394082   -0.109883     0.147456   -0.079128   -0.112681   -0.527914   -0.0325266    0.250708    0.1354      0.0158587   0.224535   -0.343391    0.442714   -0.0723931   -0.0294621     0.0434816    0.010715   -0.500237     0.221872    -0.246663   -0.0215305   0.161462
 -0.37589    -0.195132   -0.532915   -0.821153    -0.241565    0.222622    -0.366843   -0.288154   -0.320399    0.535528   -0.13223     -0.229539   -0.104977   -0.411273   -0.337372    0.429716    0.144081   -0.433116    -0.481691     -0.0227251   -0.11913     0.228432     0.0407117    0.26006    -0.174741   -0.698428
 -0.169724   -0.120206   -0.211256    0.426408    -0.58187    -0.160966    -0.370574   -0.571721   -0.415833    0.248421    0.220686    -0.338468    0.396031   -0.312594    0.182761    0.365131   -0.200362    0.265325     0.245709     -0.278885     0.464054   -0.429606     0.416965    -0.0322194   0.545382   -0.0590849
  0.51109    -0.495137    0.312883    0.0277494   -0.387443   -0.365554     0.554944   -0.159001    0.427855    0.23571     0.288006     0.0721396   0.916428   -0.018883    0.106376    0.0138071   0.151925   -0.124573     0.16108      -0.00744242  -0.23594    -0.419011    -0.304999     0.379792    0.233751    0.803795
 -0.666607   -0.0948209   0.0736007   0.409578     0.266245    0.52459     -0.190057    0.339443    0.161126   -0.486545   -0.223403     0.786036   -0.617159    0.293559    0.132789    0.528885    0.528221    0.493996    -0.231587      0.0251687   -0.408314   -0.155931     0.137183    -0.438774   -0.65273    -0.0987371
 -0.103071   -0.27325    -0.232234   -0.22867      0.0825204  -0.152762    -0.153628   -0.0607157   0.270067    0.53062     0.537992    -0.350745    0.771598   -0.303352   -0.185752   -0.642195    0.272561   -0.00771469  -0.787499      0.598306    -0.0785973  -0.220359     0.535351    -0.424463   -0.627374    0.679159[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.406853
[ Info: iteration 2, average log likelihood -1.406843
[ Info: iteration 3, average log likelihood -1.406834
[ Info: iteration 4, average log likelihood -1.406826
[ Info: iteration 5, average log likelihood -1.406818
[ Info: iteration 6, average log likelihood -1.406810
[ Info: iteration 7, average log likelihood -1.406802
[ Info: iteration 8, average log likelihood -1.406794
[ Info: iteration 9, average log likelihood -1.406787
[ Info: iteration 10, average log likelihood -1.406780
┌ Info: EM with 100000 data points 10 iterations avll -1.406780
└ 59.0 data points per parameter
[ Info: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.678561e+05
      1       2.230230e+04      -1.455538e+05 |        2
      2       7.823675e+03      -1.447862e+04 |        0
      3       7.823675e+03       0.000000e+00 |        0
K-means converged with 3 iterations (objv = 7823.67549422947)
┌ Info: K-means with 900 data points using 3 iterations
└ 150.0 data points per parameter
[ Info: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
[ Info: iteration 1, average log likelihood -2.043155
[ Info: iteration 2, average log likelihood -2.043154
[ Info: iteration 3, average log likelihood -2.043154
[ Info: iteration 4, average log likelihood -2.043154
[ Info: iteration 5, average log likelihood -2.043154
[ Info: iteration 6, average log likelihood -2.043154
[ Info: iteration 7, average log likelihood -2.043154
[ Info: iteration 8, average log likelihood -2.043154
[ Info: iteration 9, average log likelihood -2.043154
[ Info: iteration 10, average log likelihood -2.043154
┌ Info: EM with 900 data points 10 iterations avll -2.043154
└ 81.8 data points per parameter
    Testing GaussianMixtures tests passed 
