Julia Version 1.5.0-DEV.483
Commit 7f28d91b70 (2020-03-18 17:21 UTC)
Platform Info:
  OS: Linux (x86_64-linux-gnu)
  CPU: Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-9.0.1 (ORCJIT, skylake)
Environment:
  JULIA_DEPOT_PATH = ::/usr/local/share/julia
  JULIA_NUM_THREADS = 2

  Resolving package versions...
  Installed Requires ───────────────────── v1.0.1
  Installed TimerOutputs ───────────────── v0.5.3
  Installed Adapt ──────────────────────── v1.0.1
  Installed CommonSubexpressions ───────── v0.2.0
  Installed FillArrays ─────────────────── v0.8.5
  Installed GPUArrays ──────────────────── v2.0.1
  Installed Flux ───────────────────────── v0.10.3
  Installed LLVM ───────────────────────── v1.3.4
  Installed ZygoteRules ────────────────── v0.2.0
  Installed ForwardDiff ────────────────── v0.10.9
  Installed CEnum ──────────────────────── v0.2.0
  Installed IntelOpenMP_jll ────────────── v2018.0.3+0
  Installed FixedPointNumbers ──────────── v0.7.1
  Installed MacroTools ─────────────────── v0.5.4
  Installed StatsBase ──────────────────── v0.32.2
  Installed OrderedCollections ─────────── v1.1.0
  Installed StaticArrays ───────────────── v0.12.1
  Installed CuArrays ───────────────────── v1.7.3
  Installed SortingAlgorithms ──────────── v0.3.1
  Installed Media ──────────────────────── v0.5.0
  Installed FFTW ───────────────────────── v1.2.0
  Installed SpecialFunctions ───────────── v0.10.0
  Installed ZipFile ────────────────────── v0.9.1
  Installed NNlib ──────────────────────── v0.6.6
  Installed DataAPI ────────────────────── v1.1.0
  Installed DataStructures ─────────────── v0.17.10
  Installed CUDAnative ─────────────────── v2.10.2
  Installed AbstractTrees ──────────────── v0.3.2
  Installed CompilerSupportLibraries_jll ─ v0.3.0+0
  Installed DiffRules ──────────────────── v1.0.1
  Installed NaNMath ────────────────────── v0.3.3
  Installed CodecZlib ──────────────────── v0.6.0
  Installed Zygote ─────────────────────── v0.4.10
  Installed ColorTypes ─────────────────── v0.9.1
  Installed Juno ───────────────────────── v0.8.1
  Installed FFTW_jll ───────────────────── v3.3.9+4
  Installed BinaryProvider ─────────────── v0.5.8
  Installed CUDAapi ────────────────────── v3.1.0
  Installed IRTools ────────────────────── v0.3.1
  Installed OpenSpecFun_jll ────────────── v0.5.3+3
  Installed MKL_jll ────────────────────── v2019.0.117+2
  Installed Reexport ───────────────────── v0.2.0
  Installed Zlib_jll ───────────────────── v1.2.11+8
  Installed DiffResults ────────────────── v1.0.2
  Installed Colors ─────────────────────── v0.11.2
  Installed ArrayLayouts ───────────────── v0.1.5
  Installed TranscodingStreams ─────────── v0.9.5
  Installed CUDAdrv ────────────────────── v6.0.0
  Installed Missings ───────────────────── v0.4.3
  Installed AbstractFFTs ───────────────── v0.5.0
#=#=#                                                                         ###################################################                       71.3%######################################################################## 100.0%
#=#=#                                                                                                                                                    0.7%#####                                                                      7.5%###########                                                               15.7%##################                                                        25.9%############################                                              39.3%##########################################                                59.4%############################################################              83.7%######################################################################## 100.0%
#=#=#                                                                         ##################                                                        26.2%######################################################################## 100.0%
#=#=#                                                                         ######################################################################## 100.0%
#=#=#                                                                         ######################################################################## 100.0%
   Updating `~/.julia/environments/v1.5/Project.toml`
   587475ba + Flux v0.10.3
   Updating `~/.julia/environments/v1.5/Manifest.toml`
   621f4979 + AbstractFFTs v0.5.0
   1520ce14 + AbstractTrees v0.3.2
   79e6a3ab + Adapt v1.0.1
   4c555306 + ArrayLayouts v0.1.5
   b99e7846 + BinaryProvider v0.5.8
   fa961155 + CEnum v0.2.0
   3895d2a7 + CUDAapi v3.1.0
   c5f51814 + CUDAdrv v6.0.0
   be33ccc6 + CUDAnative v2.10.2
   944b1d66 + CodecZlib v0.6.0
   3da002f7 + ColorTypes v0.9.1
   5ae59095 + Colors v0.11.2
   bbf7d656 + CommonSubexpressions v0.2.0
   e66e0078 + CompilerSupportLibraries_jll v0.3.0+0
   3a865a2d + CuArrays v1.7.3
   9a962f9c + DataAPI v1.1.0
   864edb3b + DataStructures v0.17.10
   163ba53b + DiffResults v1.0.2
   b552c78f + DiffRules v1.0.1
   7a1cc6ca + FFTW v1.2.0
   f5851436 + FFTW_jll v3.3.9+4
   1a297f60 + FillArrays v0.8.5
   53c48c17 + FixedPointNumbers v0.7.1
   587475ba + Flux v0.10.3
   f6369f11 + ForwardDiff v0.10.9
   0c68f7d7 + GPUArrays v2.0.1
   7869d1d1 + IRTools v0.3.1
   1d5cc7b8 + IntelOpenMP_jll v2018.0.3+0
   e5e0dc1b + Juno v0.8.1
   929cbde3 + LLVM v1.3.4
   856f044c + MKL_jll v2019.0.117+2
   1914dd2f + MacroTools v0.5.4
   e89f7d12 + Media v0.5.0
   e1d29d7a + Missings v0.4.3
   872c559c + NNlib v0.6.6
   77ba4419 + NaNMath v0.3.3
   efe28fd5 + OpenSpecFun_jll v0.5.3+3
   bac558e1 + OrderedCollections v1.1.0
   189a3867 + Reexport v0.2.0
   ae029012 + Requires v1.0.1
   a2af1166 + SortingAlgorithms v0.3.1
   276daf66 + SpecialFunctions v0.10.0
   90137ffa + StaticArrays v0.12.1
   2913bbd2 + StatsBase v0.32.2
   a759f4b9 + TimerOutputs v0.5.3
   3bb67fe8 + TranscodingStreams v0.9.5
   a5390f91 + ZipFile v0.9.1
   83775a58 + Zlib_jll v1.2.11+8
   e88e6eb3 + Zygote v0.4.10
   700de1a5 + ZygoteRules v0.2.0
   2a0f44e3 + Base64
   ade2ca70 + Dates
   8bb1440f + DelimitedFiles
   8ba89e20 + Distributed
   b77e0a4c + InteractiveUtils
   76f85450 + LibGit2
   8f399da3 + Libdl
   37e2e46d + LinearAlgebra
   56ddb016 + Logging
   d6f4376e + Markdown
   a63ad114 + Mmap
   44cfe95a + Pkg
   de0858da + Printf
   9abbd945 + Profile
   3fa0cd96 + REPL
   9a3f8284 + Random
   ea8e919c + SHA
   9e88b42a + Serialization
   6462fe0b + Sockets
   2f01184e + SparseArrays
   10745b16 + Statistics
   8dfed614 + Test
   cf7118a7 + UUIDs
   4ec0a83e + Unicode
   Building FFTW ─────→ `~/.julia/packages/FFTW/qqcBj/deps/build.log`
   Building NNlib ────→ `~/.julia/packages/NNlib/FAI3o/deps/build.log`
   Building CodecZlib → `~/.julia/packages/CodecZlib/5t9zO/deps/build.log`
    Testing Flux
     Status `/tmp/jl_om5BMU/Project.toml`
   1520ce14 AbstractTrees v0.3.2
   79e6a3ab Adapt v1.0.1
   944b1d66 CodecZlib v0.6.0
   5ae59095 Colors v0.11.2
   3a865a2d CuArrays v1.7.3
   e30172f5 Documenter v0.24.6
   587475ba Flux v0.10.3
   c8e1da08 IterTools v1.3.0
   e5e0dc1b Juno v0.8.1
   1914dd2f MacroTools v0.5.4
   872c559c NNlib v0.6.6
   189a3867 Reexport v0.2.0
   2913bbd2 StatsBase v0.32.2
   a5390f91 ZipFile v0.9.1
   e88e6eb3 Zygote v0.4.10
   8bb1440f DelimitedFiles
   37e2e46d LinearAlgebra
   44cfe95a Pkg
   de0858da Printf
   9a3f8284 Random
   ea8e919c SHA
   10745b16 Statistics
   8dfed614 Test
     Status `/tmp/jl_om5BMU/Manifest.toml`
   621f4979 AbstractFFTs v0.5.0
   1520ce14 AbstractTrees v0.3.2
   79e6a3ab Adapt v1.0.1
   4c555306 ArrayLayouts v0.1.5
   b99e7846 BinaryProvider v0.5.8
   fa961155 CEnum v0.2.0
   3895d2a7 CUDAapi v3.1.0
   c5f51814 CUDAdrv v6.0.0
   be33ccc6 CUDAnative v2.10.2
   944b1d66 CodecZlib v0.6.0
   3da002f7 ColorTypes v0.9.1
   5ae59095 Colors v0.11.2
   bbf7d656 CommonSubexpressions v0.2.0
   e66e0078 CompilerSupportLibraries_jll v0.3.0+0
   3a865a2d CuArrays v1.7.3
   9a962f9c DataAPI v1.1.0
   864edb3b DataStructures v0.17.10
   163ba53b DiffResults v1.0.2
   b552c78f DiffRules v1.0.1
   ffbed154 DocStringExtensions v0.8.1
   e30172f5 Documenter v0.24.6
   7a1cc6ca FFTW v1.2.0
   f5851436 FFTW_jll v3.3.9+4
   1a297f60 FillArrays v0.8.5
   53c48c17 FixedPointNumbers v0.7.1
   587475ba Flux v0.10.3
   f6369f11 ForwardDiff v0.10.9
   0c68f7d7 GPUArrays v2.0.1
   7869d1d1 IRTools v0.3.1
   1d5cc7b8 IntelOpenMP_jll v2018.0.3+0
   c8e1da08 IterTools v1.3.0
   682c06a0 JSON v0.21.0
   e5e0dc1b Juno v0.8.1
   929cbde3 LLVM v1.3.4
   856f044c MKL_jll v2019.0.117+2
   1914dd2f MacroTools v0.5.4
   e89f7d12 Media v0.5.0
   e1d29d7a Missings v0.4.3
   872c559c NNlib v0.6.6
   77ba4419 NaNMath v0.3.3
   efe28fd5 OpenSpecFun_jll v0.5.3+3
   bac558e1 OrderedCollections v1.1.0
   69de0a69 Parsers v0.3.12
   189a3867 Reexport v0.2.0
   ae029012 Requires v1.0.1
   a2af1166 SortingAlgorithms v0.3.1
   276daf66 SpecialFunctions v0.10.0
   90137ffa StaticArrays v0.12.1
   2913bbd2 StatsBase v0.32.2
   a759f4b9 TimerOutputs v0.5.3
   3bb67fe8 TranscodingStreams v0.9.5
   a5390f91 ZipFile v0.9.1
   83775a58 Zlib_jll v1.2.11+8
   e88e6eb3 Zygote v0.4.10
   700de1a5 ZygoteRules v0.2.0
   2a0f44e3 Base64
   ade2ca70 Dates
   8bb1440f DelimitedFiles
   8ba89e20 Distributed
   b77e0a4c InteractiveUtils
   76f85450 LibGit2
   8f399da3 Libdl
   37e2e46d LinearAlgebra
   56ddb016 Logging
   d6f4376e Markdown
   a63ad114 Mmap
   44cfe95a Pkg
   de0858da Printf
   9abbd945 Profile
   3fa0cd96 REPL
   9a3f8284 Random
   ea8e919c SHA
   9e88b42a Serialization
   6462fe0b Sockets
   2f01184e SparseArrays
   10745b16 Statistics
   8dfed614 Test
   cf7118a7 UUIDs
   4ec0a83e Unicode
WARNING: could not import Compiler.just_construct_ssa into Wrap
┌ Warning: Incompatibility detected between CUDA and LLVM 8.0+; disabling debug info emission for CUDA kernels
└ @ CUDAnative ~/.julia/packages/CUDAnative/hfulr/src/CUDAnative.jl:114
┌ Warning: CuArrays.jl found cuda, but did not find libcudnn. Some functionality will not be available.
└ @ Flux ~/.julia/packages/Flux/NpkMm/src/Flux.jl:48
[ Info: Downloading CMUDict dataset
[ Info: Downloading MNIST dataset
[ Info: Downloading MNIST dataset
[ Info: Downloading MNIST dataset
[ Info: Downloading MNIST dataset
[ Info: Downloading Fashion-MNIST dataset
[ Info: Downloading Fashion-MNIST dataset
[ Info: Downloading Fashion-MNIST dataset
[ Info: Downloading Fashion-MNIST dataset
[ Info: Downloading sentiment treebank dataset
[ Info: Downloading iris dataset.
[ Info: Downloading the Boston housing Dataset
[ Info: Testing GPU Support
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
CuArrays: Error During Test at /home/pkgeval/.julia/packages/Flux/NpkMm/test/cuda/cuda.jl:15
  Test threw exception
  Expression: Flux.onecold(gpu([1.0, 2.0, 3.0])) == 3
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::typeof(CuArrays.mapreducedim_kernel_parallel), ::Type{Tuple{typeof(identity),typeof(max),CUDAnative.CuDeviceArray{Float32,1,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float32,1,CUDAnative.AS.Global},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Int64,Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] _mapreducedim!(::Function, ::Function, ::CuArray{Float32,1,Nothing}, ::CuArray{Float32,1,Nothing}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/mapreduce.jl:72
   [21] _reduce(::Function, ::CuArray{Float32,1,Nothing}, ::Float32, ::Colon) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/mapreduce.jl:112
   [22] #maximum#59 at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/mapreduce.jl:129 [inlined]
   [23] maximum at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/mapreduce.jl:129 [inlined]
   [24] findmax(::CuArray{Float32,1,Nothing}; dims::Function) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/indexing.jl:239
   [25] #argmax#631 at ./reducedim.jl:882 [inlined]
   [26] argmax at ./reducedim.jl:882 [inlined]
   [27] onecold at /home/pkgeval/.julia/packages/Flux/NpkMm/src/onehot.jl:120 [inlined] (repeats 2 times)
   [28] top-level scope at /home/pkgeval/.julia/packages/Flux/NpkMm/test/cuda/cuda.jl:15
   [29] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [30] top-level scope at /home/pkgeval/.julia/packages/Flux/NpkMm/test/cuda/cuda.jl:9
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
CuArrays: Error During Test at /home/pkgeval/.julia/packages/Flux/NpkMm/test/cuda/cuda.jl:20
  Test threw exception
  Expression: cx .+ 1 isa CuArray
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(+),Tuple{Base.Broadcast.Extruded{Flux.OneHotMatrix{CUDAnative.CuDeviceArray{Flux.OneHotVector,1,CUDAnative.AS.Global}},Tuple{Bool,Bool},Tuple{Int64,Int64}},Int64}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(+),Tuple{Base.Broadcast.Extruded{Flux.OneHotMatrix{CuArray{Flux.OneHotVector,1,Nothing}},Tuple{Bool,Bool},Tuple{Int64,Int64}},Int64}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(+),Tuple{Base.Broadcast.Extruded{Flux.OneHotMatrix{CuArray{Flux.OneHotVector,1,Nothing}},Tuple{Bool,Bool},Tuple{Int64,Int64}},Int64}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(+),Tuple{Flux.OneHotMatrix{CuArray{Flux.OneHotVector,1,Nothing}},Int64}}) at ./broadcast.jl:820
   [28] top-level scope at /home/pkgeval/.julia/packages/Flux/NpkMm/test/cuda/cuda.jl:20
   [29] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [30] top-level scope at /home/pkgeval/.julia/packages/Flux/NpkMm/test/cuda/cuda.jl:9
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
CuArrays: Error During Test at /home/pkgeval/.julia/packages/Flux/NpkMm/test/cuda/cuda.jl:26
  Test threw exception
  Expression: cm(gpu(rand(10, 10))) isa CuArray{Float32, 2}
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(CUDAnative.tanh),Tuple{Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(+),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,1,CUDAnative.AS.Global},Tuple{Bool},Tuple{Int64}}}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(CUDAnative.tanh),Tuple{Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(+),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{CuArray{Float32,1,Nothing},Tuple{Bool},Tuple{Int64}}}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(CUDAnative.tanh),Tuple{Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(+),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{CuArray{Float32,1,Nothing},Tuple{Bool},Tuple{Int64}}}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(CUDAnative.tanh),Tuple{Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(+),Tuple{CuArray{Float32,2,Nothing},CuArray{Float32,1,Nothing}}}}}) at ./broadcast.jl:840
   [27] materialize at ./broadcast.jl:820 [inlined]
   [28] Dense at /home/pkgeval/.julia/packages/Flux/NpkMm/src/layers/basic.jl:115 [inlined]
   [29] Dense at /home/pkgeval/.julia/packages/Flux/NpkMm/src/layers/basic.jl:126 [inlined]
   [30] applychain(::Tuple{Dense{typeof(tanh),CuArray{Float32,2,Nothing},CuArray{Float32,1,Nothing}},Dense{typeof(identity),CuArray{Float32,2,Nothing},CuArray{Float32,1,Nothing}},typeof(softmax)}, ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/Flux/NpkMm/src/layers/basic.jl:30
   [31] (::Chain{Tuple{Dense{typeof(tanh),CuArray{Float32,2,Nothing},CuArray{Float32,1,Nothing}},Dense{typeof(identity),CuArray{Float32,2,Nothing},CuArray{Float32,1,Nothing}},typeof(softmax)}})(::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/Flux/NpkMm/src/layers/basic.jl:32
   [32] top-level scope at /home/pkgeval/.julia/packages/Flux/NpkMm/test/cuda/cuda.jl:26
   [33] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [34] top-level scope at /home/pkgeval/.julia/packages/Flux/NpkMm/test/cuda/cuda.jl:9
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
CuArrays: Error During Test at /home/pkgeval/.julia/packages/Flux/NpkMm/test/cuda/cuda.jl:30
  Test threw exception
  Expression: Flux.crossentropy(x, x) ≈ Flux.crossentropy(cx, cx)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,1,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64}},typeof(*),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,1,CUDAnative.AS.Global},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CUDAnative.log),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,1,CUDAnative.AS.Global},Tuple{Bool},Tuple{Int64}}}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,1,Nothing}, ::Tuple{CuArray{Float32,1,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64}},typeof(*),Tuple{Base.Broadcast.Extruded{CuArray{Float32,1,Nothing},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CUDAnative.log),Tuple{Base.Broadcast.Extruded{CuArray{Float32,1,Nothing},Tuple{Bool},Tuple{Int64}}}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Float32,1,Nothing}, ::Tuple{CuArray{Float32,1,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64}},typeof(*),Tuple{Base.Broadcast.Extruded{CuArray{Float32,1,Nothing},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CUDAnative.log),Tuple{Base.Broadcast.Extruded{CuArray{Float32,1,Nothing},Tuple{Bool},Tuple{Int64}}}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize at ./broadcast.jl:820 [inlined]
   [28] _crossentropy(::CuArray{Float32,1,Nothing}, ::CuArray{Float32,1,Nothing}, ::Nothing) at /home/pkgeval/.julia/packages/Flux/NpkMm/src/layers/stateless.jl:49
   [29] #crossentropy#62 at /home/pkgeval/.julia/packages/Flux/NpkMm/src/layers/stateless.jl:67 [inlined]
   [30] crossentropy(::CuArray{Float32,1,Nothing}, ::CuArray{Float32,1,Nothing}) at /home/pkgeval/.julia/packages/Flux/NpkMm/src/layers/stateless.jl:67
   [31] top-level scope at /home/pkgeval/.julia/packages/Flux/NpkMm/test/cuda/cuda.jl:30
   [32] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [33] top-level scope at /home/pkgeval/.julia/packages/Flux/NpkMm/test/cuda/cuda.jl:9
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
CuArrays: Error During Test at /home/pkgeval/.julia/packages/Flux/NpkMm/test/cuda/cuda.jl:31
  Test threw exception
  Expression: Flux.crossentropy(x, x, weight = 1.0) ≈ Flux.crossentropy(cx, cx, weight = 1.0)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,1,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64}},typeof(*),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,1,CUDAnative.AS.Global},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CUDAnative.log),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,1,CUDAnative.AS.Global},Tuple{Bool},Tuple{Int64}}}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,1,Nothing}, ::Tuple{CuArray{Float32,1,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64}},typeof(*),Tuple{Base.Broadcast.Extruded{CuArray{Float32,1,Nothing},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CUDAnative.log),Tuple{Base.Broadcast.Extruded{CuArray{Float32,1,Nothing},Tuple{Bool},Tuple{Int64}}}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Float32,1,Nothing}, ::Tuple{CuArray{Float32,1,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64}},typeof(*),Tuple{Base.Broadcast.Extruded{CuArray{Float32,1,Nothing},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CUDAnative.log),Tuple{Base.Broadcast.Extruded{CuArray{Float32,1,Nothing},Tuple{Bool},Tuple{Int64}}}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize at ./broadcast.jl:820 [inlined]
   [28] _crossentropy(::CuArray{Float32,1,Nothing}, ::CuArray{Float32,1,Nothing}, ::Float64) at /home/pkgeval/.julia/packages/Flux/NpkMm/src/layers/stateless.jl:53
   [29] #crossentropy#62 at /home/pkgeval/.julia/packages/Flux/NpkMm/src/layers/stateless.jl:67 [inlined]
   [30] top-level scope at /home/pkgeval/.julia/packages/Flux/NpkMm/test/cuda/cuda.jl:31
   [31] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Flux/NpkMm/test/cuda/cuda.jl:9
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
CuArrays: Error During Test at /home/pkgeval/.julia/packages/Flux/NpkMm/test/cuda/cuda.jl:32
  Test threw exception
  Expression: Flux.crossentropy(x, x, weight = [1.0; 2.0; 3.0]) ≈ Flux.crossentropy(cx, cx, weight = cu([1.0; 2.0; 3.0]))
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,1,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64}},typeof(*),Tuple{Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(*),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,1,CUDAnative.AS.Global},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CUDAnative.log),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,1,CUDAnative.AS.Global},Tuple{Bool},Tuple{Int64}}}}}},Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,1,CUDAnative.AS.Global},Tuple{Bool},Tuple{Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,1,Nothing}, ::Tuple{CuArray{Float32,1,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64}},typeof(*),Tuple{Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(*),Tuple{Base.Broadcast.Extruded{CuArray{Float32,1,Nothing},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(CUDAnative.log),Tuple{Base.Broadcast.Extruded{CuArray{Float32,1,Nothing},Tuple{Bool},Tuple{Int64}}}}}},Base.Broadcast.Extruded{CuArray{Float32,1,Nothing},Tuple{Bool},Tuple{Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize at ./broadcast.jl:820 [inlined]
   [28] _crossentropy(::CuArray{Float32,1,Nothing}, ::CuArray{Float32,1,Nothing}, ::CuArray{Float32,1,Nothing}) at /home/pkgeval/.julia/packages/Flux/NpkMm/src/layers/stateless.jl:57
   [29] #crossentropy#62 at /home/pkgeval/.julia/packages/Flux/NpkMm/src/layers/stateless.jl:67 [inlined]
   [30] top-level scope at /home/pkgeval/.julia/packages/Flux/NpkMm/test/cuda/cuda.jl:32
   [31] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Flux/NpkMm/test/cuda/cuda.jl:9
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
CuArrays: Error During Test at /home/pkgeval/.julia/packages/Flux/NpkMm/test/cuda/cuda.jl:36
  Test threw exception
  Expression: Flux.binarycrossentropy.(σ.(x), y) ≈ Array(Flux.binarycrossentropy.(cu(σ.(x)), cu(y)))
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,1,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64}},typeof(Flux.cubinarycrossentropy),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,1,CUDAnative.AS.Global},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,1,CUDAnative.AS.Global},Tuple{Bool},Tuple{Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,1,Nothing}, ::Tuple{CuArray{Float32,1,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64}},typeof(Flux.cubinarycrossentropy),Tuple{Base.Broadcast.Extruded{CuArray{Float32,1,Nothing},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Extruded{CuArray{Float32,1,Nothing},Tuple{Bool},Tuple{Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Float32,1,Nothing}, ::Tuple{CuArray{Float32,1,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64}},typeof(Flux.cubinarycrossentropy),Tuple{Base.Broadcast.Extruded{CuArray{Float32,1,Nothing},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Extruded{CuArray{Float32,1,Nothing},Tuple{Bool},Tuple{Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(Flux.cubinarycrossentropy),Tuple{CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing}}}) at ./broadcast.jl:820
   [28] top-level scope at /home/pkgeval/.julia/packages/Flux/NpkMm/test/cuda/cuda.jl:36
   [29] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [30] top-level scope at /home/pkgeval/.julia/packages/Flux/NpkMm/test/cuda/cuda.jl:9
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
CuArrays: Error During Test at /home/pkgeval/.julia/packages/Flux/NpkMm/test/cuda/cuda.jl:37
  Test threw exception
  Expression: Flux.logitbinarycrossentropy.(x, y) ≈ Array(Flux.logitbinarycrossentropy.(cu(x), cu(y)))
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,1,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64}},typeof(Flux.culogitbinarycrossentropy),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,1,CUDAnative.AS.Global},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,1,CUDAnative.AS.Global},Tuple{Bool},Tuple{Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,1,Nothing}, ::Tuple{CuArray{Float32,1,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64}},typeof(Flux.culogitbinarycrossentropy),Tuple{Base.Broadcast.Extruded{CuArray{Float32,1,Nothing},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Extruded{CuArray{Float32,1,Nothing},Tuple{Bool},Tuple{Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Float32,1,Nothing}, ::Tuple{CuArray{Float32,1,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64}},typeof(Flux.culogitbinarycrossentropy),Tuple{Base.Broadcast.Extruded{CuArray{Float32,1,Nothing},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Extruded{CuArray{Float32,1,Nothing},Tuple{Bool},Tuple{Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(Flux.culogitbinarycrossentropy),Tuple{CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing}}}) at ./broadcast.jl:820
   [28] top-level scope at /home/pkgeval/.julia/packages/Flux/NpkMm/test/cuda/cuda.jl:37
   [29] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [30] top-level scope at /home/pkgeval/.julia/packages/Flux/NpkMm/test/cuda/cuda.jl:9
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
CuArrays: Error During Test at /home/pkgeval/.julia/packages/Flux/NpkMm/test/cuda/cuda.jl:41
  Test threw exception
  Expression: collect(cu(xs) .+ cu(ys)) ≈ collect(xs .+ ys)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(+),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Flux.OneHotMatrix{CUDAnative.CuDeviceArray{Flux.OneHotVector,1,CUDAnative.AS.Global}},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(+),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Flux.OneHotMatrix{CuArray{Flux.OneHotVector,1,Nothing}},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Float32,2,Nothing}, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(+),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Flux.OneHotMatrix{CuArray{Flux.OneHotVector,1,Nothing}},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(+),Tuple{CuArray{Float32,2,Nothing},Flux.OneHotMatrix{CuArray{Flux.OneHotVector,1,Nothing}}}}) at ./broadcast.jl:820
   [28] top-level scope at /home/pkgeval/.julia/packages/Flux/NpkMm/test/cuda/cuda.jl:41
   [29] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [30] top-level scope at /home/pkgeval/.julia/packages/Flux/NpkMm/test/cuda/cuda.jl:9
  
CuArrays: Error During Test at /home/pkgeval/.julia/packages/Flux/NpkMm/test/cuda/cuda.jl:7
  Got exception outside of a @test
  could not load library "libcudnn"
  libcudnn.so: cannot open shared object file: No such file or directory
  Stacktrace:
   [1] dlopen(::String, ::UInt32; throw_error::Bool) at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Libdl/src/Libdl.jl:109
   [2] dlopen at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Libdl/src/Libdl.jl:109 [inlined] (repeats 2 times)
   [3] (::CuArrays.CUDNN.var"#6795#lookup_fptr#99")() at /home/pkgeval/.julia/packages/CUDAapi/wYUAO/src/call.jl:29
   [4] macro expansion at /home/pkgeval/.julia/packages/CUDAapi/wYUAO/src/call.jl:37 [inlined]
   [5] macro expansion at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/dnn/error.jl:38 [inlined]
   [6] cudnnGetProperty at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/dnn/libcudnn.jl:27 [inlined]
   [7] cudnnGetProperty at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/dnn/base.jl:9 [inlined]
   [8] version() at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/dnn/base.jl:13
   [9] conv!(::CuArray{Float32,4,Nothing}, ::CuArray{Float32,4,Nothing}, ::CuArray{Float32,4,Nothing}, ::DenseConvDims{2,(2, 2),3,4,(1, 1),(0, 0, 0, 0),(1, 1),false}; alpha::Int64, algo::Int64) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/dnn/nnlib.jl:46
   [10] conv! at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/dnn/nnlib.jl:46 [inlined]
   [11] conv(::CuArray{Float32,4,Nothing}, ::CuArray{Float32,4,Nothing}, ::DenseConvDims{2,(2, 2),3,4,(1, 1),(0, 0, 0, 0),(1, 1),false}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/NNlib/FAI3o/src/conv.jl:116
   [12] conv(::CuArray{Float32,4,Nothing}, ::CuArray{Float32,4,Nothing}, ::DenseConvDims{2,(2, 2),3,4,(1, 1),(0, 0, 0, 0),(1, 1),false}) at /home/pkgeval/.julia/packages/NNlib/FAI3o/src/conv.jl:114
   [13] (::Conv{2,4,typeof(identity),CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing}})(::CuArray{Float32,4,Nothing}) at /home/pkgeval/.julia/packages/Flux/NpkMm/src/layers/conv.jl:60
   [14] top-level scope at /home/pkgeval/.julia/packages/Flux/NpkMm/test/cuda/cuda.jl:45
   [15] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [16] top-level scope at /home/pkgeval/.julia/packages/Flux/NpkMm/test/cuda/cuda.jl:9
   [17] include(::String) at ./client.jl:441
   [18] top-level scope at /home/pkgeval/.julia/packages/Flux/NpkMm/test/runtests.jl:37
   [19] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [20] top-level scope at /home/pkgeval/.julia/packages/Flux/NpkMm/test/runtests.jl:36
   [21] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [22] top-level scope at /home/pkgeval/.julia/packages/Flux/NpkMm/test/runtests.jl:12
   [23] include(::String) at ./client.jl:441
   [24] top-level scope at none:6
   [25] eval(::Module, ::Any) at ./boot.jl:331
   [26] exec_options(::Base.JLOptions) at ./client.jl:264
   [27] _start() at ./client.jl:490
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
onecold gpu: Error During Test at /home/pkgeval/.julia/packages/Flux/NpkMm/test/cuda/cuda.jl:57
  Test threw exception
  Expression: Flux.onecold(y) isa CuArray
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#21#22", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,1,CUDAnative.AS.Global},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,1,Nothing}, ::Tuple{CuArray{Int64,1,Nothing},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,1,Nothing}, ::Tuple{CuArray{Int64,1,Nothing},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] fill! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/construction.jl:10 [inlined]
   [25] reducedim_initarray at ./reducedim.jl:92 [inlined]
   [26] reducedim_initarray at ./reducedim.jl:93 [inlined]
   [27] _mapreduce_dim at ./reducedim.jl:314 [inlined]
   [28] mapreduce_impl at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/mapreduce.jl:79 [inlined]
   [29] #mapreduce#50 at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/mapreduce.jl:65 [inlined]
   [30] onecold(::Flux.OneHotMatrix{CuArray{Flux.OneHotVector,1,Nothing}}) at /home/pkgeval/.julia/packages/Flux/NpkMm/src/onehot.jl:125
   [31] top-level scope at /home/pkgeval/.julia/packages/Flux/NpkMm/test/cuda/cuda.jl:57
   [32] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [33] top-level scope at /home/pkgeval/.julia/packages/Flux/NpkMm/test/cuda/cuda.jl:56
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
onecold gpu: Error During Test at /home/pkgeval/.julia/packages/Flux/NpkMm/test/cuda/cuda.jl:58
  Test threw exception
  Expression: y[3, :] isa CuArray
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Bool,1,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64}},Flux.var"#25#26"{Int64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Flux.OneHotVector,1,CUDAnative.AS.Global},Tuple{Bool},Tuple{Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Bool,1,Nothing}, ::Tuple{CuArray{Bool,1,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64}},Flux.var"#25#26"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Flux.OneHotVector,1,Nothing},Tuple{Bool},Tuple{Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Bool,1,Nothing}, ::Tuple{CuArray{Bool,1,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64}},Flux.var"#25#26"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Flux.OneHotVector,1,Nothing},Tuple{Bool},Tuple{Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64}},Flux.var"#25#26"{Int64},Tuple{CuArray{Flux.OneHotVector,1,Nothing}}}) at ./broadcast.jl:840
   [27] materialize at ./broadcast.jl:820 [inlined]
   [28] map at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/base.jl:9 [inlined]
   [29] getindex(::Flux.OneHotMatrix{CuArray{Flux.OneHotVector,1,Nothing}}, ::Int64, ::Colon) at /home/pkgeval/.julia/packages/Flux/NpkMm/src/onehot.jl:28
   [30] top-level scope at /home/pkgeval/.julia/packages/Flux/NpkMm/test/cuda/cuda.jl:58
   [31] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [32] top-level scope at /home/pkgeval/.julia/packages/Flux/NpkMm/test/cuda/cuda.jl:56
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
restructure gpu: Error During Test at /home/pkgeval/.julia/packages/Flux/NpkMm/test/cuda/cuda.jl:61
  Got exception outside of a @test
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,1,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float32,1,CUDAnative.AS.Global},Tuple{Int64},Tuple{UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,1,Nothing}, ::Tuple{CuArray{Float32,1,Nothing},CuArray{Float32,1,CuArray{Float32,2,Nothing}},Tuple{Int64},Tuple{UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Float32,1,Nothing}, ::Tuple{CuArray{Float32,1,Nothing},CuArray{Float32,1,CuArray{Float32,2,Nothing}},Tuple{Int64},Tuple{UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] _unsafe_setindex!(::IndexLinear, ::CuArray{Float32,1,Nothing}, ::CuArray{Float32,1,CuArray{Float32,2,Nothing}}, ::UnitRange{Int64}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123
   [24] _setindex! at ./multidimensional.jl:777 [inlined]
   [25] setindex! at ./abstractarray.jl:1073 [inlined]
   [26] __cat(::CuArray{Float32,1,Nothing}, ::Tuple{Int64}, ::Tuple{Bool}, ::CuArray{Float32,1,CuArray{Float32,2,Nothing}}, ::Vararg{Any,N} where N) at ./abstractarray.jl:1463
   [27] _cat_t(::Int64, ::Type{T} where T, ::CuArray{Float32,1,CuArray{Float32,2,Nothing}}, ::Vararg{Any,N} where N) at ./abstractarray.jl:1445
   [28] cat_t(::Type{Float32}, ::CuArray{Float32,1,CuArray{Float32,2,Nothing}}, ::Vararg{Any,N} where N; dims::Int64) at ./abstractarray.jl:1437
   [29] _cat at ./abstractarray.jl:1574 [inlined]
   [30] #cat#110 at ./abstractarray.jl:1573 [inlined]
   [31] vcat(::CuArray{Float32,1,CuArray{Float32,2,Nothing}}, ::CuArray{Float32,1,Nothing}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/utils.jl:64
   [32] destructure(::Dense{typeof(identity),CuArray{Float32,2,Nothing},CuArray{Float32,1,Nothing}}) at /home/pkgeval/.julia/packages/Flux/NpkMm/src/utils.jl:145
   [33] top-level scope at /home/pkgeval/.julia/packages/Flux/NpkMm/test/cuda/cuda.jl:63
   [34] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [35] top-level scope at /home/pkgeval/.julia/packages/Flux/NpkMm/test/cuda/cuda.jl:62
   [36] include(::String) at ./client.jl:441
   [37] top-level scope at /home/pkgeval/.julia/packages/Flux/NpkMm/test/runtests.jl:37
   [38] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [39] top-level scope at /home/pkgeval/.julia/packages/Flux/NpkMm/test/runtests.jl:36
   [40] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [41] top-level scope at /home/pkgeval/.julia/packages/Flux/NpkMm/test/runtests.jl:12
   [42] include(::String) at ./client.jl:441
   [43] top-level scope at none:6
   [44] eval(::Module, ::Any) at ./boot.jl:331
   [45] exec_options(::Base.JLOptions) at ./client.jl:264
   [46] _start() at ./client.jl:490
  
┌ Warning: CUDNN unavailable, not testing GPU DNN support
└ @ Main ~/.julia/packages/Flux/NpkMm/test/cuda/cuda.jl:73
[ Info: SetupBuildDirectory: setting up build directory.
[ Info: Doctest: running doctests.
[ Info: Skipped ExpandTemplates step (doctest only).
[ Info: Skipped CrossReferences step (doctest only).
[ Info: Skipped CheckDocument step (doctest only).
[ Info: Skipped Populate step (doctest only).
[ Info: Skipped RenderDocument step (doctest only).
Test Summary:       | Pass  Error  Total
Flux                |  308     13    321
  Utils             |   40            40
  Onehot            |    6             6
  Optimise          |   20            20
  Data              |   35            35
  Layers            |  203           203
  CUDA              |    3     13     16
    CuArrays        |    3     10     13
    onecold gpu     |           2      2
    restructure gpu |           1      1
  Docs              |    1             1
ERROR: LoadError: Some tests did not pass: 308 passed, 0 failed, 13 errored, 0 broken.
in expression starting at /home/pkgeval/.julia/packages/Flux/NpkMm/test/runtests.jl:10
ERROR: Package Flux errored during testing
Stacktrace:
 [1] pkgerror(::String, ::Vararg{String,N} where N) at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/Types.jl:53
 [2] test(::Pkg.Types.Context, ::Array{Pkg.Types.PackageSpec,1}; coverage::Bool, julia_args::Cmd, test_args::Cmd, test_fn::Nothing) at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/Operations.jl:1523
 [3] test(::Pkg.Types.Context, ::Array{Pkg.Types.PackageSpec,1}; coverage::Bool, test_fn::Nothing, julia_args::Cmd, test_args::Cmd, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:316
 [4] test(::Pkg.Types.Context, ::Array{Pkg.Types.PackageSpec,1}) at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:303
 [5] #test#68 at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:297 [inlined]
 [6] test at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:297 [inlined]
 [7] #test#67 at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:296 [inlined]
 [8] test at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:296 [inlined]
 [9] test(::String; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:295
 [10] test(::String) at /workspace/srcdir/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:295
 [11] top-level scope at none:13
