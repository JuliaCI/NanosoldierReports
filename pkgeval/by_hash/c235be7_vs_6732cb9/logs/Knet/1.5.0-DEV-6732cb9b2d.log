Julia Version 1.5.0-DEV.439
Commit 6732cb9b2d (2020-03-11 21:46 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-9.0.1 (ORCJIT, skylake)
Environment:
  JULIA_DEPOT_PATH = ::/usr/local/share/julia
  JULIA_NUM_THREADS = 2

  Resolving package versions...
  Installed TimerOutputs ───────────────── v0.5.3
  Installed Requires ───────────────────── v1.0.1
  Installed AbstractFFTs ───────────────── v0.5.0
  Installed JLD2 ───────────────────────── v0.1.12
  Installed MacroTools ─────────────────── v0.5.4
  Installed AutoGrad ───────────────────── v1.2.1
  Installed NNlib ──────────────────────── v0.6.6
  Installed CompilerSupportLibraries_jll ─ v0.2.0+1
  Installed CUDAapi ────────────────────── v3.1.0
  Installed SpecialFunctions ───────────── v0.10.0
  Installed OrderedCollections ─────────── v1.1.0
  Installed TranscodingStreams ─────────── v0.9.5
  Installed GPUArrays ──────────────────── v2.0.1
  Installed BinaryProvider ─────────────── v0.5.8
  Installed CUDAdrv ────────────────────── v6.0.0
  Installed LLVM ───────────────────────── v1.3.4
  Installed CUDAnative ─────────────────── v2.10.2
  Installed Knet ───────────────────────── v1.3.4
  Installed CEnum ──────────────────────── v0.2.0
  Installed Adapt ──────────────────────── v1.0.1
  Installed OpenSpecFun_jll ────────────── v0.5.3+3
  Installed CodecZlib ──────────────────── v0.6.0
  Installed FileIO ─────────────────────── v1.2.3
  Installed DataStructures ─────────────── v0.17.10
  Installed CuArrays ───────────────────── v1.7.3
#=#=#                                                                         ##                                                                         4.1%#########                                                                 13.8%##################                                                        26.1%#############################                                             40.9%###########################################                               60.6%#############################################################             86.1%######################################################################## 100.0%
#=#=#                                                                         ######################################################################## 100.0%
   Updating `~/.julia/environments/v1.5/Project.toml`
   1902f260 + Knet v1.3.4
   Updating `~/.julia/environments/v1.5/Manifest.toml`
   621f4979 + AbstractFFTs v0.5.0
   79e6a3ab + Adapt v1.0.1
   6710c13c + AutoGrad v1.2.1
   b99e7846 + BinaryProvider v0.5.8
   fa961155 + CEnum v0.2.0
   3895d2a7 + CUDAapi v3.1.0
   c5f51814 + CUDAdrv v6.0.0
   be33ccc6 + CUDAnative v2.10.2
   944b1d66 + CodecZlib v0.6.0
   e66e0078 + CompilerSupportLibraries_jll v0.2.0+1
   3a865a2d + CuArrays v1.7.3
   864edb3b + DataStructures v0.17.10
   5789e2e9 + FileIO v1.2.3
   0c68f7d7 + GPUArrays v2.0.1
   033835bb + JLD2 v0.1.12
   1902f260 + Knet v1.3.4
   929cbde3 + LLVM v1.3.4
   1914dd2f + MacroTools v0.5.4
   872c559c + NNlib v0.6.6
   efe28fd5 + OpenSpecFun_jll v0.5.3+3
   bac558e1 + OrderedCollections v1.1.0
   ae029012 + Requires v1.0.1
   276daf66 + SpecialFunctions v0.10.0
   a759f4b9 + TimerOutputs v0.5.3
   3bb67fe8 + TranscodingStreams v0.9.5
   2a0f44e3 + Base64
   ade2ca70 + Dates
   8ba89e20 + Distributed
   b77e0a4c + InteractiveUtils
   76f85450 + LibGit2
   8f399da3 + Libdl
   37e2e46d + LinearAlgebra
   56ddb016 + Logging
   d6f4376e + Markdown
   a63ad114 + Mmap
   44cfe95a + Pkg
   de0858da + Printf
   3fa0cd96 + REPL
   9a3f8284 + Random
   ea8e919c + SHA
   9e88b42a + Serialization
   6462fe0b + Sockets
   2f01184e + SparseArrays
   10745b16 + Statistics
   8dfed614 + Test
   cf7118a7 + UUIDs
   4ec0a83e + Unicode
   Building NNlib ────→ `~/.julia/packages/NNlib/FAI3o/deps/build.log`
   Building CodecZlib → `~/.julia/packages/CodecZlib/5t9zO/deps/build.log`
   Building Knet ─────→ `~/.julia/packages/Knet/vxHRi/deps/build.log`
    Testing Knet
     Status `/tmp/jl_DLQ2Wk/Project.toml`
   6710c13c AutoGrad v1.2.1
   3895d2a7 CUDAapi v3.1.0
   3a865a2d CuArrays v1.7.3
   864edb3b DataStructures v0.17.10
   5789e2e9 FileIO v1.2.3
   033835bb JLD2 v0.1.12
   1902f260 Knet v1.3.4
   872c559c NNlib v0.6.6
   276daf66 SpecialFunctions v0.10.0
   a759f4b9 TimerOutputs v0.5.3
   8f399da3 Libdl
   37e2e46d LinearAlgebra
   de0858da Printf
   9a3f8284 Random
   10745b16 Statistics
   8dfed614 Test
     Status `/tmp/jl_DLQ2Wk/Manifest.toml`
   621f4979 AbstractFFTs v0.5.0
   79e6a3ab Adapt v1.0.1
   6710c13c AutoGrad v1.2.1
   b99e7846 BinaryProvider v0.5.8
   fa961155 CEnum v0.2.0
   3895d2a7 CUDAapi v3.1.0
   c5f51814 CUDAdrv v6.0.0
   be33ccc6 CUDAnative v2.10.2
   944b1d66 CodecZlib v0.6.0
   e66e0078 CompilerSupportLibraries_jll v0.2.0+1
   3a865a2d CuArrays v1.7.3
   864edb3b DataStructures v0.17.10
   5789e2e9 FileIO v1.2.3
   0c68f7d7 GPUArrays v2.0.1
   033835bb JLD2 v0.1.12
   1902f260 Knet v1.3.4
   929cbde3 LLVM v1.3.4
   1914dd2f MacroTools v0.5.4
   872c559c NNlib v0.6.6
   efe28fd5 OpenSpecFun_jll v0.5.3+3
   bac558e1 OrderedCollections v1.1.0
   ae029012 Requires v1.0.1
   276daf66 SpecialFunctions v0.10.0
   a759f4b9 TimerOutputs v0.5.3
   3bb67fe8 TranscodingStreams v0.9.5
   2a0f44e3 Base64
   ade2ca70 Dates
   8ba89e20 Distributed
   b77e0a4c InteractiveUtils
   76f85450 LibGit2
   8f399da3 Libdl
   37e2e46d LinearAlgebra
   56ddb016 Logging
   d6f4376e Markdown
   a63ad114 Mmap
   44cfe95a Pkg
   de0858da Printf
   3fa0cd96 REPL
   9a3f8284 Random
   ea8e919c SHA
   9e88b42a Serialization
   6462fe0b Sockets
   2f01184e SparseArrays
   10745b16 Statistics
   8dfed614 Test
   cf7118a7 UUIDs
   4ec0a83e Unicode
┌ Warning: Incompatibility detected between CUDA and LLVM 8.0+; disabling debug info emission for CUDA kernels
└ @ CUDAnative ~/.julia/packages/CUDAnative/hfulr/src/CUDAnative.jl:114
  0.751469 seconds (355.75 k allocations: 18.120 MiB, 40.66% gc time)
Knet.gpuCount() = 1
Knet.gpu() = 0
Knet.tk = ["/usr/local/cuda-10.2/targets/x86_64-linux", "/usr/local/cuda-10.2"]
Knet.libknet8 = ""
Knet.cudartfound = true
Knet.cudaRuntimeVersion = 10020
Knet.cudaDriverVersion = 10020
Knet.cudaGetDeviceCount() = 1
Knet.cudaGetDevice() = 0
Knet.cudaMemGetInfo() = (15654649856, 15843721216)
Knet.cudaDeviceSynchronize() = nothing
Knet.nvmlfound = true
Knet.nvmlDriverVersion = "440.33.01"
Knet.nvmlVersion = "10.440.33.01"
Knet.nvmlDeviceGetMemoryInfo() = (15843721216, 15654649856, 189071360)
Knet.cublashandle() = Ptr{Nothing} @0x000000000b591050
Knet.cublasVersion = 10202
gpu: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/gpu.jl:3
  Got exception outside of a @test
  Cannot find cudnn
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
   [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
   [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
   [5] top-level scope at show.jl:613
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/gpu.jl:28
   [7] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/gpu.jl:5
   [9] include(::String) at ./client.jl:441
   [10] macro expansion at ./util.jl:175 [inlined]
   [11] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:7 [inlined]
   [12] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
   [14] include(::String) at ./client.jl:441
   [15] top-level scope at none:6
   [16] eval(::Module, ::Any) at ./boot.jl:331
   [17] exec_options(::Base.JLOptions) at ./client.jl:264
   [18] _start() at ./client.jl:490
  
  7.412317 seconds (7.55 M allocations: 366.147 MiB, 2.47% gc time)
  2.921075 seconds (4.86 M allocations: 236.238 MiB, 2.65% gc time)

Stacktrace:
 [1] dropout!(::Float64, ::KnetArray{Float64,2}, ::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/dropout.jl:62
 [2] dropout(::KnetArray{Float64,2}, ::Float64; seed::Int64, drop::Bool) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/dropout.jl:25
 [3] forw(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Symbol,Integer,Tuple{Symbol,Symbol},NamedTuple{(:seed, :drop),Tuple{Int64,Bool}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] #dropout#777 at ./none:0 [inlined]
 [5] (::var"#dropout1#7")(::Param{KnetArray{Float64,2}}, ::Float64) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/dropout.jl:4
 [6] gcsum(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [7] gcsum(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [8] (::AutoGrad.var"#217#219"{Tuple{},var"#dropout1#7",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [9] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [10] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [11] gradcheck(::var"#dropout1#7", ::KnetArray{Float64,2}, ::Vararg{Any,N} where N; kw::Tuple{}, args::Int64, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [12] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/dropout.jl:9
 [13] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/dropout.jl:4
 [15] include(::String) at ./client.jl:441
 [16] macro expansion at ./util.jl:175 [inlined]
 [17] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:9 [inlined]
 [18] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [19] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [20] include(::String) at ./client.jl:441
 [21] top-level scope at none:6
 [22] eval(::Module, ::Any) at ./boot.jl:331
 [23] exec_options(::Base.JLOptions) at ./client.jl:264
 [24] _start() at ./client.jl:490
dropout: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/dropout.jl:9
  Test threw exception
  Expression: gradcheck(dropout1, k, 0.5; args = 1)
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#dropout1#7", ::KnetArray{Float64,2}, ::Vararg{Any,N} where N; kw::Tuple{}, args::Int64, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/dropout.jl:9
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/dropout.jl:4
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] dropout!(::Float64, ::KnetArray{Float64,2}, ::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/dropout.jl:62
   [2] dropout(::KnetArray{Float64,2}, ::Float64; seed::Int64, drop::Bool) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/dropout.jl:25
   [3] forw(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Symbol,Integer,Tuple{Symbol,Symbol},NamedTuple{(:seed, :drop),Tuple{Int64,Bool}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] #dropout#777 at ./none:0 [inlined]
   [5] (::var"#dropout1#7")(::Param{KnetArray{Float64,2}}, ::Float64) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/dropout.jl:4
   [6] gcsum(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [7] gcsum(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [8] (::AutoGrad.var"#217#219"{Tuple{},var"#dropout1#7",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [9] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [10] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [11] gradcheck(::var"#dropout1#7", ::KnetArray{Float64,2}, ::Vararg{Any,N} where N; kw::Tuple{}, args::Int64, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [12] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/dropout.jl:9
   [13] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/dropout.jl:4
  
dropout: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/dropout.jl:12
  Test threw exception
  Expression: isapprox(sum(abs2, dropout1(k, 0.5)), sum(abs2, dropout1(a, 0.5)), rtol = 0.1)
  UndefVarError: lib not defined
  Stacktrace:
   [1] dropout!(::Float64, ::KnetArray{Float64,2}, ::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/dropout.jl:62
   [2] dropout(::KnetArray{Float64,2}, ::Float64; seed::Int64, drop::Bool) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/dropout.jl:25
   [3] (::var"#dropout1#7")(::KnetArray{Float64,2}, ::Float64) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/dropout.jl:4
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/dropout.jl:12
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/dropout.jl:4
  
 11.884888 seconds (10.16 M allocations: 498.271 MiB, 2.09% gc time)
serialize: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/serialize.jl:5
  Got exception outside of a @test
  Cannot find cudnn
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
   [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
   [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
   [5] gethandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/rnn.jl:384
   [6] RNN(::Int64, ::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/rnn.jl:147
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/serialize.jl:6
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/serialize.jl:6
   [10] include(::String) at ./client.jl:441
   [11] macro expansion at ./util.jl:175 [inlined]
   [12] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:10 [inlined]
   [13] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
   [15] include(::String) at ./client.jl:441
   [16] top-level scope at none:6
   [17] eval(::Module, ::Any) at ./boot.jl:331
   [18] exec_options(::Base.JLOptions) at ./client.jl:264
   [19] _start() at ./client.jl:490
  
  1.011132 seconds (1.47 M allocations: 74.321 MiB, 4.38% gc time)
JLD: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/jld.jl:3
  Got exception outside of a @test
  Cannot find cudnn
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
   [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
   [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
   [5] gethandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/rnn.jl:384
   [6] RNN(::Int64, ::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/rnn.jl:147
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/jld.jl:7
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/jld.jl:6
   [10] include(::String) at ./client.jl:441
   [11] macro expansion at ./util.jl:175 [inlined]
   [12] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:11 [inlined]
   [13] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
   [15] include(::String) at ./client.jl:441
   [16] top-level scope at none:6
   [17] eval(::Module, ::Any) at ./boot.jl:331
   [18] exec_options(::Base.JLOptions) at ./client.jl:264
   [19] _start() at ./client.jl:490
  
  0.178716 seconds (5.78 k allocations: 355.062 KiB)
gcnode: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/gcnode.jl:3
  Got exception outside of a @test
  Cannot find cudnn
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] cudnnCreate() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:303
   [3] cudnnhandle(::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:210
   [4] cudnnhandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/gpu.jl:203
   [5] gethandle() at /home/pkgeval/.julia/packages/Knet/vxHRi/src/rnn.jl:384
   [6] RNN(::Int64, ::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/rnn.jl:147
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/gcnode.jl:8
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/gcnode.jl:6
   [10] include(::String) at ./client.jl:441
   [11] macro expansion at ./util.jl:175 [inlined]
   [12] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:12 [inlined]
   [13] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
   [15] include(::String) at ./client.jl:441
   [16] top-level scope at none:6
   [17] eval(::Module, ::Any) at ./boot.jl:331
   [18] exec_options(::Base.JLOptions) at ./client.jl:264
   [19] _start() at ./client.jl:490
  
  0.152782 seconds (5.47 k allocations: 332.979 KiB)
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:13
  Test threw exception
  Expression: mean(a) ≈ mean(k)
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float32,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
   [2] sum(::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
   [3] mean(::KnetArray{Float32,2}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [4] mean(::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:13
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  

Stacktrace:
 [1] sum(::KnetArray{Float32,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
 [2] sum(::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
 [3] mean(::KnetArray{Float32,2}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
 [4] mean(::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
 [5] forw(::Function, ::Param{KnetArray{Float32,2}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [6] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [7] #mean#153 at ./none:0 [inlined]
 [8] mean at ./none:0 [inlined]
 [9] (::var"#25#42"{Param{KnetArray{Float32,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [10] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [11] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [12] (::AutoGrad.var"#234#239"{Tuple{},var"#25#42"{Param{KnetArray{Float32,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [13] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [14] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [15] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [16] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [17] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:14
 [18] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [19] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
 [20] include(::String) at ./client.jl:441
 [21] macro expansion at ./util.jl:175 [inlined]
 [22] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:13 [inlined]
 [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [25] include(::String) at ./client.jl:441
 [26] top-level scope at none:6
 [27] eval(::Module, ::Any) at ./boot.jl:331
 [28] exec_options(::Base.JLOptions) at ./client.jl:264
 [29] _start() at ./client.jl:490
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:14
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:14 =# @gcheck mean(p)
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:14
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float32,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
   [2] sum(::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
   [3] mean(::KnetArray{Float32,2}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [4] mean(::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [5] forw(::Function, ::Param{KnetArray{Float32,2}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [6] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [7] #mean#153 at ./none:0 [inlined]
   [8] mean at ./none:0 [inlined]
   [9] (::var"#25#42"{Param{KnetArray{Float32,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [10] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [11] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [12] (::AutoGrad.var"#234#239"{Tuple{},var"#25#42"{Param{KnetArray{Float32,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [13] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [14] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [15] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [16] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:14
   [18] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:15
  Test threw exception
  Expression: mean(a, dims = 1) ≈ mean(k, dims = 1)
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float32,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
   [2] mean(::KnetArray{Float32,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [3] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:15
   [4] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  

Stacktrace:
 [1] sum(::KnetArray{Float32,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
 [2] mean(::KnetArray{Float32,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
 [3] forw(::Function, ::Param{KnetArray{Float32,2}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] #mean#153 at ./none:0 [inlined]
 [5] (::var"#26#43"{Param{KnetArray{Float32,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [6] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [7] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [8] (::AutoGrad.var"#234#239"{Tuple{},var"#26#43"{Param{KnetArray{Float32,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [9] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [10] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [11] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [12] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [13] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:16
 [14] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [15] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
 [16] include(::String) at ./client.jl:441
 [17] macro expansion at ./util.jl:175 [inlined]
 [18] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:13 [inlined]
 [19] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [21] include(::String) at ./client.jl:441
 [22] top-level scope at none:6
 [23] eval(::Module, ::Any) at ./boot.jl:331
 [24] exec_options(::Base.JLOptions) at ./client.jl:264
 [25] _start() at ./client.jl:490
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:16
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:16 =# @gcheck mean(p, dims = 1)
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:16
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float32,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
   [2] mean(::KnetArray{Float32,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [3] forw(::Function, ::Param{KnetArray{Float32,2}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] #mean#153 at ./none:0 [inlined]
   [5] (::var"#26#43"{Param{KnetArray{Float32,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [6] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [7] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [8] (::AutoGrad.var"#234#239"{Tuple{},var"#26#43"{Param{KnetArray{Float32,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [9] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [10] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [11] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [12] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:16
   [14] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:17
  Test threw exception
  Expression: mean(a, dims = 2) ≈ mean(k, dims = 2)
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float32,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
   [2] mean(::KnetArray{Float32,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [3] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:17
   [4] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  

Stacktrace:
 [1] sum(::KnetArray{Float32,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
 [2] mean(::KnetArray{Float32,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
 [3] forw(::Function, ::Param{KnetArray{Float32,2}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] #mean#153 at ./none:0 [inlined]
 [5] (::var"#27#44"{Param{KnetArray{Float32,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [6] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [7] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [8] (::AutoGrad.var"#234#239"{Tuple{},var"#27#44"{Param{KnetArray{Float32,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [9] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [10] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [11] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [12] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [13] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:18
 [14] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [15] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
 [16] include(::String) at ./client.jl:441
 [17] macro expansion at ./util.jl:175 [inlined]
 [18] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:13 [inlined]
 [19] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [21] include(::String) at ./client.jl:441
 [22] top-level scope at none:6
 [23] eval(::Module, ::Any) at ./boot.jl:331
 [24] exec_options(::Base.JLOptions) at ./client.jl:264
 [25] _start() at ./client.jl:490
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:18
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:18 =# @gcheck mean(p, dims = 2)
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:18
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float32,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
   [2] mean(::KnetArray{Float32,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [3] forw(::Function, ::Param{KnetArray{Float32,2}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] #mean#153 at ./none:0 [inlined]
   [5] (::var"#27#44"{Param{KnetArray{Float32,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [6] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [7] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [8] (::AutoGrad.var"#234#239"{Tuple{},var"#27#44"{Param{KnetArray{Float32,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [9] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [10] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [11] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [12] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:18
   [14] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:19
  Test threw exception
  Expression: mean(abs, a) ≈ mean(abs, k)
  UndefVarError: lib not defined
  Stacktrace:
   [1] sumabs(::KnetArray{Float32,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
   [2] sum(::typeof(abs), ::KnetArray{Float32,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:6
   [3] sum(::typeof(abs), ::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:6
   [4] mean(::Function, ::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:4
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:19
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  

Stacktrace:
 [1] sumabs(::KnetArray{Float32,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
 [2] sum(::typeof(abs), ::KnetArray{Float32,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:6
 [3] sum(::typeof(abs), ::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:6
 [4] mean(::Function, ::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:4
 [5] forw(::Function, ::Function, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [6] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [7] #mean#163 at ./none:0 [inlined]
 [8] mean at ./none:0 [inlined]
 [9] (::var"#28#45"{Param{KnetArray{Float32,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [10] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [11] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [12] (::AutoGrad.var"#234#239"{Tuple{},var"#28#45"{Param{KnetArray{Float32,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [13] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [14] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [15] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [16] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [17] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:20
 [18] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [19] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
 [20] include(::String) at ./client.jl:441
 [21] macro expansion at ./util.jl:175 [inlined]
 [22] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:13 [inlined]
 [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [25] include(::String) at ./client.jl:441
 [26] top-level scope at none:6
 [27] eval(::Module, ::Any) at ./boot.jl:331
 [28] exec_options(::Base.JLOptions) at ./client.jl:264
 [29] _start() at ./client.jl:490
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:20
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:20 =# @gcheck mean(abs, p)
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:20
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sumabs(::KnetArray{Float32,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
   [2] sum(::typeof(abs), ::KnetArray{Float32,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:6
   [3] sum(::typeof(abs), ::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:6
   [4] mean(::Function, ::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:4
   [5] forw(::Function, ::Function, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [6] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [7] #mean#163 at ./none:0 [inlined]
   [8] mean at ./none:0 [inlined]
   [9] (::var"#28#45"{Param{KnetArray{Float32,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [10] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [11] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [12] (::AutoGrad.var"#234#239"{Tuple{},var"#28#45"{Param{KnetArray{Float32,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [13] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [14] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [15] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [16] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:20
   [18] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:21
  Test threw exception
  Expression: mean(abs2, a) ≈ mean(abs2, k)
  UndefVarError: lib not defined
  Stacktrace:
   [1] sumabs2(::KnetArray{Float32,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
   [2] sum(::typeof(abs2), ::KnetArray{Float32,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:7
   [3] sum(::typeof(abs2), ::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:7
   [4] mean(::Function, ::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:4
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:21
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  

Stacktrace:
 [1] sumabs2(::KnetArray{Float32,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
 [2] sum(::typeof(abs2), ::KnetArray{Float32,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:7
 [3] sum(::typeof(abs2), ::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:7
 [4] mean(::Function, ::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:4
 [5] forw(::Function, ::Function, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [6] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [7] #mean#181 at ./none:0 [inlined]
 [8] mean at ./none:0 [inlined]
 [9] (::var"#29#46"{Param{KnetArray{Float32,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [10] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [11] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [12] (::AutoGrad.var"#234#239"{Tuple{},var"#29#46"{Param{KnetArray{Float32,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [13] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [14] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [15] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [16] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [17] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:22
 [18] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [19] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
 [20] include(::String) at ./client.jl:441
 [21] macro expansion at ./util.jl:175 [inlined]
 [22] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:13 [inlined]
 [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [25] include(::String) at ./client.jl:441
 [26] top-level scope at none:6
 [27] eval(::Module, ::Any) at ./boot.jl:331
 [28] exec_options(::Base.JLOptions) at ./client.jl:264
 [29] _start() at ./client.jl:490
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:22
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:22 =# @gcheck mean(abs2, p)
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:22
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sumabs2(::KnetArray{Float32,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
   [2] sum(::typeof(abs2), ::KnetArray{Float32,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:7
   [3] sum(::typeof(abs2), ::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:7
   [4] mean(::Function, ::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:4
   [5] forw(::Function, ::Function, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [6] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [7] #mean#181 at ./none:0 [inlined]
   [8] mean at ./none:0 [inlined]
   [9] (::var"#29#46"{Param{KnetArray{Float32,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [10] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [11] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [12] (::AutoGrad.var"#234#239"{Tuple{},var"#29#46"{Param{KnetArray{Float32,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [13] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [14] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [15] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [16] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:22
   [18] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:23
  Test threw exception
  Expression: std(a) ≈ std(k)
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float32,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
   [2] mean(::KnetArray{Float32,2}; o::Base.Iterators.Pairs{Symbol,Colon,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Colon}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [3] _varm(::KnetArray{Float32,2}, ::Nothing; corrected::Bool, dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:11
   [4] var(::KnetArray{Float32,2}; corrected::Bool, mean::Nothing, dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:7
   [5] var(::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:7
   [6] std(::KnetArray{Float32,2}; kws::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:5
   [7] std(::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:5
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:23
   [9] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [10] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  

Stacktrace:
 [1] sum(::KnetArray{Float32,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
 [2] mean(::KnetArray{Float32,2}; o::Base.Iterators.Pairs{Symbol,Colon,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Colon}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
 [3] forw(::Function, ::Param{KnetArray{Float32,2}}; kwargs::Base.Iterators.Pairs{Symbol,Colon,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Colon}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] #mean#153 at ./none:0 [inlined]
 [5] _varm(::Param{KnetArray{Float32,2}}, ::Nothing; corrected::Bool, dims::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:31
 [6] #var#195 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:27 [inlined]
 [7] var at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:27 [inlined]
 [8] std(::Param{KnetArray{Float32,2}}; kws::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:24
 [9] std at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:24 [inlined]
 [10] (::var"#30#47"{Param{KnetArray{Float32,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [11] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [12] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [13] (::AutoGrad.var"#234#239"{Tuple{},var"#30#47"{Param{KnetArray{Float32,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [14] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [15] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [16] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [17] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [18] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:24
 [19] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
 [21] include(::String) at ./client.jl:441
 [22] macro expansion at ./util.jl:175 [inlined]
 [23] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:13 [inlined]
 [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [26] include(::String) at ./client.jl:441
 [27] top-level scope at none:6
 [28] eval(::Module, ::Any) at ./boot.jl:331
 [29] exec_options(::Base.JLOptions) at ./client.jl:264
 [30] _start() at ./client.jl:490
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:24
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:24 =# @gcheck std(p)
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:24
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float32,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
   [2] mean(::KnetArray{Float32,2}; o::Base.Iterators.Pairs{Symbol,Colon,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Colon}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [3] forw(::Function, ::Param{KnetArray{Float32,2}}; kwargs::Base.Iterators.Pairs{Symbol,Colon,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Colon}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] #mean#153 at ./none:0 [inlined]
   [5] _varm(::Param{KnetArray{Float32,2}}, ::Nothing; corrected::Bool, dims::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:31
   [6] #var#195 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:27 [inlined]
   [7] var at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:27 [inlined]
   [8] std(::Param{KnetArray{Float32,2}}; kws::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:24
   [9] std at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:24 [inlined]
   [10] (::var"#30#47"{Param{KnetArray{Float32,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [11] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [12] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [13] (::AutoGrad.var"#234#239"{Tuple{},var"#30#47"{Param{KnetArray{Float32,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [14] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [15] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [16] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [17] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:24
   [19] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:25
  Test threw exception
  Expression: std(a, dims = 1) ≈ std(k, dims = 1)
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float32,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
   [2] mean(::KnetArray{Float32,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [3] _varm(::KnetArray{Float32,2}, ::Nothing; corrected::Bool, dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:11
   [4] var(::KnetArray{Float32,2}; corrected::Bool, mean::Nothing, dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:7
   [5] std(::KnetArray{Float32,2}; kws::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:5
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:25
   [7] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  

Stacktrace:
 [1] sum(::KnetArray{Float32,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
 [2] mean(::KnetArray{Float32,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
 [3] forw(::Function, ::Param{KnetArray{Float32,2}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] #mean#153 at ./none:0 [inlined]
 [5] _varm(::Param{KnetArray{Float32,2}}, ::Nothing; corrected::Bool, dims::Int64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:31
 [6] #var#195 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:27 [inlined]
 [7] std(::Param{KnetArray{Float32,2}}; kws::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:24
 [8] (::var"#31#48"{Param{KnetArray{Float32,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [9] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [10] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [11] (::AutoGrad.var"#234#239"{Tuple{},var"#31#48"{Param{KnetArray{Float32,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [12] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [13] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [14] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [15] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:26
 [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [18] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
 [19] include(::String) at ./client.jl:441
 [20] macro expansion at ./util.jl:175 [inlined]
 [21] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:13 [inlined]
 [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [24] include(::String) at ./client.jl:441
 [25] top-level scope at none:6
 [26] eval(::Module, ::Any) at ./boot.jl:331
 [27] exec_options(::Base.JLOptions) at ./client.jl:264
 [28] _start() at ./client.jl:490
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:26
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:26 =# @gcheck std(p, dims = 1)
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:26
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float32,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
   [2] mean(::KnetArray{Float32,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [3] forw(::Function, ::Param{KnetArray{Float32,2}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] #mean#153 at ./none:0 [inlined]
   [5] _varm(::Param{KnetArray{Float32,2}}, ::Nothing; corrected::Bool, dims::Int64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:31
   [6] #var#195 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:27 [inlined]
   [7] std(::Param{KnetArray{Float32,2}}; kws::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:24
   [8] (::var"#31#48"{Param{KnetArray{Float32,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [9] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [10] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [11] (::AutoGrad.var"#234#239"{Tuple{},var"#31#48"{Param{KnetArray{Float32,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [12] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [13] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [14] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [15] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:26
   [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:27
  Test threw exception
  Expression: std(a, dims = 2) ≈ std(k, dims = 2)
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float32,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
   [2] mean(::KnetArray{Float32,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [3] _varm(::KnetArray{Float32,2}, ::Nothing; corrected::Bool, dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:11
   [4] var(::KnetArray{Float32,2}; corrected::Bool, mean::Nothing, dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:7
   [5] std(::KnetArray{Float32,2}; kws::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:5
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:27
   [7] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  

Stacktrace:
 [1] sum(::KnetArray{Float32,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
 [2] mean(::KnetArray{Float32,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
 [3] forw(::Function, ::Param{KnetArray{Float32,2}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] #mean#153 at ./none:0 [inlined]
 [5] _varm(::Param{KnetArray{Float32,2}}, ::Nothing; corrected::Bool, dims::Int64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:31
 [6] #var#195 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:27 [inlined]
 [7] std(::Param{KnetArray{Float32,2}}; kws::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:24
 [8] (::var"#32#49"{Param{KnetArray{Float32,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [9] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [10] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [11] (::AutoGrad.var"#234#239"{Tuple{},var"#32#49"{Param{KnetArray{Float32,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [12] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [13] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [14] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [15] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:28
 [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [18] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
 [19] include(::String) at ./client.jl:441
 [20] macro expansion at ./util.jl:175 [inlined]
 [21] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:13 [inlined]
 [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [24] include(::String) at ./client.jl:441
 [25] top-level scope at none:6
 [26] eval(::Module, ::Any) at ./boot.jl:331
 [27] exec_options(::Base.JLOptions) at ./client.jl:264
 [28] _start() at ./client.jl:490
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:28
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:28 =# @gcheck std(p, dims = 2)
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:28
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float32,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
   [2] mean(::KnetArray{Float32,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [3] forw(::Function, ::Param{KnetArray{Float32,2}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] #mean#153 at ./none:0 [inlined]
   [5] _varm(::Param{KnetArray{Float32,2}}, ::Nothing; corrected::Bool, dims::Int64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:31
   [6] #var#195 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:27 [inlined]
   [7] std(::Param{KnetArray{Float32,2}}; kws::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:24
   [8] (::var"#32#49"{Param{KnetArray{Float32,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [9] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [10] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [11] (::AutoGrad.var"#234#239"{Tuple{},var"#32#49"{Param{KnetArray{Float32,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [12] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [13] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [14] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [15] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:28
   [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:29
  Test threw exception
  Expression: stdm(a, mean(a)) ≈ stdm(k, mean(k))
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float32,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
   [2] sum(::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
   [3] mean(::KnetArray{Float32,2}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [4] mean(::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:29
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  

Stacktrace:
 [1] sum(::KnetArray{Float32,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
 [2] sum(::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
 [3] mean(::KnetArray{Float32,2}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
 [4] mean(::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
 [5] forw(::Function, ::Param{KnetArray{Float32,2}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [6] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [7] #mean#153 at ./none:0 [inlined]
 [8] mean at ./none:0 [inlined]
 [9] (::var"#33#50"{Param{KnetArray{Float32,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [10] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [11] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [12] (::AutoGrad.var"#234#239"{Tuple{},var"#33#50"{Param{KnetArray{Float32,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [13] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [14] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [15] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [16] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [17] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:30
 [18] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [19] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
 [20] include(::String) at ./client.jl:441
 [21] macro expansion at ./util.jl:175 [inlined]
 [22] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:13 [inlined]
 [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [25] include(::String) at ./client.jl:441
 [26] top-level scope at none:6
 [27] eval(::Module, ::Any) at ./boot.jl:331
 [28] exec_options(::Base.JLOptions) at ./client.jl:264
 [29] _start() at ./client.jl:490
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:30
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:30 =# @gcheck stdm(p, mean(p))
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:30
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float32,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
   [2] sum(::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
   [3] mean(::KnetArray{Float32,2}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [4] mean(::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [5] forw(::Function, ::Param{KnetArray{Float32,2}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [6] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [7] #mean#153 at ./none:0 [inlined]
   [8] mean at ./none:0 [inlined]
   [9] (::var"#33#50"{Param{KnetArray{Float32,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [10] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [11] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [12] (::AutoGrad.var"#234#239"{Tuple{},var"#33#50"{Param{KnetArray{Float32,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [13] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [14] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [15] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [16] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:30
   [18] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:31
  Test threw exception
  Expression: stdm(a, mean(a, dims = 1), dims = 1) ≈ stdm(k, mean(k, dims = 1), dims = 1)
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float32,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
   [2] mean(::KnetArray{Float32,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [3] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:31
   [4] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  

Stacktrace:
 [1] sum(::KnetArray{Float32,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
 [2] mean(::KnetArray{Float32,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
 [3] forw(::Function, ::Param{KnetArray{Float32,2}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] #mean#153 at ./none:0 [inlined]
 [5] (::var"#34#51"{Param{KnetArray{Float32,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [6] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [7] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [8] (::AutoGrad.var"#234#239"{Tuple{},var"#34#51"{Param{KnetArray{Float32,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [9] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [10] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [11] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [12] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [13] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:32
 [14] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [15] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
 [16] include(::String) at ./client.jl:441
 [17] macro expansion at ./util.jl:175 [inlined]
 [18] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:13 [inlined]
 [19] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [21] include(::String) at ./client.jl:441
 [22] top-level scope at none:6
 [23] eval(::Module, ::Any) at ./boot.jl:331
 [24] exec_options(::Base.JLOptions) at ./client.jl:264
 [25] _start() at ./client.jl:490
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:32
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:32 =# @gcheck stdm(p, mean(p, dims = 1), dims = 1)
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:32
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float32,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
   [2] mean(::KnetArray{Float32,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [3] forw(::Function, ::Param{KnetArray{Float32,2}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] #mean#153 at ./none:0 [inlined]
   [5] (::var"#34#51"{Param{KnetArray{Float32,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [6] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [7] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [8] (::AutoGrad.var"#234#239"{Tuple{},var"#34#51"{Param{KnetArray{Float32,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [9] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [10] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [11] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [12] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:32
   [14] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:33
  Test threw exception
  Expression: stdm(a, mean(a, dims = 2), dims = 2) ≈ stdm(k, mean(k, dims = 2), dims = 2)
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float32,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
   [2] mean(::KnetArray{Float32,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [3] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:33
   [4] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  

Stacktrace:
 [1] sum(::KnetArray{Float32,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
 [2] mean(::KnetArray{Float32,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
 [3] forw(::Function, ::Param{KnetArray{Float32,2}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] #mean#153 at ./none:0 [inlined]
 [5] (::var"#35#52"{Param{KnetArray{Float32,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [6] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [7] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [8] (::AutoGrad.var"#234#239"{Tuple{},var"#35#52"{Param{KnetArray{Float32,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [9] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [10] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [11] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [12] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [13] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:34
 [14] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [15] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
 [16] include(::String) at ./client.jl:441
 [17] macro expansion at ./util.jl:175 [inlined]
 [18] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:13 [inlined]
 [19] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [21] include(::String) at ./client.jl:441
 [22] top-level scope at none:6
 [23] eval(::Module, ::Any) at ./boot.jl:331
 [24] exec_options(::Base.JLOptions) at ./client.jl:264
 [25] _start() at ./client.jl:490
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:34
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:34 =# @gcheck stdm(p, mean(p, dims = 2), dims = 2)
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:34
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float32,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
   [2] mean(::KnetArray{Float32,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [3] forw(::Function, ::Param{KnetArray{Float32,2}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] #mean#153 at ./none:0 [inlined]
   [5] (::var"#35#52"{Param{KnetArray{Float32,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [6] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [7] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [8] (::AutoGrad.var"#234#239"{Tuple{},var"#35#52"{Param{KnetArray{Float32,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [9] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [10] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [11] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [12] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:34
   [14] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:35
  Test threw exception
  Expression: var(a) ≈ var(k)
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float32,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
   [2] mean(::KnetArray{Float32,2}; o::Base.Iterators.Pairs{Symbol,Colon,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Colon}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [3] _varm(::KnetArray{Float32,2}, ::Nothing; corrected::Bool, dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:11
   [4] var(::KnetArray{Float32,2}; corrected::Bool, mean::Nothing, dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:7
   [5] var(::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:7
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:35
   [7] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  

Stacktrace:
 [1] sum(::KnetArray{Float32,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
 [2] mean(::KnetArray{Float32,2}; o::Base.Iterators.Pairs{Symbol,Colon,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Colon}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
 [3] forw(::Function, ::Param{KnetArray{Float32,2}}; kwargs::Base.Iterators.Pairs{Symbol,Colon,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Colon}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] #mean#153 at ./none:0 [inlined]
 [5] _varm(::Param{KnetArray{Float32,2}}, ::Nothing; corrected::Bool, dims::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:31
 [6] #var#195 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:27 [inlined]
 [7] var at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:27 [inlined]
 [8] (::var"#36#53"{Param{KnetArray{Float32,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [9] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [10] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [11] (::AutoGrad.var"#234#239"{Tuple{},var"#36#53"{Param{KnetArray{Float32,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [12] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [13] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [14] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [15] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:36
 [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [18] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
 [19] include(::String) at ./client.jl:441
 [20] macro expansion at ./util.jl:175 [inlined]
 [21] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:13 [inlined]
 [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [24] include(::String) at ./client.jl:441
 [25] top-level scope at none:6
 [26] eval(::Module, ::Any) at ./boot.jl:331
 [27] exec_options(::Base.JLOptions) at ./client.jl:264
 [28] _start() at ./client.jl:490
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:36
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:36 =# @gcheck var(p)
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:36
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float32,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
   [2] mean(::KnetArray{Float32,2}; o::Base.Iterators.Pairs{Symbol,Colon,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Colon}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [3] forw(::Function, ::Param{KnetArray{Float32,2}}; kwargs::Base.Iterators.Pairs{Symbol,Colon,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Colon}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] #mean#153 at ./none:0 [inlined]
   [5] _varm(::Param{KnetArray{Float32,2}}, ::Nothing; corrected::Bool, dims::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:31
   [6] #var#195 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:27 [inlined]
   [7] var at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:27 [inlined]
   [8] (::var"#36#53"{Param{KnetArray{Float32,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [9] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [10] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [11] (::AutoGrad.var"#234#239"{Tuple{},var"#36#53"{Param{KnetArray{Float32,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [12] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [13] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [14] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [15] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:36
   [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:37
  Test threw exception
  Expression: var(a, dims = 1) ≈ var(k, dims = 1)
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float32,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
   [2] mean(::KnetArray{Float32,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [3] _varm(::KnetArray{Float32,2}, ::Nothing; corrected::Bool, dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:11
   [4] var(::KnetArray{Float32,2}; corrected::Bool, mean::Nothing, dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:7
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:37
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  

Stacktrace:
 [1] sum(::KnetArray{Float32,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
 [2] mean(::KnetArray{Float32,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
 [3] forw(::Function, ::Param{KnetArray{Float32,2}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] #mean#153 at ./none:0 [inlined]
 [5] _varm(::Param{KnetArray{Float32,2}}, ::Nothing; corrected::Bool, dims::Int64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:31
 [6] #var#195 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:27 [inlined]
 [7] (::var"#37#54"{Param{KnetArray{Float32,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [8] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [9] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [10] (::AutoGrad.var"#234#239"{Tuple{},var"#37#54"{Param{KnetArray{Float32,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [11] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [12] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [13] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [14] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [15] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:38
 [16] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [17] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
 [18] include(::String) at ./client.jl:441
 [19] macro expansion at ./util.jl:175 [inlined]
 [20] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:13 [inlined]
 [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [23] include(::String) at ./client.jl:441
 [24] top-level scope at none:6
 [25] eval(::Module, ::Any) at ./boot.jl:331
 [26] exec_options(::Base.JLOptions) at ./client.jl:264
 [27] _start() at ./client.jl:490
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:38
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:38 =# @gcheck var(p, dims = 1)
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:38
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float32,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
   [2] mean(::KnetArray{Float32,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [3] forw(::Function, ::Param{KnetArray{Float32,2}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] #mean#153 at ./none:0 [inlined]
   [5] _varm(::Param{KnetArray{Float32,2}}, ::Nothing; corrected::Bool, dims::Int64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:31
   [6] #var#195 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:27 [inlined]
   [7] (::var"#37#54"{Param{KnetArray{Float32,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [8] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [9] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [10] (::AutoGrad.var"#234#239"{Tuple{},var"#37#54"{Param{KnetArray{Float32,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [11] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [12] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [13] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [14] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:38
   [16] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:39
  Test threw exception
  Expression: var(a, dims = 2) ≈ var(k, dims = 2)
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float32,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
   [2] mean(::KnetArray{Float32,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [3] _varm(::KnetArray{Float32,2}, ::Nothing; corrected::Bool, dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:11
   [4] var(::KnetArray{Float32,2}; corrected::Bool, mean::Nothing, dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:7
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:39
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  

Stacktrace:
 [1] sum(::KnetArray{Float32,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
 [2] mean(::KnetArray{Float32,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
 [3] forw(::Function, ::Param{KnetArray{Float32,2}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] #mean#153 at ./none:0 [inlined]
 [5] _varm(::Param{KnetArray{Float32,2}}, ::Nothing; corrected::Bool, dims::Int64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:31
 [6] #var#195 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:27 [inlined]
 [7] (::var"#38#55"{Param{KnetArray{Float32,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [8] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [9] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [10] (::AutoGrad.var"#234#239"{Tuple{},var"#38#55"{Param{KnetArray{Float32,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [11] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [12] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [13] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [14] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [15] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:40
 [16] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [17] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
 [18] include(::String) at ./client.jl:441
 [19] macro expansion at ./util.jl:175 [inlined]
 [20] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:13 [inlined]
 [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [23] include(::String) at ./client.jl:441
 [24] top-level scope at none:6
 [25] eval(::Module, ::Any) at ./boot.jl:331
 [26] exec_options(::Base.JLOptions) at ./client.jl:264
 [27] _start() at ./client.jl:490
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:40
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:40 =# @gcheck var(p, dims = 2)
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:40
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float32,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
   [2] mean(::KnetArray{Float32,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [3] forw(::Function, ::Param{KnetArray{Float32,2}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] #mean#153 at ./none:0 [inlined]
   [5] _varm(::Param{KnetArray{Float32,2}}, ::Nothing; corrected::Bool, dims::Int64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:31
   [6] #var#195 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:27 [inlined]
   [7] (::var"#38#55"{Param{KnetArray{Float32,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [8] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [9] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [10] (::AutoGrad.var"#234#239"{Tuple{},var"#38#55"{Param{KnetArray{Float32,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [11] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [12] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [13] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [14] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:40
   [16] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:41
  Test threw exception
  Expression: varm(a, mean(a)) ≈ varm(k, mean(k))
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float32,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
   [2] sum(::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
   [3] mean(::KnetArray{Float32,2}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [4] mean(::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:41
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  

Stacktrace:
 [1] sum(::KnetArray{Float32,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
 [2] sum(::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
 [3] mean(::KnetArray{Float32,2}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
 [4] mean(::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
 [5] forw(::Function, ::Param{KnetArray{Float32,2}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [6] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [7] #mean#153 at ./none:0 [inlined]
 [8] mean at ./none:0 [inlined]
 [9] (::var"#39#56"{Param{KnetArray{Float32,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [10] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [11] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [12] (::AutoGrad.var"#234#239"{Tuple{},var"#39#56"{Param{KnetArray{Float32,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [13] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [14] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [15] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [16] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [17] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:42
 [18] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [19] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
 [20] include(::String) at ./client.jl:441
 [21] macro expansion at ./util.jl:175 [inlined]
 [22] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:13 [inlined]
 [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [25] include(::String) at ./client.jl:441
 [26] top-level scope at none:6
 [27] eval(::Module, ::Any) at ./boot.jl:331
 [28] exec_options(::Base.JLOptions) at ./client.jl:264
 [29] _start() at ./client.jl:490
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:42
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:42 =# @gcheck varm(p, mean(p))
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:42
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float32,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
   [2] sum(::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
   [3] mean(::KnetArray{Float32,2}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [4] mean(::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [5] forw(::Function, ::Param{KnetArray{Float32,2}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [6] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [7] #mean#153 at ./none:0 [inlined]
   [8] mean at ./none:0 [inlined]
   [9] (::var"#39#56"{Param{KnetArray{Float32,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [10] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [11] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [12] (::AutoGrad.var"#234#239"{Tuple{},var"#39#56"{Param{KnetArray{Float32,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [13] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [14] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [15] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [16] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:42
   [18] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:43
  Test threw exception
  Expression: varm(a, mean(a, dims = 1), dims = 1) ≈ varm(k, mean(k, dims = 1), dims = 1)
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float32,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
   [2] mean(::KnetArray{Float32,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [3] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:43
   [4] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  

Stacktrace:
 [1] sum(::KnetArray{Float32,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
 [2] mean(::KnetArray{Float32,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
 [3] forw(::Function, ::Param{KnetArray{Float32,2}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] #mean#153 at ./none:0 [inlined]
 [5] (::var"#40#57"{Param{KnetArray{Float32,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [6] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [7] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [8] (::AutoGrad.var"#234#239"{Tuple{},var"#40#57"{Param{KnetArray{Float32,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [9] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [10] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [11] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [12] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [13] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:44
 [14] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [15] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
 [16] include(::String) at ./client.jl:441
 [17] macro expansion at ./util.jl:175 [inlined]
 [18] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:13 [inlined]
 [19] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [21] include(::String) at ./client.jl:441
 [22] top-level scope at none:6
 [23] eval(::Module, ::Any) at ./boot.jl:331
 [24] exec_options(::Base.JLOptions) at ./client.jl:264
 [25] _start() at ./client.jl:490
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:44
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:44 =# @gcheck varm(p, mean(p, dims = 1), dims = 1)
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:44
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float32,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
   [2] mean(::KnetArray{Float32,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [3] forw(::Function, ::Param{KnetArray{Float32,2}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] #mean#153 at ./none:0 [inlined]
   [5] (::var"#40#57"{Param{KnetArray{Float32,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [6] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [7] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [8] (::AutoGrad.var"#234#239"{Tuple{},var"#40#57"{Param{KnetArray{Float32,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [9] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [10] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [11] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [12] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:44
   [14] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:45
  Test threw exception
  Expression: varm(a, mean(a, dims = 2), dims = 2) ≈ varm(k, mean(k, dims = 2), dims = 2)
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float32,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
   [2] mean(::KnetArray{Float32,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [3] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:45
   [4] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  

Stacktrace:
 [1] sum(::KnetArray{Float32,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
 [2] mean(::KnetArray{Float32,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
 [3] forw(::Function, ::Param{KnetArray{Float32,2}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] #mean#153 at ./none:0 [inlined]
 [5] (::var"#41#58"{Param{KnetArray{Float32,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [6] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [7] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [8] (::AutoGrad.var"#234#239"{Tuple{},var"#41#58"{Param{KnetArray{Float32,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [9] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [10] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [11] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [12] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [13] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:46
 [14] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [15] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
 [16] include(::String) at ./client.jl:441
 [17] macro expansion at ./util.jl:175 [inlined]
 [18] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:13 [inlined]
 [19] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [21] include(::String) at ./client.jl:441
 [22] top-level scope at none:6
 [23] eval(::Module, ::Any) at ./boot.jl:331
 [24] exec_options(::Base.JLOptions) at ./client.jl:264
 [25] _start() at ./client.jl:490
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:46
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:46 =# @gcheck varm(p, mean(p, dims = 2), dims = 2)
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:46
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float32,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
   [2] mean(::KnetArray{Float32,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [3] forw(::Function, ::Param{KnetArray{Float32,2}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] #mean#153 at ./none:0 [inlined]
   [5] (::var"#41#58"{Param{KnetArray{Float32,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [6] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [7] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [8] (::AutoGrad.var"#234#239"{Tuple{},var"#41#58"{Param{KnetArray{Float32,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [9] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [10] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [11] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [12] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:46
   [14] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:13
  Test threw exception
  Expression: mean(a) ≈ mean(k)
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float64,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
   [2] sum(::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
   [3] mean(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [4] mean(::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:13
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  

Stacktrace:
 [1] sum(::KnetArray{Float64,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
 [2] sum(::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
 [3] mean(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
 [4] mean(::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
 [5] forw(::Function, ::Param{KnetArray{Float64,2}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [6] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [7] #mean#153 at ./none:0 [inlined]
 [8] mean at ./none:0 [inlined]
 [9] (::var"#25#42"{Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [10] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [11] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [12] (::AutoGrad.var"#234#239"{Tuple{},var"#25#42"{Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [13] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [14] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [15] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [16] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [17] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:14
 [18] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [19] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
 [20] include(::String) at ./client.jl:441
 [21] macro expansion at ./util.jl:175 [inlined]
 [22] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:13 [inlined]
 [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [25] include(::String) at ./client.jl:441
 [26] top-level scope at none:6
 [27] eval(::Module, ::Any) at ./boot.jl:331
 [28] exec_options(::Base.JLOptions) at ./client.jl:264
 [29] _start() at ./client.jl:490
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:14
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:14 =# @gcheck mean(p)
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:14
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float64,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
   [2] sum(::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
   [3] mean(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [4] mean(::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [5] forw(::Function, ::Param{KnetArray{Float64,2}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [6] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [7] #mean#153 at ./none:0 [inlined]
   [8] mean at ./none:0 [inlined]
   [9] (::var"#25#42"{Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [10] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [11] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [12] (::AutoGrad.var"#234#239"{Tuple{},var"#25#42"{Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [13] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [14] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [15] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [16] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:14
   [18] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:15
  Test threw exception
  Expression: mean(a, dims = 1) ≈ mean(k, dims = 1)
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float64,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
   [2] mean(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [3] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:15
   [4] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  

Stacktrace:
 [1] sum(::KnetArray{Float64,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
 [2] mean(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
 [3] forw(::Function, ::Param{KnetArray{Float64,2}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] #mean#153 at ./none:0 [inlined]
 [5] (::var"#26#43"{Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [6] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [7] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [8] (::AutoGrad.var"#234#239"{Tuple{},var"#26#43"{Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [9] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [10] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [11] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [12] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [13] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:16
 [14] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [15] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
 [16] include(::String) at ./client.jl:441
 [17] macro expansion at ./util.jl:175 [inlined]
 [18] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:13 [inlined]
 [19] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [21] include(::String) at ./client.jl:441
 [22] top-level scope at none:6
 [23] eval(::Module, ::Any) at ./boot.jl:331
 [24] exec_options(::Base.JLOptions) at ./client.jl:264
 [25] _start() at ./client.jl:490
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:16
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:16 =# @gcheck mean(p, dims = 1)
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:16
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float64,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
   [2] mean(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [3] forw(::Function, ::Param{KnetArray{Float64,2}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] #mean#153 at ./none:0 [inlined]
   [5] (::var"#26#43"{Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [6] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [7] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [8] (::AutoGrad.var"#234#239"{Tuple{},var"#26#43"{Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [9] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [10] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [11] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [12] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:16
   [14] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:17
  Test threw exception
  Expression: mean(a, dims = 2) ≈ mean(k, dims = 2)
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float64,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
   [2] mean(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [3] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:17
   [4] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  

Stacktrace:
 [1] sum(::KnetArray{Float64,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
 [2] mean(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
 [3] forw(::Function, ::Param{KnetArray{Float64,2}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] #mean#153 at ./none:0 [inlined]
 [5] (::var"#27#44"{Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [6] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [7] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [8] (::AutoGrad.var"#234#239"{Tuple{},var"#27#44"{Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [9] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [10] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [11] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [12] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [13] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:18
 [14] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [15] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
 [16] include(::String) at ./client.jl:441
 [17] macro expansion at ./util.jl:175 [inlined]
 [18] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:13 [inlined]
 [19] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [21] include(::String) at ./client.jl:441
 [22] top-level scope at none:6
 [23] eval(::Module, ::Any) at ./boot.jl:331
 [24] exec_options(::Base.JLOptions) at ./client.jl:264
 [25] _start() at ./client.jl:490
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:18
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:18 =# @gcheck mean(p, dims = 2)
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:18
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float64,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
   [2] mean(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [3] forw(::Function, ::Param{KnetArray{Float64,2}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] #mean#153 at ./none:0 [inlined]
   [5] (::var"#27#44"{Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [6] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [7] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [8] (::AutoGrad.var"#234#239"{Tuple{},var"#27#44"{Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [9] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [10] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [11] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [12] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:18
   [14] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:19
  Test threw exception
  Expression: mean(abs, a) ≈ mean(abs, k)
  UndefVarError: lib not defined
  Stacktrace:
   [1] sumabs(::KnetArray{Float64,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
   [2] sum(::typeof(abs), ::KnetArray{Float64,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:6
   [3] sum(::typeof(abs), ::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:6
   [4] mean(::Function, ::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:4
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:19
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  

Stacktrace:
 [1] sumabs(::KnetArray{Float64,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
 [2] sum(::typeof(abs), ::KnetArray{Float64,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:6
 [3] sum(::typeof(abs), ::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:6
 [4] mean(::Function, ::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:4
 [5] forw(::Function, ::Function, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [6] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [7] #mean#163 at ./none:0 [inlined]
 [8] mean at ./none:0 [inlined]
 [9] (::var"#28#45"{Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [10] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [11] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [12] (::AutoGrad.var"#234#239"{Tuple{},var"#28#45"{Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [13] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [14] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [15] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [16] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [17] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:20
 [18] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [19] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
 [20] include(::String) at ./client.jl:441
 [21] macro expansion at ./util.jl:175 [inlined]
 [22] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:13 [inlined]
 [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [25] include(::String) at ./client.jl:441
 [26] top-level scope at none:6
 [27] eval(::Module, ::Any) at ./boot.jl:331
 [28] exec_options(::Base.JLOptions) at ./client.jl:264
 [29] _start() at ./client.jl:490
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:20
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:20 =# @gcheck mean(abs, p)
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:20
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sumabs(::KnetArray{Float64,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
   [2] sum(::typeof(abs), ::KnetArray{Float64,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:6
   [3] sum(::typeof(abs), ::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:6
   [4] mean(::Function, ::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:4
   [5] forw(::Function, ::Function, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [6] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [7] #mean#163 at ./none:0 [inlined]
   [8] mean at ./none:0 [inlined]
   [9] (::var"#28#45"{Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [10] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [11] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [12] (::AutoGrad.var"#234#239"{Tuple{},var"#28#45"{Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [13] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [14] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [15] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [16] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:20
   [18] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:21
  Test threw exception
  Expression: mean(abs2, a) ≈ mean(abs2, k)
  UndefVarError: lib not defined
  Stacktrace:
   [1] sumabs2(::KnetArray{Float64,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
   [2] sum(::typeof(abs2), ::KnetArray{Float64,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:7
   [3] sum(::typeof(abs2), ::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:7
   [4] mean(::Function, ::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:4
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:21
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  

Stacktrace:
 [1] sumabs2(::KnetArray{Float64,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
 [2] sum(::typeof(abs2), ::KnetArray{Float64,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:7
 [3] sum(::typeof(abs2), ::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:7
 [4] mean(::Function, ::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:4
 [5] forw(::Function, ::Function, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [6] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [7] #mean#181 at ./none:0 [inlined]
 [8] mean at ./none:0 [inlined]
 [9] (::var"#29#46"{Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [10] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [11] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [12] (::AutoGrad.var"#234#239"{Tuple{},var"#29#46"{Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [13] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [14] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [15] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [16] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [17] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:22
 [18] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [19] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
 [20] include(::String) at ./client.jl:441
 [21] macro expansion at ./util.jl:175 [inlined]
 [22] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:13 [inlined]
 [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [25] include(::String) at ./client.jl:441
 [26] top-level scope at none:6
 [27] eval(::Module, ::Any) at ./boot.jl:331
 [28] exec_options(::Base.JLOptions) at ./client.jl:264
 [29] _start() at ./client.jl:490
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:22
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:22 =# @gcheck mean(abs2, p)
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:22
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sumabs2(::KnetArray{Float64,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
   [2] sum(::typeof(abs2), ::KnetArray{Float64,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:7
   [3] sum(::typeof(abs2), ::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:7
   [4] mean(::Function, ::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:4
   [5] forw(::Function, ::Function, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [6] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [7] #mean#181 at ./none:0 [inlined]
   [8] mean at ./none:0 [inlined]
   [9] (::var"#29#46"{Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [10] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [11] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [12] (::AutoGrad.var"#234#239"{Tuple{},var"#29#46"{Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [13] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [14] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [15] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [16] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:22
   [18] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:23
  Test threw exception
  Expression: std(a) ≈ std(k)
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float64,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
   [2] mean(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Symbol,Colon,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Colon}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [3] _varm(::KnetArray{Float64,2}, ::Nothing; corrected::Bool, dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:11
   [4] var(::KnetArray{Float64,2}; corrected::Bool, mean::Nothing, dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:7
   [5] var(::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:7
   [6] std(::KnetArray{Float64,2}; kws::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:5
   [7] std(::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:5
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:23
   [9] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [10] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  

Stacktrace:
 [1] sum(::KnetArray{Float64,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
 [2] mean(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Symbol,Colon,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Colon}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
 [3] forw(::Function, ::Param{KnetArray{Float64,2}}; kwargs::Base.Iterators.Pairs{Symbol,Colon,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Colon}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] #mean#153 at ./none:0 [inlined]
 [5] _varm(::Param{KnetArray{Float64,2}}, ::Nothing; corrected::Bool, dims::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:31
 [6] #var#195 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:27 [inlined]
 [7] var at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:27 [inlined]
 [8] std(::Param{KnetArray{Float64,2}}; kws::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:24
 [9] std at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:24 [inlined]
 [10] (::var"#30#47"{Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [11] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [12] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [13] (::AutoGrad.var"#234#239"{Tuple{},var"#30#47"{Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [14] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [15] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [16] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [17] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [18] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:24
 [19] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
 [21] include(::String) at ./client.jl:441
 [22] macro expansion at ./util.jl:175 [inlined]
 [23] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:13 [inlined]
 [24] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [26] include(::String) at ./client.jl:441
 [27] top-level scope at none:6
 [28] eval(::Module, ::Any) at ./boot.jl:331
 [29] exec_options(::Base.JLOptions) at ./client.jl:264
 [30] _start() at ./client.jl:490
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:24
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:24 =# @gcheck std(p)
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:24
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float64,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
   [2] mean(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Symbol,Colon,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Colon}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [3] forw(::Function, ::Param{KnetArray{Float64,2}}; kwargs::Base.Iterators.Pairs{Symbol,Colon,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Colon}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] #mean#153 at ./none:0 [inlined]
   [5] _varm(::Param{KnetArray{Float64,2}}, ::Nothing; corrected::Bool, dims::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:31
   [6] #var#195 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:27 [inlined]
   [7] var at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:27 [inlined]
   [8] std(::Param{KnetArray{Float64,2}}; kws::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:24
   [9] std at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:24 [inlined]
   [10] (::var"#30#47"{Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [11] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [12] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [13] (::AutoGrad.var"#234#239"{Tuple{},var"#30#47"{Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [14] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [15] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [16] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [17] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:24
   [19] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:25
  Test threw exception
  Expression: std(a, dims = 1) ≈ std(k, dims = 1)
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float64,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
   [2] mean(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [3] _varm(::KnetArray{Float64,2}, ::Nothing; corrected::Bool, dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:11
   [4] var(::KnetArray{Float64,2}; corrected::Bool, mean::Nothing, dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:7
   [5] std(::KnetArray{Float64,2}; kws::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:5
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:25
   [7] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  

Stacktrace:
 [1] sum(::KnetArray{Float64,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
 [2] mean(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
 [3] forw(::Function, ::Param{KnetArray{Float64,2}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] #mean#153 at ./none:0 [inlined]
 [5] _varm(::Param{KnetArray{Float64,2}}, ::Nothing; corrected::Bool, dims::Int64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:31
 [6] #var#195 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:27 [inlined]
 [7] std(::Param{KnetArray{Float64,2}}; kws::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:24
 [8] (::var"#31#48"{Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [9] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [10] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [11] (::AutoGrad.var"#234#239"{Tuple{},var"#31#48"{Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [12] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [13] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [14] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [15] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:26
 [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [18] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
 [19] include(::String) at ./client.jl:441
 [20] macro expansion at ./util.jl:175 [inlined]
 [21] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:13 [inlined]
 [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [24] include(::String) at ./client.jl:441
 [25] top-level scope at none:6
 [26] eval(::Module, ::Any) at ./boot.jl:331
 [27] exec_options(::Base.JLOptions) at ./client.jl:264
 [28] _start() at ./client.jl:490
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:26
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:26 =# @gcheck std(p, dims = 1)
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:26
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float64,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
   [2] mean(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [3] forw(::Function, ::Param{KnetArray{Float64,2}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] #mean#153 at ./none:0 [inlined]
   [5] _varm(::Param{KnetArray{Float64,2}}, ::Nothing; corrected::Bool, dims::Int64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:31
   [6] #var#195 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:27 [inlined]
   [7] std(::Param{KnetArray{Float64,2}}; kws::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:24
   [8] (::var"#31#48"{Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [9] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [10] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [11] (::AutoGrad.var"#234#239"{Tuple{},var"#31#48"{Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [12] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [13] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [14] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [15] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:26
   [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:27
  Test threw exception
  Expression: std(a, dims = 2) ≈ std(k, dims = 2)
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float64,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
   [2] mean(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [3] _varm(::KnetArray{Float64,2}, ::Nothing; corrected::Bool, dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:11
   [4] var(::KnetArray{Float64,2}; corrected::Bool, mean::Nothing, dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:7
   [5] std(::KnetArray{Float64,2}; kws::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:5
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:27
   [7] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  

Stacktrace:
 [1] sum(::KnetArray{Float64,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
 [2] mean(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
 [3] forw(::Function, ::Param{KnetArray{Float64,2}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] #mean#153 at ./none:0 [inlined]
 [5] _varm(::Param{KnetArray{Float64,2}}, ::Nothing; corrected::Bool, dims::Int64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:31
 [6] #var#195 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:27 [inlined]
 [7] std(::Param{KnetArray{Float64,2}}; kws::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:24
 [8] (::var"#32#49"{Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [9] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [10] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [11] (::AutoGrad.var"#234#239"{Tuple{},var"#32#49"{Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [12] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [13] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [14] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [15] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:28
 [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [18] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
 [19] include(::String) at ./client.jl:441
 [20] macro expansion at ./util.jl:175 [inlined]
 [21] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:13 [inlined]
 [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [24] include(::String) at ./client.jl:441
 [25] top-level scope at none:6
 [26] eval(::Module, ::Any) at ./boot.jl:331
 [27] exec_options(::Base.JLOptions) at ./client.jl:264
 [28] _start() at ./client.jl:490
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:28
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:28 =# @gcheck std(p, dims = 2)
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:28
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float64,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
   [2] mean(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [3] forw(::Function, ::Param{KnetArray{Float64,2}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] #mean#153 at ./none:0 [inlined]
   [5] _varm(::Param{KnetArray{Float64,2}}, ::Nothing; corrected::Bool, dims::Int64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:31
   [6] #var#195 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:27 [inlined]
   [7] std(::Param{KnetArray{Float64,2}}; kws::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:24
   [8] (::var"#32#49"{Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [9] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [10] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [11] (::AutoGrad.var"#234#239"{Tuple{},var"#32#49"{Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [12] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [13] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [14] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [15] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:28
   [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:29
  Test threw exception
  Expression: stdm(a, mean(a)) ≈ stdm(k, mean(k))
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float64,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
   [2] sum(::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
   [3] mean(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [4] mean(::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:29
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  

Stacktrace:
 [1] sum(::KnetArray{Float64,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
 [2] sum(::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
 [3] mean(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
 [4] mean(::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
 [5] forw(::Function, ::Param{KnetArray{Float64,2}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [6] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [7] #mean#153 at ./none:0 [inlined]
 [8] mean at ./none:0 [inlined]
 [9] (::var"#33#50"{Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [10] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [11] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [12] (::AutoGrad.var"#234#239"{Tuple{},var"#33#50"{Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [13] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [14] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [15] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [16] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [17] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:30
 [18] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [19] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
 [20] include(::String) at ./client.jl:441
 [21] macro expansion at ./util.jl:175 [inlined]
 [22] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:13 [inlined]
 [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [25] include(::String) at ./client.jl:441
 [26] top-level scope at none:6
 [27] eval(::Module, ::Any) at ./boot.jl:331
 [28] exec_options(::Base.JLOptions) at ./client.jl:264
 [29] _start() at ./client.jl:490
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:30
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:30 =# @gcheck stdm(p, mean(p))
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:30
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float64,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
   [2] sum(::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
   [3] mean(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [4] mean(::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [5] forw(::Function, ::Param{KnetArray{Float64,2}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [6] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [7] #mean#153 at ./none:0 [inlined]
   [8] mean at ./none:0 [inlined]
   [9] (::var"#33#50"{Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [10] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [11] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [12] (::AutoGrad.var"#234#239"{Tuple{},var"#33#50"{Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [13] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [14] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [15] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [16] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:30
   [18] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:31
  Test threw exception
  Expression: stdm(a, mean(a, dims = 1), dims = 1) ≈ stdm(k, mean(k, dims = 1), dims = 1)
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float64,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
   [2] mean(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [3] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:31
   [4] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  

Stacktrace:
 [1] sum(::KnetArray{Float64,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
 [2] mean(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
 [3] forw(::Function, ::Param{KnetArray{Float64,2}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] #mean#153 at ./none:0 [inlined]
 [5] (::var"#34#51"{Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [6] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [7] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [8] (::AutoGrad.var"#234#239"{Tuple{},var"#34#51"{Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [9] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [10] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [11] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [12] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [13] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:32
 [14] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [15] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
 [16] include(::String) at ./client.jl:441
 [17] macro expansion at ./util.jl:175 [inlined]
 [18] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:13 [inlined]
 [19] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [21] include(::String) at ./client.jl:441
 [22] top-level scope at none:6
 [23] eval(::Module, ::Any) at ./boot.jl:331
 [24] exec_options(::Base.JLOptions) at ./client.jl:264
 [25] _start() at ./client.jl:490
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:32
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:32 =# @gcheck stdm(p, mean(p, dims = 1), dims = 1)
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:32
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float64,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
   [2] mean(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [3] forw(::Function, ::Param{KnetArray{Float64,2}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] #mean#153 at ./none:0 [inlined]
   [5] (::var"#34#51"{Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [6] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [7] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [8] (::AutoGrad.var"#234#239"{Tuple{},var"#34#51"{Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [9] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [10] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [11] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [12] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:32
   [14] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:33
  Test threw exception
  Expression: stdm(a, mean(a, dims = 2), dims = 2) ≈ stdm(k, mean(k, dims = 2), dims = 2)
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float64,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
   [2] mean(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [3] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:33
   [4] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  

Stacktrace:
 [1] sum(::KnetArray{Float64,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
 [2] mean(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
 [3] forw(::Function, ::Param{KnetArray{Float64,2}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] #mean#153 at ./none:0 [inlined]
 [5] (::var"#35#52"{Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [6] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [7] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [8] (::AutoGrad.var"#234#239"{Tuple{},var"#35#52"{Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [9] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [10] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [11] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [12] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [13] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:34
 [14] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [15] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
 [16] include(::String) at ./client.jl:441
 [17] macro expansion at ./util.jl:175 [inlined]
 [18] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:13 [inlined]
 [19] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [21] include(::String) at ./client.jl:441
 [22] top-level scope at none:6
 [23] eval(::Module, ::Any) at ./boot.jl:331
 [24] exec_options(::Base.JLOptions) at ./client.jl:264
 [25] _start() at ./client.jl:490
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:34
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:34 =# @gcheck stdm(p, mean(p, dims = 2), dims = 2)
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:34
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float64,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
   [2] mean(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [3] forw(::Function, ::Param{KnetArray{Float64,2}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] #mean#153 at ./none:0 [inlined]
   [5] (::var"#35#52"{Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [6] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [7] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [8] (::AutoGrad.var"#234#239"{Tuple{},var"#35#52"{Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [9] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [10] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [11] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [12] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:34
   [14] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:35
  Test threw exception
  Expression: var(a) ≈ var(k)
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float64,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
   [2] mean(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Symbol,Colon,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Colon}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [3] _varm(::KnetArray{Float64,2}, ::Nothing; corrected::Bool, dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:11
   [4] var(::KnetArray{Float64,2}; corrected::Bool, mean::Nothing, dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:7
   [5] var(::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:7
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:35
   [7] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  

Stacktrace:
 [1] sum(::KnetArray{Float64,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
 [2] mean(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Symbol,Colon,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Colon}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
 [3] forw(::Function, ::Param{KnetArray{Float64,2}}; kwargs::Base.Iterators.Pairs{Symbol,Colon,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Colon}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] #mean#153 at ./none:0 [inlined]
 [5] _varm(::Param{KnetArray{Float64,2}}, ::Nothing; corrected::Bool, dims::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:31
 [6] #var#195 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:27 [inlined]
 [7] var at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:27 [inlined]
 [8] (::var"#36#53"{Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [9] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [10] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [11] (::AutoGrad.var"#234#239"{Tuple{},var"#36#53"{Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [12] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [13] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [14] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [15] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:36
 [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [18] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
 [19] include(::String) at ./client.jl:441
 [20] macro expansion at ./util.jl:175 [inlined]
 [21] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:13 [inlined]
 [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [24] include(::String) at ./client.jl:441
 [25] top-level scope at none:6
 [26] eval(::Module, ::Any) at ./boot.jl:331
 [27] exec_options(::Base.JLOptions) at ./client.jl:264
 [28] _start() at ./client.jl:490
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:36
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:36 =# @gcheck var(p)
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:36
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float64,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
   [2] mean(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Symbol,Colon,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Colon}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [3] forw(::Function, ::Param{KnetArray{Float64,2}}; kwargs::Base.Iterators.Pairs{Symbol,Colon,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Colon}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] #mean#153 at ./none:0 [inlined]
   [5] _varm(::Param{KnetArray{Float64,2}}, ::Nothing; corrected::Bool, dims::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:31
   [6] #var#195 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:27 [inlined]
   [7] var at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:27 [inlined]
   [8] (::var"#36#53"{Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [9] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [10] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [11] (::AutoGrad.var"#234#239"{Tuple{},var"#36#53"{Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [12] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [13] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [14] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [15] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:36
   [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:37
  Test threw exception
  Expression: var(a, dims = 1) ≈ var(k, dims = 1)
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float64,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
   [2] mean(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [3] _varm(::KnetArray{Float64,2}, ::Nothing; corrected::Bool, dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:11
   [4] var(::KnetArray{Float64,2}; corrected::Bool, mean::Nothing, dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:7
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:37
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  

Stacktrace:
 [1] sum(::KnetArray{Float64,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
 [2] mean(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
 [3] forw(::Function, ::Param{KnetArray{Float64,2}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] #mean#153 at ./none:0 [inlined]
 [5] _varm(::Param{KnetArray{Float64,2}}, ::Nothing; corrected::Bool, dims::Int64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:31
 [6] #var#195 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:27 [inlined]
 [7] (::var"#37#54"{Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [8] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [9] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [10] (::AutoGrad.var"#234#239"{Tuple{},var"#37#54"{Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [11] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [12] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [13] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [14] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [15] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:38
 [16] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [17] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
 [18] include(::String) at ./client.jl:441
 [19] macro expansion at ./util.jl:175 [inlined]
 [20] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:13 [inlined]
 [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [23] include(::String) at ./client.jl:441
 [24] top-level scope at none:6
 [25] eval(::Module, ::Any) at ./boot.jl:331
 [26] exec_options(::Base.JLOptions) at ./client.jl:264
 [27] _start() at ./client.jl:490
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:38
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:38 =# @gcheck var(p, dims = 1)
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:38
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float64,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
   [2] mean(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [3] forw(::Function, ::Param{KnetArray{Float64,2}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] #mean#153 at ./none:0 [inlined]
   [5] _varm(::Param{KnetArray{Float64,2}}, ::Nothing; corrected::Bool, dims::Int64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:31
   [6] #var#195 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:27 [inlined]
   [7] (::var"#37#54"{Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [8] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [9] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [10] (::AutoGrad.var"#234#239"{Tuple{},var"#37#54"{Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [11] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [12] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [13] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [14] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:38
   [16] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:39
  Test threw exception
  Expression: var(a, dims = 2) ≈ var(k, dims = 2)
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float64,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
   [2] mean(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [3] _varm(::KnetArray{Float64,2}, ::Nothing; corrected::Bool, dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:11
   [4] var(::KnetArray{Float64,2}; corrected::Bool, mean::Nothing, dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:7
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:39
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  

Stacktrace:
 [1] sum(::KnetArray{Float64,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
 [2] mean(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
 [3] forw(::Function, ::Param{KnetArray{Float64,2}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] #mean#153 at ./none:0 [inlined]
 [5] _varm(::Param{KnetArray{Float64,2}}, ::Nothing; corrected::Bool, dims::Int64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:31
 [6] #var#195 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:27 [inlined]
 [7] (::var"#38#55"{Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [8] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [9] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [10] (::AutoGrad.var"#234#239"{Tuple{},var"#38#55"{Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [11] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [12] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [13] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [14] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [15] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:40
 [16] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [17] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
 [18] include(::String) at ./client.jl:441
 [19] macro expansion at ./util.jl:175 [inlined]
 [20] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:13 [inlined]
 [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [23] include(::String) at ./client.jl:441
 [24] top-level scope at none:6
 [25] eval(::Module, ::Any) at ./boot.jl:331
 [26] exec_options(::Base.JLOptions) at ./client.jl:264
 [27] _start() at ./client.jl:490
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:40
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:40 =# @gcheck var(p, dims = 2)
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:40
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float64,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
   [2] mean(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [3] forw(::Function, ::Param{KnetArray{Float64,2}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] #mean#153 at ./none:0 [inlined]
   [5] _varm(::Param{KnetArray{Float64,2}}, ::Nothing; corrected::Bool, dims::Int64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:31
   [6] #var#195 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/statistics.jl:27 [inlined]
   [7] (::var"#38#55"{Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [8] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [9] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [10] (::AutoGrad.var"#234#239"{Tuple{},var"#38#55"{Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [11] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [12] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [13] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [14] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:40
   [16] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:41
  Test threw exception
  Expression: varm(a, mean(a)) ≈ varm(k, mean(k))
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float64,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
   [2] sum(::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
   [3] mean(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [4] mean(::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:41
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  

Stacktrace:
 [1] sum(::KnetArray{Float64,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
 [2] sum(::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
 [3] mean(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
 [4] mean(::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
 [5] forw(::Function, ::Param{KnetArray{Float64,2}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [6] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [7] #mean#153 at ./none:0 [inlined]
 [8] mean at ./none:0 [inlined]
 [9] (::var"#39#56"{Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [10] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [11] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [12] (::AutoGrad.var"#234#239"{Tuple{},var"#39#56"{Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [13] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [14] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [15] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [16] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [17] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:42
 [18] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [19] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
 [20] include(::String) at ./client.jl:441
 [21] macro expansion at ./util.jl:175 [inlined]
 [22] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:13 [inlined]
 [23] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [25] include(::String) at ./client.jl:441
 [26] top-level scope at none:6
 [27] eval(::Module, ::Any) at ./boot.jl:331
 [28] exec_options(::Base.JLOptions) at ./client.jl:264
 [29] _start() at ./client.jl:490
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:42
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:42 =# @gcheck varm(p, mean(p))
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:42
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float64,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
   [2] sum(::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
   [3] mean(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [4] mean(::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [5] forw(::Function, ::Param{KnetArray{Float64,2}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [6] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [7] #mean#153 at ./none:0 [inlined]
   [8] mean at ./none:0 [inlined]
   [9] (::var"#39#56"{Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [10] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [11] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [12] (::AutoGrad.var"#234#239"{Tuple{},var"#39#56"{Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [13] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [14] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [15] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [16] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:42
   [18] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:43
  Test threw exception
  Expression: varm(a, mean(a, dims = 1), dims = 1) ≈ varm(k, mean(k, dims = 1), dims = 1)
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float64,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
   [2] mean(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [3] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:43
   [4] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  

Stacktrace:
 [1] sum(::KnetArray{Float64,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
 [2] mean(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
 [3] forw(::Function, ::Param{KnetArray{Float64,2}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] #mean#153 at ./none:0 [inlined]
 [5] (::var"#40#57"{Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [6] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [7] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [8] (::AutoGrad.var"#234#239"{Tuple{},var"#40#57"{Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [9] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [10] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [11] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [12] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [13] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:44
 [14] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [15] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
 [16] include(::String) at ./client.jl:441
 [17] macro expansion at ./util.jl:175 [inlined]
 [18] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:13 [inlined]
 [19] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [21] include(::String) at ./client.jl:441
 [22] top-level scope at none:6
 [23] eval(::Module, ::Any) at ./boot.jl:331
 [24] exec_options(::Base.JLOptions) at ./client.jl:264
 [25] _start() at ./client.jl:490
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:44
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:44 =# @gcheck varm(p, mean(p, dims = 1), dims = 1)
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:44
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float64,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
   [2] mean(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [3] forw(::Function, ::Param{KnetArray{Float64,2}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] #mean#153 at ./none:0 [inlined]
   [5] (::var"#40#57"{Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [6] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [7] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [8] (::AutoGrad.var"#234#239"{Tuple{},var"#40#57"{Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [9] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [10] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [11] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [12] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:44
   [14] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:45
  Test threw exception
  Expression: varm(a, mean(a, dims = 2), dims = 2) ≈ varm(k, mean(k, dims = 2), dims = 2)
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float64,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
   [2] mean(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [3] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:45
   [4] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  

Stacktrace:
 [1] sum(::KnetArray{Float64,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
 [2] mean(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
 [3] forw(::Function, ::Param{KnetArray{Float64,2}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] #mean#153 at ./none:0 [inlined]
 [5] (::var"#41#58"{Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [6] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [7] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [8] (::AutoGrad.var"#234#239"{Tuple{},var"#41#58"{Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [9] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [10] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [11] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [12] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [13] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:46
 [14] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [15] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
 [16] include(::String) at ./client.jl:441
 [17] macro expansion at ./util.jl:175 [inlined]
 [18] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:13 [inlined]
 [19] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [20] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [21] include(::String) at ./client.jl:441
 [22] top-level scope at none:6
 [23] eval(::Module, ::Any) at ./boot.jl:331
 [24] exec_options(::Base.JLOptions) at ./client.jl:264
 [25] _start() at ./client.jl:490
statistics: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:46
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:46 =# @gcheck varm(p, mean(p, dims = 2), dims = 2)
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:46
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float64,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:54
   [2] mean(::KnetArray{Float64,2}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/statistics.jl:3
   [3] forw(::Function, ::Param{KnetArray{Float64,2}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] #mean#153 at ./none:0 [inlined]
   [5] (::var"#41#58"{Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [6] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [7] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [8] (::AutoGrad.var"#234#239"{Tuple{},var"#41#58"{Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [9] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [10] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [11] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [12] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:46
   [14] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/statistics.jl:9
  
 22.272024 seconds (8.58 M allocations: 432.043 MiB, 1.24% gc time)

Stacktrace:
 [1] sum(::KnetArray{Float64,1}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
 [2] sum(::KnetArray{Float64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
 [3] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,1}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [5] #sum#75 at ./none:0 [inlined]
 [6] sum(::AutoGrad.Result{KnetArray{Float64,1}}) at ./none:0
 [7] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:57
 [8] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [9] (::AutoGrad.var"#234#239"{Tuple{},var"#62#82"{Param{KnetArray{Float64,1}},Tuple{UnitRange{Int64}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [12] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [13] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:13
 [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
 [17] include(::String) at ./client.jl:441
 [18] macro expansion at ./util.jl:175 [inlined]
 [19] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:14 [inlined]
 [20] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [22] include(::String) at ./client.jl:441
 [23] top-level scope at none:6
 [24] eval(::Module, ::Any) at ./boot.jl:331
 [25] exec_options(::Base.JLOptions) at ./client.jl:264
 [26] _start() at ./client.jl:490
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:13
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:13 =# @gcheck getindex(a3, idx...)
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:13
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float64,1}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
   [2] sum(::KnetArray{Float64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
   [3] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,1}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [5] #sum#75 at ./none:0 [inlined]
   [6] sum(::AutoGrad.Result{KnetArray{Float64,1}}) at ./none:0
   [7] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:57
   [8] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [9] (::AutoGrad.var"#234#239"{Tuple{},var"#62#82"{Param{KnetArray{Float64,1}},Tuple{UnitRange{Int64}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [12] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [13] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:13
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  

Stacktrace:
 [1] sum(::KnetArray{Float64,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
 [2] sum(::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
 [3] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,2}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [5] #sum#75 at ./none:0 [inlined]
 [6] sum(::AutoGrad.Result{KnetArray{Float64,2}}) at ./none:0
 [7] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:57
 [8] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [9] (::AutoGrad.var"#234#239"{Tuple{},var"#64#84"{Param{KnetArray{Float64,1}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [12] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [13] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:17
 [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
 [17] include(::String) at ./client.jl:441
 [18] macro expansion at ./util.jl:175 [inlined]
 [19] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:14 [inlined]
 [20] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [22] include(::String) at ./client.jl:441
 [23] top-level scope at none:6
 [24] eval(::Module, ::Any) at ./boot.jl:331
 [25] exec_options(::Base.JLOptions) at ./client.jl:264
 [26] _start() at ./client.jl:490
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:17
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:17 =# @gcheck permutedims(a3)
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:17
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float64,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
   [2] sum(::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
   [3] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,2}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [5] #sum#75 at ./none:0 [inlined]
   [6] sum(::AutoGrad.Result{KnetArray{Float64,2}}) at ./none:0
   [7] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:57
   [8] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [9] (::AutoGrad.var"#234#239"{Tuple{},var"#64#84"{Param{KnetArray{Float64,1}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [12] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [13] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:17
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...

Stacktrace:
 [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
 [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
 [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
 [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
 [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
 [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
 [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
 [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [18] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,1,CUDAnative.AS.Global},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,1,Nothing},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [22] gpu_call(::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,1,Nothing},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
 [23] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
 [24] _setindex! at ./multidimensional.jl:777 [inlined]
 [25] setindex! at ./abstractarray.jl:1073 [inlined]
 [26] __cat(::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{Int64,Int64}, ::Tuple{Bool,Bool}, ::CuArrays.CuArray{Float64,1,Nothing}, ::Vararg{CuArrays.CuArray{Float64,1,Nothing},N} where N) at ./abstractarray.jl:1463
 [27] cat(::KnetArray{Float64,1}, ::Vararg{KnetArray{Float64,1},N} where N; dims::Val{2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
 [28] forw(::Function, ::Param{KnetArray{Float64,1}}, ::Vararg{Param{KnetArray{Float64,1}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Val{2},Tuple{Symbol},NamedTuple{(:dims,),Tuple{Val{2}}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [29] #cat#199 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:30 [inlined]
 [30] hcat at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:119 [inlined]
 [31] (::var"#75#95"{Param{KnetArray{Float64,1}},Param{KnetArray{Float64,1}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [32] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [33] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [34] (::AutoGrad.var"#234#239"{Tuple{},var"#75#95"{Param{KnetArray{Float64,1}},Param{KnetArray{Float64,1}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [35] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [36] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [37] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [38] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [39] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:37
 [40] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [41] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
 [42] include(::String) at ./client.jl:441
 [43] macro expansion at ./util.jl:175 [inlined]
 [44] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:14 [inlined]
 [45] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [46] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [47] include(::String) at ./client.jl:441
 [48] top-level scope at none:6
 [49] eval(::Module, ::Any) at ./boot.jl:331
 [50] exec_options(::Base.JLOptions) at ./client.jl:264
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:37
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:37 =# @gcheck hcat(a3, b3)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:37
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  caused by [exception 1]
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,1,CUDAnative.AS.Global},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,1,Nothing},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,1,Nothing},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
   [24] _setindex! at ./multidimensional.jl:777 [inlined]
   [25] setindex! at ./abstractarray.jl:1073 [inlined]
   [26] __cat(::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{Int64,Int64}, ::Tuple{Bool,Bool}, ::CuArrays.CuArray{Float64,1,Nothing}, ::Vararg{CuArrays.CuArray{Float64,1,Nothing},N} where N) at ./abstractarray.jl:1463
   [27] cat(::KnetArray{Float64,1}, ::Vararg{KnetArray{Float64,1},N} where N; dims::Val{2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
   [28] forw(::Function, ::Param{KnetArray{Float64,1}}, ::Vararg{Param{KnetArray{Float64,1}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Val{2},Tuple{Symbol},NamedTuple{(:dims,),Tuple{Val{2}}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [29] #cat#199 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:30 [inlined]
   [30] hcat at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:119 [inlined]
   [31] (::var"#75#95"{Param{KnetArray{Float64,1}},Param{KnetArray{Float64,1}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [32] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [33] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [34] (::AutoGrad.var"#234#239"{Tuple{},var"#75#95"{Param{KnetArray{Float64,1}},Param{KnetArray{Float64,1}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [35] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [36] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [37] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [38] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [39] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:37
   [40] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [41] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...

Stacktrace:
 [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
 [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
 [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
 [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
 [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
 [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
 [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
 [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [18] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,1,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,1,CUDAnative.AS.Global},Tuple{Int64},Tuple{UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,1,Nothing}, ::Tuple{CuArrays.CuArray{Float64,1,Nothing},CuArrays.CuArray{Float64,1,Nothing},Tuple{Int64},Tuple{UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [22] gpu_call(::Function, ::CuArrays.CuArray{Float64,1,Nothing}, ::Tuple{CuArrays.CuArray{Float64,1,Nothing},CuArrays.CuArray{Float64,1,Nothing},Tuple{Int64},Tuple{UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
 [23] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
 [24] _setindex! at ./multidimensional.jl:777 [inlined]
 [25] setindex! at ./abstractarray.jl:1073 [inlined]
 [26] __cat(::CuArrays.CuArray{Float64,1,Nothing}, ::Tuple{Int64}, ::Tuple{Bool}, ::CuArrays.CuArray{Float64,1,Nothing}, ::Vararg{CuArrays.CuArray{Float64,1,Nothing},N} where N) at ./abstractarray.jl:1463
 [27] cat(::KnetArray{Float64,1}, ::Vararg{KnetArray{Float64,1},N} where N; dims::Val{1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
 [28] forw(::Function, ::Param{KnetArray{Float64,1}}, ::Vararg{Param{KnetArray{Float64,1}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Val{1},Tuple{Symbol},NamedTuple{(:dims,),Tuple{Val{1}}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [29] #cat#199 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:30 [inlined]
 [30] vcat at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:118 [inlined]
 [31] (::var"#76#96"{Param{KnetArray{Float64,1}},Param{KnetArray{Float64,1}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [32] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [33] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [34] (::AutoGrad.var"#234#239"{Tuple{},var"#76#96"{Param{KnetArray{Float64,1}},Param{KnetArray{Float64,1}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [35] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [36] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [37] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [38] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [39] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:38
 [40] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [41] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
 [42] include(::String) at ./client.jl:441
 [43] macro expansion at ./util.jl:175 [inlined]
 [44] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:14 [inlined]
 [45] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [46] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [47] include(::String) at ./client.jl:441
 [48] top-level scope at none:6
 [49] eval(::Module, ::Any) at ./boot.jl:331
 [50] exec_options(::Base.JLOptions) at ./client.jl:264
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:38
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:38 =# @gcheck vcat(a3, b3)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:38
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  caused by [exception 1]
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,1,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,1,CUDAnative.AS.Global},Tuple{Int64},Tuple{UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,1,Nothing}, ::Tuple{CuArrays.CuArray{Float64,1,Nothing},CuArrays.CuArray{Float64,1,Nothing},Tuple{Int64},Tuple{UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArrays.CuArray{Float64,1,Nothing}, ::Tuple{CuArrays.CuArray{Float64,1,Nothing},CuArrays.CuArray{Float64,1,Nothing},Tuple{Int64},Tuple{UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
   [24] _setindex! at ./multidimensional.jl:777 [inlined]
   [25] setindex! at ./abstractarray.jl:1073 [inlined]
   [26] __cat(::CuArrays.CuArray{Float64,1,Nothing}, ::Tuple{Int64}, ::Tuple{Bool}, ::CuArrays.CuArray{Float64,1,Nothing}, ::Vararg{CuArrays.CuArray{Float64,1,Nothing},N} where N) at ./abstractarray.jl:1463
   [27] cat(::KnetArray{Float64,1}, ::Vararg{KnetArray{Float64,1},N} where N; dims::Val{1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
   [28] forw(::Function, ::Param{KnetArray{Float64,1}}, ::Vararg{Param{KnetArray{Float64,1}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Val{1},Tuple{Symbol},NamedTuple{(:dims,),Tuple{Val{1}}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [29] #cat#199 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:30 [inlined]
   [30] vcat at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:118 [inlined]
   [31] (::var"#76#96"{Param{KnetArray{Float64,1}},Param{KnetArray{Float64,1}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [32] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [33] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [34] (::AutoGrad.var"#234#239"{Tuple{},var"#76#96"{Param{KnetArray{Float64,1}},Param{KnetArray{Float64,1}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [35] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [36] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [37] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [38] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [39] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:38
   [40] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [41] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:40
  Test threw exception
  Expression: cat(a0, b0, dims = i) == cat(a1, b1, dims = i)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,1,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,1,CUDAnative.AS.Global},Tuple{Int64},Tuple{UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,1,Nothing}, ::Tuple{CuArrays.CuArray{Float64,1,Nothing},CuArrays.CuArray{Float64,1,Nothing},Tuple{Int64},Tuple{UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArrays.CuArray{Float64,1,Nothing}, ::Tuple{CuArrays.CuArray{Float64,1,Nothing},CuArrays.CuArray{Float64,1,Nothing},Tuple{Int64},Tuple{UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
   [24] _setindex! at ./multidimensional.jl:777 [inlined]
   [25] setindex! at ./abstractarray.jl:1073 [inlined]
   [26] __cat(::CuArrays.CuArray{Float64,1,Nothing}, ::Tuple{Int64}, ::Tuple{Bool}, ::CuArrays.CuArray{Float64,1,Nothing}, ::Vararg{CuArrays.CuArray{Float64,1,Nothing},N} where N) at ./abstractarray.jl:1463
   [27] cat(::KnetArray{Float64,1}, ::Vararg{KnetArray{Float64,1},N} where N; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:40
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...

Stacktrace:
 [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
 [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
 [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
 [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
 [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
 [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
 [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
 [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [18] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,1,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,1,CUDAnative.AS.Global},Tuple{Int64},Tuple{UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,1,Nothing}, ::Tuple{CuArrays.CuArray{Float64,1,Nothing},CuArrays.CuArray{Float64,1,Nothing},Tuple{Int64},Tuple{UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [22] gpu_call(::Function, ::CuArrays.CuArray{Float64,1,Nothing}, ::Tuple{CuArrays.CuArray{Float64,1,Nothing},CuArrays.CuArray{Float64,1,Nothing},Tuple{Int64},Tuple{UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
 [23] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
 [24] _setindex! at ./multidimensional.jl:777 [inlined]
 [25] setindex! at ./abstractarray.jl:1073 [inlined]
 [26] __cat(::CuArrays.CuArray{Float64,1,Nothing}, ::Tuple{Int64}, ::Tuple{Bool}, ::CuArrays.CuArray{Float64,1,Nothing}, ::Vararg{CuArrays.CuArray{Float64,1,Nothing},N} where N) at ./abstractarray.jl:1463
 [27] cat(::KnetArray{Float64,1}, ::Vararg{KnetArray{Float64,1},N} where N; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
 [28] forw(::Function, ::Param{KnetArray{Float64,1}}, ::Vararg{Param{KnetArray{Float64,1}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [29] #cat#199 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:30 [inlined]
 [30] (::var"#78#98"{Param{KnetArray{Float64,1}},Param{KnetArray{Float64,1}},Int64})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [31] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [32] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [33] (::AutoGrad.var"#234#239"{Tuple{},var"#78#98"{Param{KnetArray{Float64,1}},Param{KnetArray{Float64,1}},Int64},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [34] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [35] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [36] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [37] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [38] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42
 [39] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [40] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
 [41] include(::String) at ./client.jl:441
 [42] macro expansion at ./util.jl:175 [inlined]
 [43] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:14 [inlined]
 [44] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [45] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [46] include(::String) at ./client.jl:441
 [47] top-level scope at none:6
 [48] eval(::Module, ::Any) at ./boot.jl:331
 [49] exec_options(::Base.JLOptions) at ./client.jl:264
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42 =# @gcheck cat(a3, b3, dims = i)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  caused by [exception 1]
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,1,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,1,CUDAnative.AS.Global},Tuple{Int64},Tuple{UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,1,Nothing}, ::Tuple{CuArrays.CuArray{Float64,1,Nothing},CuArrays.CuArray{Float64,1,Nothing},Tuple{Int64},Tuple{UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArrays.CuArray{Float64,1,Nothing}, ::Tuple{CuArrays.CuArray{Float64,1,Nothing},CuArrays.CuArray{Float64,1,Nothing},Tuple{Int64},Tuple{UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
   [24] _setindex! at ./multidimensional.jl:777 [inlined]
   [25] setindex! at ./abstractarray.jl:1073 [inlined]
   [26] __cat(::CuArrays.CuArray{Float64,1,Nothing}, ::Tuple{Int64}, ::Tuple{Bool}, ::CuArrays.CuArray{Float64,1,Nothing}, ::Vararg{CuArrays.CuArray{Float64,1,Nothing},N} where N) at ./abstractarray.jl:1463
   [27] cat(::KnetArray{Float64,1}, ::Vararg{KnetArray{Float64,1},N} where N; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
   [28] forw(::Function, ::Param{KnetArray{Float64,1}}, ::Vararg{Param{KnetArray{Float64,1}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [29] #cat#199 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:30 [inlined]
   [30] (::var"#78#98"{Param{KnetArray{Float64,1}},Param{KnetArray{Float64,1}},Int64})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [31] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [32] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [33] (::AutoGrad.var"#234#239"{Tuple{},var"#78#98"{Param{KnetArray{Float64,1}},Param{KnetArray{Float64,1}},Int64},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [34] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [35] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [36] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [37] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [38] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42
   [39] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [40] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:11
  Test threw exception
  Expression: getindex(a0, idx...) == getindex(a1, idx...)
  UndefVarError: lib not defined
  Stacktrace:
   [1] getindex2(::KnetArray{Float64,2}, ::UnitRange{Int64}, ::UnitRange{Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/karray.jl:984
   [2] getindex(::KnetArray{Float64,2}, ::UnitRange{Int64}, ::UnitRange{Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/karray.jl:938
   [3] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:11
   [4] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  

Stacktrace:
 [1] getindex2(::KnetArray{Float64,2}, ::UnitRange{Int64}, ::UnitRange{Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/karray.jl:984
 [2] getindex(::KnetArray{Float64,2}, ::UnitRange{Int64}, ::UnitRange{Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/karray.jl:938
 [3] forw(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [5] getindex at ./none:0 [inlined]
 [6] (::var"#62#82"{Param{KnetArray{Float64,2}},Tuple{UnitRange{Int64},UnitRange{Int64}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [7] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [8] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [9] (::AutoGrad.var"#234#239"{Tuple{},var"#62#82"{Param{KnetArray{Float64,2}},Tuple{UnitRange{Int64},UnitRange{Int64}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [12] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [13] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:13
 [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
 [17] include(::String) at ./client.jl:441
 [18] macro expansion at ./util.jl:175 [inlined]
 [19] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:14 [inlined]
 [20] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [22] include(::String) at ./client.jl:441
 [23] top-level scope at none:6
 [24] eval(::Module, ::Any) at ./boot.jl:331
 [25] exec_options(::Base.JLOptions) at ./client.jl:264
 [26] _start() at ./client.jl:490
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:13
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:13 =# @gcheck getindex(a3, idx...)
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:13
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] getindex2(::KnetArray{Float64,2}, ::UnitRange{Int64}, ::UnitRange{Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/karray.jl:984
   [2] getindex(::KnetArray{Float64,2}, ::UnitRange{Int64}, ::UnitRange{Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/karray.jl:938
   [3] forw(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [5] getindex at ./none:0 [inlined]
   [6] (::var"#62#82"{Param{KnetArray{Float64,2}},Tuple{UnitRange{Int64},UnitRange{Int64}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [7] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [8] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [9] (::AutoGrad.var"#234#239"{Tuple{},var"#62#82"{Param{KnetArray{Float64,2}},Tuple{UnitRange{Int64},UnitRange{Int64}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [12] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [13] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:13
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:20
  Test threw exception
  Expression: permutedims(a0, (2, 1)) == permutedims(a1, (2, 1))
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] permutedims!(::CuArrays.CuArray{Float64,2,Nothing}, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{Int64,Int64}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [25] permutedims!(::KnetArray{Float64,2}, ::KnetArray{Float64,2}, ::Tuple{Int64,Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [26] permutedims(::KnetArray{Float64,2}, ::Tuple{Int64,Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [27] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:20
   [28] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [29] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:21
  Test threw exception
  Expression: permutedims(a0, (1, 2)) == permutedims(a1, (1, 2))
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] permutedims!(::CuArrays.CuArray{Float64,2,Nothing}, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{Int64,Int64}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [25] permutedims!(::KnetArray{Float64,2}, ::KnetArray{Float64,2}, ::Tuple{Int64,Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [26] permutedims(::KnetArray{Float64,2}, ::Tuple{Int64,Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [27] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:21
   [28] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [29] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  

Stacktrace:
 [1] sum(::KnetArray{Float64,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
 [2] sum(::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
 [3] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,2}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [5] #sum#75 at ./none:0 [inlined]
 [6] sum(::AutoGrad.Result{KnetArray{Float64,2}}) at ./none:0
 [7] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:57
 [8] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [9] (::AutoGrad.var"#234#239"{Tuple{},var"#68#88"{Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [12] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [13] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:25
 [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
 [17] include(::String) at ./client.jl:441
 [18] macro expansion at ./util.jl:175 [inlined]
 [19] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:14 [inlined]
 [20] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [22] include(::String) at ./client.jl:441
 [23] top-level scope at none:6
 [24] eval(::Module, ::Any) at ./boot.jl:331
 [25] exec_options(::Base.JLOptions) at ./client.jl:264
 [26] _start() at ./client.jl:490
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:25
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:25 =# @gcheck permutedims(a3)
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:25
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float64,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
   [2] sum(::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
   [3] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,2}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [5] #sum#75 at ./none:0 [inlined]
   [6] sum(::AutoGrad.Result{KnetArray{Float64,2}}) at ./none:0
   [7] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:57
   [8] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [9] (::AutoGrad.var"#234#239"{Tuple{},var"#68#88"{Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [12] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [13] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:25
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...

Stacktrace:
 [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
 [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
 [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
 [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
 [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
 [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
 [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
 [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [18] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [22] gpu_call(::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
 [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
 [24] permutedims!(::CuArrays.CuArray{Float64,2,Nothing}, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{Int64,Int64}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [25] permutedims!(::KnetArray{Float64,2}, ::KnetArray{Float64,2}, ::Tuple{Int64,Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [26] permutedims(::KnetArray{Float64,2}, ::Tuple{Int64,Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [27] forw(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [28] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [29] permutedims at ./none:0 [inlined]
 [30] (::var"#69#89"{Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [31] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [32] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [33] (::AutoGrad.var"#234#239"{Tuple{},var"#69#89"{Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [34] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [35] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [36] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [37] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [38] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:26
 [39] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [40] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
 [41] include(::String) at ./client.jl:441
 [42] macro expansion at ./util.jl:175 [inlined]
 [43] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:14 [inlined]
 [44] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [45] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [46] include(::String) at ./client.jl:441
 [47] top-level scope at none:6
 [48] eval(::Module, ::Any) at ./boot.jl:331
 [49] exec_options(::Base.JLOptions) at ./client.jl:264
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:26
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:26 =# @gcheck permutedims(a3, (2, 1))
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:26
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  caused by [exception 1]
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] permutedims!(::CuArrays.CuArray{Float64,2,Nothing}, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{Int64,Int64}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [25] permutedims!(::KnetArray{Float64,2}, ::KnetArray{Float64,2}, ::Tuple{Int64,Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [26] permutedims(::KnetArray{Float64,2}, ::Tuple{Int64,Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [27] forw(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [28] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [29] permutedims at ./none:0 [inlined]
   [30] (::var"#69#89"{Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [31] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [32] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [33] (::AutoGrad.var"#234#239"{Tuple{},var"#69#89"{Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [34] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [35] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [36] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [37] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [38] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:26
   [39] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [40] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...

Stacktrace:
 [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
 [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
 [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
 [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
 [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
 [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
 [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
 [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [18] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [22] gpu_call(::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
 [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
 [24] permutedims!(::CuArrays.CuArray{Float64,2,Nothing}, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{Int64,Int64}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [25] permutedims!(::KnetArray{Float64,2}, ::KnetArray{Float64,2}, ::Tuple{Int64,Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [26] permutedims(::KnetArray{Float64,2}, ::Tuple{Int64,Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [27] forw(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [28] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [29] permutedims at ./none:0 [inlined]
 [30] (::var"#70#90"{Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [31] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [32] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [33] (::AutoGrad.var"#234#239"{Tuple{},var"#70#90"{Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [34] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [35] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [36] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [37] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [38] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:27
 [39] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [40] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
 [41] include(::String) at ./client.jl:441
 [42] macro expansion at ./util.jl:175 [inlined]
 [43] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:14 [inlined]
 [44] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [45] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [46] include(::String) at ./client.jl:441
 [47] top-level scope at none:6
 [48] eval(::Module, ::Any) at ./boot.jl:331
 [49] exec_options(::Base.JLOptions) at ./client.jl:264
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:27
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:27 =# @gcheck permutedims(a3, (1, 2))
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:27
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  caused by [exception 1]
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] permutedims!(::CuArrays.CuArray{Float64,2,Nothing}, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{Int64,Int64}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [25] permutedims!(::KnetArray{Float64,2}, ::KnetArray{Float64,2}, ::Tuple{Int64,Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [26] permutedims(::KnetArray{Float64,2}, ::Tuple{Int64,Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [27] forw(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [28] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [29] permutedims at ./none:0 [inlined]
   [30] (::var"#70#90"{Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [31] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [32] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [33] (::AutoGrad.var"#234#239"{Tuple{},var"#70#90"{Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [34] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [35] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [36] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [37] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [38] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:27
   [39] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [40] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:34
  Test threw exception
  Expression: vcat(a0, b0) == vcat(a1, b1)
  UndefVarError: lib not defined
  Stacktrace:
   [1] setindex2!(::KnetArray{Float64,2}, ::KnetArray{Float64,2}, ::UnitRange{Int64}, ::Colon) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/karray.jl:1024
   [2] setindex! at /home/pkgeval/.julia/packages/Knet/vxHRi/src/karray.jl:949 [inlined]
   [3] vcat(::KnetArray{Float64,2}, ::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/karray.jl:268
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:34
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...

Stacktrace:
 [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
 [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
 [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
 [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
 [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
 [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
 [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
 [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [18] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [22] gpu_call(::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
 [23] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
 [24] _setindex! at ./multidimensional.jl:777 [inlined]
 [25] setindex! at ./abstractarray.jl:1073 [inlined]
 [26] __cat(::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{Int64,Int64}, ::Tuple{Bool,Bool}, ::CuArrays.CuArray{Float64,2,Nothing}, ::Vararg{CuArrays.CuArray{Float64,2,Nothing},N} where N) at ./abstractarray.jl:1463
 [27] cat(::KnetArray{Float64,2}, ::Vararg{KnetArray{Float64,2},N} where N; dims::Val{2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
 [28] forw(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Param{KnetArray{Float64,2}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Val{2},Tuple{Symbol},NamedTuple{(:dims,),Tuple{Val{2}}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [29] #cat#199 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:30 [inlined]
 [30] hcat at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:119 [inlined]
 [31] (::var"#75#95"{Param{KnetArray{Float64,2}},Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [32] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [33] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [34] (::AutoGrad.var"#234#239"{Tuple{},var"#75#95"{Param{KnetArray{Float64,2}},Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [35] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [36] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [37] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [38] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [39] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:37
 [40] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [41] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
 [42] include(::String) at ./client.jl:441
 [43] macro expansion at ./util.jl:175 [inlined]
 [44] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:14 [inlined]
 [45] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [46] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [47] include(::String) at ./client.jl:441
 [48] top-level scope at none:6
 [49] eval(::Module, ::Any) at ./boot.jl:331
 [50] exec_options(::Base.JLOptions) at ./client.jl:264
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:37
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:37 =# @gcheck hcat(a3, b3)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:37
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  caused by [exception 1]
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
   [24] _setindex! at ./multidimensional.jl:777 [inlined]
   [25] setindex! at ./abstractarray.jl:1073 [inlined]
   [26] __cat(::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{Int64,Int64}, ::Tuple{Bool,Bool}, ::CuArrays.CuArray{Float64,2,Nothing}, ::Vararg{CuArrays.CuArray{Float64,2,Nothing},N} where N) at ./abstractarray.jl:1463
   [27] cat(::KnetArray{Float64,2}, ::Vararg{KnetArray{Float64,2},N} where N; dims::Val{2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
   [28] forw(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Param{KnetArray{Float64,2}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Val{2},Tuple{Symbol},NamedTuple{(:dims,),Tuple{Val{2}}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [29] #cat#199 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:30 [inlined]
   [30] hcat at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:119 [inlined]
   [31] (::var"#75#95"{Param{KnetArray{Float64,2}},Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [32] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [33] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [34] (::AutoGrad.var"#234#239"{Tuple{},var"#75#95"{Param{KnetArray{Float64,2}},Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [35] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [36] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [37] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [38] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [39] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:37
   [40] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [41] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...

Stacktrace:
 [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
 [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
 [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
 [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
 [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
 [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
 [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
 [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [18] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [22] gpu_call(::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
 [23] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
 [24] _setindex! at ./multidimensional.jl:777 [inlined]
 [25] setindex! at ./abstractarray.jl:1073 [inlined]
 [26] __cat(::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{Int64,Int64}, ::Tuple{Bool}, ::CuArrays.CuArray{Float64,2,Nothing}, ::Vararg{CuArrays.CuArray{Float64,2,Nothing},N} where N) at ./abstractarray.jl:1463
 [27] cat(::KnetArray{Float64,2}, ::Vararg{KnetArray{Float64,2},N} where N; dims::Val{1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
 [28] forw(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Param{KnetArray{Float64,2}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Val{1},Tuple{Symbol},NamedTuple{(:dims,),Tuple{Val{1}}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [29] #cat#199 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:30 [inlined]
 [30] vcat at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:118 [inlined]
 [31] (::var"#76#96"{Param{KnetArray{Float64,2}},Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [32] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [33] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [34] (::AutoGrad.var"#234#239"{Tuple{},var"#76#96"{Param{KnetArray{Float64,2}},Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [35] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [36] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [37] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [38] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [39] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:38
 [40] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [41] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
 [42] include(::String) at ./client.jl:441
 [43] macro expansion at ./util.jl:175 [inlined]
 [44] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:14 [inlined]
 [45] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [46] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [47] include(::String) at ./client.jl:441
 [48] top-level scope at none:6
 [49] eval(::Module, ::Any) at ./boot.jl:331
 [50] exec_options(::Base.JLOptions) at ./client.jl:264
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:38
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:38 =# @gcheck vcat(a3, b3)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:38
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  caused by [exception 1]
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
   [24] _setindex! at ./multidimensional.jl:777 [inlined]
   [25] setindex! at ./abstractarray.jl:1073 [inlined]
   [26] __cat(::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{Int64,Int64}, ::Tuple{Bool}, ::CuArrays.CuArray{Float64,2,Nothing}, ::Vararg{CuArrays.CuArray{Float64,2,Nothing},N} where N) at ./abstractarray.jl:1463
   [27] cat(::KnetArray{Float64,2}, ::Vararg{KnetArray{Float64,2},N} where N; dims::Val{1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
   [28] forw(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Param{KnetArray{Float64,2}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Val{1},Tuple{Symbol},NamedTuple{(:dims,),Tuple{Val{1}}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [29] #cat#199 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:30 [inlined]
   [30] vcat at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:118 [inlined]
   [31] (::var"#76#96"{Param{KnetArray{Float64,2}},Param{KnetArray{Float64,2}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [32] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [33] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [34] (::AutoGrad.var"#234#239"{Tuple{},var"#76#96"{Param{KnetArray{Float64,2}},Param{KnetArray{Float64,2}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [35] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [36] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [37] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [38] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [39] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:38
   [40] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [41] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:40
  Test threw exception
  Expression: cat(a0, b0, dims = i) == cat(a1, b1, dims = i)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
   [24] _setindex! at ./multidimensional.jl:777 [inlined]
   [25] setindex! at ./abstractarray.jl:1073 [inlined]
   [26] __cat(::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{Int64,Int64}, ::Tuple{Bool}, ::CuArrays.CuArray{Float64,2,Nothing}, ::Vararg{CuArrays.CuArray{Float64,2,Nothing},N} where N) at ./abstractarray.jl:1463
   [27] cat(::KnetArray{Float64,2}, ::Vararg{KnetArray{Float64,2},N} where N; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:40
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...

Stacktrace:
 [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
 [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
 [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
 [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
 [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
 [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
 [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
 [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [18] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [22] gpu_call(::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
 [23] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
 [24] _setindex! at ./multidimensional.jl:777 [inlined]
 [25] setindex! at ./abstractarray.jl:1073 [inlined]
 [26] __cat(::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{Int64,Int64}, ::Tuple{Bool}, ::CuArrays.CuArray{Float64,2,Nothing}, ::Vararg{CuArrays.CuArray{Float64,2,Nothing},N} where N) at ./abstractarray.jl:1463
 [27] cat(::KnetArray{Float64,2}, ::Vararg{KnetArray{Float64,2},N} where N; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
 [28] forw(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Param{KnetArray{Float64,2}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [29] #cat#199 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:30 [inlined]
 [30] (::var"#78#98"{Param{KnetArray{Float64,2}},Param{KnetArray{Float64,2}},Int64})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [31] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [32] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [33] (::AutoGrad.var"#234#239"{Tuple{},var"#78#98"{Param{KnetArray{Float64,2}},Param{KnetArray{Float64,2}},Int64},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [34] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [35] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [36] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [37] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [38] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42
 [39] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [40] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
 [41] include(::String) at ./client.jl:441
 [42] macro expansion at ./util.jl:175 [inlined]
 [43] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:14 [inlined]
 [44] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [45] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [46] include(::String) at ./client.jl:441
 [47] top-level scope at none:6
 [48] eval(::Module, ::Any) at ./boot.jl:331
 [49] exec_options(::Base.JLOptions) at ./client.jl:264
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42 =# @gcheck cat(a3, b3, dims = i)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  caused by [exception 1]
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
   [24] _setindex! at ./multidimensional.jl:777 [inlined]
   [25] setindex! at ./abstractarray.jl:1073 [inlined]
   [26] __cat(::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{Int64,Int64}, ::Tuple{Bool}, ::CuArrays.CuArray{Float64,2,Nothing}, ::Vararg{CuArrays.CuArray{Float64,2,Nothing},N} where N) at ./abstractarray.jl:1463
   [27] cat(::KnetArray{Float64,2}, ::Vararg{KnetArray{Float64,2},N} where N; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
   [28] forw(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Param{KnetArray{Float64,2}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [29] #cat#199 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:30 [inlined]
   [30] (::var"#78#98"{Param{KnetArray{Float64,2}},Param{KnetArray{Float64,2}},Int64})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [31] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [32] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [33] (::AutoGrad.var"#234#239"{Tuple{},var"#78#98"{Param{KnetArray{Float64,2}},Param{KnetArray{Float64,2}},Int64},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [34] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [35] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [36] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [37] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [38] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42
   [39] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [40] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:40
  Test threw exception
  Expression: cat(a0, b0, dims = i) == cat(a1, b1, dims = i)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
   [24] _setindex! at ./multidimensional.jl:777 [inlined]
   [25] setindex! at ./abstractarray.jl:1073 [inlined]
   [26] __cat(::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{Int64,Int64}, ::Tuple{Bool,Bool}, ::CuArrays.CuArray{Float64,2,Nothing}, ::Vararg{CuArrays.CuArray{Float64,2,Nothing},N} where N) at ./abstractarray.jl:1463
   [27] cat(::KnetArray{Float64,2}, ::Vararg{KnetArray{Float64,2},N} where N; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:40
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...

Stacktrace:
 [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
 [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
 [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
 [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
 [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
 [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
 [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
 [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [18] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [22] gpu_call(::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
 [23] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
 [24] _setindex! at ./multidimensional.jl:777 [inlined]
 [25] setindex! at ./abstractarray.jl:1073 [inlined]
 [26] __cat(::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{Int64,Int64}, ::Tuple{Bool,Bool}, ::CuArrays.CuArray{Float64,2,Nothing}, ::Vararg{CuArrays.CuArray{Float64,2,Nothing},N} where N) at ./abstractarray.jl:1463
 [27] cat(::KnetArray{Float64,2}, ::Vararg{KnetArray{Float64,2},N} where N; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
 [28] forw(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Param{KnetArray{Float64,2}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [29] #cat#199 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:30 [inlined]
 [30] (::var"#78#98"{Param{KnetArray{Float64,2}},Param{KnetArray{Float64,2}},Int64})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [31] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [32] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [33] (::AutoGrad.var"#234#239"{Tuple{},var"#78#98"{Param{KnetArray{Float64,2}},Param{KnetArray{Float64,2}},Int64},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [34] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [35] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [36] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [37] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [38] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42
 [39] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [40] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
 [41] include(::String) at ./client.jl:441
 [42] macro expansion at ./util.jl:175 [inlined]
 [43] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:14 [inlined]
 [44] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [45] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [46] include(::String) at ./client.jl:441
 [47] top-level scope at none:6
 [48] eval(::Module, ::Any) at ./boot.jl:331
 [49] exec_options(::Base.JLOptions) at ./client.jl:264
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42 =# @gcheck cat(a3, b3, dims = i)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  caused by [exception 1]
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{CuArrays.CuArray{Float64,2,Nothing},CuArrays.CuArray{Float64,2,Nothing},Tuple{Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
   [24] _setindex! at ./multidimensional.jl:777 [inlined]
   [25] setindex! at ./abstractarray.jl:1073 [inlined]
   [26] __cat(::CuArrays.CuArray{Float64,2,Nothing}, ::Tuple{Int64,Int64}, ::Tuple{Bool,Bool}, ::CuArrays.CuArray{Float64,2,Nothing}, ::Vararg{CuArrays.CuArray{Float64,2,Nothing},N} where N) at ./abstractarray.jl:1463
   [27] cat(::KnetArray{Float64,2}, ::Vararg{KnetArray{Float64,2},N} where N; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
   [28] forw(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Param{KnetArray{Float64,2}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [29] #cat#199 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:30 [inlined]
   [30] (::var"#78#98"{Param{KnetArray{Float64,2}},Param{KnetArray{Float64,2}},Int64})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [31] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [32] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [33] (::AutoGrad.var"#234#239"{Tuple{},var"#78#98"{Param{KnetArray{Float64,2}},Param{KnetArray{Float64,2}},Int64},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [34] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [35] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [36] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [37] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [38] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42
   [39] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [40] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:44
  Test threw exception
  Expression: setindex!(a0, b0[idx...], idx...) == setindex!(a1, b1[idx...], idx...)
  UndefVarError: lib not defined
  Stacktrace:
   [1] getindex2(::KnetArray{Float64,2}, ::UnitRange{Int64}, ::UnitRange{Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/karray.jl:984
   [2] getindex(::KnetArray{Float64,2}, ::UnitRange{Int64}, ::UnitRange{Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/karray.jl:938
   [3] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:44
   [4] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
cuarray: Test Failed at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:52
  Expression: argmax(a0, dims = i) == argmax(a1, dims = i)
   Evaluated: CartesianIndex{2}[CartesianIndex(2, 1) CartesianIndex(8, 2) … CartesianIndex(1, 7) CartesianIndex(2, 8)] == CartesianIndex{2}[CartesianIndex(2, 1) CartesianIndex(8, 2) … CartesianIndex(1, 7) CartesianIndex(2, 8)]
Stacktrace:
 [1] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:52
 [2] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [3] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
cuarray: Test Failed at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:54
  Expression: findmax(a0, dims = i) == findmax(a1, dims = i)
   Evaluated: ([0.7840834548932483 0.8122368931472519 … 0.9137799467019243 0.9871517086591859], CartesianIndex{2}[CartesianIndex(2, 1) CartesianIndex(8, 2) … CartesianIndex(1, 7) CartesianIndex(2, 8)]) == ([0.7840834548932483 0.8122368931472519 … 0.9137799467019243 0.9871517086591859], CartesianIndex{2}[CartesianIndex(2, 1) CartesianIndex(8, 2) … CartesianIndex(1, 7) CartesianIndex(2, 8)])
Stacktrace:
 [1] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:54
 [2] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [3] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
cuarray: Test Failed at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:55
  Expression: findmin(a0, dims = i) == findmin(a1, dims = i)
   Evaluated: ([0.06028766880302161 0.0049972483350402275 … 0.08361295338293484 0.005069543577594482], CartesianIndex{2}[CartesianIndex(8, 1) CartesianIndex(1, 2) … CartesianIndex(4, 7) CartesianIndex(3, 8)]) == ([0.06028766880302161 0.0049972483350402275 … 0.08361295338293484 0.005069543577594482], CartesianIndex{2}[CartesianIndex(8, 1) CartesianIndex(1, 2) … CartesianIndex(4, 7) CartesianIndex(3, 8)])
Stacktrace:
 [1] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:55
 [2] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [3] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
cuarray: Test Failed at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:52
  Expression: argmax(a0, dims = i) == argmax(a1, dims = i)
   Evaluated: CartesianIndex{2}[CartesianIndex(1, 4); CartesianIndex(2, 8); … ; CartesianIndex(7, 8); CartesianIndex(8, 2)] == CartesianIndex{2}[CartesianIndex(1, 4); CartesianIndex(2, 8); … ; CartesianIndex(7, 8); CartesianIndex(8, 2)]
Stacktrace:
 [1] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:52
 [2] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [3] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
cuarray: Test Failed at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:53
  Expression: argmin(a0, dims = i) == argmin(a1, dims = i)
   Evaluated: CartesianIndex{2}[CartesianIndex(1, 2); CartesianIndex(2, 3); … ; CartesianIndex(7, 2); CartesianIndex(8, 1)] == CartesianIndex{2}[CartesianIndex(1, 2); CartesianIndex(2, 7); … ; CartesianIndex(7, 2); CartesianIndex(8, 1)]
Stacktrace:
 [1] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:53
 [2] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [3] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
cuarray: Test Failed at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:54
  Expression: findmax(a0, dims = i) == findmax(a1, dims = i)
   Evaluated: ([0.9380448589747328; 0.9871517086591859; … ; 0.9319483912815572; 0.8122368931472519], CartesianIndex{2}[CartesianIndex(1, 4); CartesianIndex(2, 8); … ; CartesianIndex(7, 8); CartesianIndex(8, 2)]) == ([0.9380448589747328; 0.9871517086591859; … ; 0.9319483912815572; 0.8122368931472519], CartesianIndex{2}[CartesianIndex(1, 4); CartesianIndex(2, 8); … ; CartesianIndex(7, 8); CartesianIndex(8, 2)])
Stacktrace:
 [1] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:54
 [2] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [3] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
cuarray: Test Failed at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:55
  Expression: findmin(a0, dims = i) == findmin(a1, dims = i)
   Evaluated: ([0.0049972483350402275; 0.04616972682681131; … ; 0.11637681019798762; 0.06028766880302161], CartesianIndex{2}[CartesianIndex(1, 2); CartesianIndex(2, 3); … ; CartesianIndex(7, 2); CartesianIndex(8, 1)]) == ([0.0049972483350402275; 0.11276148256160612; … ; 0.11637681019798762; 0.06028766880302161], CartesianIndex{2}[CartesianIndex(1, 2); CartesianIndex(2, 7); … ; CartesianIndex(7, 2); CartesianIndex(8, 1)])
Stacktrace:
 [1] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:55
 [2] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [3] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:11
  Test threw exception
  Expression: getindex(a0, idx...) == getindex(a1, idx...)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::typeof(GPUArrays.index_kernel), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] _unsafe_getindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:96 [inlined]
   [25] getindex(::KnetArray{Float64,3}, ::UnitRange{Int64}, ::UnitRange{Int64}, ::UnitRange{Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:28
   [26] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:11
   [27] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...

Stacktrace:
 [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
 [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
 [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
 [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
 [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
 [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
 [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
 [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [18] cufunction(::typeof(GPUArrays.index_kernel), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [22] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
 [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
 [24] _unsafe_getindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:96 [inlined]
 [25] getindex(::KnetArray{Float64,3}, ::UnitRange{Int64}, ::UnitRange{Int64}, ::UnitRange{Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:28
 [26] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [27] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [28] getindex at ./none:0 [inlined]
 [29] (::var"#62#82"{Param{KnetArray{Float64,3}},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [30] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [31] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [32] (::AutoGrad.var"#234#239"{Tuple{},var"#62#82"{Param{KnetArray{Float64,3}},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [33] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [34] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [35] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [36] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:13
 [38] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [39] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
 [40] include(::String) at ./client.jl:441
 [41] macro expansion at ./util.jl:175 [inlined]
 [42] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:14 [inlined]
 [43] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [44] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [45] include(::String) at ./client.jl:441
 [46] top-level scope at none:6
 [47] eval(::Module, ::Any) at ./boot.jl:331
 [48] exec_options(::Base.JLOptions) at ./client.jl:264
 [49] _start() at ./client.jl:490
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:13
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:13 =# @gcheck getindex(a3, idx...)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:13
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  caused by [exception 1]
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::typeof(GPUArrays.index_kernel), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] _unsafe_getindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:96 [inlined]
   [25] getindex(::KnetArray{Float64,3}, ::UnitRange{Int64}, ::UnitRange{Int64}, ::UnitRange{Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:28
   [26] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [27] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [28] getindex at ./none:0 [inlined]
   [29] (::var"#62#82"{Param{KnetArray{Float64,3}},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [30] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [31] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [32] (::AutoGrad.var"#234#239"{Tuple{},var"#62#82"{Param{KnetArray{Float64,3}},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [33] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [34] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [35] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [36] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [37] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:13
   [38] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [39] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:29
  Test threw exception
  Expression: permutedims(a0, (1, 3, 2)) == permutedims(a1, (1, 3, 2))
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{Int64,Int64,Int64}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [25] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Tuple{Int64,Int64,Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [26] permutedims(::KnetArray{Float64,3}, ::Tuple{Int64,Int64,Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [27] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:29
   [28] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [29] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...

Stacktrace:
 [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
 [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
 [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
 [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
 [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
 [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
 [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
 [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [18] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [22] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
 [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
 [24] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{Int64,Int64,Int64}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [25] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Tuple{Int64,Int64,Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [26] permutedims(::KnetArray{Float64,3}, ::Tuple{Int64,Int64,Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [27] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [28] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [29] permutedims at ./none:0 [inlined]
 [30] (::var"#72#92"{Param{KnetArray{Float64,3}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [31] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [32] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [33] (::AutoGrad.var"#234#239"{Tuple{},var"#72#92"{Param{KnetArray{Float64,3}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [34] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [35] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [36] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [37] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [38] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:31
 [39] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [40] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
 [41] include(::String) at ./client.jl:441
 [42] macro expansion at ./util.jl:175 [inlined]
 [43] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:14 [inlined]
 [44] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [45] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [46] include(::String) at ./client.jl:441
 [47] top-level scope at none:6
 [48] eval(::Module, ::Any) at ./boot.jl:331
 [49] exec_options(::Base.JLOptions) at ./client.jl:264
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:31
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:31 =# @gcheck permutedims(a3, (1, 3, 2))
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:31
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  caused by [exception 1]
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{Int64,Int64,Int64}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [25] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Tuple{Int64,Int64,Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [26] permutedims(::KnetArray{Float64,3}, ::Tuple{Int64,Int64,Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [27] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [28] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [29] permutedims at ./none:0 [inlined]
   [30] (::var"#72#92"{Param{KnetArray{Float64,3}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [31] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [32] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [33] (::AutoGrad.var"#234#239"{Tuple{},var"#72#92"{Param{KnetArray{Float64,3}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [34] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [35] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [36] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [37] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [38] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:31
   [39] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [40] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:33
  Test threw exception
  Expression: hcat(a0, b0) == hcat(a1, b1)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
   [24] _setindex! at ./multidimensional.jl:777 [inlined]
   [25] setindex! at ./abstractarray.jl:1073 [inlined]
   [26] __cat(::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{Int64,Int64,Int64}, ::Tuple{Bool,Bool}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Vararg{CuArrays.CuArray{Float64,3,Nothing},N} where N) at ./abstractarray.jl:1463
   [27] cat(::KnetArray{Float64,3}, ::Vararg{KnetArray{Float64,3},N} where N; dims::Val{2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
   [28] hcat(::KnetArray{Float64,3}, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:59
   [29] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:33
   [30] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [31] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:34
  Test threw exception
  Expression: vcat(a0, b0) == vcat(a1, b1)
  UndefVarError: lib not defined
  Stacktrace:
   [1] setindex2!(::KnetArray{Float64,2}, ::KnetArray{Float64,2}, ::UnitRange{Int64}, ::Colon) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/karray.jl:1024
   [2] setindex! at /home/pkgeval/.julia/packages/Knet/vxHRi/src/karray.jl:949 [inlined]
   [3] vcat(::KnetArray{Float64,2}, ::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/karray.jl:268
   [4] vcat(::KnetArray{Float64,3}, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/karray.jl:283
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:34
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...

Stacktrace:
 [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
 [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
 [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
 [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
 [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
 [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
 [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
 [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [18] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [22] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
 [23] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
 [24] _setindex! at ./multidimensional.jl:777 [inlined]
 [25] setindex! at ./abstractarray.jl:1073 [inlined]
 [26] __cat(::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{Int64,Int64,Int64}, ::Tuple{Bool,Bool}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Vararg{CuArrays.CuArray{Float64,3,Nothing},N} where N) at ./abstractarray.jl:1463
 [27] cat(::KnetArray{Float64,3}, ::Vararg{KnetArray{Float64,3},N} where N; dims::Val{2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
 [28] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Param{KnetArray{Float64,3}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Val{2},Tuple{Symbol},NamedTuple{(:dims,),Tuple{Val{2}}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [29] #cat#199 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:30 [inlined]
 [30] hcat at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:119 [inlined]
 [31] (::var"#75#95"{Param{KnetArray{Float64,3}},Param{KnetArray{Float64,3}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [32] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [33] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [34] (::AutoGrad.var"#234#239"{Tuple{},var"#75#95"{Param{KnetArray{Float64,3}},Param{KnetArray{Float64,3}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [35] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [36] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [37] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [38] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [39] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:37
 [40] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [41] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
 [42] include(::String) at ./client.jl:441
 [43] macro expansion at ./util.jl:175 [inlined]
 [44] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:14 [inlined]
 [45] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [46] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [47] include(::String) at ./client.jl:441
 [48] top-level scope at none:6
 [49] eval(::Module, ::Any) at ./boot.jl:331
 [50] exec_options(::Base.JLOptions) at ./client.jl:264
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:37
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:37 =# @gcheck hcat(a3, b3)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:37
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  caused by [exception 1]
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
   [24] _setindex! at ./multidimensional.jl:777 [inlined]
   [25] setindex! at ./abstractarray.jl:1073 [inlined]
   [26] __cat(::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{Int64,Int64,Int64}, ::Tuple{Bool,Bool}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Vararg{CuArrays.CuArray{Float64,3,Nothing},N} where N) at ./abstractarray.jl:1463
   [27] cat(::KnetArray{Float64,3}, ::Vararg{KnetArray{Float64,3},N} where N; dims::Val{2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
   [28] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Param{KnetArray{Float64,3}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Val{2},Tuple{Symbol},NamedTuple{(:dims,),Tuple{Val{2}}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [29] #cat#199 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:30 [inlined]
   [30] hcat at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:119 [inlined]
   [31] (::var"#75#95"{Param{KnetArray{Float64,3}},Param{KnetArray{Float64,3}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [32] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [33] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [34] (::AutoGrad.var"#234#239"{Tuple{},var"#75#95"{Param{KnetArray{Float64,3}},Param{KnetArray{Float64,3}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [35] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [36] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [37] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [38] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [39] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:37
   [40] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [41] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...

Stacktrace:
 [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
 [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
 [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
 [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
 [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
 [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
 [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
 [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [18] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [22] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
 [23] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
 [24] _setindex! at ./multidimensional.jl:777 [inlined]
 [25] setindex! at ./abstractarray.jl:1073 [inlined]
 [26] __cat(::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{Int64,Int64,Int64}, ::Tuple{Bool}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Vararg{CuArrays.CuArray{Float64,3,Nothing},N} where N) at ./abstractarray.jl:1463
 [27] cat(::KnetArray{Float64,3}, ::Vararg{KnetArray{Float64,3},N} where N; dims::Val{1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
 [28] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Param{KnetArray{Float64,3}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Val{1},Tuple{Symbol},NamedTuple{(:dims,),Tuple{Val{1}}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [29] #cat#199 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:30 [inlined]
 [30] vcat at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:118 [inlined]
 [31] (::var"#76#96"{Param{KnetArray{Float64,3}},Param{KnetArray{Float64,3}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [32] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [33] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [34] (::AutoGrad.var"#234#239"{Tuple{},var"#76#96"{Param{KnetArray{Float64,3}},Param{KnetArray{Float64,3}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [35] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [36] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [37] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [38] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [39] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:38
 [40] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [41] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
 [42] include(::String) at ./client.jl:441
 [43] macro expansion at ./util.jl:175 [inlined]
 [44] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:14 [inlined]
 [45] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [46] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [47] include(::String) at ./client.jl:441
 [48] top-level scope at none:6
 [49] eval(::Module, ::Any) at ./boot.jl:331
 [50] exec_options(::Base.JLOptions) at ./client.jl:264
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:38
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:38 =# @gcheck vcat(a3, b3)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:38
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  caused by [exception 1]
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
   [24] _setindex! at ./multidimensional.jl:777 [inlined]
   [25] setindex! at ./abstractarray.jl:1073 [inlined]
   [26] __cat(::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{Int64,Int64,Int64}, ::Tuple{Bool}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Vararg{CuArrays.CuArray{Float64,3,Nothing},N} where N) at ./abstractarray.jl:1463
   [27] cat(::KnetArray{Float64,3}, ::Vararg{KnetArray{Float64,3},N} where N; dims::Val{1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
   [28] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Param{KnetArray{Float64,3}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Val{1},Tuple{Symbol},NamedTuple{(:dims,),Tuple{Val{1}}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [29] #cat#199 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:30 [inlined]
   [30] vcat at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:118 [inlined]
   [31] (::var"#76#96"{Param{KnetArray{Float64,3}},Param{KnetArray{Float64,3}}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [32] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [33] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [34] (::AutoGrad.var"#234#239"{Tuple{},var"#76#96"{Param{KnetArray{Float64,3}},Param{KnetArray{Float64,3}}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [35] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [36] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [37] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [38] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [39] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:38
   [40] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [41] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:40
  Test threw exception
  Expression: cat(a0, b0, dims = i) == cat(a1, b1, dims = i)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
   [24] _setindex! at ./multidimensional.jl:777 [inlined]
   [25] setindex! at ./abstractarray.jl:1073 [inlined]
   [26] __cat(::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{Int64,Int64,Int64}, ::Tuple{Bool}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Vararg{CuArrays.CuArray{Float64,3,Nothing},N} where N) at ./abstractarray.jl:1463
   [27] cat(::KnetArray{Float64,3}, ::Vararg{KnetArray{Float64,3},N} where N; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:40
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...

Stacktrace:
 [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
 [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
 [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
 [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
 [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
 [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
 [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
 [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [18] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [22] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
 [23] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
 [24] _setindex! at ./multidimensional.jl:777 [inlined]
 [25] setindex! at ./abstractarray.jl:1073 [inlined]
 [26] __cat(::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{Int64,Int64,Int64}, ::Tuple{Bool}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Vararg{CuArrays.CuArray{Float64,3,Nothing},N} where N) at ./abstractarray.jl:1463
 [27] cat(::KnetArray{Float64,3}, ::Vararg{KnetArray{Float64,3},N} where N; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
 [28] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Param{KnetArray{Float64,3}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [29] #cat#199 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:30 [inlined]
 [30] (::var"#78#98"{Param{KnetArray{Float64,3}},Param{KnetArray{Float64,3}},Int64})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [31] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [32] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [33] (::AutoGrad.var"#234#239"{Tuple{},var"#78#98"{Param{KnetArray{Float64,3}},Param{KnetArray{Float64,3}},Int64},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [34] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [35] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [36] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [37] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [38] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42
 [39] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [40] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
 [41] include(::String) at ./client.jl:441
 [42] macro expansion at ./util.jl:175 [inlined]
 [43] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:14 [inlined]
 [44] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [45] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [46] include(::String) at ./client.jl:441
 [47] top-level scope at none:6
 [48] eval(::Module, ::Any) at ./boot.jl:331
 [49] exec_options(::Base.JLOptions) at ./client.jl:264
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42 =# @gcheck cat(a3, b3, dims = i)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  caused by [exception 1]
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
   [24] _setindex! at ./multidimensional.jl:777 [inlined]
   [25] setindex! at ./abstractarray.jl:1073 [inlined]
   [26] __cat(::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{Int64,Int64,Int64}, ::Tuple{Bool}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Vararg{CuArrays.CuArray{Float64,3,Nothing},N} where N) at ./abstractarray.jl:1463
   [27] cat(::KnetArray{Float64,3}, ::Vararg{KnetArray{Float64,3},N} where N; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
   [28] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Param{KnetArray{Float64,3}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [29] #cat#199 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:30 [inlined]
   [30] (::var"#78#98"{Param{KnetArray{Float64,3}},Param{KnetArray{Float64,3}},Int64})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [31] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [32] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [33] (::AutoGrad.var"#234#239"{Tuple{},var"#78#98"{Param{KnetArray{Float64,3}},Param{KnetArray{Float64,3}},Int64},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [34] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [35] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [36] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [37] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [38] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42
   [39] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [40] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:40
  Test threw exception
  Expression: cat(a0, b0, dims = i) == cat(a1, b1, dims = i)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
   [24] _setindex! at ./multidimensional.jl:777 [inlined]
   [25] setindex! at ./abstractarray.jl:1073 [inlined]
   [26] __cat(::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{Int64,Int64,Int64}, ::Tuple{Bool,Bool}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Vararg{CuArrays.CuArray{Float64,3,Nothing},N} where N) at ./abstractarray.jl:1463
   [27] cat(::KnetArray{Float64,3}, ::Vararg{KnetArray{Float64,3},N} where N; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:40
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...

Stacktrace:
 [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
 [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
 [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
 [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
 [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
 [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
 [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
 [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [18] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [22] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
 [23] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
 [24] _setindex! at ./multidimensional.jl:777 [inlined]
 [25] setindex! at ./abstractarray.jl:1073 [inlined]
 [26] __cat(::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{Int64,Int64,Int64}, ::Tuple{Bool,Bool}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Vararg{CuArrays.CuArray{Float64,3,Nothing},N} where N) at ./abstractarray.jl:1463
 [27] cat(::KnetArray{Float64,3}, ::Vararg{KnetArray{Float64,3},N} where N; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
 [28] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Param{KnetArray{Float64,3}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [29] #cat#199 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:30 [inlined]
 [30] (::var"#78#98"{Param{KnetArray{Float64,3}},Param{KnetArray{Float64,3}},Int64})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [31] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [32] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [33] (::AutoGrad.var"#234#239"{Tuple{},var"#78#98"{Param{KnetArray{Float64,3}},Param{KnetArray{Float64,3}},Int64},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [34] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [35] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [36] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [37] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [38] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42
 [39] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [40] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
 [41] include(::String) at ./client.jl:441
 [42] macro expansion at ./util.jl:175 [inlined]
 [43] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:14 [inlined]
 [44] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [45] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [46] include(::String) at ./client.jl:441
 [47] top-level scope at none:6
 [48] eval(::Module, ::Any) at ./boot.jl:331
 [49] exec_options(::Base.JLOptions) at ./client.jl:264
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42 =# @gcheck cat(a3, b3, dims = i)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  caused by [exception 1]
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
   [24] _setindex! at ./multidimensional.jl:777 [inlined]
   [25] setindex! at ./abstractarray.jl:1073 [inlined]
   [26] __cat(::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{Int64,Int64,Int64}, ::Tuple{Bool,Bool}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Vararg{CuArrays.CuArray{Float64,3,Nothing},N} where N) at ./abstractarray.jl:1463
   [27] cat(::KnetArray{Float64,3}, ::Vararg{KnetArray{Float64,3},N} where N; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
   [28] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Param{KnetArray{Float64,3}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [29] #cat#199 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:30 [inlined]
   [30] (::var"#78#98"{Param{KnetArray{Float64,3}},Param{KnetArray{Float64,3}},Int64})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [31] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [32] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [33] (::AutoGrad.var"#234#239"{Tuple{},var"#78#98"{Param{KnetArray{Float64,3}},Param{KnetArray{Float64,3}},Int64},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [34] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [35] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [36] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [37] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [38] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42
   [39] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [40] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:40
  Test threw exception
  Expression: cat(a0, b0, dims = i) == cat(a1, b1, dims = i)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
   [24] _setindex! at ./multidimensional.jl:777 [inlined]
   [25] setindex! at ./abstractarray.jl:1073 [inlined]
   [26] __cat(::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{Int64,Int64,Int64}, ::Tuple{Bool,Bool,Bool}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Vararg{CuArrays.CuArray{Float64,3,Nothing},N} where N) at ./abstractarray.jl:1463
   [27] cat(::KnetArray{Float64,3}, ::Vararg{KnetArray{Float64,3},N} where N; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:40
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...

Stacktrace:
 [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
 [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
 [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
 [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
 [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
 [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
 [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
 [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [18] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [22] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
 [23] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
 [24] _setindex! at ./multidimensional.jl:777 [inlined]
 [25] setindex! at ./abstractarray.jl:1073 [inlined]
 [26] __cat(::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{Int64,Int64,Int64}, ::Tuple{Bool,Bool,Bool}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Vararg{CuArrays.CuArray{Float64,3,Nothing},N} where N) at ./abstractarray.jl:1463
 [27] cat(::KnetArray{Float64,3}, ::Vararg{KnetArray{Float64,3},N} where N; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
 [28] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Param{KnetArray{Float64,3}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [29] #cat#199 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:30 [inlined]
 [30] (::var"#78#98"{Param{KnetArray{Float64,3}},Param{KnetArray{Float64,3}},Int64})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
 [31] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [32] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
 [33] (::AutoGrad.var"#234#239"{Tuple{},var"#78#98"{Param{KnetArray{Float64,3}},Param{KnetArray{Float64,3}},Int64},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [34] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [35] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [36] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [37] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
 [38] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42
 [39] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [40] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
 [41] include(::String) at ./client.jl:441
 [42] macro expansion at ./util.jl:175 [inlined]
 [43] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:14 [inlined]
 [44] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [45] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [46] include(::String) at ./client.jl:441
 [47] top-level scope at none:6
 [48] eval(::Module, ::Any) at ./boot.jl:331
 [49] exec_options(::Base.JLOptions) at ./client.jl:264
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42 =# @gcheck cat(a3, b3, dims = i)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  caused by [exception 1]
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::typeof(GPUArrays.setindex_kernel!), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},Int64}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] _unsafe_setindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:123 [inlined]
   [24] _setindex! at ./multidimensional.jl:777 [inlined]
   [25] setindex! at ./abstractarray.jl:1073 [inlined]
   [26] __cat(::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{Int64,Int64,Int64}, ::Tuple{Bool,Bool,Bool}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Vararg{CuArrays.CuArray{Float64,3,Nothing},N} where N) at ./abstractarray.jl:1463
   [27] cat(::KnetArray{Float64,3}, ::Vararg{KnetArray{Float64,3},N} where N; dims::Int64) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:71
   [28] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Param{KnetArray{Float64,3}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [29] #cat#199 at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/cat.jl:30 [inlined]
   [30] (::var"#78#98"{Param{KnetArray{Float64,3}},Param{KnetArray{Float64,3}},Int64})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:172
   [31] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [32] gcsum at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50 [inlined]
   [33] (::AutoGrad.var"#234#239"{Tuple{},var"#78#98"{Param{KnetArray{Float64,3}},Param{KnetArray{Float64,3}},Int64},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [34] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [35] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [36] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [37] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:158
   [38] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:42
   [39] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [40] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
cuarray: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:44
  Test threw exception
  Expression: setindex!(a0, b0[idx...], idx...) == setindex!(a1, b1[idx...], idx...)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::typeof(GPUArrays.index_kernel), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] _unsafe_getindex! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:96 [inlined]
   [25] getindex(::KnetArray{Float64,3}, ::UnitRange{Int64}, ::UnitRange{Int64}, ::UnitRange{Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:28
   [26] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:44
   [27] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
  
cuarray: Test Failed at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:52
  Expression: argmax(a0, dims = i) == argmax(a1, dims = i)
   Evaluated: CartesianIndex{3}[CartesianIndex(5, 1, 1) CartesianIndex(3, 2, 1) … CartesianIndex(8, 7, 1) CartesianIndex(7, 8, 1)]

CartesianIndex{3}[CartesianIndex(4, 1, 2) CartesianIndex(6, 2, 2) … CartesianIndex(7, 7, 2) CartesianIndex(1, 8, 2)]

CartesianIndex{3}[CartesianIndex(2, 1, 3) CartesianIndex(6, 2, 3) … CartesianIndex(3, 7, 3) CartesianIndex(3, 8, 3)]

CartesianIndex{3}[CartesianIndex(4, 1, 4) CartesianIndex(1, 2, 4) … CartesianIndex(7, 7, 4) CartesianIndex(2, 8, 4)]

CartesianIndex{3}[CartesianIndex(3, 1, 5) CartesianIndex(5, 2, 5) … CartesianIndex(1, 7, 5) CartesianIndex(8, 8, 5)]

CartesianIndex{3}[CartesianIndex(2, 1, 6) CartesianIndex(2, 2, 6) … CartesianIndex(8, 7, 6) CartesianIndex(7, 8, 6)]

CartesianIndex{3}[CartesianIndex(4, 1, 7) CartesianIndex(1, 2, 7) … CartesianIndex(5, 7, 7) CartesianIndex(2, 8, 7)]

CartesianIndex{3}[CartesianIndex(8, 1, 8) CartesianIndex(6, 2, 8) … CartesianIndex(7, 7, 8) CartesianIndex(1, 8, 8)] == CartesianIndex{3}[CartesianIndex(5, 1, 1) CartesianIndex(3, 2, 1) … CartesianIndex(8, 7, 1) CartesianIndex(7, 8, 1)]

CartesianIndex{3}[CartesianIndex(4, 1, 2) CartesianIndex(2, 2, 2) … CartesianIndex(7, 7, 2) CartesianIndex(1, 8, 2)]

CartesianIndex{3}[CartesianIndex(2, 1, 3) CartesianIndex(6, 2, 3) … CartesianIndex(3, 7, 3) CartesianIndex(3, 8, 3)]

CartesianIndex{3}[CartesianIndex(4, 1, 4) CartesianIndex(1, 2, 4) … CartesianIndex(7, 7, 4) CartesianIndex(2, 8, 4)]

CartesianIndex{3}[CartesianIndex(3, 1, 5) CartesianIndex(5, 2, 5) … CartesianIndex(1, 7, 5) CartesianIndex(8, 8, 5)]

CartesianIndex{3}[CartesianIndex(2, 1, 6) CartesianIndex(2, 2, 6) … CartesianIndex(8, 7, 6) CartesianIndex(7, 8, 6)]

CartesianIndex{3}[CartesianIndex(4, 1, 7) CartesianIndex(1, 2, 7) … CartesianIndex(5, 7, 7) CartesianIndex(2, 8, 7)]

CartesianIndex{3}[CartesianIndex(8, 1, 8) CartesianIndex(6, 2, 8) … CartesianIndex(7, 7, 8) CartesianIndex(1, 8, 8)]
Stacktrace:
 [1] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:52
 [2] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [3] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
cuarray: Test Failed at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:53
  Expression: argmin(a0, dims = i) == argmin(a1, dims = i)
   Evaluated: CartesianIndex{3}[CartesianIndex(2, 1, 1) CartesianIndex(6, 2, 1) … CartesianIndex(5, 7, 1) CartesianIndex(2, 8, 1)]

CartesianIndex{3}[CartesianIndex(5, 1, 2) CartesianIndex(1, 2, 2) … CartesianIndex(8, 7, 2) CartesianIndex(5, 8, 2)]

CartesianIndex{3}[CartesianIndex(7, 1, 3) CartesianIndex(1, 2, 3) … CartesianIndex(1, 7, 3) CartesianIndex(8, 8, 3)]

CartesianIndex{3}[CartesianIndex(3, 1, 4) CartesianIndex(7, 2, 4) … CartesianIndex(1, 7, 4) CartesianIndex(7, 8, 4)]

CartesianIndex{3}[CartesianIndex(2, 1, 5) CartesianIndex(8, 2, 5) … CartesianIndex(2, 7, 5) CartesianIndex(5, 8, 5)]

CartesianIndex{3}[CartesianIndex(6, 1, 6) CartesianIndex(3, 2, 6) … CartesianIndex(6, 7, 6) CartesianIndex(3, 8, 6)]

CartesianIndex{3}[CartesianIndex(6, 1, 7) CartesianIndex(5, 2, 7) … CartesianIndex(7, 7, 7) CartesianIndex(5, 8, 7)]

CartesianIndex{3}[CartesianIndex(5, 1, 8) CartesianIndex(3, 2, 8) … CartesianIndex(4, 7, 8) CartesianIndex(2, 8, 8)] == CartesianIndex{3}[CartesianIndex(2, 1, 1) CartesianIndex(6, 2, 1) … CartesianIndex(5, 7, 1) CartesianIndex(2, 8, 1)]

CartesianIndex{3}[CartesianIndex(5, 1, 2) CartesianIndex(1, 2, 2) … CartesianIndex(8, 7, 2) CartesianIndex(5, 8, 2)]

CartesianIndex{3}[CartesianIndex(7, 1, 3) CartesianIndex(1, 2, 3) … CartesianIndex(1, 7, 3) CartesianIndex(8, 8, 3)]

CartesianIndex{3}[CartesianIndex(3, 1, 4) CartesianIndex(7, 2, 4) … CartesianIndex(1, 7, 4) CartesianIndex(7, 8, 4)]

CartesianIndex{3}[CartesianIndex(2, 1, 5) CartesianIndex(8, 2, 5) … CartesianIndex(2, 7, 5) CartesianIndex(5, 8, 5)]

CartesianIndex{3}[CartesianIndex(6, 1, 6) CartesianIndex(3, 2, 6) … CartesianIndex(6, 7, 6) CartesianIndex(3, 8, 6)]

CartesianIndex{3}[CartesianIndex(6, 1, 7) CartesianIndex(5, 2, 7) … CartesianIndex(7, 7, 7) CartesianIndex(5, 8, 7)]

CartesianIndex{3}[CartesianIndex(5, 1, 8) CartesianIndex(3, 2, 8) … CartesianIndex(4, 7, 8) CartesianIndex(2, 8, 8)]
Stacktrace:
 [1] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:53
 [2] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [3] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
cuarray: Test Failed at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:54
  Expression: findmax(a0, dims = i) == findmax(a1, dims = i)
   Evaluated: ([0.5305014129624956 0.9969194321792736 … 0.9985907294952077 0.7670876910352851]

[0.8236826830719026 0.820900203045333 … 0.9665823026527851 0.7333723428645496]

[0.9983877928529141 0.9990511235935093 … 0.9228034489861254 0.8293716647788951]

[0.43463475161928145 0.9054948401760607 … 0.9867426396136794 0.9581728807624663]

[0.9359817821721259 0.9857322286808097 … 0.8696821757671549 0.9342016969208025]

[0.7363301161999574 0.9481576694971006 … 0.9865947885903439 0.9518768731170459]

[0.7858768340283941 0.8896403357801166 … 0.7930396232470585 0.946923828031283]

[0.9490759545813638 0.9796031635173152 … 0.988687814048945 0.9580190209814774], CartesianIndex{3}[CartesianIndex(5, 1, 1) CartesianIndex(3, 2, 1) … CartesianIndex(8, 7, 1) CartesianIndex(7, 8, 1)]

CartesianIndex{3}[CartesianIndex(4, 1, 2) CartesianIndex(6, 2, 2) … CartesianIndex(7, 7, 2) CartesianIndex(1, 8, 2)]

CartesianIndex{3}[CartesianIndex(2, 1, 3) CartesianIndex(6, 2, 3) … CartesianIndex(3, 7, 3) CartesianIndex(3, 8, 3)]

CartesianIndex{3}[CartesianIndex(4, 1, 4) CartesianIndex(1, 2, 4) … CartesianIndex(7, 7, 4) CartesianIndex(2, 8, 4)]

CartesianIndex{3}[CartesianIndex(3, 1, 5) CartesianIndex(5, 2, 5) … CartesianIndex(1, 7, 5) CartesianIndex(8, 8, 5)]

CartesianIndex{3}[CartesianIndex(2, 1, 6) CartesianIndex(2, 2, 6) … CartesianIndex(8, 7, 6) CartesianIndex(7, 8, 6)]

CartesianIndex{3}[CartesianIndex(4, 1, 7) CartesianIndex(1, 2, 7) … CartesianIndex(5, 7, 7) CartesianIndex(2, 8, 7)]

CartesianIndex{3}[CartesianIndex(8, 1, 8) CartesianIndex(6, 2, 8) … CartesianIndex(7, 7, 8) CartesianIndex(1, 8, 8)]) == ([0.5305014129624956 0.9969194321792736 … 0.9985907294952077 0.7670876910352851]

[0.8236826830719026 0.8787849343805092 … 0.9665823026527851 0.7333723428645496]

[0.9983877928529141 0.9990511235935093 … 0.9228034489861254 0.8293716647788951]

[0.43463475161928145 0.9054948401760607 … 0.9867426396136794 0.9581728807624663]

[0.9359817821721259 0.9857322286808097 … 0.8696821757671549 0.9342016969208025]

[0.7363301161999574 0.9481576694971006 … 0.9865947885903439 0.9518768731170459]

[0.7858768340283941 0.8896403357801166 … 0.7930396232470585 0.946923828031283]

[0.9490759545813638 0.9796031635173152 … 0.988687814048945 0.9580190209814774], CartesianIndex{3}[CartesianIndex(5, 1, 1) CartesianIndex(3, 2, 1) … CartesianIndex(8, 7, 1) CartesianIndex(7, 8, 1)]

CartesianIndex{3}[CartesianIndex(4, 1, 2) CartesianIndex(2, 2, 2) … CartesianIndex(7, 7, 2) CartesianIndex(1, 8, 2)]

CartesianIndex{3}[CartesianIndex(2, 1, 3) CartesianIndex(6, 2, 3) … CartesianIndex(3, 7, 3) CartesianIndex(3, 8, 3)]

CartesianIndex{3}[CartesianIndex(4, 1, 4) CartesianIndex(1, 2, 4) … CartesianIndex(7, 7, 4) CartesianIndex(2, 8, 4)]

CartesianIndex{3}[CartesianIndex(3, 1, 5) CartesianIndex(5, 2, 5) … CartesianIndex(1, 7, 5) CartesianIndex(8, 8, 5)]

CartesianIndex{3}[CartesianIndex(2, 1, 6) CartesianIndex(2, 2, 6) … CartesianIndex(8, 7, 6) CartesianIndex(7, 8, 6)]

CartesianIndex{3}[CartesianIndex(4, 1, 7) CartesianIndex(1, 2, 7) … CartesianIndex(5, 7, 7) CartesianIndex(2, 8, 7)]

CartesianIndex{3}[CartesianIndex(8, 1, 8) CartesianIndex(6, 2, 8) … CartesianIndex(7, 7, 8) CartesianIndex(1, 8, 8)])
Stacktrace:
 [1] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:54
 [2] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [3] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
cuarray: Test Failed at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:55
  Expression: findmin(a0, dims = i) == findmin(a1, dims = i)
   Evaluated: ([0.08052841884358886 0.13767854585072836 … 0.025438724570175042 0.04082455595115464]

[0.027594799825710226 0.027288485750718516 … 0.08624435502721162 0.08106764813263401]

[0.15934554114343746 0.09179960405049381 … 0.05151152490095434 0.011722046002706321]

[0.006314729297426824 0.026646165446342573 … 0.08415122830388477 0.14817048516401377]

[0.06324586881250327 0.28768061217137997 … 0.04395095307674701 0.08754393727593968]

[0.060609869337986844 0.01901722339751899 … 0.19500245859008936 0.00142757545191885]

[0.01694086124333083 0.2065059098714097 … 0.025712447796486293 0.059306725077281186]

[0.1754965147115437 0.1509898435424426 … 0.21764184149853438 0.23447545224686017], CartesianIndex{3}[CartesianIndex(2, 1, 1) CartesianIndex(6, 2, 1) … CartesianIndex(5, 7, 1) CartesianIndex(2, 8, 1)]

CartesianIndex{3}[CartesianIndex(5, 1, 2) CartesianIndex(1, 2, 2) … CartesianIndex(8, 7, 2) CartesianIndex(5, 8, 2)]

CartesianIndex{3}[CartesianIndex(7, 1, 3) CartesianIndex(1, 2, 3) … CartesianIndex(1, 7, 3) CartesianIndex(8, 8, 3)]

CartesianIndex{3}[CartesianIndex(3, 1, 4) CartesianIndex(7, 2, 4) … CartesianIndex(1, 7, 4) CartesianIndex(7, 8, 4)]

CartesianIndex{3}[CartesianIndex(2, 1, 5) CartesianIndex(8, 2, 5) … CartesianIndex(2, 7, 5) CartesianIndex(5, 8, 5)]

CartesianIndex{3}[CartesianIndex(6, 1, 6) CartesianIndex(3, 2, 6) … CartesianIndex(6, 7, 6) CartesianIndex(3, 8, 6)]

CartesianIndex{3}[CartesianIndex(6, 1, 7) CartesianIndex(5, 2, 7) … CartesianIndex(7, 7, 7) CartesianIndex(5, 8, 7)]

CartesianIndex{3}[CartesianIndex(5, 1, 8) CartesianIndex(3, 2, 8) … CartesianIndex(4, 7, 8) CartesianIndex(2, 8, 8)]) == ([0.08052841884358886 0.13767854585072836 … 0.025438724570175042 0.04082455595115464]

[0.027594799825710226 0.027288485750718516 … 0.08624435502721162 0.08106764813263401]

[0.15934554114343746 0.09179960405049381 … 0.05151152490095434 0.011722046002706321]

[0.006314729297426824 0.026646165446342573 … 0.08415122830388477 0.14817048516401377]

[0.06324586881250327 0.28768061217137997 … 0.04395095307674701 0.08754393727593968]

[0.060609869337986844 0.01901722339751899 … 0.19500245859008936 0.00142757545191885]

[0.01694086124333083 0.2065059098714097 … 0.025712447796486293 0.059306725077281186]

[0.1754965147115437 0.1509898435424426 … 0.21764184149853438 0.23447545224686017], CartesianIndex{3}[CartesianIndex(2, 1, 1) CartesianIndex(6, 2, 1) … CartesianIndex(5, 7, 1) CartesianIndex(2, 8, 1)]

CartesianIndex{3}[CartesianIndex(5, 1, 2) CartesianIndex(1, 2, 2) … CartesianIndex(8, 7, 2) CartesianIndex(5, 8, 2)]

CartesianIndex{3}[CartesianIndex(7, 1, 3) CartesianIndex(1, 2, 3) … CartesianIndex(1, 7, 3) CartesianIndex(8, 8, 3)]

CartesianIndex{3}[CartesianIndex(3, 1, 4) CartesianIndex(7, 2, 4) … CartesianIndex(1, 7, 4) CartesianIndex(7, 8, 4)]

CartesianIndex{3}[CartesianIndex(2, 1, 5) CartesianIndex(8, 2, 5) … CartesianIndex(2, 7, 5) CartesianIndex(5, 8, 5)]

CartesianIndex{3}[CartesianIndex(6, 1, 6) CartesianIndex(3, 2, 6) … CartesianIndex(6, 7, 6) CartesianIndex(3, 8, 6)]

CartesianIndex{3}[CartesianIndex(6, 1, 7) CartesianIndex(5, 2, 7) … CartesianIndex(7, 7, 7) CartesianIndex(5, 8, 7)]

CartesianIndex{3}[CartesianIndex(5, 1, 8) CartesianIndex(3, 2, 8) … CartesianIndex(4, 7, 8) CartesianIndex(2, 8, 8)])
Stacktrace:
 [1] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:55
 [2] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [3] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
cuarray: Test Failed at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:52
  Expression: argmax(a0, dims = i) == argmax(a1, dims = i)
   Evaluated: CartesianIndex{3}[CartesianIndex(1, 4, 1); CartesianIndex(2, 6, 1); … ; CartesianIndex(7, 4, 1); CartesianIndex(8, 7, 1)]

CartesianIndex{3}[CartesianIndex(1, 6, 2); CartesianIndex(2, 6, 2); … ; CartesianIndex(7, 7, 2); CartesianIndex(8, 6, 2)]

CartesianIndex{3}[CartesianIndex(1, 3, 3); CartesianIndex(2, 1, 3); … ; CartesianIndex(7, 4, 3); CartesianIndex(8, 7, 3)]

CartesianIndex{3}[CartesianIndex(1, 8, 4); CartesianIndex(2, 7, 4); … ; CartesianIndex(7, 7, 4); CartesianIndex(8, 6, 4)]

CartesianIndex{3}[CartesianIndex(1, 6, 5); CartesianIndex(2, 6, 5); … ; CartesianIndex(7, 3, 5); CartesianIndex(8, 8, 5)]

CartesianIndex{3}[CartesianIndex(1, 2, 6); CartesianIndex(2, 2, 6); … ; CartesianIndex(7, 8, 6); CartesianIndex(8, 7, 6)]

CartesianIndex{3}[CartesianIndex(1, 2, 7); CartesianIndex(2, 8, 7); … ; CartesianIndex(7, 4, 7); CartesianIndex(8, 4, 7)]

CartesianIndex{3}[CartesianIndex(1, 3, 8); CartesianIndex(2, 3, 8); … ; CartesianIndex(7, 4, 8); CartesianIndex(8, 1, 8)] == CartesianIndex{3}[CartesianIndex(1, 4, 1); CartesianIndex(2, 6, 1); … ; CartesianIndex(7, 4, 1); CartesianIndex(8, 7, 1)]

CartesianIndex{3}[CartesianIndex(1, 6, 2); CartesianIndex(2, 6, 2); … ; CartesianIndex(7, 7, 2); CartesianIndex(8, 6, 2)]

CartesianIndex{3}[CartesianIndex(1, 3, 3); CartesianIndex(2, 1, 3); … ; CartesianIndex(7, 4, 3); CartesianIndex(8, 7, 3)]

CartesianIndex{3}[CartesianIndex(1, 8, 4); CartesianIndex(2, 7, 4); … ; CartesianIndex(7, 7, 4); CartesianIndex(8, 6, 4)]

CartesianIndex{3}[CartesianIndex(1, 6, 5); CartesianIndex(2, 6, 5); … ; CartesianIndex(7, 3, 5); CartesianIndex(8, 8, 5)]

CartesianIndex{3}[CartesianIndex(1, 2, 6); CartesianIndex(2, 2, 6); … ; CartesianIndex(7, 8, 6); CartesianIndex(8, 7, 6)]

CartesianIndex{3}[CartesianIndex(1, 2, 7); CartesianIndex(2, 8, 7); … ; CartesianIndex(7, 4, 7); CartesianIndex(8, 4, 7)]

CartesianIndex{3}[CartesianIndex(1, 3, 8); CartesianIndex(2, 3, 8); … ; CartesianIndex(7, 4, 8); CartesianIndex(8, 1, 8)]
Stacktrace:
 [1] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:52
 [2] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [3] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
cuarray: Test Failed at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:53
  Expression: argmin(a0, dims = i) == argmin(a1, dims = i)
   Evaluated: CartesianIndex{3}[CartesianIndex(1, 8, 1); CartesianIndex(2, 7, 1); … ; CartesianIndex(7, 1, 1); CartesianIndex(8, 6, 1)]

CartesianIndex{3}[CartesianIndex(1, 2, 2); CartesianIndex(2, 2, 2); … ; CartesianIndex(7, 8, 2); CartesianIndex(8, 7, 2)]

CartesianIndex{3}[CartesianIndex(1, 7, 3); CartesianIndex(2, 4, 3); … ; CartesianIndex(7, 1, 3); CartesianIndex(8, 8, 3)]

CartesianIndex{3}[CartesianIndex(1, 7, 4); CartesianIndex(2, 3, 4); … ; CartesianIndex(7, 2, 4); CartesianIndex(8, 4, 4)]

CartesianIndex{3}[CartesianIndex(1, 3, 5); CartesianIndex(2, 7, 5); … ; CartesianIndex(7, 1, 5); CartesianIndex(8, 2, 5)]

CartesianIndex{3}[CartesianIndex(1, 4, 6); CartesianIndex(2, 8, 6); … ; CartesianIndex(7, 4, 6); CartesianIndex(8, 2, 6)]

CartesianIndex{3}[CartesianIndex(1, 1, 7); CartesianIndex(2, 6, 7); … ; CartesianIndex(7, 7, 7); CartesianIndex(8, 5, 7)]

CartesianIndex{3}[CartesianIndex(1, 6, 8); CartesianIndex(2, 8, 8); … ; CartesianIndex(7, 5, 8); CartesianIndex(8, 3, 8)] == CartesianIndex{3}[CartesianIndex(1, 8, 1); CartesianIndex(2, 7, 1); … ; CartesianIndex(7, 1, 1); CartesianIndex(8, 6, 1)]

CartesianIndex{3}[CartesianIndex(1, 2, 2); CartesianIndex(2, 4, 2); … ; CartesianIndex(7, 8, 2); CartesianIndex(8, 7, 2)]

CartesianIndex{3}[CartesianIndex(1, 7, 3); CartesianIndex(2, 6, 3); … ; CartesianIndex(7, 1, 3); CartesianIndex(8, 8, 3)]

CartesianIndex{3}[CartesianIndex(1, 7, 4); CartesianIndex(2, 4, 4); … ; CartesianIndex(7, 2, 4); CartesianIndex(8, 4, 4)]

CartesianIndex{3}[CartesianIndex(1, 3, 5); CartesianIndex(2, 7, 5); … ; CartesianIndex(7, 1, 5); CartesianIndex(8, 2, 5)]

CartesianIndex{3}[CartesianIndex(1, 4, 6); CartesianIndex(2, 8, 6); … ; CartesianIndex(7, 4, 6); CartesianIndex(8, 2, 6)]

CartesianIndex{3}[CartesianIndex(1, 1, 7); CartesianIndex(2, 6, 7); … ; CartesianIndex(7, 7, 7); CartesianIndex(8, 5, 7)]

CartesianIndex{3}[CartesianIndex(1, 6, 8); CartesianIndex(2, 8, 8); … ; CartesianIndex(7, 5, 8); CartesianIndex(8, 3, 8)]
Stacktrace:
 [1] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:53
 [2] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [3] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
cuarray: Test Failed at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:54
  Expression: findmax(a0, dims = i) == findmax(a1, dims = i)
   Evaluated: ([0.9384605473816359; 0.9436194917596172; … ; 0.9299567474746888; 0.9985907294952077]

[0.7348742958926497; 0.9616170917140132; … ; 0.9665823026527851; 0.8110808054237018]

[0.699273005625588; 0.9983877928529141; … ; 0.918618351206262; 0.8435981410769193]

[0.9282631799828198; 0.9827029819847197; … ; 0.9867426396136794; 0.9040088038942311]

[0.9569338833623366; 0.7645353423507368; … ; 0.9683627034791202; 0.9342016969208025]

[0.7146591847194359; 0.9481576694971006; … ; 0.9518768731170459; 0.9865947885903439]

[0.8896403357801166; 0.946923828031283; … ; 0.8008368193157471; 0.9412011044954753]

[0.9604273665302745; 0.8015521338507714; … ; 0.9992709325427171; 0.9490759545813638], CartesianIndex{3}[CartesianIndex(1, 4, 1); CartesianIndex(2, 6, 1); … ; CartesianIndex(7, 4, 1); CartesianIndex(8, 7, 1)]

CartesianIndex{3}[CartesianIndex(1, 6, 2); CartesianIndex(2, 6, 2); … ; CartesianIndex(7, 7, 2); CartesianIndex(8, 6, 2)]

CartesianIndex{3}[CartesianIndex(1, 3, 3); CartesianIndex(2, 1, 3); … ; CartesianIndex(7, 4, 3); CartesianIndex(8, 7, 3)]

CartesianIndex{3}[CartesianIndex(1, 8, 4); CartesianIndex(2, 7, 4); … ; CartesianIndex(7, 7, 4); CartesianIndex(8, 6, 4)]

CartesianIndex{3}[CartesianIndex(1, 6, 5); CartesianIndex(2, 6, 5); … ; CartesianIndex(7, 3, 5); CartesianIndex(8, 8, 5)]

CartesianIndex{3}[CartesianIndex(1, 2, 6); CartesianIndex(2, 2, 6); … ; CartesianIndex(7, 8, 6); CartesianIndex(8, 7, 6)]

CartesianIndex{3}[CartesianIndex(1, 2, 7); CartesianIndex(2, 8, 7); … ; CartesianIndex(7, 4, 7); CartesianIndex(8, 4, 7)]

CartesianIndex{3}[CartesianIndex(1, 3, 8); CartesianIndex(2, 3, 8); … ; CartesianIndex(7, 4, 8); CartesianIndex(8, 1, 8)]) == ([0.9384605473816359; 0.9436194917596172; … ; 0.9299567474746888; 0.9985907294952077]

[0.7348742958926497; 0.9616170917140132; … ; 0.9665823026527851; 0.8110808054237018]

[0.699273005625588; 0.9983877928529141; … ; 0.918618351206262; 0.8435981410769193]

[0.9282631799828198; 0.9827029819847197; … ; 0.9867426396136794; 0.9040088038942311]

[0.9569338833623366; 0.7645353423507368; … ; 0.9683627034791202; 0.9342016969208025]

[0.7146591847194359; 0.9481576694971006; … ; 0.9518768731170459; 0.9865947885903439]

[0.8896403357801166; 0.946923828031283; … ; 0.8008368193157471; 0.9412011044954753]

[0.9604273665302745; 0.8015521338507714; … ; 0.9992709325427171; 0.9490759545813638], CartesianIndex{3}[CartesianIndex(1, 4, 1); CartesianIndex(2, 6, 1); … ; CartesianIndex(7, 4, 1); CartesianIndex(8, 7, 1)]

CartesianIndex{3}[CartesianIndex(1, 6, 2); CartesianIndex(2, 6, 2); … ; CartesianIndex(7, 7, 2); CartesianIndex(8, 6, 2)]

CartesianIndex{3}[CartesianIndex(1, 3, 3); CartesianIndex(2, 1, 3); … ; CartesianIndex(7, 4, 3); CartesianIndex(8, 7, 3)]

CartesianIndex{3}[CartesianIndex(1, 8, 4); CartesianIndex(2, 7, 4); … ; CartesianIndex(7, 7, 4); CartesianIndex(8, 6, 4)]

CartesianIndex{3}[CartesianIndex(1, 6, 5); CartesianIndex(2, 6, 5); … ; CartesianIndex(7, 3, 5); CartesianIndex(8, 8, 5)]

CartesianIndex{3}[CartesianIndex(1, 2, 6); CartesianIndex(2, 2, 6); … ; CartesianIndex(7, 8, 6); CartesianIndex(8, 7, 6)]

CartesianIndex{3}[CartesianIndex(1, 2, 7); CartesianIndex(2, 8, 7); … ; CartesianIndex(7, 4, 7); CartesianIndex(8, 4, 7)]

CartesianIndex{3}[CartesianIndex(1, 3, 8); CartesianIndex(2, 3, 8); … ; CartesianIndex(7, 4, 8); CartesianIndex(8, 1, 8)])
Stacktrace:
 [1] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:54
 [2] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [3] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
cuarray: Test Failed at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:55
  Expression: findmin(a0, dims = i) == findmin(a1, dims = i)
   Evaluated: ([0.35811281090306957; 0.031997039282309014; … ; 0.18390048699719808; 0.26359026684763065]

[0.027288485750718516; 0.3735466343407079; … ; 0.1141249100402899; 0.08624435502721162]

[0.05151152490095434; 0.05592187148052252; … ; 0.15934554114343746; 0.011722046002706321]

[0.08415122830388477; 0.21967949473605652; … ; 0.026646165446342573; 0.15305096970565235]

[0.25012029030858307; 0.04395095307674701; … ; 0.06963684153211891; 0.28768061217137997]

[0.03300037855491467; 0.10186177037934052; … ; 0.11635770546471402; 0.13842954645772987]

[0.18177037762662263; 0.03379864669350141; … ; 0.025712447796486293; 0.27053954853251727]

[0.09595607245760562; 0.23447545224686017; … ; 0.09713994264725034; 0.038626062938367456], CartesianIndex{3}[CartesianIndex(1, 8, 1); CartesianIndex(2, 7, 1); … ; CartesianIndex(7, 1, 1); CartesianIndex(8, 6, 1)]

CartesianIndex{3}[CartesianIndex(1, 2, 2); CartesianIndex(2, 2, 2); … ; CartesianIndex(7, 8, 2); CartesianIndex(8, 7, 2)]

CartesianIndex{3}[CartesianIndex(1, 7, 3); CartesianIndex(2, 4, 3); … ; CartesianIndex(7, 1, 3); CartesianIndex(8, 8, 3)]

CartesianIndex{3}[CartesianIndex(1, 7, 4); CartesianIndex(2, 3, 4); … ; CartesianIndex(7, 2, 4); CartesianIndex(8, 4, 4)]

CartesianIndex{3}[CartesianIndex(1, 3, 5); CartesianIndex(2, 7, 5); … ; CartesianIndex(7, 1, 5); CartesianIndex(8, 2, 5)]

CartesianIndex{3}[CartesianIndex(1, 4, 6); CartesianIndex(2, 8, 6); … ; CartesianIndex(7, 4, 6); CartesianIndex(8, 2, 6)]

CartesianIndex{3}[CartesianIndex(1, 1, 7); CartesianIndex(2, 6, 7); … ; CartesianIndex(7, 7, 7); CartesianIndex(8, 5, 7)]

CartesianIndex{3}[CartesianIndex(1, 6, 8); CartesianIndex(2, 8, 8); … ; CartesianIndex(7, 5, 8); CartesianIndex(8, 3, 8)]) == ([0.35811281090306957; 0.031997039282309014; … ; 0.18390048699719808; 0.26359026684763065]

[0.027288485750718516; 0.40597627078965437; … ; 0.1141249100402899; 0.08624435502721162]

[0.05151152490095434; 0.07973528240823802; … ; 0.15934554114343746; 0.011722046002706321]

[0.08415122830388477; 0.03250036748917018; … ; 0.026646165446342573; 0.15305096970565235]

[0.25012029030858307; 0.04395095307674701; … ; 0.06963684153211891; 0.28768061217137997]

[0.03300037855491467; 0.10186177037934052; … ; 0.11635770546471402; 0.13842954645772987]

[0.18177037762662263; 0.03379864669350141; … ; 0.025712447796486293; 0.27053954853251727]

[0.09595607245760562; 0.23447545224686017; … ; 0.09713994264725034; 0.038626062938367456], CartesianIndex{3}[CartesianIndex(1, 8, 1); CartesianIndex(2, 7, 1); … ; CartesianIndex(7, 1, 1); CartesianIndex(8, 6, 1)]

CartesianIndex{3}[CartesianIndex(1, 2, 2); CartesianIndex(2, 4, 2); … ; CartesianIndex(7, 8, 2); CartesianIndex(8, 7, 2)]

CartesianIndex{3}[CartesianIndex(1, 7, 3); CartesianIndex(2, 6, 3); … ; CartesianIndex(7, 1, 3); CartesianIndex(8, 8, 3)]

CartesianIndex{3}[CartesianIndex(1, 7, 4); CartesianIndex(2, 4, 4); … ; CartesianIndex(7, 2, 4); CartesianIndex(8, 4, 4)]

CartesianIndex{3}[CartesianIndex(1, 3, 5); CartesianIndex(2, 7, 5); … ; CartesianIndex(7, 1, 5); CartesianIndex(8, 2, 5)]

CartesianIndex{3}[CartesianIndex(1, 4, 6); CartesianIndex(2, 8, 6); … ; CartesianIndex(7, 4, 6); CartesianIndex(8, 2, 6)]

CartesianIndex{3}[CartesianIndex(1, 1, 7); CartesianIndex(2, 6, 7); … ; CartesianIndex(7, 7, 7); CartesianIndex(8, 5, 7)]

CartesianIndex{3}[CartesianIndex(1, 6, 8); CartesianIndex(2, 8, 8); … ; CartesianIndex(7, 5, 8); CartesianIndex(8, 3, 8)])
Stacktrace:
 [1] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:55
 [2] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [3] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
cuarray: Test Failed at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:52
  Expression: argmax(a0, dims = i) == argmax(a1, dims = i)
   Evaluated: CartesianIndex{3}[CartesianIndex(1, 1, 8) CartesianIndex(1, 2, 5) … CartesianIndex(1, 7, 5) CartesianIndex(1, 8, 8); CartesianIndex(2, 1, 3) CartesianIndex(2, 2, 6) … CartesianIndex(2, 7, 4) CartesianIndex(2, 8, 4); … ; CartesianIndex(7, 1, 8) CartesianIndex(7, 2, 3) … CartesianIndex(7, 7, 8) CartesianIndex(7, 8, 6); CartesianIndex(8, 1, 8) CartesianIndex(8, 2, 7) … CartesianIndex(8, 7, 1) CartesianIndex(8, 8, 5)] == CartesianIndex{3}[CartesianIndex(1, 1, 8) CartesianIndex(1, 2, 5) … CartesianIndex(1, 7, 5) CartesianIndex(1, 8, 8); CartesianIndex(2, 1, 3) CartesianIndex(2, 2, 6) … CartesianIndex(2, 7, 4) CartesianIndex(2, 8, 4); … ; CartesianIndex(7, 1, 8) CartesianIndex(7, 2, 3) … CartesianIndex(7, 7, 8) CartesianIndex(7, 8, 6); CartesianIndex(8, 1, 8) CartesianIndex(8, 2, 7) … CartesianIndex(8, 7, 1) CartesianIndex(8, 8, 5)]
Stacktrace:
 [1] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:52
 [2] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [3] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
cuarray: Test Failed at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:53
  Expression: argmin(a0, dims = i) == argmin(a1, dims = i)
   Evaluated: CartesianIndex{3}[CartesianIndex(1, 1, 7) CartesianIndex(1, 2, 2) … CartesianIndex(1, 7, 3) CartesianIndex(1, 8, 5); CartesianIndex(2, 1, 5) CartesianIndex(2, 2, 2) … CartesianIndex(2, 7, 1) CartesianIndex(2, 8, 1); … ; CartesianIndex(7, 1, 5) CartesianIndex(7, 2, 4) … CartesianIndex(7, 7, 7) CartesianIndex(7, 8, 7); CartesianIndex(8, 1, 2) CartesianIndex(8, 2, 6) … CartesianIndex(8, 7, 2) CartesianIndex(8, 8, 3)] == CartesianIndex{3}[CartesianIndex(1, 1, 7) CartesianIndex(1, 2, 2) … CartesianIndex(1, 7, 3) CartesianIndex(1, 8, 5); CartesianIndex(2, 1, 5) CartesianIndex(2, 2, 4) … CartesianIndex(2, 7, 1) CartesianIndex(2, 8, 1); … ; CartesianIndex(7, 1, 5) CartesianIndex(7, 2, 4) … CartesianIndex(7, 7, 7) CartesianIndex(7, 8, 7); CartesianIndex(8, 1, 2) CartesianIndex(8, 2, 6) … CartesianIndex(8, 7, 2) CartesianIndex(8, 8, 3)]
Stacktrace:
 [1] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:53
 [2] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [3] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
cuarray: Test Failed at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:54
  Expression: findmax(a0, dims = i) == findmax(a1, dims = i)
   Evaluated: ([0.6396503470021402 0.955626828225381 … 0.8696821757671549 0.9580190209814774; 0.9983877928529141 0.9481576694971006 … 0.9827029819847197 0.9581728807624663; … ; 0.9060022303590731 0.7561414911679136 … 0.988687814048945 0.9518768731170459; 0.9490759545813638 0.8799292977015383 … 0.9985907294952077 0.9342016969208025], CartesianIndex{3}[CartesianIndex(1, 1, 8) CartesianIndex(1, 2, 5) … CartesianIndex(1, 7, 5) CartesianIndex(1, 8, 8); CartesianIndex(2, 1, 3) CartesianIndex(2, 2, 6) … CartesianIndex(2, 7, 4) CartesianIndex(2, 8, 4); … ; CartesianIndex(7, 1, 8) CartesianIndex(7, 2, 3) … CartesianIndex(7, 7, 8) CartesianIndex(7, 8, 6); CartesianIndex(8, 1, 8) CartesianIndex(8, 2, 7) … CartesianIndex(8, 7, 1) CartesianIndex(8, 8, 5)]) == ([0.6396503470021402 0.955626828225381 … 0.8696821757671549 0.9580190209814774; 0.9983877928529141 0.9481576694971006 … 0.9827029819847197 0.9581728807624663; … ; 0.9060022303590731 0.7561414911679136 … 0.988687814048945 0.9518768731170459; 0.9490759545813638 0.8799292977015383 … 0.9985907294952077 0.9342016969208025], CartesianIndex{3}[CartesianIndex(1, 1, 8) CartesianIndex(1, 2, 5) … CartesianIndex(1, 7, 5) CartesianIndex(1, 8, 8); CartesianIndex(2, 1, 3) CartesianIndex(2, 2, 6) … CartesianIndex(2, 7, 4) CartesianIndex(2, 8, 4); … ; CartesianIndex(7, 1, 8) CartesianIndex(7, 2, 3) … CartesianIndex(7, 7, 8) CartesianIndex(7, 8, 6); CartesianIndex(8, 1, 8) CartesianIndex(8, 2, 7) … CartesianIndex(8, 7, 1) CartesianIndex(8, 8, 5)])
Stacktrace:
 [1] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:54
 [2] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [3] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
cuarray: Test Failed at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:55
  Expression: findmin(a0, dims = i) == findmin(a1, dims = i)
   Evaluated: ([0.18177037762662263 0.027288485750718516 … 0.05151152490095434 0.30827570933347026; 0.06324586881250327 0.3735466343407079 … 0.031997039282309014 0.04082455595115464; … ; 0.06963684153211891 0.026646165446342573 … 0.025712447796486293 0.07368050436774909; 0.2349364827214322 0.13842954645772987 … 0.08624435502721162 0.011722046002706321], CartesianIndex{3}[CartesianIndex(1, 1, 7) CartesianIndex(1, 2, 2) … CartesianIndex(1, 7, 3) CartesianIndex(1, 8, 5); CartesianIndex(2, 1, 5) CartesianIndex(2, 2, 2) … CartesianIndex(2, 7, 1) CartesianIndex(2, 8, 1); … ; CartesianIndex(7, 1, 5) CartesianIndex(7, 2, 4) … CartesianIndex(7, 7, 7) CartesianIndex(7, 8, 7); CartesianIndex(8, 1, 2) CartesianIndex(8, 2, 6) … CartesianIndex(8, 7, 2) CartesianIndex(8, 8, 3)]) == ([0.18177037762662263 0.027288485750718516 … 0.05151152490095434 0.30827570933347026; 0.06324586881250327 0.05240752843274543 … 0.031997039282309014 0.04082455595115464; … ; 0.06963684153211891 0.026646165446342573 … 0.025712447796486293 0.07368050436774909; 0.2349364827214322 0.13842954645772987 … 0.08624435502721162 0.011722046002706321], CartesianIndex{3}[CartesianIndex(1, 1, 7) CartesianIndex(1, 2, 2) … CartesianIndex(1, 7, 3) CartesianIndex(1, 8, 5); CartesianIndex(2, 1, 5) CartesianIndex(2, 2, 4) … CartesianIndex(2, 7, 1) CartesianIndex(2, 8, 1); … ; CartesianIndex(7, 1, 5) CartesianIndex(7, 2, 4) … CartesianIndex(7, 7, 7) CartesianIndex(7, 8, 7); CartesianIndex(8, 1, 2) CartesianIndex(8, 2, 6) … CartesianIndex(8, 7, 2) CartesianIndex(8, 8, 3)])
Stacktrace:
 [1] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:55
 [2] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [3] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/cuarray.jl:4
 53.551721 seconds (49.89 M allocations: 2.420 GiB, 3.55% gc time)

Stacktrace:
 [1] sum(::KnetArray{Float32,3}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
 [2] sum(::KnetArray{Float32,3}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
 [3] forw(::Function, ::AutoGrad.Result{KnetArray{Float32,3}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [5] #sum#75 at ./none:0 [inlined]
 [6] sum(::AutoGrad.Result{KnetArray{Float32,3}}) at ./none:0
 [7] gcsum(::Function, ::Param{Array{KnetArray{Float32,3},1}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:57
 [8] gcsum(::Function, ::Param{Array{KnetArray{Float32,3},1}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [9] (::AutoGrad.var"#217#219"{Tuple{},var"#bmmul1#99",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [12] gradcheck(::var"#bmmul1#99", ::Array{KnetArray{Float32,3},1}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [13] gradcheck(::Function, ::Array{KnetArray{Float32,3},1}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/bmm.jl:30
 [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/bmm.jl:6
 [17] include(::String) at ./client.jl:441
 [18] macro expansion at ./util.jl:175 [inlined]
 [19] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:15 [inlined]
 [20] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [22] include(::String) at ./client.jl:441
 [23] top-level scope at none:6
 [24] eval(::Module, ::Any) at ./boot.jl:331
 [25] exec_options(::Base.JLOptions) at ./client.jl:264
 [26] _start() at ./client.jl:490
bmm: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/bmm.jl:30
  Test threw exception
  Expression: gradcheck(bmmul1, w)
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#bmmul1#99", ::Array{KnetArray{Float32,3},1}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::Array{KnetArray{Float32,3},1}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/bmm.jl:30
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/bmm.jl:6
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float32,3}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
   [2] sum(::KnetArray{Float32,3}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
   [3] forw(::Function, ::AutoGrad.Result{KnetArray{Float32,3}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [5] #sum#75 at ./none:0 [inlined]
   [6] sum(::AutoGrad.Result{KnetArray{Float32,3}}) at ./none:0
   [7] gcsum(::Function, ::Param{Array{KnetArray{Float32,3},1}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:57
   [8] gcsum(::Function, ::Param{Array{KnetArray{Float32,3},1}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [9] (::AutoGrad.var"#217#219"{Tuple{},var"#bmmul1#99",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [12] gradcheck(::var"#bmmul1#99", ::Array{KnetArray{Float32,3},1}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [13] gradcheck(::Function, ::Array{KnetArray{Float32,3},1}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/bmm.jl:30
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/bmm.jl:6
  

Stacktrace:
 [1] sum(::KnetArray{Float32,3}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
 [2] sum(::KnetArray{Float32,3}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
 [3] forw(::Function, ::AutoGrad.Result{KnetArray{Float32,3}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [5] #sum#75 at ./none:0 [inlined]
 [6] sum(::AutoGrad.Result{KnetArray{Float32,3}}) at ./none:0
 [7] gcsum(::Function, ::Param{Array{KnetArray{Float32,3},1}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:57
 [8] gcsum(::Function, ::Param{Array{KnetArray{Float32,3},1}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [9] (::AutoGrad.var"#217#219"{Tuple{},var"#bmmul1#99",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [12] gradcheck(::var"#bmmul1#99", ::Array{KnetArray{Float32,3},1}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [13] gradcheck(::Function, ::Array{KnetArray{Float32,3},1}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/bmm.jl:30
 [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/bmm.jl:6
 [17] include(::String) at ./client.jl:441
 [18] macro expansion at ./util.jl:175 [inlined]
 [19] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:15 [inlined]
 [20] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [22] include(::String) at ./client.jl:441
 [23] top-level scope at none:6
 [24] eval(::Module, ::Any) at ./boot.jl:331
 [25] exec_options(::Base.JLOptions) at ./client.jl:264
 [26] _start() at ./client.jl:490
bmm: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/bmm.jl:30
  Test threw exception
  Expression: gradcheck(bmmul1, w)
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#bmmul1#99", ::Array{KnetArray{Float32,3},1}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::Array{KnetArray{Float32,3},1}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/bmm.jl:30
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/bmm.jl:6
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float32,3}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
   [2] sum(::KnetArray{Float32,3}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
   [3] forw(::Function, ::AutoGrad.Result{KnetArray{Float32,3}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [5] #sum#75 at ./none:0 [inlined]
   [6] sum(::AutoGrad.Result{KnetArray{Float32,3}}) at ./none:0
   [7] gcsum(::Function, ::Param{Array{KnetArray{Float32,3},1}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:57
   [8] gcsum(::Function, ::Param{Array{KnetArray{Float32,3},1}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [9] (::AutoGrad.var"#217#219"{Tuple{},var"#bmmul1#99",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [12] gradcheck(::var"#bmmul1#99", ::Array{KnetArray{Float32,3},1}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [13] gradcheck(::Function, ::Array{KnetArray{Float32,3},1}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/bmm.jl:30
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/bmm.jl:6
  

Stacktrace:
 [1] sum(::KnetArray{Float32,4}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
 [2] sum(::KnetArray{Float32,4}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
 [3] forw(::Function, ::AutoGrad.Result{KnetArray{Float32,4}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [5] #sum#75 at ./none:0 [inlined]
 [6] sum(::AutoGrad.Result{KnetArray{Float32,4}}) at ./none:0
 [7] gcsum(::Function, ::Param{Array{KnetArray{Float32,4},1}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:57
 [8] gcsum(::Function, ::Param{Array{KnetArray{Float32,4},1}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [9] (::AutoGrad.var"#217#219"{Tuple{},var"#bmmul1#99",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [12] gradcheck(::var"#bmmul1#99", ::Array{KnetArray{Float32,4},1}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [13] gradcheck(::Function, ::Array{KnetArray{Float32,4},1}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/bmm.jl:30
 [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/bmm.jl:6
 [17] include(::String) at ./client.jl:441
 [18] macro expansion at ./util.jl:175 [inlined]
 [19] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:15 [inlined]
 [20] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [22] include(::String) at ./client.jl:441
 [23] top-level scope at none:6
 [24] eval(::Module, ::Any) at ./boot.jl:331
 [25] exec_options(::Base.JLOptions) at ./client.jl:264
 [26] _start() at ./client.jl:490
bmm: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/bmm.jl:30
  Test threw exception
  Expression: gradcheck(bmmul1, w)
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#bmmul1#99", ::Array{KnetArray{Float32,4},1}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::Array{KnetArray{Float32,4},1}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/bmm.jl:30
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/bmm.jl:6
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float32,4}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
   [2] sum(::KnetArray{Float32,4}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
   [3] forw(::Function, ::AutoGrad.Result{KnetArray{Float32,4}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [5] #sum#75 at ./none:0 [inlined]
   [6] sum(::AutoGrad.Result{KnetArray{Float32,4}}) at ./none:0
   [7] gcsum(::Function, ::Param{Array{KnetArray{Float32,4},1}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:57
   [8] gcsum(::Function, ::Param{Array{KnetArray{Float32,4},1}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [9] (::AutoGrad.var"#217#219"{Tuple{},var"#bmmul1#99",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [12] gradcheck(::var"#bmmul1#99", ::Array{KnetArray{Float32,4},1}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [13] gradcheck(::Function, ::Array{KnetArray{Float32,4},1}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/bmm.jl:30
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/bmm.jl:6
  

Stacktrace:
 [1] sum(::KnetArray{Float64,3}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
 [2] sum(::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
 [3] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,3}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [5] #sum#75 at ./none:0 [inlined]
 [6] sum(::AutoGrad.Result{KnetArray{Float64,3}}) at ./none:0
 [7] gcsum(::Function, ::Param{Array{KnetArray{Float64,3},1}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:57
 [8] gcsum(::Function, ::Param{Array{KnetArray{Float64,3},1}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [9] (::AutoGrad.var"#217#219"{Tuple{},var"#bmmul1#99",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [12] gradcheck(::var"#bmmul1#99", ::Array{KnetArray{Float64,3},1}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [13] gradcheck(::Function, ::Array{KnetArray{Float64,3},1}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/bmm.jl:30
 [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/bmm.jl:6
 [17] include(::String) at ./client.jl:441
 [18] macro expansion at ./util.jl:175 [inlined]
 [19] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:15 [inlined]
 [20] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [22] include(::String) at ./client.jl:441
 [23] top-level scope at none:6
 [24] eval(::Module, ::Any) at ./boot.jl:331
 [25] exec_options(::Base.JLOptions) at ./client.jl:264
 [26] _start() at ./client.jl:490
bmm: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/bmm.jl:30
  Test threw exception
  Expression: gradcheck(bmmul1, w)
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#bmmul1#99", ::Array{KnetArray{Float64,3},1}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::Array{KnetArray{Float64,3},1}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/bmm.jl:30
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/bmm.jl:6
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float64,3}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
   [2] sum(::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
   [3] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,3}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [5] #sum#75 at ./none:0 [inlined]
   [6] sum(::AutoGrad.Result{KnetArray{Float64,3}}) at ./none:0
   [7] gcsum(::Function, ::Param{Array{KnetArray{Float64,3},1}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:57
   [8] gcsum(::Function, ::Param{Array{KnetArray{Float64,3},1}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [9] (::AutoGrad.var"#217#219"{Tuple{},var"#bmmul1#99",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [12] gradcheck(::var"#bmmul1#99", ::Array{KnetArray{Float64,3},1}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [13] gradcheck(::Function, ::Array{KnetArray{Float64,3},1}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/bmm.jl:30
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/bmm.jl:6
  

Stacktrace:
 [1] sum(::KnetArray{Float64,3}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
 [2] sum(::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
 [3] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,3}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [5] #sum#75 at ./none:0 [inlined]
 [6] sum(::AutoGrad.Result{KnetArray{Float64,3}}) at ./none:0
 [7] gcsum(::Function, ::Param{Array{KnetArray{Float64,3},1}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:57
 [8] gcsum(::Function, ::Param{Array{KnetArray{Float64,3},1}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [9] (::AutoGrad.var"#217#219"{Tuple{},var"#bmmul1#99",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [12] gradcheck(::var"#bmmul1#99", ::Array{KnetArray{Float64,3},1}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [13] gradcheck(::Function, ::Array{KnetArray{Float64,3},1}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/bmm.jl:30
 [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/bmm.jl:6
 [17] include(::String) at ./client.jl:441
 [18] macro expansion at ./util.jl:175 [inlined]
 [19] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:15 [inlined]
 [20] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [22] include(::String) at ./client.jl:441
 [23] top-level scope at none:6
 [24] eval(::Module, ::Any) at ./boot.jl:331
 [25] exec_options(::Base.JLOptions) at ./client.jl:264
 [26] _start() at ./client.jl:490
bmm: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/bmm.jl:30
  Test threw exception
  Expression: gradcheck(bmmul1, w)
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#bmmul1#99", ::Array{KnetArray{Float64,3},1}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::Array{KnetArray{Float64,3},1}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/bmm.jl:30
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/bmm.jl:6
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float64,3}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
   [2] sum(::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
   [3] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,3}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [5] #sum#75 at ./none:0 [inlined]
   [6] sum(::AutoGrad.Result{KnetArray{Float64,3}}) at ./none:0
   [7] gcsum(::Function, ::Param{Array{KnetArray{Float64,3},1}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:57
   [8] gcsum(::Function, ::Param{Array{KnetArray{Float64,3},1}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [9] (::AutoGrad.var"#217#219"{Tuple{},var"#bmmul1#99",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [12] gradcheck(::var"#bmmul1#99", ::Array{KnetArray{Float64,3},1}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [13] gradcheck(::Function, ::Array{KnetArray{Float64,3},1}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/bmm.jl:30
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/bmm.jl:6
  

Stacktrace:
 [1] sum(::KnetArray{Float64,4}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
 [2] sum(::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
 [3] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [5] #sum#75 at ./none:0 [inlined]
 [6] sum(::AutoGrad.Result{KnetArray{Float64,4}}) at ./none:0
 [7] gcsum(::Function, ::Param{Array{KnetArray{Float64,4},1}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:57
 [8] gcsum(::Function, ::Param{Array{KnetArray{Float64,4},1}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [9] (::AutoGrad.var"#217#219"{Tuple{},var"#bmmul1#99",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [12] gradcheck(::var"#bmmul1#99", ::Array{KnetArray{Float64,4},1}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [13] gradcheck(::Function, ::Array{KnetArray{Float64,4},1}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/bmm.jl:30
 [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/bmm.jl:6
 [17] include(::String) at ./client.jl:441
 [18] macro expansion at ./util.jl:175 [inlined]
 [19] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:15 [inlined]
 [20] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [22] include(::String) at ./client.jl:441
 [23] top-level scope at none:6
 [24] eval(::Module, ::Any) at ./boot.jl:331
 [25] exec_options(::Base.JLOptions) at ./client.jl:264
 [26] _start() at ./client.jl:490
bmm: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/bmm.jl:30
  Test threw exception
  Expression: gradcheck(bmmul1, w)
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#bmmul1#99", ::Array{KnetArray{Float64,4},1}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::Array{KnetArray{Float64,4},1}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/bmm.jl:30
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/bmm.jl:6
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float64,4}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
   [2] sum(::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
   [3] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [5] #sum#75 at ./none:0 [inlined]
   [6] sum(::AutoGrad.Result{KnetArray{Float64,4}}) at ./none:0
   [7] gcsum(::Function, ::Param{Array{KnetArray{Float64,4},1}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:57
   [8] gcsum(::Function, ::Param{Array{KnetArray{Float64,4},1}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [9] (::AutoGrad.var"#217#219"{Tuple{},var"#bmmul1#99",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [12] gradcheck(::var"#bmmul1#99", ::Array{KnetArray{Float64,4},1}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [13] gradcheck(::Function, ::Array{KnetArray{Float64,4},1}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/bmm.jl:30
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/bmm.jl:6
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
bmm: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/bmm.jl:5
  Got exception outside of a @test
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{Int64,Int64,Int64}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [25] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Tuple{Int64,Int64,Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [26] permutedims(::KnetArray{Float64,3}, ::Tuple{Int64,Int64,Int64}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [27] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/bmm.jl:38
   [28] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [29] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/bmm.jl:6
   [30] include(::String) at ./client.jl:441
   [31] macro expansion at ./util.jl:175 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:15 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
   [35] include(::String) at ./client.jl:441
   [36] top-level scope at none:6
   [37] eval(::Module, ::Any) at ./boot.jl:331
   [38] exec_options(::Base.JLOptions) at ./client.jl:264
   [39] _start() at ./client.jl:490
  
 22.494146 seconds (21.44 M allocations: 1.042 GiB, 2.83% gc time)

Stacktrace:
 [1] sum(::KnetArray{Float32,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
 [2] sum(::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
 [3] forw(::Function, ::AutoGrad.Result{KnetArray{Float32,2}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [5] #sum#75 at ./none:0 [inlined]
 [6] sum(::AutoGrad.Result{KnetArray{Float32,2}}) at ./none:0
 [7] gcsum(::Function, ::Param{Tuple{KnetArray{Float32,2},KnetArray{Float32,2}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:57
 [8] gcsum(::Function, ::Param{Tuple{KnetArray{Float32,2},KnetArray{Float32,2}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [9] (::AutoGrad.var"#217#219"{Tuple{},var"#mmul#108",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [12] gradcheck(::var"#mmul#108", ::Tuple{KnetArray{Float32,2},KnetArray{Float32,2}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [13] gradcheck(::Function, ::Tuple{KnetArray{Float32,2},KnetArray{Float32,2}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:19
 [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [17] include(::String) at ./client.jl:441
 [18] macro expansion at ./util.jl:175 [inlined]
 [19] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [20] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [22] include(::String) at ./client.jl:441
 [23] top-level scope at none:6
 [24] eval(::Module, ::Any) at ./boot.jl:331
 [25] exec_options(::Base.JLOptions) at ./client.jl:264
 [26] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:19
  Test threw exception
  Expression: gradcheck(mmul, (ka, kb))
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#mmul#108", ::Tuple{KnetArray{Float32,2},KnetArray{Float32,2}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::Tuple{KnetArray{Float32,2},KnetArray{Float32,2}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:19
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float32,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
   [2] sum(::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
   [3] forw(::Function, ::AutoGrad.Result{KnetArray{Float32,2}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [5] #sum#75 at ./none:0 [inlined]
   [6] sum(::AutoGrad.Result{KnetArray{Float32,2}}) at ./none:0
   [7] gcsum(::Function, ::Param{Tuple{KnetArray{Float32,2},KnetArray{Float32,2}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:57
   [8] gcsum(::Function, ::Param{Tuple{KnetArray{Float32,2},KnetArray{Float32,2}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [9] (::AutoGrad.var"#217#219"{Tuple{},var"#mmul#108",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [12] gradcheck(::var"#mmul#108", ::Tuple{KnetArray{Float32,2},KnetArray{Float32,2}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [13] gradcheck(::Function, ::Tuple{KnetArray{Float32,2},KnetArray{Float32,2}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:19
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  

Stacktrace:
 [1] sum(::KnetArray{Float32,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
 [2] sum(::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
 [3] forw(::Function, ::AutoGrad.Result{KnetArray{Float32,2}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [5] #sum#75 at ./none:0 [inlined]
 [6] sum(::AutoGrad.Result{KnetArray{Float32,2}}) at ./none:0
 [7] gcsum(::Function, ::Param{Tuple{KnetArray{Float32,2},KnetArray{Float32,2}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:57
 [8] gcsum(::Function, ::Param{Tuple{KnetArray{Float32,2},KnetArray{Float32,2}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [9] (::AutoGrad.var"#217#219"{Tuple{},var"#mmulABt#109",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [12] gradcheck(::var"#mmulABt#109", ::Tuple{KnetArray{Float32,2},KnetArray{Float32,2}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [13] gradcheck(::Function, ::Tuple{KnetArray{Float32,2},KnetArray{Float32,2}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:24
 [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [17] include(::String) at ./client.jl:441
 [18] macro expansion at ./util.jl:175 [inlined]
 [19] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [20] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [22] include(::String) at ./client.jl:441
 [23] top-level scope at none:6
 [24] eval(::Module, ::Any) at ./boot.jl:331
 [25] exec_options(::Base.JLOptions) at ./client.jl:264
 [26] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:24
  Test threw exception
  Expression: gradcheck(mmulABt, (ka, kb'))
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#mmulABt#109", ::Tuple{KnetArray{Float32,2},KnetArray{Float32,2}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::Tuple{KnetArray{Float32,2},KnetArray{Float32,2}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:24
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float32,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
   [2] sum(::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
   [3] forw(::Function, ::AutoGrad.Result{KnetArray{Float32,2}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [5] #sum#75 at ./none:0 [inlined]
   [6] sum(::AutoGrad.Result{KnetArray{Float32,2}}) at ./none:0
   [7] gcsum(::Function, ::Param{Tuple{KnetArray{Float32,2},KnetArray{Float32,2}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:57
   [8] gcsum(::Function, ::Param{Tuple{KnetArray{Float32,2},KnetArray{Float32,2}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [9] (::AutoGrad.var"#217#219"{Tuple{},var"#mmulABt#109",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [12] gradcheck(::var"#mmulABt#109", ::Tuple{KnetArray{Float32,2},KnetArray{Float32,2}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [13] gradcheck(::Function, ::Tuple{KnetArray{Float32,2},KnetArray{Float32,2}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:24
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  

Stacktrace:
 [1] sum(::KnetArray{Float32,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
 [2] sum(::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
 [3] forw(::Function, ::AutoGrad.Result{KnetArray{Float32,2}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [5] #sum#75 at ./none:0 [inlined]
 [6] sum(::AutoGrad.Result{KnetArray{Float32,2}}) at ./none:0
 [7] gcsum(::Function, ::Param{Tuple{KnetArray{Float32,2},KnetArray{Float32,2}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:57
 [8] gcsum(::Function, ::Param{Tuple{KnetArray{Float32,2},KnetArray{Float32,2}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [9] (::AutoGrad.var"#217#219"{Tuple{},var"#mmulAtB#110",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [12] gradcheck(::var"#mmulAtB#110", ::Tuple{KnetArray{Float32,2},KnetArray{Float32,2}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [13] gradcheck(::Function, ::Tuple{KnetArray{Float32,2},KnetArray{Float32,2}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:25
 [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [17] include(::String) at ./client.jl:441
 [18] macro expansion at ./util.jl:175 [inlined]
 [19] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [20] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [22] include(::String) at ./client.jl:441
 [23] top-level scope at none:6
 [24] eval(::Module, ::Any) at ./boot.jl:331
 [25] exec_options(::Base.JLOptions) at ./client.jl:264
 [26] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:25
  Test threw exception
  Expression: gradcheck(mmulAtB, (ka', kb))
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#mmulAtB#110", ::Tuple{KnetArray{Float32,2},KnetArray{Float32,2}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::Tuple{KnetArray{Float32,2},KnetArray{Float32,2}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:25
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float32,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
   [2] sum(::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
   [3] forw(::Function, ::AutoGrad.Result{KnetArray{Float32,2}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [5] #sum#75 at ./none:0 [inlined]
   [6] sum(::AutoGrad.Result{KnetArray{Float32,2}}) at ./none:0
   [7] gcsum(::Function, ::Param{Tuple{KnetArray{Float32,2},KnetArray{Float32,2}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:57
   [8] gcsum(::Function, ::Param{Tuple{KnetArray{Float32,2},KnetArray{Float32,2}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [9] (::AutoGrad.var"#217#219"{Tuple{},var"#mmulAtB#110",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [12] gradcheck(::var"#mmulAtB#110", ::Tuple{KnetArray{Float32,2},KnetArray{Float32,2}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [13] gradcheck(::Function, ::Tuple{KnetArray{Float32,2},KnetArray{Float32,2}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:25
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  

Stacktrace:
 [1] sum(::KnetArray{Float32,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
 [2] sum(::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
 [3] forw(::Function, ::AutoGrad.Result{KnetArray{Float32,2}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [5] #sum#75 at ./none:0 [inlined]
 [6] sum(::AutoGrad.Result{KnetArray{Float32,2}}) at ./none:0
 [7] gcsum(::Function, ::Param{Tuple{KnetArray{Float32,2},KnetArray{Float32,2}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:57
 [8] gcsum(::Function, ::Param{Tuple{KnetArray{Float32,2},KnetArray{Float32,2}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [9] (::AutoGrad.var"#217#219"{Tuple{},var"#mmulAtBt#111",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [12] gradcheck(::var"#mmulAtBt#111", ::Tuple{KnetArray{Float32,2},KnetArray{Float32,2}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [13] gradcheck(::Function, ::Tuple{KnetArray{Float32,2},KnetArray{Float32,2}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:26
 [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [17] include(::String) at ./client.jl:441
 [18] macro expansion at ./util.jl:175 [inlined]
 [19] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [20] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [22] include(::String) at ./client.jl:441
 [23] top-level scope at none:6
 [24] eval(::Module, ::Any) at ./boot.jl:331
 [25] exec_options(::Base.JLOptions) at ./client.jl:264
 [26] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:26
  Test threw exception
  Expression: gradcheck(mmulAtBt, (ka', kb'))
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#mmulAtBt#111", ::Tuple{KnetArray{Float32,2},KnetArray{Float32,2}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::Tuple{KnetArray{Float32,2},KnetArray{Float32,2}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:26
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float32,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
   [2] sum(::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
   [3] forw(::Function, ::AutoGrad.Result{KnetArray{Float32,2}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [5] #sum#75 at ./none:0 [inlined]
   [6] sum(::AutoGrad.Result{KnetArray{Float32,2}}) at ./none:0
   [7] gcsum(::Function, ::Param{Tuple{KnetArray{Float32,2},KnetArray{Float32,2}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:57
   [8] gcsum(::Function, ::Param{Tuple{KnetArray{Float32,2},KnetArray{Float32,2}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [9] (::AutoGrad.var"#217#219"{Tuple{},var"#mmulAtBt#111",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [12] gradcheck(::var"#mmulAtBt#111", ::Tuple{KnetArray{Float32,2},KnetArray{Float32,2}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [13] gradcheck(::Function, ::Tuple{KnetArray{Float32,2},KnetArray{Float32,2}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:26
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  

Stacktrace:
 [1] sum(::KnetArray{Float32,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
 [2] sum(::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
 [3] forw(::Function, ::AutoGrad.Result{KnetArray{Float32,2}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [5] #sum#75 at ./none:0 [inlined]
 [6] sum(::AutoGrad.Result{KnetArray{Float32,2}}) at ./none:0
 [7] gcsum(::Function, ::Param{KnetArray{Float32,2}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:57
 [8] gcsum(::Function, ::Param{KnetArray{Float32,2}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [9] (::AutoGrad.var"#217#219"{Tuple{},typeof(transpose),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [12] gradcheck(::typeof(transpose), ::KnetArray{Float32,2}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [13] gradcheck(::Function, ::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:59
 [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [17] include(::String) at ./client.jl:441
 [18] macro expansion at ./util.jl:175 [inlined]
 [19] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [20] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [22] include(::String) at ./client.jl:441
 [23] top-level scope at none:6
 [24] eval(::Module, ::Any) at ./boot.jl:331
 [25] exec_options(::Base.JLOptions) at ./client.jl:264
 [26] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:59
  Test threw exception
  Expression: gradcheck(transpose, ka)
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(transpose), ::KnetArray{Float32,2}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:59
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float32,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
   [2] sum(::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
   [3] forw(::Function, ::AutoGrad.Result{KnetArray{Float32,2}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [5] #sum#75 at ./none:0 [inlined]
   [6] sum(::AutoGrad.Result{KnetArray{Float32,2}}) at ./none:0
   [7] gcsum(::Function, ::Param{KnetArray{Float32,2}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:57
   [8] gcsum(::Function, ::Param{KnetArray{Float32,2}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [9] (::AutoGrad.var"#217#219"{Tuple{},typeof(transpose),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [12] gradcheck(::typeof(transpose), ::KnetArray{Float32,2}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [13] gradcheck(::Function, ::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:59
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  

Stacktrace:
 [1] sum(::KnetArray{Float32,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
 [2] sum(::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
 [3] forw(::Function, ::AutoGrad.Result{KnetArray{Float32,2}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [5] #sum#75 at ./none:0 [inlined]
 [6] sum(::AutoGrad.Result{KnetArray{Float32,2}}) at ./none:0
 [7] gcsum(::Function, ::Param{Array{Any,1}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:57
 [8] gcsum(::Function, ::Param{Array{Any,1}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [9] (::AutoGrad.var"#217#219"{Tuple{},var"#mmul1#112",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [12] gradcheck(::var"#mmul1#112", ::Array{Any,1}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [13] gradcheck(::Function, ::Array{Any,1}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:63
 [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [17] include(::String) at ./client.jl:441
 [18] macro expansion at ./util.jl:175 [inlined]
 [19] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [20] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [22] include(::String) at ./client.jl:441
 [23] top-level scope at none:6
 [24] eval(::Module, ::Any) at ./boot.jl:331
 [25] exec_options(::Base.JLOptions) at ./client.jl:264
 [26] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:63
  Test threw exception
  Expression: gradcheck(mmul1, Any[kat, kb])
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#mmul1#112", ::Array{Any,1}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::Array{Any,1}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:63
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float32,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
   [2] sum(::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
   [3] forw(::Function, ::AutoGrad.Result{KnetArray{Float32,2}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [5] #sum#75 at ./none:0 [inlined]
   [6] sum(::AutoGrad.Result{KnetArray{Float32,2}}) at ./none:0
   [7] gcsum(::Function, ::Param{Array{Any,1}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:57
   [8] gcsum(::Function, ::Param{Array{Any,1}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [9] (::AutoGrad.var"#217#219"{Tuple{},var"#mmul1#112",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [12] gradcheck(::var"#mmul1#112", ::Array{Any,1}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [13] gradcheck(::Function, ::Array{Any,1}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:63
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  

Stacktrace:
 [1] sum(::KnetArray{Float32,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
 [2] sum(::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
 [3] forw(::Function, ::AutoGrad.Result{KnetArray{Float32,2}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [5] #sum#75 at ./none:0 [inlined]
 [6] sum(::AutoGrad.Result{KnetArray{Float32,2}}) at ./none:0
 [7] gcsum(::Function, ::Param{Array{Any,1}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:57
 [8] gcsum(::Function, ::Param{Array{Any,1}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [9] (::AutoGrad.var"#217#219"{Tuple{},var"#mmul2#113",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [12] gradcheck(::var"#mmul2#113", ::Array{Any,1}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [13] gradcheck(::Function, ::Array{Any,1}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:64
 [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [17] include(::String) at ./client.jl:441
 [18] macro expansion at ./util.jl:175 [inlined]
 [19] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [20] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [22] include(::String) at ./client.jl:441
 [23] top-level scope at none:6
 [24] eval(::Module, ::Any) at ./boot.jl:331
 [25] exec_options(::Base.JLOptions) at ./client.jl:264
 [26] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:64
  Test threw exception
  Expression: gradcheck(mmul2, Any[ka, kbt])
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#mmul2#113", ::Array{Any,1}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::Array{Any,1}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:64
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float32,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
   [2] sum(::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
   [3] forw(::Function, ::AutoGrad.Result{KnetArray{Float32,2}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [5] #sum#75 at ./none:0 [inlined]
   [6] sum(::AutoGrad.Result{KnetArray{Float32,2}}) at ./none:0
   [7] gcsum(::Function, ::Param{Array{Any,1}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:57
   [8] gcsum(::Function, ::Param{Array{Any,1}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [9] (::AutoGrad.var"#217#219"{Tuple{},var"#mmul2#113",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [12] gradcheck(::var"#mmul2#113", ::Array{Any,1}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [13] gradcheck(::Function, ::Array{Any,1}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:64
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  

Stacktrace:
 [1] sum(::KnetArray{Float32,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
 [2] sum(::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
 [3] forw(::Function, ::AutoGrad.Result{KnetArray{Float32,2}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [5] #sum#75 at ./none:0 [inlined]
 [6] sum(::AutoGrad.Result{KnetArray{Float32,2}}) at ./none:0
 [7] gcsum(::Function, ::Param{Array{Any,1}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:57
 [8] gcsum(::Function, ::Param{Array{Any,1}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [9] (::AutoGrad.var"#217#219"{Tuple{},var"#mmul3#114",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [12] gradcheck(::var"#mmul3#114", ::Array{Any,1}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [13] gradcheck(::Function, ::Array{Any,1}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:65
 [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [17] include(::String) at ./client.jl:441
 [18] macro expansion at ./util.jl:175 [inlined]
 [19] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [20] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [22] include(::String) at ./client.jl:441
 [23] top-level scope at none:6
 [24] eval(::Module, ::Any) at ./boot.jl:331
 [25] exec_options(::Base.JLOptions) at ./client.jl:264
 [26] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:65
  Test threw exception
  Expression: gradcheck(mmul3, Any[kat, kbt])
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#mmul3#114", ::Array{Any,1}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::Array{Any,1}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:65
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float32,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
   [2] sum(::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
   [3] forw(::Function, ::AutoGrad.Result{KnetArray{Float32,2}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [5] #sum#75 at ./none:0 [inlined]
   [6] sum(::AutoGrad.Result{KnetArray{Float32,2}}) at ./none:0
   [7] gcsum(::Function, ::Param{Array{Any,1}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:57
   [8] gcsum(::Function, ::Param{Array{Any,1}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [9] (::AutoGrad.var"#217#219"{Tuple{},var"#mmul3#114",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [12] gradcheck(::var"#mmul3#114", ::Array{Any,1}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [13] gradcheck(::Function, ::Array{Any,1}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:65
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  

Stacktrace:
 [1] broadcasted(::typeof(*), ::Float32, ::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/binary.jl:31
 [2] *(::KnetArray{Float32,2}, ::Float32) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/binary.jl:305
 [3] forw(::Function, ::AutoGrad.Result{KnetArray{Float32,2}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [5] *(::AutoGrad.Result{KnetArray{Float32,2}}, ::AutoGrad.Result{Float32}) at ./none:0
 [6] (::var"#mmul1#112")(::Param{Array{Any,1}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:43
 [7] gcsum(::Function, ::Param{Array{Any,1}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [8] gcsum(::Function, ::Param{Array{Any,1}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [9] (::AutoGrad.var"#217#219"{Tuple{},var"#mmul1#112",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [12] gradcheck(::var"#mmul1#112", ::Array{Any,1}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [13] gradcheck(::Function, ::Array{Any,1}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:66
 [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [17] include(::String) at ./client.jl:441
 [18] macro expansion at ./util.jl:175 [inlined]
 [19] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [20] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [22] include(::String) at ./client.jl:441
 [23] top-level scope at none:6
 [24] eval(::Module, ::Any) at ./boot.jl:331
 [25] exec_options(::Base.JLOptions) at ./client.jl:264
 [26] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:66
  Test threw exception
  Expression: gradcheck(mmul1, Any[ka, s])
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#mmul1#112", ::Array{Any,1}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::Array{Any,1}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:66
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] broadcasted(::typeof(*), ::Float32, ::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/binary.jl:31
   [2] *(::KnetArray{Float32,2}, ::Float32) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/binary.jl:305
   [3] forw(::Function, ::AutoGrad.Result{KnetArray{Float32,2}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [5] *(::AutoGrad.Result{KnetArray{Float32,2}}, ::AutoGrad.Result{Float32}) at ./none:0
   [6] (::var"#mmul1#112")(::Param{Array{Any,1}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:43
   [7] gcsum(::Function, ::Param{Array{Any,1}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [8] gcsum(::Function, ::Param{Array{Any,1}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [9] (::AutoGrad.var"#217#219"{Tuple{},var"#mmul1#112",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [12] gradcheck(::var"#mmul1#112", ::Array{Any,1}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [13] gradcheck(::Function, ::Array{Any,1}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:66
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  

Stacktrace:
 [1] broadcasted(::typeof(*), ::Float32, ::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/binary.jl:31
 [2] *(::Float32, ::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/binary.jl:306
 [3] forw(::Function, ::AutoGrad.Result{Float32}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [5] *(::AutoGrad.Result{Float32}, ::AutoGrad.Result{KnetArray{Float32,2}}) at ./none:0
 [6] (::var"#mmul1#112")(::Param{Array{Any,1}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:43
 [7] gcsum(::Function, ::Param{Array{Any,1}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [8] gcsum(::Function, ::Param{Array{Any,1}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [9] (::AutoGrad.var"#217#219"{Tuple{},var"#mmul1#112",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [12] gradcheck(::var"#mmul1#112", ::Array{Any,1}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [13] gradcheck(::Function, ::Array{Any,1}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:67
 [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [17] include(::String) at ./client.jl:441
 [18] macro expansion at ./util.jl:175 [inlined]
 [19] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [20] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [22] include(::String) at ./client.jl:441
 [23] top-level scope at none:6
 [24] eval(::Module, ::Any) at ./boot.jl:331
 [25] exec_options(::Base.JLOptions) at ./client.jl:264
 [26] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:67
  Test threw exception
  Expression: gradcheck(mmul1, Any[s, kb])
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#mmul1#112", ::Array{Any,1}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::Array{Any,1}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:67
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] broadcasted(::typeof(*), ::Float32, ::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/binary.jl:31
   [2] *(::Float32, ::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/binary.jl:306
   [3] forw(::Function, ::AutoGrad.Result{Float32}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [5] *(::AutoGrad.Result{Float32}, ::AutoGrad.Result{KnetArray{Float32,2}}) at ./none:0
   [6] (::var"#mmul1#112")(::Param{Array{Any,1}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:43
   [7] gcsum(::Function, ::Param{Array{Any,1}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [8] gcsum(::Function, ::Param{Array{Any,1}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [9] (::AutoGrad.var"#217#219"{Tuple{},var"#mmul1#112",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [12] gradcheck(::var"#mmul1#112", ::Array{Any,1}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [13] gradcheck(::Function, ::Array{Any,1}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:67
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  

Stacktrace:
 [1] broadcasted(::typeof(*), ::Float32, ::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/binary.jl:31
 [2] *(::KnetArray{Float32,2}, ::Float32) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/binary.jl:305
 [3] forw(::Function, ::AutoGrad.Result{KnetArray{Float32,2}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [5] *(::AutoGrad.Result{KnetArray{Float32,2}}, ::AutoGrad.Result{Float32}) at ./none:0
 [6] (::var"#mmul2#113")(::Param{Array{Any,1}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:44
 [7] gcsum(::Function, ::Param{Array{Any,1}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [8] gcsum(::Function, ::Param{Array{Any,1}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [9] (::AutoGrad.var"#217#219"{Tuple{},var"#mmul2#113",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [12] gradcheck(::var"#mmul2#113", ::Array{Any,1}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [13] gradcheck(::Function, ::Array{Any,1}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:68
 [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [17] include(::String) at ./client.jl:441
 [18] macro expansion at ./util.jl:175 [inlined]
 [19] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [20] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [22] include(::String) at ./client.jl:441
 [23] top-level scope at none:6
 [24] eval(::Module, ::Any) at ./boot.jl:331
 [25] exec_options(::Base.JLOptions) at ./client.jl:264
 [26] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:68
  Test threw exception
  Expression: gradcheck(mmul2, Any[ka, s])
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#mmul2#113", ::Array{Any,1}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::Array{Any,1}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:68
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] broadcasted(::typeof(*), ::Float32, ::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/binary.jl:31
   [2] *(::KnetArray{Float32,2}, ::Float32) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/binary.jl:305
   [3] forw(::Function, ::AutoGrad.Result{KnetArray{Float32,2}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [5] *(::AutoGrad.Result{KnetArray{Float32,2}}, ::AutoGrad.Result{Float32}) at ./none:0
   [6] (::var"#mmul2#113")(::Param{Array{Any,1}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:44
   [7] gcsum(::Function, ::Param{Array{Any,1}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [8] gcsum(::Function, ::Param{Array{Any,1}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [9] (::AutoGrad.var"#217#219"{Tuple{},var"#mmul2#113",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [12] gradcheck(::var"#mmul2#113", ::Array{Any,1}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [13] gradcheck(::Function, ::Array{Any,1}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:68
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  

Stacktrace:
 [1] broadcasted(::typeof(*), ::Float32, ::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/binary.jl:31
 [2] *(::Float32, ::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/binary.jl:306
 [3] forw(::Function, ::AutoGrad.Result{Float32}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [5] *(::AutoGrad.Result{Float32}, ::AutoGrad.Result{KnetArray{Float32,2}}) at ./none:0
 [6] (::var"#mmul2#113")(::Param{Array{Any,1}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:44
 [7] gcsum(::Function, ::Param{Array{Any,1}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [8] gcsum(::Function, ::Param{Array{Any,1}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [9] (::AutoGrad.var"#217#219"{Tuple{},var"#mmul2#113",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [12] gradcheck(::var"#mmul2#113", ::Array{Any,1}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [13] gradcheck(::Function, ::Array{Any,1}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:69
 [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [17] include(::String) at ./client.jl:441
 [18] macro expansion at ./util.jl:175 [inlined]
 [19] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [20] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [22] include(::String) at ./client.jl:441
 [23] top-level scope at none:6
 [24] eval(::Module, ::Any) at ./boot.jl:331
 [25] exec_options(::Base.JLOptions) at ./client.jl:264
 [26] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:69
  Test threw exception
  Expression: gradcheck(mmul2, Any[s, kb])
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#mmul2#113", ::Array{Any,1}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::Array{Any,1}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:69
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] broadcasted(::typeof(*), ::Float32, ::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/binary.jl:31
   [2] *(::Float32, ::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/binary.jl:306
   [3] forw(::Function, ::AutoGrad.Result{Float32}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [5] *(::AutoGrad.Result{Float32}, ::AutoGrad.Result{KnetArray{Float32,2}}) at ./none:0
   [6] (::var"#mmul2#113")(::Param{Array{Any,1}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:44
   [7] gcsum(::Function, ::Param{Array{Any,1}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [8] gcsum(::Function, ::Param{Array{Any,1}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [9] (::AutoGrad.var"#217#219"{Tuple{},var"#mmul2#113",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [12] gradcheck(::var"#mmul2#113", ::Array{Any,1}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [13] gradcheck(::Function, ::Array{Any,1}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:69
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  

Stacktrace:
 [1] broadcasted(::typeof(*), ::Float32, ::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/binary.jl:31
 [2] *(::KnetArray{Float32,2}, ::Float32) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/binary.jl:305
 [3] forw(::Function, ::AutoGrad.Result{KnetArray{Float32,2}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [5] *(::AutoGrad.Result{KnetArray{Float32,2}}, ::AutoGrad.Result{Float32}) at ./none:0
 [6] (::var"#mmul3#114")(::Param{Array{Any,1}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:45
 [7] gcsum(::Function, ::Param{Array{Any,1}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [8] gcsum(::Function, ::Param{Array{Any,1}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [9] (::AutoGrad.var"#217#219"{Tuple{},var"#mmul3#114",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [12] gradcheck(::var"#mmul3#114", ::Array{Any,1}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [13] gradcheck(::Function, ::Array{Any,1}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:70
 [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [17] include(::String) at ./client.jl:441
 [18] macro expansion at ./util.jl:175 [inlined]
 [19] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [20] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [22] include(::String) at ./client.jl:441
 [23] top-level scope at none:6
 [24] eval(::Module, ::Any) at ./boot.jl:331
 [25] exec_options(::Base.JLOptions) at ./client.jl:264
 [26] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:70
  Test threw exception
  Expression: gradcheck(mmul3, Any[ka, s])
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#mmul3#114", ::Array{Any,1}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::Array{Any,1}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:70
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] broadcasted(::typeof(*), ::Float32, ::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/binary.jl:31
   [2] *(::KnetArray{Float32,2}, ::Float32) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/binary.jl:305
   [3] forw(::Function, ::AutoGrad.Result{KnetArray{Float32,2}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [5] *(::AutoGrad.Result{KnetArray{Float32,2}}, ::AutoGrad.Result{Float32}) at ./none:0
   [6] (::var"#mmul3#114")(::Param{Array{Any,1}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:45
   [7] gcsum(::Function, ::Param{Array{Any,1}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [8] gcsum(::Function, ::Param{Array{Any,1}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [9] (::AutoGrad.var"#217#219"{Tuple{},var"#mmul3#114",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [12] gradcheck(::var"#mmul3#114", ::Array{Any,1}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [13] gradcheck(::Function, ::Array{Any,1}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:70
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  

Stacktrace:
 [1] broadcasted(::typeof(*), ::Float32, ::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/binary.jl:31
 [2] *(::Float32, ::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/binary.jl:306
 [3] forw(::Function, ::AutoGrad.Result{Float32}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [5] *(::AutoGrad.Result{Float32}, ::AutoGrad.Result{KnetArray{Float32,2}}) at ./none:0
 [6] (::var"#mmul3#114")(::Param{Array{Any,1}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:45
 [7] gcsum(::Function, ::Param{Array{Any,1}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [8] gcsum(::Function, ::Param{Array{Any,1}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [9] (::AutoGrad.var"#217#219"{Tuple{},var"#mmul3#114",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [12] gradcheck(::var"#mmul3#114", ::Array{Any,1}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [13] gradcheck(::Function, ::Array{Any,1}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:71
 [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [17] include(::String) at ./client.jl:441
 [18] macro expansion at ./util.jl:175 [inlined]
 [19] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [20] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [22] include(::String) at ./client.jl:441
 [23] top-level scope at none:6
 [24] eval(::Module, ::Any) at ./boot.jl:331
 [25] exec_options(::Base.JLOptions) at ./client.jl:264
 [26] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:71
  Test threw exception
  Expression: gradcheck(mmul3, Any[s, kb])
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#mmul3#114", ::Array{Any,1}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::Array{Any,1}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:71
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] broadcasted(::typeof(*), ::Float32, ::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/binary.jl:31
   [2] *(::Float32, ::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/binary.jl:306
   [3] forw(::Function, ::AutoGrad.Result{Float32}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [5] *(::AutoGrad.Result{Float32}, ::AutoGrad.Result{KnetArray{Float32,2}}) at ./none:0
   [6] (::var"#mmul3#114")(::Param{Array{Any,1}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:45
   [7] gcsum(::Function, ::Param{Array{Any,1}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [8] gcsum(::Function, ::Param{Array{Any,1}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [9] (::AutoGrad.var"#217#219"{Tuple{},var"#mmul3#114",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [12] gradcheck(::var"#mmul3#114", ::Array{Any,1}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [13] gradcheck(::Function, ::Array{Any,1}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:71
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  

Stacktrace:
 [1] sum(::KnetArray{Float32,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
 [2] sum(::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
 [3] forw(::Function, ::AutoGrad.Result{KnetArray{Float32,2}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [5] #sum#75 at ./none:0 [inlined]
 [6] sum(::AutoGrad.Result{KnetArray{Float32,2}}) at ./none:0
 [7] gcsum(::Function, ::Param{KnetArray{Float32,2}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:57
 [8] gcsum(::Function, ::Param{KnetArray{Float32,2}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [9] (::AutoGrad.var"#217#219"{Tuple{},typeof(mat),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [12] gradcheck(::typeof(mat), ::KnetArray{Float32,2}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [13] gradcheck(::Function, ::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:77
 [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [17] include(::String) at ./client.jl:441
 [18] macro expansion at ./util.jl:175 [inlined]
 [19] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [20] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [22] include(::String) at ./client.jl:441
 [23] top-level scope at none:6
 [24] eval(::Module, ::Any) at ./boot.jl:331
 [25] exec_options(::Base.JLOptions) at ./client.jl:264
 [26] _start() at ./client.jl:490
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:77
  Test threw exception
  Expression: gradcheck(mat, ka)
  UndefVarError: lib not defined
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(mat), ::KnetArray{Float32,2}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:77
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  UndefVarError: lib not defined
  Stacktrace:
   [1] sum(::KnetArray{Float32,2}; dims::Function) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:30
   [2] sum(::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/reduction.jl:29
   [3] forw(::Function, ::AutoGrad.Result{KnetArray{Float32,2}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [4] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [5] #sum#75 at ./none:0 [inlined]
   [6] sum(::AutoGrad.Result{KnetArray{Float32,2}}) at ./none:0
   [7] gcsum(::Function, ::Param{KnetArray{Float32,2}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:57
   [8] gcsum(::Function, ::Param{KnetArray{Float32,2}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [9] (::AutoGrad.var"#217#219"{Tuple{},typeof(mat),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [10] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [11] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [12] gradcheck(::typeof(mat), ::KnetArray{Float32,2}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [13] gradcheck(::Function, ::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:77
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:84
  Test threw exception
  Expression: isapprox(p2(a), Array(p2(ka)))
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float32,2,Nothing}, ::Tuple{CuArrays.CuArray{Float32,2,Nothing},CuArrays.CuArray{Float32,2,Nothing},Tuple{Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [23] gpu_call(::Function, ::CuArrays.CuArray{Float32,2,Nothing}, ::Tuple{CuArrays.CuArray{Float32,2,Nothing},CuArrays.CuArray{Float32,2,Nothing},Tuple{Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [24] permutedims!(::CuArrays.CuArray{Float32,2,Nothing}, ::CuArrays.CuArray{Float32,2,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [25] permutedims!(::KnetArray{Float32,2}, ::KnetArray{Float32,2}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [26] permutedims at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49 [inlined]
   [27] (::var"#p2#115"{Array{Int64,1}})(::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:81
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:84
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...

Stacktrace:
 [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
 [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
 [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
 [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
 [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
 [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
 [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
 [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [18] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float32,2,Nothing}, ::Tuple{CuArrays.CuArray{Float32,2,Nothing},CuArrays.CuArray{Float32,2,Nothing},Tuple{Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [22] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [23] gpu_call(::Function, ::CuArrays.CuArray{Float32,2,Nothing}, ::Tuple{CuArrays.CuArray{Float32,2,Nothing},CuArrays.CuArray{Float32,2,Nothing},Tuple{Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [24] permutedims!(::CuArrays.CuArray{Float32,2,Nothing}, ::CuArrays.CuArray{Float32,2,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [25] permutedims!(::KnetArray{Float32,2}, ::KnetArray{Float32,2}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [26] permutedims(::KnetArray{Float32,2}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [27] forw(::Function, ::Param{KnetArray{Float32,2}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [28] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [29] permutedims at ./none:0 [inlined]
 [30] (::var"#p2#115"{Array{Int64,1}})(::Param{KnetArray{Float32,2}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:81
 [31] gcsum(::Function, ::Param{KnetArray{Float32,2}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [32] gcsum(::Function, ::Param{KnetArray{Float32,2}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [33] (::AutoGrad.var"#217#219"{Tuple{},var"#p2#115"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [34] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [35] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [36] gradcheck(::var"#p2#115"{Array{Int64,1}}, ::KnetArray{Float32,2}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [37] gradcheck(::Function, ::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [38] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:85
 [39] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [40] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [41] include(::String) at ./client.jl:441
 [42] macro expansion at ./util.jl:175 [inlined]
 [43] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [44] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [45] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [46] include(::String) at ./client.jl:441
 [47] top-level scope at none:6
 [48] eval(::Module, ::Any) at ./boot.jl:331
 [49] exec_options(::Base.JLOptions) at ./client.jl:264
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:85
  Test threw exception
  Expression: gradcheck(p2, ka)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p2#115"{Array{Int64,1}}, ::KnetArray{Float32,2}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:85
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float32,2,Nothing}, ::Tuple{CuArrays.CuArray{Float32,2,Nothing},CuArrays.CuArray{Float32,2,Nothing},Tuple{Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [23] gpu_call(::Function, ::CuArrays.CuArray{Float32,2,Nothing}, ::Tuple{CuArrays.CuArray{Float32,2,Nothing},CuArrays.CuArray{Float32,2,Nothing},Tuple{Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [24] permutedims!(::CuArrays.CuArray{Float32,2,Nothing}, ::CuArrays.CuArray{Float32,2,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [25] permutedims!(::KnetArray{Float32,2}, ::KnetArray{Float32,2}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [26] permutedims(::KnetArray{Float32,2}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [27] forw(::Function, ::Param{KnetArray{Float32,2}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [28] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [29] permutedims at ./none:0 [inlined]
   [30] (::var"#p2#115"{Array{Int64,1}})(::Param{KnetArray{Float32,2}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:81
   [31] gcsum(::Function, ::Param{KnetArray{Float32,2}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [32] gcsum(::Function, ::Param{KnetArray{Float32,2}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [33] (::AutoGrad.var"#217#219"{Tuple{},var"#p2#115"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [34] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [35] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [36] gradcheck(::var"#p2#115"{Array{Int64,1}}, ::KnetArray{Float32,2}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [37] gradcheck(::Function, ::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [38] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:85
   [39] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [40] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:84
  Test threw exception
  Expression: isapprox(p2(a), Array(p2(ka)))
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float32,2,Nothing}, ::Tuple{CuArrays.CuArray{Float32,2,Nothing},CuArrays.CuArray{Float32,2,Nothing},Tuple{Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [23] gpu_call(::Function, ::CuArrays.CuArray{Float32,2,Nothing}, ::Tuple{CuArrays.CuArray{Float32,2,Nothing},CuArrays.CuArray{Float32,2,Nothing},Tuple{Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [24] permutedims!(::CuArrays.CuArray{Float32,2,Nothing}, ::CuArrays.CuArray{Float32,2,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [25] permutedims!(::KnetArray{Float32,2}, ::KnetArray{Float32,2}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [26] permutedims at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49 [inlined]
   [27] (::var"#p2#115"{Array{Int64,1}})(::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:81
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:84
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...

Stacktrace:
 [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
 [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
 [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
 [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
 [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
 [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
 [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
 [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [18] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float32,2,Nothing}, ::Tuple{CuArrays.CuArray{Float32,2,Nothing},CuArrays.CuArray{Float32,2,Nothing},Tuple{Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [22] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [23] gpu_call(::Function, ::CuArrays.CuArray{Float32,2,Nothing}, ::Tuple{CuArrays.CuArray{Float32,2,Nothing},CuArrays.CuArray{Float32,2,Nothing},Tuple{Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [24] permutedims!(::CuArrays.CuArray{Float32,2,Nothing}, ::CuArrays.CuArray{Float32,2,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [25] permutedims!(::KnetArray{Float32,2}, ::KnetArray{Float32,2}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [26] permutedims(::KnetArray{Float32,2}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [27] forw(::Function, ::Param{KnetArray{Float32,2}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [28] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [29] permutedims at ./none:0 [inlined]
 [30] (::var"#p2#115"{Array{Int64,1}})(::Param{KnetArray{Float32,2}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:81
 [31] gcsum(::Function, ::Param{KnetArray{Float32,2}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [32] gcsum(::Function, ::Param{KnetArray{Float32,2}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [33] (::AutoGrad.var"#217#219"{Tuple{},var"#p2#115"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [34] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [35] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [36] gradcheck(::var"#p2#115"{Array{Int64,1}}, ::KnetArray{Float32,2}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [37] gradcheck(::Function, ::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [38] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:85
 [39] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [40] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [41] include(::String) at ./client.jl:441
 [42] macro expansion at ./util.jl:175 [inlined]
 [43] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [44] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [45] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [46] include(::String) at ./client.jl:441
 [47] top-level scope at none:6
 [48] eval(::Module, ::Any) at ./boot.jl:331
 [49] exec_options(::Base.JLOptions) at ./client.jl:264
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:85
  Test threw exception
  Expression: gradcheck(p2, ka)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p2#115"{Array{Int64,1}}, ::KnetArray{Float32,2}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:85
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float32,2,Nothing}, ::Tuple{CuArrays.CuArray{Float32,2,Nothing},CuArrays.CuArray{Float32,2,Nothing},Tuple{Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [23] gpu_call(::Function, ::CuArrays.CuArray{Float32,2,Nothing}, ::Tuple{CuArrays.CuArray{Float32,2,Nothing},CuArrays.CuArray{Float32,2,Nothing},Tuple{Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [24] permutedims!(::CuArrays.CuArray{Float32,2,Nothing}, ::CuArrays.CuArray{Float32,2,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [25] permutedims!(::KnetArray{Float32,2}, ::KnetArray{Float32,2}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [26] permutedims(::KnetArray{Float32,2}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [27] forw(::Function, ::Param{KnetArray{Float32,2}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [28] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [29] permutedims at ./none:0 [inlined]
   [30] (::var"#p2#115"{Array{Int64,1}})(::Param{KnetArray{Float32,2}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:81
   [31] gcsum(::Function, ::Param{KnetArray{Float32,2}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [32] gcsum(::Function, ::Param{KnetArray{Float32,2}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [33] (::AutoGrad.var"#217#219"{Tuple{},var"#p2#115"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [34] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [35] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [36] gradcheck(::var"#p2#115"{Array{Int64,1}}, ::KnetArray{Float32,2}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [37] gradcheck(::Function, ::KnetArray{Float32,2}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [38] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:85
   [39] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [40] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:95
  Test threw exception
  Expression: isapprox(p3(a3), Array(p3(k3)))
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [23] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [24] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [25] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [26] permutedims at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49 [inlined]
   [27] (::var"#p3#116"{Array{Int64,1}})(::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:95
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...

Stacktrace:
 [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
 [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
 [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
 [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
 [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
 [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
 [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
 [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [18] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [22] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [23] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [24] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [25] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [26] permutedims(::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [27] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [28] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [29] permutedims at ./none:0 [inlined]
 [30] (::var"#p3#116"{Array{Int64,1}})(::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
 [31] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [32] gcsum(::Function, ::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [33] (::AutoGrad.var"#217#219"{Tuple{},var"#p3#116"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [34] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [35] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [36] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [37] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [38] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
 [39] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [40] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [41] include(::String) at ./client.jl:441
 [42] macro expansion at ./util.jl:175 [inlined]
 [43] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [44] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [45] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [46] include(::String) at ./client.jl:441
 [47] top-level scope at none:6
 [48] eval(::Module, ::Any) at ./boot.jl:331
 [49] exec_options(::Base.JLOptions) at ./client.jl:264
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
  Test threw exception
  Expression: gradcheck(p3, k3)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [23] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [24] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [25] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [26] permutedims(::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [27] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [28] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [29] permutedims at ./none:0 [inlined]
   [30] (::var"#p3#116"{Array{Int64,1}})(::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
   [31] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [32] gcsum(::Function, ::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [33] (::AutoGrad.var"#217#219"{Tuple{},var"#p3#116"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [34] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [35] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [36] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [37] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [38] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
   [39] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [40] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:95
  Test threw exception
  Expression: isapprox(p3(a3), Array(p3(k3)))
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [23] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [24] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [25] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [26] permutedims at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49 [inlined]
   [27] (::var"#p3#116"{Array{Int64,1}})(::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:95
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...

Stacktrace:
 [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
 [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
 [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
 [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
 [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
 [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
 [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
 [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [18] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [22] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [23] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [24] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [25] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [26] permutedims(::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [27] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [28] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [29] permutedims at ./none:0 [inlined]
 [30] (::var"#p3#116"{Array{Int64,1}})(::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
 [31] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [32] gcsum(::Function, ::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [33] (::AutoGrad.var"#217#219"{Tuple{},var"#p3#116"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [34] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [35] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [36] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [37] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [38] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
 [39] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [40] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [41] include(::String) at ./client.jl:441
 [42] macro expansion at ./util.jl:175 [inlined]
 [43] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [44] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [45] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [46] include(::String) at ./client.jl:441
 [47] top-level scope at none:6
 [48] eval(::Module, ::Any) at ./boot.jl:331
 [49] exec_options(::Base.JLOptions) at ./client.jl:264
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
  Test threw exception
  Expression: gradcheck(p3, k3)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [23] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [24] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [25] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [26] permutedims(::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [27] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [28] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [29] permutedims at ./none:0 [inlined]
   [30] (::var"#p3#116"{Array{Int64,1}})(::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
   [31] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [32] gcsum(::Function, ::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [33] (::AutoGrad.var"#217#219"{Tuple{},var"#p3#116"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [34] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [35] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [36] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [37] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [38] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
   [39] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [40] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:95
  Test threw exception
  Expression: isapprox(p3(a3), Array(p3(k3)))
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [23] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [24] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [25] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [26] permutedims at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49 [inlined]
   [27] (::var"#p3#116"{Array{Int64,1}})(::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:95
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...

Stacktrace:
 [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
 [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
 [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
 [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
 [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
 [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
 [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
 [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [18] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [22] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [23] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [24] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [25] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [26] permutedims(::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [27] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [28] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [29] permutedims at ./none:0 [inlined]
 [30] (::var"#p3#116"{Array{Int64,1}})(::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
 [31] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [32] gcsum(::Function, ::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [33] (::AutoGrad.var"#217#219"{Tuple{},var"#p3#116"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [34] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [35] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [36] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [37] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [38] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
 [39] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [40] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [41] include(::String) at ./client.jl:441
 [42] macro expansion at ./util.jl:175 [inlined]
 [43] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [44] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [45] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [46] include(::String) at ./client.jl:441
 [47] top-level scope at none:6
 [48] eval(::Module, ::Any) at ./boot.jl:331
 [49] exec_options(::Base.JLOptions) at ./client.jl:264
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
  Test threw exception
  Expression: gradcheck(p3, k3)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [23] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [24] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [25] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [26] permutedims(::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [27] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [28] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [29] permutedims at ./none:0 [inlined]
   [30] (::var"#p3#116"{Array{Int64,1}})(::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
   [31] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [32] gcsum(::Function, ::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [33] (::AutoGrad.var"#217#219"{Tuple{},var"#p3#116"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [34] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [35] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [36] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [37] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [38] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
   [39] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [40] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:95
  Test threw exception
  Expression: isapprox(p3(a3), Array(p3(k3)))
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [23] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [24] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [25] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [26] permutedims at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49 [inlined]
   [27] (::var"#p3#116"{Array{Int64,1}})(::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:95
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...

Stacktrace:
 [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
 [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
 [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
 [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
 [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
 [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
 [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
 [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [18] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [22] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [23] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [24] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [25] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [26] permutedims(::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [27] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [28] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [29] permutedims at ./none:0 [inlined]
 [30] (::var"#p3#116"{Array{Int64,1}})(::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
 [31] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [32] gcsum(::Function, ::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [33] (::AutoGrad.var"#217#219"{Tuple{},var"#p3#116"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [34] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [35] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [36] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [37] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [38] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
 [39] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [40] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [41] include(::String) at ./client.jl:441
 [42] macro expansion at ./util.jl:175 [inlined]
 [43] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [44] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [45] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [46] include(::String) at ./client.jl:441
 [47] top-level scope at none:6
 [48] eval(::Module, ::Any) at ./boot.jl:331
 [49] exec_options(::Base.JLOptions) at ./client.jl:264
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
  Test threw exception
  Expression: gradcheck(p3, k3)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [23] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [24] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [25] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [26] permutedims(::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [27] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [28] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [29] permutedims at ./none:0 [inlined]
   [30] (::var"#p3#116"{Array{Int64,1}})(::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
   [31] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [32] gcsum(::Function, ::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [33] (::AutoGrad.var"#217#219"{Tuple{},var"#p3#116"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [34] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [35] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [36] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [37] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [38] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
   [39] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [40] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:95
  Test threw exception
  Expression: isapprox(p3(a3), Array(p3(k3)))
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [23] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [24] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [25] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [26] permutedims at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49 [inlined]
   [27] (::var"#p3#116"{Array{Int64,1}})(::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:95
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...

Stacktrace:
 [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
 [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
 [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
 [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
 [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
 [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
 [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
 [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [18] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [22] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [23] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [24] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [25] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [26] permutedims(::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [27] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [28] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [29] permutedims at ./none:0 [inlined]
 [30] (::var"#p3#116"{Array{Int64,1}})(::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
 [31] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [32] gcsum(::Function, ::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [33] (::AutoGrad.var"#217#219"{Tuple{},var"#p3#116"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [34] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [35] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [36] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [37] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [38] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
 [39] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [40] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [41] include(::String) at ./client.jl:441
 [42] macro expansion at ./util.jl:175 [inlined]
 [43] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [44] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [45] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [46] include(::String) at ./client.jl:441
 [47] top-level scope at none:6
 [48] eval(::Module, ::Any) at ./boot.jl:331
 [49] exec_options(::Base.JLOptions) at ./client.jl:264
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
  Test threw exception
  Expression: gradcheck(p3, k3)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [23] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [24] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [25] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [26] permutedims(::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [27] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [28] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [29] permutedims at ./none:0 [inlined]
   [30] (::var"#p3#116"{Array{Int64,1}})(::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
   [31] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [32] gcsum(::Function, ::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [33] (::AutoGrad.var"#217#219"{Tuple{},var"#p3#116"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [34] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [35] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [36] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [37] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [38] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
   [39] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [40] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:95
  Test threw exception
  Expression: isapprox(p3(a3), Array(p3(k3)))
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [23] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [24] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [25] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [26] permutedims at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49 [inlined]
   [27] (::var"#p3#116"{Array{Int64,1}})(::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:95
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...

Stacktrace:
 [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
 [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
 [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
 [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
 [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
 [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
 [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
 [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [18] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [22] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [23] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [24] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [25] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [26] permutedims(::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [27] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [28] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [29] permutedims at ./none:0 [inlined]
 [30] (::var"#p3#116"{Array{Int64,1}})(::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
 [31] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [32] gcsum(::Function, ::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [33] (::AutoGrad.var"#217#219"{Tuple{},var"#p3#116"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [34] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [35] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [36] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [37] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [38] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
 [39] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [40] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [41] include(::String) at ./client.jl:441
 [42] macro expansion at ./util.jl:175 [inlined]
 [43] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [44] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [45] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [46] include(::String) at ./client.jl:441
 [47] top-level scope at none:6
 [48] eval(::Module, ::Any) at ./boot.jl:331
 [49] exec_options(::Base.JLOptions) at ./client.jl:264
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
  Test threw exception
  Expression: gradcheck(p3, k3)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Int64,Int64,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [23] gpu_call(::Function, ::CuArrays.CuArray{Float64,3,Nothing}, ::Tuple{CuArrays.CuArray{Float64,3,Nothing},CuArrays.CuArray{Float64,3,Nothing},Tuple{Int64,Int64,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [24] permutedims!(::CuArrays.CuArray{Float64,3,Nothing}, ::CuArrays.CuArray{Float64,3,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [25] permutedims!(::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [26] permutedims(::KnetArray{Float64,3}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [27] forw(::Function, ::Param{KnetArray{Float64,3}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [28] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [29] permutedims at ./none:0 [inlined]
   [30] (::var"#p3#116"{Array{Int64,1}})(::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:92
   [31] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [32] gcsum(::Function, ::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [33] (::AutoGrad.var"#217#219"{Tuple{},var"#p3#116"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [34] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [35] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [36] gradcheck(::var"#p3#116"{Array{Int64,1}}, ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [37] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [38] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:96
   [39] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [40] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:106
  Test threw exception
  Expression: isapprox(p4(a4), Array(p4(k4)))
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [23] gpu_call(::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [24] permutedims!(::CuArrays.CuArray{Float64,4,Nothing}, ::CuArrays.CuArray{Float64,4,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [25] permutedims!(::KnetArray{Float64,4}, ::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [26] permutedims at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49 [inlined]
   [27] (::var"#p4#117"{Array{Int64,1}})(::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:103
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:106
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...

Stacktrace:
 [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
 [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
 [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
 [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
 [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
 [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
 [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
 [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [18] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [22] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [23] gpu_call(::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [24] permutedims!(::CuArrays.CuArray{Float64,4,Nothing}, ::CuArrays.CuArray{Float64,4,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [25] permutedims!(::KnetArray{Float64,4}, ::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [26] permutedims(::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [27] forw(::Function, ::Param{KnetArray{Float64,4}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [28] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [29] permutedims at ./none:0 [inlined]
 [30] (::var"#p4#117"{Array{Int64,1}})(::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:103
 [31] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [32] gcsum(::Function, ::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [33] (::AutoGrad.var"#217#219"{Tuple{},var"#p4#117"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [34] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [35] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [36] gradcheck(::var"#p4#117"{Array{Int64,1}}, ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [37] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [38] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
 [39] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [40] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [41] include(::String) at ./client.jl:441
 [42] macro expansion at ./util.jl:175 [inlined]
 [43] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [44] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [45] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [46] include(::String) at ./client.jl:441
 [47] top-level scope at none:6
 [48] eval(::Module, ::Any) at ./boot.jl:331
 [49] exec_options(::Base.JLOptions) at ./client.jl:264
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
  Test threw exception
  Expression: gradcheck(p4, k4)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#p4#117"{Array{Int64,1}}, ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  caused by [exception 1]
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [23] gpu_call(::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [24] permutedims!(::CuArrays.CuArray{Float64,4,Nothing}, ::CuArrays.CuArray{Float64,4,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [25] permutedims!(::KnetArray{Float64,4}, ::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [26] permutedims(::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
   [27] forw(::Function, ::Param{KnetArray{Float64,4}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
   [28] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
   [29] permutedims at ./none:0 [inlined]
   [30] (::var"#p4#117"{Array{Int64,1}})(::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:103
   [31] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [32] gcsum(::Function, ::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
   [33] (::AutoGrad.var"#217#219"{Tuple{},var"#p4#117"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
   [34] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
   [35] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
   [36] gradcheck(::var"#p4#117"{Array{Int64,1}}, ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
   [37] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
   [38] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
   [39] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [40] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
linalg: Error During Test at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:106
  Test threw exception
  Expression: isapprox(p4(a4), Array(p4(k4)))
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
   [23] gpu_call(::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
   [24] permutedims!(::CuArrays.CuArray{Float64,4,Nothing}, ::CuArrays.CuArray{Float64,4,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
   [25] permutedims!(::KnetArray{Float64,4}, ::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
   [26] permutedims at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49 [inlined]
   [27] (::var"#p4#117"{Array{Int64,1}})(::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:103
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:106
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...

Stacktrace:
 [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
 [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
 [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
 [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
 [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
 [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
 [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
 [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
 [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
 [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
 [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
 [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
 [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
 [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
 [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
 [18] cufunction(::GPUArrays.var"#48#49", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Int64}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
 [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
 [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
 [22] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151 [inlined]
 [23] gpu_call(::Function, ::CuArrays.CuArray{Float64,4,Nothing}, ::Tuple{CuArrays.CuArray{Float64,4,Nothing},CuArrays.CuArray{Float64,4,Nothing},NTuple{4,Int64}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128
 [24] permutedims!(::CuArrays.CuArray{Float64,4,Nothing}, ::CuArrays.CuArray{Float64,4,Nothing}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/linalg.jl:85
 [25] permutedims!(::KnetArray{Float64,4}, ::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:40
 [26] permutedims(::KnetArray{Float64,4}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/vxHRi/src/cuarray.jl:49
 [27] forw(::Function, ::Param{KnetArray{Float64,4}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:66
 [28] forw at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:65 [inlined]
 [29] permutedims at ./none:0 [inlined]
 [30] (::var"#p4#117"{Array{Int64,1}})(::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:103
 [31] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [32] gcsum(::Function, ::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:50
 [33] (::AutoGrad.var"#217#219"{Tuple{},var"#p4#117"{Array{Int64,1}},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:205
 [34] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:144
 [35] differentiate at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]
 [36] gradcheck(::var"#p4#117"{Array{Int64,1}}, ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:39
 [37] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/FSgUc/test/gradcheck.jl:36
 [38] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:107
 [39] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
 [40] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/linalg.jl:8
 [41] include(::String) at ./client.jl:441
 [42] macro expansion at ./util.jl:175 [inlined]
 [43] macro expansion at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:16 [inlined]
 [44] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
 [45] top-level scope at /home/pkgeval/.julia/packages/Knet/vxHRi/test/runtests.jl:6
 [46] include(::String) at ./client.jl:441
 [47] top-level scope at none:6
 [48] eval(::Module, ::Any) at ./boot.jl:331
 [49] exec_options(::Base.JLOptions) at ./client.jl:264

signal (15): Terminated
in expression starting at none:13
pthread_cond_wait at /lib/x86_64-linux-gnu/libpthread.so.0 (unknown line)
