Julia Version 1.5.0-DEV.439
Commit 6732cb9b2d (2020-03-11 21:46 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-9.0.1 (ORCJIT, skylake)
Environment:
  JULIA_DEPOT_PATH = ::/usr/local/share/julia
  JULIA_NUM_THREADS = 2

  Resolving package versions...
  Installed IntelOpenMP_jll ────────────── v2018.0.3+0
  Installed ZygoteRules ────────────────── v0.2.0
  Installed MKL_jll ────────────────────── v2019.0.117+2
  Installed ArrayLayouts ───────────────── v0.1.5
  Installed Zlib_jll ───────────────────── v1.2.11+8
  Installed Missings ───────────────────── v0.4.3
  Installed FFTW ───────────────────────── v1.2.0
  Installed GeometricFlux ──────────────── v0.3.0
  Installed TimerOutputs ───────────────── v0.5.3
  Installed Juno ───────────────────────── v0.8.1
  Installed AbstractFFTs ───────────────── v0.5.0
  Installed Requires ───────────────────── v1.0.1
  Installed MacroTools ─────────────────── v0.5.4
  Installed NaNMath ────────────────────── v0.3.3
  Installed IRTools ────────────────────── v0.3.1
  Installed NNlib ──────────────────────── v0.6.6
  Installed CompilerSupportLibraries_jll ─ v0.2.0+1
  Installed Reexport ───────────────────── v0.2.0
  Installed CUDAapi ────────────────────── v3.1.0
  Installed SpecialFunctions ───────────── v0.10.0
  Installed StaticArrays ───────────────── v0.12.1
  Installed BinaryProvider ─────────────── v0.5.8
  Installed TranscodingStreams ─────────── v0.9.5
  Installed GPUArrays ──────────────────── v2.0.1
  Installed FFTW_jll ───────────────────── v3.3.9+4
  Installed DataAPI ────────────────────── v1.1.0
  Installed Media ──────────────────────── v0.5.0
  Installed SortingAlgorithms ──────────── v0.3.1
  Installed StatsBase ──────────────────── v0.32.2
  Installed Flux ───────────────────────── v0.10.3
  Installed LLVM ───────────────────────── v1.3.4
  Installed CUDAnative ─────────────────── v2.10.2
  Installed ZipFile ────────────────────── v0.9.1
  Installed OrderedCollections ─────────── v1.1.0
  Installed CommonSubexpressions ───────── v0.2.0
  Installed DiffRules ──────────────────── v1.0.1
  Installed Colors ─────────────────────── v0.11.2
  Installed ForwardDiff ────────────────── v0.10.9
  Installed Zygote ─────────────────────── v0.4.9
  Installed CUDAdrv ────────────────────── v6.0.0
  Installed CEnum ──────────────────────── v0.2.0
  Installed FillArrays ─────────────────── v0.8.5
  Installed OpenSpecFun_jll ────────────── v0.5.3+3
  Installed Adapt ──────────────────────── v1.0.1
  Installed FixedPointNumbers ──────────── v0.7.1
  Installed ColorTypes ─────────────────── v0.9.1
  Installed DataStructures ─────────────── v0.17.10
  Installed CodecZlib ──────────────────── v0.6.0
  Installed DiffResults ────────────────── v1.0.2
  Installed CuArrays ───────────────────── v1.7.3
  Installed AbstractTrees ──────────────── v0.3.2
#=#=#                                                                         ######################################################################## 100.0%
#=#=#                                                                         ######################################################################## 100.0%
#=#=#                                                                         #                                                                          1.5%#####                                                                      8.3%############                                                              17.4%#####################                                                     29.6%################################                                          45.1%#############################################                             63.9%###############################################################           87.8%######################################################################## 100.0%
#=#=#                                                                         #############################                                             40.8%######################################################################## 100.0%
#=#=#                                                                         ######################################################################## 100.0%
   Updating `~/.julia/environments/v1.5/Project.toml`
   7e08b658 + GeometricFlux v0.3.0
   Updating `~/.julia/environments/v1.5/Manifest.toml`
   621f4979 + AbstractFFTs v0.5.0
   1520ce14 + AbstractTrees v0.3.2
   79e6a3ab + Adapt v1.0.1
   4c555306 + ArrayLayouts v0.1.5
   b99e7846 + BinaryProvider v0.5.8
   fa961155 + CEnum v0.2.0
   3895d2a7 + CUDAapi v3.1.0
   c5f51814 + CUDAdrv v6.0.0
   be33ccc6 + CUDAnative v2.10.2
   944b1d66 + CodecZlib v0.6.0
   3da002f7 + ColorTypes v0.9.1
   5ae59095 + Colors v0.11.2
   bbf7d656 + CommonSubexpressions v0.2.0
   e66e0078 + CompilerSupportLibraries_jll v0.2.0+1
   3a865a2d + CuArrays v1.7.3
   9a962f9c + DataAPI v1.1.0
   864edb3b + DataStructures v0.17.10
   163ba53b + DiffResults v1.0.2
   b552c78f + DiffRules v1.0.1
   7a1cc6ca + FFTW v1.2.0
   f5851436 + FFTW_jll v3.3.9+4
   1a297f60 + FillArrays v0.8.5
   53c48c17 + FixedPointNumbers v0.7.1
   587475ba + Flux v0.10.3
   f6369f11 + ForwardDiff v0.10.9
   0c68f7d7 + GPUArrays v2.0.1
   7e08b658 + GeometricFlux v0.3.0
   7869d1d1 + IRTools v0.3.1
   1d5cc7b8 + IntelOpenMP_jll v2018.0.3+0
   e5e0dc1b + Juno v0.8.1
   929cbde3 + LLVM v1.3.4
   856f044c + MKL_jll v2019.0.117+2
   1914dd2f + MacroTools v0.5.4
   e89f7d12 + Media v0.5.0
   e1d29d7a + Missings v0.4.3
   872c559c + NNlib v0.6.6
   77ba4419 + NaNMath v0.3.3
   efe28fd5 + OpenSpecFun_jll v0.5.3+3
   bac558e1 + OrderedCollections v1.1.0
   189a3867 + Reexport v0.2.0
   ae029012 + Requires v1.0.1
   a2af1166 + SortingAlgorithms v0.3.1
   276daf66 + SpecialFunctions v0.10.0
   90137ffa + StaticArrays v0.12.1
   2913bbd2 + StatsBase v0.32.2
   a759f4b9 + TimerOutputs v0.5.3
   3bb67fe8 + TranscodingStreams v0.9.5
   a5390f91 + ZipFile v0.9.1
   83775a58 + Zlib_jll v1.2.11+8
   e88e6eb3 + Zygote v0.4.9
   700de1a5 + ZygoteRules v0.2.0
   2a0f44e3 + Base64
   ade2ca70 + Dates
   8bb1440f + DelimitedFiles
   8ba89e20 + Distributed
   b77e0a4c + InteractiveUtils
   76f85450 + LibGit2
   8f399da3 + Libdl
   37e2e46d + LinearAlgebra
   56ddb016 + Logging
   d6f4376e + Markdown
   a63ad114 + Mmap
   44cfe95a + Pkg
   de0858da + Printf
   9abbd945 + Profile
   3fa0cd96 + REPL
   9a3f8284 + Random
   ea8e919c + SHA
   9e88b42a + Serialization
   6462fe0b + Sockets
   2f01184e + SparseArrays
   10745b16 + Statistics
   8dfed614 + Test
   cf7118a7 + UUIDs
   4ec0a83e + Unicode
   Building FFTW ─────→ `~/.julia/packages/FFTW/qqcBj/deps/build.log`
   Building NNlib ────→ `~/.julia/packages/NNlib/FAI3o/deps/build.log`
   Building CodecZlib → `~/.julia/packages/CodecZlib/5t9zO/deps/build.log`
    Testing GeometricFlux
     Status `/tmp/jl_1s4f4b/Project.toml`
   3895d2a7 CUDAapi v3.1.0
   be33ccc6 CUDAnative v2.10.2
   3a865a2d CuArrays v1.7.3
   864edb3b DataStructures v0.17.10
   587475ba Flux v0.10.3
   7e08b658 GeometricFlux v0.3.0
   7869d1d1 IRTools v0.3.1
   093fc24a LightGraphs v1.3.1
   626554b9 MetaGraphs v0.6.5
   ae029012 Requires v1.0.1
   47aef6b3 SimpleWeightedGraphs v1.1.1
   e88e6eb3 Zygote v0.4.9
   700de1a5 ZygoteRules v0.2.0
   37e2e46d LinearAlgebra
   9a3f8284 Random
   2f01184e SparseArrays
   10745b16 Statistics
   8dfed614 Test
     Status `/tmp/jl_1s4f4b/Manifest.toml`
   621f4979 AbstractFFTs v0.5.0
   1520ce14 AbstractTrees v0.3.2
   79e6a3ab Adapt v1.0.1
   ec485272 ArnoldiMethod v0.0.4
   4c555306 ArrayLayouts v0.1.5
   b99e7846 BinaryProvider v0.5.8
   fa961155 CEnum v0.2.0
   3895d2a7 CUDAapi v3.1.0
   c5f51814 CUDAdrv v6.0.0
   be33ccc6 CUDAnative v2.10.2
   944b1d66 CodecZlib v0.6.0
   3da002f7 ColorTypes v0.9.1
   5ae59095 Colors v0.11.2
   bbf7d656 CommonSubexpressions v0.2.0
   e66e0078 CompilerSupportLibraries_jll v0.2.0+1
   3a865a2d CuArrays v1.7.3
   9a962f9c DataAPI v1.1.0
   864edb3b DataStructures v0.17.10
   163ba53b DiffResults v1.0.2
   b552c78f DiffRules v1.0.1
   7a1cc6ca FFTW v1.2.0
   f5851436 FFTW_jll v3.3.9+4
   5789e2e9 FileIO v1.2.3
   1a297f60 FillArrays v0.8.5
   53c48c17 FixedPointNumbers v0.7.1
   587475ba Flux v0.10.3
   f6369f11 ForwardDiff v0.10.9
   0c68f7d7 GPUArrays v2.0.1
   7e08b658 GeometricFlux v0.3.0
   7869d1d1 IRTools v0.3.1
   d25df0c9 Inflate v0.1.1
   1d5cc7b8 IntelOpenMP_jll v2018.0.3+0
   033835bb JLD2 v0.1.12
   e5e0dc1b Juno v0.8.1
   929cbde3 LLVM v1.3.4
   093fc24a LightGraphs v1.3.1
   856f044c MKL_jll v2019.0.117+2
   1914dd2f MacroTools v0.5.4
   e89f7d12 Media v0.5.0
   626554b9 MetaGraphs v0.6.5
   e1d29d7a Missings v0.4.3
   872c559c NNlib v0.6.6
   77ba4419 NaNMath v0.3.3
   efe28fd5 OpenSpecFun_jll v0.5.3+3
   bac558e1 OrderedCollections v1.1.0
   189a3867 Reexport v0.2.0
   ae029012 Requires v1.0.1
   699a6c99 SimpleTraits v0.9.1
   47aef6b3 SimpleWeightedGraphs v1.1.1
   a2af1166 SortingAlgorithms v0.3.1
   276daf66 SpecialFunctions v0.10.0
   90137ffa StaticArrays v0.12.1
   2913bbd2 StatsBase v0.32.2
   a759f4b9 TimerOutputs v0.5.3
   3bb67fe8 TranscodingStreams v0.9.5
   a5390f91 ZipFile v0.9.1
   83775a58 Zlib_jll v1.2.11+8
   e88e6eb3 Zygote v0.4.9
   700de1a5 ZygoteRules v0.2.0
   2a0f44e3 Base64
   ade2ca70 Dates
   8bb1440f DelimitedFiles
   8ba89e20 Distributed
   b77e0a4c InteractiveUtils
   76f85450 LibGit2
   8f399da3 Libdl
   37e2e46d LinearAlgebra
   56ddb016 Logging
   d6f4376e Markdown
   a63ad114 Mmap
   44cfe95a Pkg
   de0858da Printf
   9abbd945 Profile
   3fa0cd96 REPL
   9a3f8284 Random
   ea8e919c SHA
   9e88b42a Serialization
   1a1011a3 SharedArrays
   6462fe0b Sockets
   2f01184e SparseArrays
   10745b16 Statistics
   8dfed614 Test
   cf7118a7 UUIDs
   4ec0a83e Unicode
WARNING: could not import Compiler.just_construct_ssa into Wrap
┌ Warning: CuArrays.jl found cuda, but did not find libcudnn. Some functionality will not be available.
└ @ Flux ~/.julia/packages/Flux/NpkMm/src/Flux.jl:48
┌ Warning: Incompatibility detected between CUDA and LLVM 8.0+; disabling debug info emission for CUDA kernels
└ @ CUDAnative ~/.julia/packages/CUDAnative/hfulr/src/CUDAnative.jl:114
┌ Warning: CuArrays.jl found cuda, but did not find libcudnn. Some functionality will not be available.
└ @ Flux ~/.julia/packages/Flux/NpkMm/src/Flux.jl:48
┌ Warning: Package GeometricFlux does not have LightGraphs in its dependencies:
│ - If you have GeometricFlux checked out for development and have
│   added LightGraphs as a dependency but haven't updated your primary
│   environment's manifest file, try `Pkg.resolve()`.
│ - Otherwise you may need to report an issue with GeometricFlux
└ Loading LightGraphs into GeometricFlux from project dependency, future warnings for GeometricFlux are suppressed.
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:15
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_add!(x, us, xs))
            end), ys) == (ones(2, 5),)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#3#4"{Array{Int64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/scatter.jl:41
   [2] (::GeometricFlux.var"#36#back#5"{GeometricFlux.var"#3#4"{Array{Int64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #4 at ./none:0 [inlined]
   [4] (::typeof(∂(#4)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#4))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Float64,2}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:15
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:15
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:16
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_add!(copy(ys), x, xs))
            end), us) == (ones(2, 3, 4),)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#3#4"{Array{Int64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/scatter.jl:41
   [2] (::GeometricFlux.var"#36#back#5"{GeometricFlux.var"#3#4"{Array{Int64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #5 at ./none:0 [inlined]
   [4] (::typeof(∂(#5)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#5))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Float64,3}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:16
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:15
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:17
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_add!(copy(ys), us, x))
            end), xs) == (nothing,)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#3#4"{Array{Int64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/scatter.jl:41
   [2] (::GeometricFlux.var"#36#back#5"{GeometricFlux.var"#3#4"{Array{Int64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #6 at ./none:0 [inlined]
   [4] (::typeof(∂(#6)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#6))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Int64,2}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:17
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:15
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:19
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_sub!(x, us, xs))
            end), ys) == (ones(2, 5),)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#7#8"{Array{Int64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/scatter.jl:47
   [2] (::GeometricFlux.var"#48#back#9"{GeometricFlux.var"#7#8"{Array{Int64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #7 at ./none:0 [inlined]
   [4] (::typeof(∂(#7)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#7))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Float64,2}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:19
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:15
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:20
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_sub!(copy(ys), x, xs))
            end), us) == (-(ones(2, 3, 4)),)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#7#8"{Array{Int64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/scatter.jl:47
   [2] (::GeometricFlux.var"#48#back#9"{GeometricFlux.var"#7#8"{Array{Int64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #8 at ./none:0 [inlined]
   [4] (::typeof(∂(#8)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#8))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Float64,3}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:20
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:15
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:21
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_sub!(copy(ys), us, x))
            end), xs) == (nothing,)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#7#8"{Array{Int64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/scatter.jl:47
   [2] (::GeometricFlux.var"#48#back#9"{GeometricFlux.var"#7#8"{Array{Int64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #9 at ./none:0 [inlined]
   [4] (::typeof(∂(#9)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#9))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Int64,2}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:21
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:15
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:23
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_max!(x, us, xs))
            end), ys) == (ones(2, 5),)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#27#28"{Array{Float64,2},Array{Float64,3},Array{Int64,2},Array{Float64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/scatter.jl:99
   [2] (::GeometricFlux.var"#90#back#29"{GeometricFlux.var"#27#28"{Array{Float64,2},Array{Float64,3},Array{Int64,2},Array{Float64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #10 at ./none:0 [inlined]
   [4] (::typeof(∂(#10)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#10))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Float64,2}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:23
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:15
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:24
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_max!(copy(ys), x, xs))
            end), us) == (zeros(2, 3, 4),)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#27#28"{Array{Float64,2},Array{Float64,3},Array{Int64,2},Array{Float64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/scatter.jl:99
   [2] (::GeometricFlux.var"#90#back#29"{GeometricFlux.var"#27#28"{Array{Float64,2},Array{Float64,3},Array{Int64,2},Array{Float64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #11 at ./none:0 [inlined]
   [4] (::typeof(∂(#11)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#11))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Float64,3}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:24
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:15
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:25
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_max!(copy(ys), us, x))
            end), xs) == (nothing,)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#27#28"{Array{Float64,2},Array{Float64,3},Array{Int64,2},Array{Float64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/scatter.jl:99
   [2] (::GeometricFlux.var"#90#back#29"{GeometricFlux.var"#27#28"{Array{Float64,2},Array{Float64,3},Array{Int64,2},Array{Float64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #12 at ./none:0 [inlined]
   [4] (::typeof(∂(#12)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#12))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Int64,2}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:25
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:15
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:27
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_min!(x, us, xs))
            end), ys) == (zeros(2, 5),)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#31#32"{Array{Float64,2},Array{Float64,3},Array{Int64,2},Array{Float64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/scatter.jl:109
   [2] (::GeometricFlux.var"#102#back#33"{GeometricFlux.var"#31#32"{Array{Float64,2},Array{Float64,3},Array{Int64,2},Array{Float64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #13 at ./none:0 [inlined]
   [4] (::typeof(∂(#13)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#13))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Float64,2}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:27
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:15
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:28
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_min!(copy(ys), x, xs))
            end), us) == (ones(2, 3, 4),)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#31#32"{Array{Float64,2},Array{Float64,3},Array{Int64,2},Array{Float64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/scatter.jl:109
   [2] (::GeometricFlux.var"#102#back#33"{GeometricFlux.var"#31#32"{Array{Float64,2},Array{Float64,3},Array{Int64,2},Array{Float64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #14 at ./none:0 [inlined]
   [4] (::typeof(∂(#14)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#14))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Float64,3}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:28
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:15
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:29
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_min!(copy(ys), us, x))
            end), xs) == (nothing,)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#31#32"{Array{Float64,2},Array{Float64,3},Array{Int64,2},Array{Float64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/scatter.jl:109
   [2] (::GeometricFlux.var"#102#back#33"{GeometricFlux.var"#31#32"{Array{Float64,2},Array{Float64,3},Array{Int64,2},Array{Float64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #15 at ./none:0 [inlined]
   [4] (::typeof(∂(#15)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#15))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Int64,2}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:29
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:15
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:31
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_mul!(x, us, xs))
            end), ys) == (∇y_mul,)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#11#14"{Array{Float64,2},Array{Float64,3},Array{Int64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/scatter.jl:57
   [2] (::GeometricFlux.var"#61#back#17"{GeometricFlux.var"#11#14"{Array{Float64,2},Array{Float64,3},Array{Int64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #16 at ./none:0 [inlined]
   [4] (::typeof(∂(#16)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#16))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Float64,2}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:31
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:15
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:32
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_mul!(copy(ys), x, xs))
            end), us) == (2048 * gather(ys, xs),)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#11#14"{Array{Float64,2},Array{Float64,3},Array{Int64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/scatter.jl:57
   [2] (::GeometricFlux.var"#61#back#17"{GeometricFlux.var"#11#14"{Array{Float64,2},Array{Float64,3},Array{Int64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #17 at ./none:0 [inlined]
   [4] (::typeof(∂(#17)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#17))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Float64,3}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:32
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:15
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:33
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_mul!(copy(ys), us, x))
            end), xs) == (nothing,)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#11#14"{Array{Float64,2},Array{Float64,3},Array{Int64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/scatter.jl:57
   [2] (::GeometricFlux.var"#61#back#17"{GeometricFlux.var"#11#14"{Array{Float64,2},Array{Float64,3},Array{Int64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #18 at ./none:0 [inlined]
   [4] (::typeof(∂(#18)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#18))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Int64,2}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:33
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:15
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:35
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_div!(x, us, xs))
            end), ys) == (∇y_div,)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#19#22"{Array{Float64,2},Array{Float64,3},Array{Int64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/scatter.jl:75
   [2] (::GeometricFlux.var"#77#back#25"{GeometricFlux.var"#19#22"{Array{Float64,2},Array{Float64,3},Array{Int64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #19 at ./none:0 [inlined]
   [4] (::typeof(∂(#19)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#19))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Float64,2}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:35
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:15
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:36
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_div!(copy(ys), x, xs))
            end), us) == (-(gather(ys, xs)) / 8192,)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#19#22"{Array{Float64,2},Array{Float64,3},Array{Int64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/scatter.jl:75
   [2] (::GeometricFlux.var"#77#back#25"{GeometricFlux.var"#19#22"{Array{Float64,2},Array{Float64,3},Array{Int64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #20 at ./none:0 [inlined]
   [4] (::typeof(∂(#20)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#20))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Float64,3}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:36
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:15
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:37
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_div!(copy(ys), us, x))
            end), xs) == (nothing,)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#19#22"{Array{Float64,2},Array{Float64,3},Array{Int64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/scatter.jl:75
   [2] (::GeometricFlux.var"#77#back#25"{GeometricFlux.var"#19#22"{Array{Float64,2},Array{Float64,3},Array{Int64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #21 at ./none:0 [inlined]
   [4] (::typeof(∂(#21)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#21))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Int64,2}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:37
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:15
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:39
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_mean!(x, us, xs))
            end), ys) == (ones(2, 5),)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#35#36"{Array{Float64,2},Array{Int64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/scatter.jl:118
   [2] (::GeometricFlux.var"#116#back#37"{GeometricFlux.var"#35#36"{Array{Float64,2},Array{Int64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #22 at ./none:0 [inlined]
   [4] (::typeof(∂(#22)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#22))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Float64,2}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:39
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:15
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:40
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_mean!(copy(ys), x, xs))
            end), us) == (∇u_mean,)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#35#36"{Array{Float64,2},Array{Int64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/scatter.jl:118
   [2] (::GeometricFlux.var"#116#back#37"{GeometricFlux.var"#35#36"{Array{Float64,2},Array{Int64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #23 at ./none:0 [inlined]
   [4] (::typeof(∂(#23)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#23))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Float64,3}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:40
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:15
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:41
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_mean!(copy(ys), us, x))
            end), xs) == (nothing,)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#35#36"{Array{Float64,2},Array{Int64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/scatter.jl:118
   [2] (::GeometricFlux.var"#116#back#37"{GeometricFlux.var"#35#36"{Array{Float64,2},Array{Int64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #24 at ./none:0 [inlined]
   [4] (::typeof(∂(#24)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#24))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Int64,2}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:41
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:15
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:45
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(sumpool(x, us))
            end), xs) == (nothing,)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#88#89"{Array{Int64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/layers/pool.jl:108
   [2] (::GeometricFlux.var"#170#back#90"{GeometricFlux.var"#88#89"{Array{Int64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #25 at ./none:0 [inlined]
   [4] (::typeof(∂(#25)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#25))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Int64,2}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:45
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:45
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:46
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(sumpool(xs, x))
            end), us) == (ones(2, 3, 4),)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#88#89"{Array{Int64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/layers/pool.jl:108
   [2] (::GeometricFlux.var"#170#back#90"{GeometricFlux.var"#88#89"{Array{Int64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #26 at ./none:0 [inlined]
   [4] (::typeof(∂(#26)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#26))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Float64,3}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:46
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:45
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:48
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(subpool(x, us))
            end), xs) == (nothing,)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#92#93"{Array{Int64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/layers/pool.jl:110
   [2] (::GeometricFlux.var"#182#back#94"{GeometricFlux.var"#92#93"{Array{Int64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #27 at ./none:0 [inlined]
   [4] (::typeof(∂(#27)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#27))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Int64,2}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:48
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:45
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:49
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(subpool(xs, x))
            end), us) == (-(ones(2, 3, 4)),)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#92#93"{Array{Int64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/layers/pool.jl:110
   [2] (::GeometricFlux.var"#182#back#94"{GeometricFlux.var"#92#93"{Array{Int64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #28 at ./none:0 [inlined]
   [4] (::typeof(∂(#28)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#28))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Float64,3}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:49
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:45
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:51
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(maxpool(x, us))
            end), xs) == (nothing,)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#112#113"{Array{Int64,2},Array{Float64,3},Array{Float64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/layers/pool.jl:143
   [2] (::GeometricFlux.var"#220#back#114"{GeometricFlux.var"#112#113"{Array{Int64,2},Array{Float64,3},Array{Float64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #29 at ./none:0 [inlined]
   [4] (::typeof(∂(#29)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#29))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Int64,2}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:51
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:45
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:52
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(maxpool(xs, x))
            end), us) == (ones(2, 3, 4),)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#112#113"{Array{Int64,2},Array{Float64,3},Array{Float64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/layers/pool.jl:143
   [2] (::GeometricFlux.var"#220#back#114"{GeometricFlux.var"#112#113"{Array{Int64,2},Array{Float64,3},Array{Float64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #30 at ./none:0 [inlined]
   [4] (::typeof(∂(#30)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#30))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Float64,3}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:52
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:45
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:54
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(minpool(x, us))
            end), xs) == (nothing,)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#116#117"{Array{Int64,2},Array{Float64,3},Array{Float64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/layers/pool.jl:151
   [2] (::GeometricFlux.var"#232#back#118"{GeometricFlux.var"#116#117"{Array{Int64,2},Array{Float64,3},Array{Float64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #31 at ./none:0 [inlined]
   [4] (::typeof(∂(#31)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#31))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Int64,2}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:54
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:45
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:55
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(minpool(xs, x))
            end), us) == (ones(2, 3, 4),)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#116#117"{Array{Int64,2},Array{Float64,3},Array{Float64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/layers/pool.jl:151
   [2] (::GeometricFlux.var"#232#back#118"{GeometricFlux.var"#116#117"{Array{Int64,2},Array{Float64,3},Array{Float64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #32 at ./none:0 [inlined]
   [4] (::typeof(∂(#32)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#32))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Float64,3}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:55
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:45
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:57
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(prodpool(x, us))
            end), xs) == (nothing,)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#96#99"{Array{Int64,2},Array{Float64,3}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/layers/pool.jl:115
   [2] (::GeometricFlux.var"#195#back#102"{GeometricFlux.var"#96#99"{Array{Int64,2},Array{Float64,3}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #33 at ./none:0 [inlined]
   [4] (::typeof(∂(#33)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#33))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Int64,2}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:57
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:45
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:58
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(prodpool(xs, x))
            end), us) == (2048 * ones(2, 3, 4),)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#96#99"{Array{Int64,2},Array{Float64,3}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/layers/pool.jl:115
   [2] (::GeometricFlux.var"#195#back#102"{GeometricFlux.var"#96#99"{Array{Int64,2},Array{Float64,3}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #34 at ./none:0 [inlined]
   [4] (::typeof(∂(#34)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#34))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Float64,3}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:58
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:45
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:60
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(divpool(x, us))
            end), xs) == (nothing,)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#104#107"{Array{Int64,2},Array{Float64,3}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/layers/pool.jl:129
   [2] (::GeometricFlux.var"#208#back#110"{GeometricFlux.var"#104#107"{Array{Int64,2},Array{Float64,3}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #35 at ./none:0 [inlined]
   [4] (::typeof(∂(#35)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#35))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Int64,2}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:60
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:45
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:61
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(divpool(xs, x))
            end), us) == (-(ones(2, 3, 4)) / 8192,)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#104#107"{Array{Int64,2},Array{Float64,3}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/layers/pool.jl:129
   [2] (::GeometricFlux.var"#208#back#110"{GeometricFlux.var"#104#107"{Array{Int64,2},Array{Float64,3}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #36 at ./none:0 [inlined]
   [4] (::typeof(∂(#36)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#36))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Float64,3}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:61
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:45
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:63
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(meanpool(x, us))
            end), xs) == (nothing,)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#120#121"{Array{Int64,2},Array{Float64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/layers/pool.jl:159
   [2] (::GeometricFlux.var"#246#back#122"{GeometricFlux.var"#120#121"{Array{Int64,2},Array{Float64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #37 at ./none:0 [inlined]
   [4] (::typeof(∂(#37)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#37))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Int64,2}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:63
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:45
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:64
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(meanpool(xs, x))
            end), us) == (∇u_mean,)
  MethodError: no method matching gather(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, ::Array{Int64,2})
  Closest candidates are:
    gather(::AbstractArray{T,N}, ::AbstractArray{var"#s27",N} where var"#s27"<:Integer, !Matched::Integer; out) where {T, N} at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:3
    gather(!Matched::Array{T,2}, ::Array{Int64,N} where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/utils.jl:15
    gather(::AbstractArray{T,N} where N, !Matched::CuArray{Int64,N,P} where P where N) where T at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/cuda/utils.jl:1
  Stacktrace:
   [1] (::GeometricFlux.var"#120#121"{Array{Int64,2},Array{Float64,2}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/src/layers/pool.jl:159
   [2] (::GeometricFlux.var"#246#back#122"{GeometricFlux.var"#120#121"{Array{Int64,2},Array{Float64,2}}})(::FillArrays.Fill{Float64,2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}) at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49
   [3] #38 at ./none:0 [inlined]
   [4] (::typeof(∂(#38)))(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface2.jl:0
   [5] (::Zygote.var"#36#37"{typeof(∂(#38))})(::Float64) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:36
   [6] gradient(::Function, ::Array{Float64,3}) at /home/pkgeval/.julia/packages/Zygote/ApBXe/src/compiler/interface.jl:45
   [7] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:64
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [9] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:45
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114
   [11] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/grad.jl:14
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
add: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:17
  Test threw exception
  Expression: scatter_add!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:17 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:15 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
add: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:18
  Test threw exception
  Expression: scatter!(:add, T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:18 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:15 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
sub: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:24
  Test threw exception
  Expression: scatter_sub!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:24 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:22 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
sub: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:25
  Test threw exception
  Expression: scatter!(:sub, T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:25 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:22 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
max: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:31
  Test threw exception
  Expression: scatter_max!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:31 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:29 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
max: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:32
  Test threw exception
  Expression: scatter!(:max, T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:32 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:29 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
min: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:38
  Test threw exception
  Expression: scatter_min!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:38 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:36 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
min: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:39
  Test threw exception
  Expression: scatter!(:min, T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:39 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:36 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
add: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:17
  Test threw exception
  Expression: scatter_add!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:17 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:15 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
add: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:18
  Test threw exception
  Expression: scatter!(:add, T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:18 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:15 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
sub: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:24
  Test threw exception
  Expression: scatter_sub!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:24 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:22 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
sub: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:25
  Test threw exception
  Expression: scatter!(:sub, T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:25 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:22 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
max: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:31
  Test threw exception
  Expression: scatter_max!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:31 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:29 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
max: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:32
  Test threw exception
  Expression: scatter!(:max, T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:32 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:29 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
min: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:38
  Test threw exception
  Expression: scatter_min!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:38 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:36 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
min: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:39
  Test threw exception
  Expression: scatter!(:min, T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:39 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:36 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
add: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:17
  Test threw exception
  Expression: scatter_add!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:17 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:15 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
add: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:18
  Test threw exception
  Expression: scatter!(:add, T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:18 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:15 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
sub: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:24
  Test threw exception
  Expression: scatter_sub!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:24 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:22 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
sub: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:25
  Test threw exception
  Expression: scatter!(:sub, T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:25 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:22 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
max: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:31
  Test threw exception
  Expression: scatter_max!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:31 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:29 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
max: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:32
  Test threw exception
  Expression: scatter!(:max, T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:32 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:29 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
min: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:38
  Test threw exception
  Expression: scatter_min!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:38 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:36 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
min: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:39
  Test threw exception
  Expression: scatter!(:min, T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:39 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:36 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
add: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:17
  Test threw exception
  Expression: scatter_add!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:17 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:15 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
add: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:18
  Test threw exception
  Expression: scatter!(:add, T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:18 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:15 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
sub: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:24
  Test threw exception
  Expression: scatter_sub!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:24 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:22 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
sub: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:25
  Test threw exception
  Expression: scatter!(:sub, T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:25 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:22 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
max: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:31
  Test threw exception
  Expression: scatter_max!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:31 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:29 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
max: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:32
  Test threw exception
  Expression: scatter!(:max, T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:32 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:29 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
min: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:38
  Test threw exception
  Expression: scatter_min!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:38 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:36 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
min: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:39
  Test threw exception
  Expression: scatter!(:min, T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:39 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:36 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:14 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
add: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:50
  Test threw exception
  Expression: scatter_add!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:50 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:48 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
add: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:51
  Test threw exception
  Expression: scatter!(:add, T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:51 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:48 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
sub: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:57
  Test threw exception
  Expression: scatter_sub!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:57 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:55 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
sub: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:58
  Test threw exception
  Expression: scatter!(:sub, T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:58 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:55 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
max: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:64
  Test threw exception
  Expression: scatter_max!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:64 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:62 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
max: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:65
  Test threw exception
  Expression: scatter!(:max, T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:65 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:62 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
min: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:71
  Test threw exception
  Expression: scatter_min!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:71 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:69 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
min: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:72
  Test threw exception
  Expression: scatter!(:min, T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:72 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:69 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
mul: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:78
  Test threw exception
  Expression: scatter_mul!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:78 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:76 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
mul: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:79
  Test threw exception
  Expression: scatter!(:mul, T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:79 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:76 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
div: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:82
  Got exception outside of a @test
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},typeof(*),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}},Int64}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},typeof(*),Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}},Int64}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},typeof(*),Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}},Int64}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},typeof(*),Tuple{CuArray{Int64,3,Nothing},Int64}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(*),Tuple{CuArray{Int64,3,Nothing},Int64}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:83 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:83 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
   [35] include(::String) at ./client.jl:441
   [36] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/runtests.jl:52 [inlined]
   [37] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [38] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/runtests.jl:51
   [39] include(::String) at ./client.jl:441
   [40] top-level scope at none:6
   [41] eval(::Module, ::Any) at ./boot.jl:331
   [42] exec_options(::Base.JLOptions) at ./client.jl:264
   [43] _start() at ./client.jl:490
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
mean: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:93
  Test threw exception
  Expression: scatter_mean!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:93 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:91 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
mean: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:94
  Test threw exception
  Expression: scatter!(:mean, T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:94 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:91 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
add: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:50
  Test threw exception
  Expression: scatter_add!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:50 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:48 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
add: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:51
  Test threw exception
  Expression: scatter!(:add, T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:51 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:48 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
sub: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:57
  Test threw exception
  Expression: scatter_sub!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:57 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:55 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
sub: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:58
  Test threw exception
  Expression: scatter!(:sub, T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:58 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:55 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
max: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:64
  Test threw exception
  Expression: scatter_max!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:64 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:62 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
max: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:65
  Test threw exception
  Expression: scatter!(:max, T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:65 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:62 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
min: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:71
  Test threw exception
  Expression: scatter_min!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:71 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:69 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
min: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:72
  Test threw exception
  Expression: scatter!(:min, T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:72 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:69 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
mul: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:78
  Test threw exception
  Expression: scatter_mul!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:78 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:76 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
mul: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:79
  Test threw exception
  Expression: scatter!(:mul, T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:79 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:76 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
div: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:82
  Got exception outside of a @test
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},typeof(*),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}},Int64}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},typeof(*),Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}},Int64}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},typeof(*),Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}},Int64}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},typeof(*),Tuple{CuArray{Int64,3,Nothing},Int64}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(*),Tuple{CuArray{Int64,3,Nothing},Int64}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:83 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:83 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
   [35] include(::String) at ./client.jl:441
   [36] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/runtests.jl:52 [inlined]
   [37] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [38] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/runtests.jl:51
   [39] include(::String) at ./client.jl:441
   [40] top-level scope at none:6
   [41] eval(::Module, ::Any) at ./boot.jl:331
   [42] exec_options(::Base.JLOptions) at ./client.jl:264
   [43] _start() at ./client.jl:490
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
mean: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:93
  Test threw exception
  Expression: scatter_mean!(T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:93 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:91 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
mean: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:94
  Test threw exception
  Expression: scatter!(:mean, T.(copy(ys)), T.(us), xs) == T.(ys_)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,2,Nothing}, ::Tuple{CuArray{Int64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Base.Broadcast.Extruded{CuArray{Int64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48
   [25] copyto!(::CuArray{Int64,2,Nothing}, ::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:864
   [26] copy(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:840
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,typeof(identity),Tuple{CuArray{Int64,2,Nothing}}}) at ./broadcast.jl:820
   [28] copy(::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstractarray.jl:180
   [29] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:94 [inlined]
   [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [31] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:91 [inlined]
   [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [33] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:47 [inlined]
   [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/scatter.jl:12
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
sumpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:12
  Test threw exception
  Expression: sumpool(CuArray{Int64}(cluster), T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{UInt32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{UInt32,3,Nothing}, ::Tuple{CuArray{UInt32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{UInt32,3,Nothing}, ::Tuple{CuArray{UInt32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{UInt32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:12 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:10 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:9 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
sumpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:13
  Test threw exception
  Expression: pool(:add, CuArray{Int64}(cluster), T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{UInt32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{UInt32,3,Nothing}, ::Tuple{CuArray{UInt32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{UInt32,3,Nothing}, ::Tuple{CuArray{UInt32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{UInt32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:13 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:10 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:9 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
sumpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:14
  Test threw exception
  Expression: sumpool(cluster, T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{UInt32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{UInt32,3,Nothing}, ::Tuple{CuArray{UInt32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{UInt32,3,Nothing}, ::Tuple{CuArray{UInt32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{UInt32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:14 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:10 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:9 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
sumpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:15
  Test threw exception
  Expression: pool(:add, cluster, T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{UInt32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{UInt32,3,Nothing}, ::Tuple{CuArray{UInt32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{UInt32,3,Nothing}, ::Tuple{CuArray{UInt32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{UInt32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:15 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:10 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:9 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
maxpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:21
  Test threw exception
  Expression: maxpool(CuArray{Int64}(cluster), T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{UInt32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{UInt32,3,Nothing}, ::Tuple{CuArray{UInt32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{UInt32,3,Nothing}, ::Tuple{CuArray{UInt32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{UInt32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:21 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:19 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:9 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
maxpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:22
  Test threw exception
  Expression: pool(:max, CuArray{Int64}(cluster), T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{UInt32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{UInt32,3,Nothing}, ::Tuple{CuArray{UInt32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{UInt32,3,Nothing}, ::Tuple{CuArray{UInt32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{UInt32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:22 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:19 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:9 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
maxpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:23
  Test threw exception
  Expression: maxpool(cluster, T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{UInt32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{UInt32,3,Nothing}, ::Tuple{CuArray{UInt32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{UInt32,3,Nothing}, ::Tuple{CuArray{UInt32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{UInt32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:23 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:19 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:9 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
maxpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:24
  Test threw exception
  Expression: pool(:max, cluster, T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{UInt32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{UInt32,3,Nothing}, ::Tuple{CuArray{UInt32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{UInt32,3,Nothing}, ::Tuple{CuArray{UInt32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{UInt32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:24 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:19 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:9 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
minpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:30
  Test threw exception
  Expression: minpool(CuArray{Int64}(cluster), T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{UInt32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{UInt32,3,Nothing}, ::Tuple{CuArray{UInt32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{UInt32,3,Nothing}, ::Tuple{CuArray{UInt32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{UInt32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:30 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:28 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:9 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
minpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:31
  Test threw exception
  Expression: pool(:min, CuArray{Int64}(cluster), T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{UInt32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{UInt32,3,Nothing}, ::Tuple{CuArray{UInt32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{UInt32,3,Nothing}, ::Tuple{CuArray{UInt32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{UInt32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:31 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:28 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:9 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
minpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:32
  Test threw exception
  Expression: minpool(cluster, T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{UInt32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{UInt32,3,Nothing}, ::Tuple{CuArray{UInt32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{UInt32,3,Nothing}, ::Tuple{CuArray{UInt32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{UInt32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:32 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:28 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:9 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
minpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:33
  Test threw exception
  Expression: pool(:min, cluster, T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{UInt32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{UInt32,3,Nothing}, ::Tuple{CuArray{UInt32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{UInt32,3,Nothing}, ::Tuple{CuArray{UInt32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{UInt32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:33 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:28 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:9 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
sumpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:12
  Test threw exception
  Expression: sumpool(CuArray{Int64}(cluster), T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{UInt64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{UInt64,3,Nothing}, ::Tuple{CuArray{UInt64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{UInt64,3,Nothing}, ::Tuple{CuArray{UInt64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{UInt64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:12 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:10 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:9 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
sumpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:13
  Test threw exception
  Expression: pool(:add, CuArray{Int64}(cluster), T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{UInt64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{UInt64,3,Nothing}, ::Tuple{CuArray{UInt64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{UInt64,3,Nothing}, ::Tuple{CuArray{UInt64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{UInt64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:13 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:10 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:9 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
sumpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:14
  Test threw exception
  Expression: sumpool(cluster, T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{UInt64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{UInt64,3,Nothing}, ::Tuple{CuArray{UInt64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{UInt64,3,Nothing}, ::Tuple{CuArray{UInt64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{UInt64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:14 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:10 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:9 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
sumpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:15
  Test threw exception
  Expression: pool(:add, cluster, T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{UInt64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{UInt64,3,Nothing}, ::Tuple{CuArray{UInt64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{UInt64,3,Nothing}, ::Tuple{CuArray{UInt64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{UInt64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:15 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:10 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:9 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
maxpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:21
  Test threw exception
  Expression: maxpool(CuArray{Int64}(cluster), T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{UInt64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{UInt64,3,Nothing}, ::Tuple{CuArray{UInt64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{UInt64,3,Nothing}, ::Tuple{CuArray{UInt64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{UInt64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:21 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:19 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:9 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
maxpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:22
  Test threw exception
  Expression: pool(:max, CuArray{Int64}(cluster), T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{UInt64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{UInt64,3,Nothing}, ::Tuple{CuArray{UInt64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{UInt64,3,Nothing}, ::Tuple{CuArray{UInt64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{UInt64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:22 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:19 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:9 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
maxpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:23
  Test threw exception
  Expression: maxpool(cluster, T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{UInt64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{UInt64,3,Nothing}, ::Tuple{CuArray{UInt64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{UInt64,3,Nothing}, ::Tuple{CuArray{UInt64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{UInt64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:23 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:19 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:9 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
maxpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:24
  Test threw exception
  Expression: pool(:max, cluster, T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{UInt64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{UInt64,3,Nothing}, ::Tuple{CuArray{UInt64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{UInt64,3,Nothing}, ::Tuple{CuArray{UInt64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{UInt64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:24 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:19 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:9 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
minpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:30
  Test threw exception
  Expression: minpool(CuArray{Int64}(cluster), T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{UInt64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{UInt64,3,Nothing}, ::Tuple{CuArray{UInt64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{UInt64,3,Nothing}, ::Tuple{CuArray{UInt64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{UInt64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:30 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:28 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:9 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
minpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:31
  Test threw exception
  Expression: pool(:min, CuArray{Int64}(cluster), T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{UInt64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{UInt64,3,Nothing}, ::Tuple{CuArray{UInt64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{UInt64,3,Nothing}, ::Tuple{CuArray{UInt64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{UInt64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:31 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:28 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:9 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
minpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:32
  Test threw exception
  Expression: minpool(cluster, T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{UInt64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{UInt64,3,Nothing}, ::Tuple{CuArray{UInt64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{UInt64,3,Nothing}, ::Tuple{CuArray{UInt64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{UInt64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:32 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:28 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:9 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
minpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:33
  Test threw exception
  Expression: pool(:min, cluster, T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{UInt64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{UInt64,3,Nothing}, ::Tuple{CuArray{UInt64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{UInt64,3,Nothing}, ::Tuple{CuArray{UInt64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{UInt64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{UInt64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:33 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:28 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:9 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
sumpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:43
  Test threw exception
  Expression: sumpool(CuArray{Int64}(cluster), T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:43 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:41 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
sumpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:44
  Test threw exception
  Expression: pool(:add, CuArray{Int64}(cluster), T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:44 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:41 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
sumpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:45
  Test threw exception
  Expression: sumpool(cluster, T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:45 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:41 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
sumpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:46
  Test threw exception
  Expression: pool(:add, cluster, T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:46 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:41 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
subpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:52
  Test threw exception
  Expression: subpool(CuArray{Int64}(cluster), T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:52 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:50 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
subpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:53
  Test threw exception
  Expression: pool(:sub, CuArray{Int64}(cluster), T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:53 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:50 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
subpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:54
  Test threw exception
  Expression: subpool(cluster, T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:54 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:50 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
subpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:55
  Test threw exception
  Expression: pool(:sub, cluster, T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:55 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:50 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
maxpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:61
  Test threw exception
  Expression: maxpool(CuArray{Int64}(cluster), T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:61 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:59 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
maxpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:62
  Test threw exception
  Expression: pool(:max, CuArray{Int64}(cluster), T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:62 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:59 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
maxpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:63
  Test threw exception
  Expression: maxpool(cluster, T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:63 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:59 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
maxpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:64
  Test threw exception
  Expression: pool(:max, cluster, T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:64 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:59 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
minpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:70
  Test threw exception
  Expression: minpool(CuArray{Int64}(cluster), T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:70 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:68 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
minpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:71
  Test threw exception
  Expression: pool(:min, CuArray{Int64}(cluster), T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:71 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:68 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
minpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:72
  Test threw exception
  Expression: minpool(cluster, T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:72 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:68 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
minpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:73
  Test threw exception
  Expression: pool(:min, cluster, T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int32,3,Nothing}, ::Tuple{CuArray{Int32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:73 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:68 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
sumpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:43
  Test threw exception
  Expression: sumpool(CuArray{Int64}(cluster), T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:43 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:41 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
sumpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:44
  Test threw exception
  Expression: pool(:add, CuArray{Int64}(cluster), T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:44 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:41 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
sumpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:45
  Test threw exception
  Expression: sumpool(cluster, T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:45 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:41 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
sumpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:46
  Test threw exception
  Expression: pool(:add, cluster, T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:46 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:41 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
subpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:52
  Test threw exception
  Expression: subpool(CuArray{Int64}(cluster), T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:52 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:50 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
subpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:53
  Test threw exception
  Expression: pool(:sub, CuArray{Int64}(cluster), T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:53 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:50 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
subpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:54
  Test threw exception
  Expression: subpool(cluster, T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:54 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:50 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
subpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:55
  Test threw exception
  Expression: pool(:sub, cluster, T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:55 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:50 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
maxpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:61
  Test threw exception
  Expression: maxpool(CuArray{Int64}(cluster), T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:61 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:59 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
maxpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:62
  Test threw exception
  Expression: pool(:max, CuArray{Int64}(cluster), T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:62 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:59 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
maxpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:63
  Test threw exception
  Expression: maxpool(cluster, T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:63 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:59 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
maxpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:64
  Test threw exception
  Expression: pool(:max, cluster, T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:64 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:59 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
minpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:70
  Test threw exception
  Expression: minpool(CuArray{Int64}(cluster), T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:70 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:68 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
minpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:71
  Test threw exception
  Expression: pool(:min, CuArray{Int64}(cluster), T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:71 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:68 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
minpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:72
  Test threw exception
  Expression: minpool(cluster, T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:72 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:68 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
minpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:73
  Test threw exception
  Expression: pool(:min, cluster, T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Int64,3,Nothing}, ::Tuple{CuArray{Int64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Int64},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Int64},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:73 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:68 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:40 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
sumpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:83
  Test threw exception
  Expression: sumpool(CuArray{Int64}(cluster), T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:83 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:81 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
sumpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:84
  Test threw exception
  Expression: pool(:add, CuArray{Int64}(cluster), T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:84 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:81 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
sumpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:85
  Test threw exception
  Expression: sumpool(cluster, T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:85 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:81 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
sumpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:86
  Test threw exception
  Expression: pool(:add, cluster, T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:86 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:81 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
subpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:92
  Test threw exception
  Expression: subpool(CuArray{Int64}(cluster), T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:92 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:90 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
subpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:93
  Test threw exception
  Expression: pool(:sub, CuArray{Int64}(cluster), T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:93 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:90 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
subpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:94
  Test threw exception
  Expression: subpool(cluster, T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:94 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:90 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
subpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:95
  Test threw exception
  Expression: pool(:sub, cluster, T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:95 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:90 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
maxpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:101
  Test threw exception
  Expression: maxpool(CuArray{Int64}(cluster), T.(X)) == T.(y)
  MethodError: no method matching Base.CodegenParams(; cached=false, track_allocations=false, code_coverage=false, static_alloc=false, prefer_specsig=true, module_setup=CUDAnative.var"#hook_module_setup#95"(Core.Box(#undef)), module_activation=CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), CUDAnative.var"#postprocess#94"(), Core.Box(nothing), DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}(Dict{Core.MethodInstance,Array{LLVM.Function,1}}()), Core.Box(#undef), Core.Box(#undef)), emit_function=CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.MethodInstance[]), emitted_function=CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}}(CUDAnative.CompilerJob(CUDAnative.Runtime.unbox_uint64, Tuple{Any}, v"7.5.0", false, nothing, nothing, nothing, nothing, nothing), Core.Box(nothing), Core.MethodInstance[]), gnu_pubnames=false, debug_info_kind=0)
  Closest candidates are:
    Base.CodegenParams(; track_allocations, code_coverage, static_alloc, prefer_specsig, gnu_pubnames, debug_info_kind, module_setup, module_activation, raise_exception, emit_function, emitted_function) at reflection.jl:986 got unsupported keyword argument "cached"
  Stacktrace:
   [1] kwerr(::NamedTuple{(:cached, :track_allocations, :code_coverage, :static_alloc, :prefer_specsig, :module_setup, :module_activation, :emit_function, :emitted_function, :gnu_pubnames, :debug_info_kind),Tuple{Bool,Bool,Bool,Bool,Bool,CUDAnative.var"#hook_module_setup#95",CUDAnative.var"#hook_module_activation#96"{CUDAnative.CompilerJob,CUDAnative.var"#postprocess#94",DataStructures.MultiDict{Core.MethodInstance,LLVM.Function}},CUDAnative.var"#hook_emit_function#99"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},CUDAnative.var"#hook_emitted_function#100"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}},Bool,Int32}}, ::Type{T} where T) at ./error.jl:157
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:148
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/irgen.jl:165
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:104 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:103
   [9] emit_function!(::LLVM.Module, ::VersionNumber, ::Function, ::Tuple{DataType}, ::String) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:144
   [10] build_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:154
   [11] (::CUDAnative.var"#141#144"{VersionNumber,String})() at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:189
   [12] get!(::CUDAnative.var"#141#144"{VersionNumber,String}, ::Dict{String,LLVM.Module}, ::String) at ./dict.jl:450
   [13] load_runtime(::VersionNumber) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/rtlib.jl:182
   [14] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:99
   [15] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:52
   [16] #compile#152 at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/compiler/driver.jl:33 [inlined]
   [17] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:393 [inlined]
   [18] cufunction(::GPUArrays.var"#25#26", ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float32,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Int64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [19] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:360
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/hfulr/src/execution.jl:179 [inlined]
   [21] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/pkgeval/.julia/packages/CuArrays/A6GUx/src/gpuarray_interface.jl:62
   [22] gpu_call(::Function, ::CuArray{Float32,3,Nothing}, ::Tuple{CuArray{Float32,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},CuArrays.var"#48#49"{Float32},Tuple{Base.Broadcast.Extruded{CuArray{Int64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64) at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:151
   [23] gpu_call at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/abstract_gpu_interface.jl:128 [inlined]
   [24] copyto! at /home/pkgeval/.julia/packages/GPUArrays/1wgPO/src/broadcast.jl:48 [inlined]
   [25] copyto! at ./broadcast.jl:864 [inlined]
   [26] copy at ./broadcast.jl:840 [inlined]
   [27] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.ArrayStyle{CuArray},Nothing,CuArrays.var"#48#49"{Float32},Tuple{CuArray{Int64,3,Nothing}}}) at ./broadcast.jl:820
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:101 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:99 [inlined]
   [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:80 [inlined]
   [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1114 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/5F8t4/test/cuda/pool.jl:7
  
[ Info: Building the CUDAnative run-time library for your sm_75 device, this might take a while...
