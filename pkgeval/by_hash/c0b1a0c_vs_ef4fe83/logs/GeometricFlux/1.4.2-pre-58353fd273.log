Julia Version 1.4.2-pre.13
Commit 58353fd273 (2020-05-10 17:03 UTC)
Platform Info:
  OS: Linux (x86_64-linux-gnu)
  CPU: Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-8.0.1 (ORCJIT, skylake)
Environment:
  JULIA_DEPOT_PATH = ::/usr/local/share/julia
  JULIA_NUM_THREADS = 2

  Resolving package versions...
  Installed ZipFile ────────────────────── v0.9.1
  Installed SpecialFunctions ───────────── v0.10.0
  Installed ExprTools ──────────────────── v0.1.1
  Installed OrderedCollections ─────────── v1.2.0
  Installed GPUCompiler ────────────────── v0.2.0
  Installed LLVM ───────────────────────── v1.4.1
  Installed GeometricFlux ──────────────── v0.5.0
  Installed CUDAnative ─────────────────── v3.1.0
  Installed Reexport ───────────────────── v0.2.0
  Installed CUDAapi ────────────────────── v4.0.0
  Installed Cthulhu ────────────────────── v1.0.2
  Installed DataAPI ────────────────────── v1.3.0
  Installed MacroTools ─────────────────── v0.5.5
  Installed NaNMath ────────────────────── v0.3.3
  Installed Missings ───────────────────── v0.4.3
  Installed Media ──────────────────────── v0.5.0
  Installed Juno ───────────────────────── v0.8.1
  Installed Zlib_jll ───────────────────── v1.2.11+9
  Installed AbstractTrees ──────────────── v0.3.3
  Installed OpenSpecFun_jll ────────────── v0.5.3+3
  Installed SortingAlgorithms ──────────── v0.3.1
  Installed CompilerSupportLibraries_jll ─ v0.3.3+0
  Installed Zygote ─────────────────────── v0.4.20
  Installed TranscodingStreams ─────────── v0.9.5
  Installed ColorTypes ─────────────────── v0.10.3
  Installed DiffResults ────────────────── v1.0.2
  Installed ArrayLayouts ───────────────── v0.2.6
  Installed CEnum ──────────────────────── v0.3.0
  Installed DataStructures ─────────────── v0.17.15
  Installed GPUArrays ──────────────────── v3.3.0
  Installed IRTools ────────────────────── v0.3.2
  Installed NNlib ──────────────────────── v0.6.6
  Installed CuArrays ───────────────────── v2.2.0
  Installed Flux ───────────────────────── v0.10.4
  Installed TimerOutputs ───────────────── v0.5.5
  Installed CodecZlib ──────────────────── v0.7.0
  Installed ZygoteRules ────────────────── v0.2.0
  Installed FixedPointNumbers ──────────── v0.8.0
  Installed AbstractFFTs ───────────────── v0.5.0
  Installed Colors ─────────────────────── v0.12.0
  Installed StatsBase ──────────────────── v0.33.0
  Installed StaticArrays ───────────────── v0.12.3
  Installed Adapt ──────────────────────── v1.0.1
  Installed CommonSubexpressions ───────── v0.2.0
  Installed FillArrays ─────────────────── v0.8.9
  Installed Requires ───────────────────── v1.0.1
  Installed DiffRules ──────────────────── v1.0.1
  Installed BinaryProvider ─────────────── v0.5.9
  Installed CUDAdrv ────────────────────── v6.3.0
  Installed ForwardDiff ────────────────── v0.10.10
  Installed CodeTracking ───────────────── v0.5.11
   Updating `~/.julia/environments/v1.4/Project.toml`
  [7e08b658] + GeometricFlux v0.5.0
   Updating `~/.julia/environments/v1.4/Manifest.toml`
  [621f4979] + AbstractFFTs v0.5.0
  [1520ce14] + AbstractTrees v0.3.3
  [79e6a3ab] + Adapt v1.0.1
  [4c555306] + ArrayLayouts v0.2.6
  [b99e7846] + BinaryProvider v0.5.9
  [fa961155] + CEnum v0.3.0
  [3895d2a7] + CUDAapi v4.0.0
  [c5f51814] + CUDAdrv v6.3.0
  [be33ccc6] + CUDAnative v3.1.0
  [da1fd8a2] + CodeTracking v0.5.11
  [944b1d66] + CodecZlib v0.7.0
  [3da002f7] + ColorTypes v0.10.3
  [5ae59095] + Colors v0.12.0
  [bbf7d656] + CommonSubexpressions v0.2.0
  [e66e0078] + CompilerSupportLibraries_jll v0.3.3+0
  [f68482b8] + Cthulhu v1.0.2
  [3a865a2d] + CuArrays v2.2.0
  [9a962f9c] + DataAPI v1.3.0
  [864edb3b] + DataStructures v0.17.15
  [163ba53b] + DiffResults v1.0.2
  [b552c78f] + DiffRules v1.0.1
  [e2ba6199] + ExprTools v0.1.1
  [1a297f60] + FillArrays v0.8.9
  [53c48c17] + FixedPointNumbers v0.8.0
  [587475ba] + Flux v0.10.4
  [f6369f11] + ForwardDiff v0.10.10
  [0c68f7d7] + GPUArrays v3.3.0
  [61eb1bfa] + GPUCompiler v0.2.0
  [7e08b658] + GeometricFlux v0.5.0
  [7869d1d1] + IRTools v0.3.2
  [e5e0dc1b] + Juno v0.8.1
  [929cbde3] + LLVM v1.4.1
  [1914dd2f] + MacroTools v0.5.5
  [e89f7d12] + Media v0.5.0
  [e1d29d7a] + Missings v0.4.3
  [872c559c] + NNlib v0.6.6
  [77ba4419] + NaNMath v0.3.3
  [efe28fd5] + OpenSpecFun_jll v0.5.3+3
  [bac558e1] + OrderedCollections v1.2.0
  [189a3867] + Reexport v0.2.0
  [ae029012] + Requires v1.0.1
  [a2af1166] + SortingAlgorithms v0.3.1
  [276daf66] + SpecialFunctions v0.10.0
  [90137ffa] + StaticArrays v0.12.3
  [2913bbd2] + StatsBase v0.33.0
  [a759f4b9] + TimerOutputs v0.5.5
  [3bb67fe8] + TranscodingStreams v0.9.5
  [a5390f91] + ZipFile v0.9.1
  [83775a58] + Zlib_jll v1.2.11+9
  [e88e6eb3] + Zygote v0.4.20
  [700de1a5] + ZygoteRules v0.2.0
  [2a0f44e3] + Base64 
  [ade2ca70] + Dates 
  [8bb1440f] + DelimitedFiles 
  [8ba89e20] + Distributed 
  [9fa8497b] + Future 
  [b77e0a4c] + InteractiveUtils 
  [76f85450] + LibGit2 
  [8f399da3] + Libdl 
  [37e2e46d] + LinearAlgebra 
  [56ddb016] + Logging 
  [d6f4376e] + Markdown 
  [a63ad114] + Mmap 
  [44cfe95a] + Pkg 
  [de0858da] + Printf 
  [9abbd945] + Profile 
  [3fa0cd96] + REPL 
  [9a3f8284] + Random 
  [ea8e919c] + SHA 
  [9e88b42a] + Serialization 
  [6462fe0b] + Sockets 
  [2f01184e] + SparseArrays 
  [10745b16] + Statistics 
  [8dfed614] + Test 
  [cf7118a7] + UUIDs 
  [4ec0a83e] + Unicode 
   Building NNlib → `~/.julia/packages/NNlib/FAI3o/deps/build.log`
    Testing GeometricFlux
Status `/tmp/jl_CLWpOY/Manifest.toml`
  [621f4979] AbstractFFTs v0.5.0
  [1520ce14] AbstractTrees v0.3.3
  [79e6a3ab] Adapt v1.0.1
  [ec485272] ArnoldiMethod v0.0.4
  [4c555306] ArrayLayouts v0.2.6
  [b99e7846] BinaryProvider v0.5.9
  [fa961155] CEnum v0.3.0
  [3895d2a7] CUDAapi v4.0.0
  [c5f51814] CUDAdrv v6.3.0
  [be33ccc6] CUDAnative v3.1.0
  [da1fd8a2] CodeTracking v0.5.11
  [944b1d66] CodecZlib v0.7.0
  [3da002f7] ColorTypes v0.10.3
  [5ae59095] Colors v0.12.0
  [bbf7d656] CommonSubexpressions v0.2.0
  [e66e0078] CompilerSupportLibraries_jll v0.3.3+0
  [f68482b8] Cthulhu v1.0.2
  [3a865a2d] CuArrays v2.2.0
  [9a962f9c] DataAPI v1.3.0
  [864edb3b] DataStructures v0.17.15
  [163ba53b] DiffResults v1.0.2
  [b552c78f] DiffRules v1.0.1
  [e2ba6199] ExprTools v0.1.1
  [5789e2e9] FileIO v1.3.0
  [1a297f60] FillArrays v0.8.9
  [53c48c17] FixedPointNumbers v0.8.0
  [587475ba] Flux v0.10.4
  [f6369f11] ForwardDiff v0.10.10
  [0c68f7d7] GPUArrays v3.3.0
  [61eb1bfa] GPUCompiler v0.2.0
  [7e08b658] GeometricFlux v0.5.0
  [7869d1d1] IRTools v0.3.2
  [d25df0c9] Inflate v0.1.2
  [033835bb] JLD2 v0.1.13
  [e5e0dc1b] Juno v0.8.1
  [929cbde3] LLVM v1.4.1
  [093fc24a] LightGraphs v1.3.3
  [1914dd2f] MacroTools v0.5.5
  [e89f7d12] Media v0.5.0
  [626554b9] MetaGraphs v0.6.5
  [e1d29d7a] Missings v0.4.3
  [872c559c] NNlib v0.6.6
  [77ba4419] NaNMath v0.3.3
  [efe28fd5] OpenSpecFun_jll v0.5.3+3
  [bac558e1] OrderedCollections v1.2.0
  [189a3867] Reexport v0.2.0
  [ae029012] Requires v1.0.1
  [699a6c99] SimpleTraits v0.9.2
  [47aef6b3] SimpleWeightedGraphs v1.1.1
  [a2af1166] SortingAlgorithms v0.3.1
  [276daf66] SpecialFunctions v0.10.0
  [90137ffa] StaticArrays v0.12.3
  [2913bbd2] StatsBase v0.33.0
  [a759f4b9] TimerOutputs v0.5.5
  [3bb67fe8] TranscodingStreams v0.9.5
  [a5390f91] ZipFile v0.9.1
  [83775a58] Zlib_jll v1.2.11+9
  [e88e6eb3] Zygote v0.4.20
  [700de1a5] ZygoteRules v0.2.0
  [2a0f44e3] Base64 
  [ade2ca70] Dates 
  [8bb1440f] DelimitedFiles 
  [8ba89e20] Distributed 
  [9fa8497b] Future 
  [b77e0a4c] InteractiveUtils 
  [76f85450] LibGit2 
  [8f399da3] Libdl 
  [37e2e46d] LinearAlgebra 
  [56ddb016] Logging 
  [d6f4376e] Markdown 
  [a63ad114] Mmap 
  [44cfe95a] Pkg 
  [de0858da] Printf 
  [9abbd945] Profile 
  [3fa0cd96] REPL 
  [9a3f8284] Random 
  [ea8e919c] SHA 
  [9e88b42a] Serialization 
  [1a1011a3] SharedArrays 
  [6462fe0b] Sockets 
  [2f01184e] SparseArrays 
  [10745b16] Statistics 
  [8dfed614] Test 
  [cf7118a7] UUIDs 
  [4ec0a83e] Unicode 
┌ Warning: Package GeometricFlux does not have LightGraphs in its dependencies:
│ - If you have GeometricFlux checked out for development and have
│   added LightGraphs as a dependency but haven't updated your primary
│   environment's manifest file, try `Pkg.resolve()`.
│ - Otherwise you may need to report an issue with GeometricFlux
└ Loading LightGraphs into GeometricFlux from project dependency, future warnings for GeometricFlux are suppressed.
┌ Warning: Performing scalar operations on GPU arrays: This is very slow, consider disallowing these operations with `allowscalar(false)`
└ @ GPUArrays ~/.julia/packages/GPUArrays/OXvxB/src/host/indexing.jl:43
divpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:128
  Test threw exception
  Expression: divpool(CuArray{Int64}(cluster), T.(X)) ≈ T.(y)
  GPU compilation of kernel broadcast(CuArrays.CuKernelContext, CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global}, Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}) failed
  KernelError: passing and using non-bitstype argument
  
  Argument 4 to your kernel function is of type Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}, which is not isbits:
    .args is of type Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}} which is not isbits.
      .2 is of type Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}} which is not isbits.
        .x is of type Array{Float32,2} which is not isbits.
  
  Passing non-isbits types is only allowed if they they are unused by the kernel.
  
  Stacktrace:
   [1] check_invocation(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::LLVM.Function) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/validation.jl:75
   [2] macro expansion at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:184 [inlined]
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [4] codegen(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:183
   [5] compile(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:36
   [6] _cufunction(::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:308
   [7] _cufunction at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:302 [inlined]
   [8] #77 at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:21 [inlined]
   [9] get!(::GPUCompiler.var"#77#78"{Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CUDAnative._cufunction),GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}}, ::Dict{UInt64,Any}, ::UInt64) at ./dict.jl:452
   [10] macro expansion at ./lock.jl:183 [inlined]
   [11] check_cache(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:19
   [12] + at ./int.jl:53 [inlined]
   [13] hash_64_64 at ./hashing.jl:35 [inlined]
   [14] hash_uint64 at ./hashing.jl:62 [inlined]
   [15] hx at ./float.jl:568 [inlined]
   [16] hash at ./float.jl:571 [inlined]
   [17] cached_compilation(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:0
   [18] cached_compilation(::Function, ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:37
   [19] cufunction(::Function, ::Type; name::String, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:296
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:108 [inlined]
   [21] gpu_call(::CuArrays.CuArrayBackend, ::Function, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64; name::String) at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/gpuarrays.jl:32
   [22] #gpu_call#1 at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/device/execution.jl:61 [inlined]
   [23] copyto! at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/broadcast.jl:63 [inlined]
   [24] copyto! at ./broadcast.jl:864 [inlined]
   [25] copy at ./broadcast.jl:840 [inlined]
   [26] materialize at ./broadcast.jl:820 [inlined]
   [27] broadcast_preserving_zero_d at ./broadcast.jl:809 [inlined]
   [28] -(::CuArray{Float32,2,Nothing}, ::Array{Float32,2}) at ./arraymath.jl:39
   [29] isapprox(::CuArray{Float32,2,Nothing}, ::Array{Float32,2}; atol::Int64, rtol::Float32, nans::Bool, norm::typeof(LinearAlgebra.norm)) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/generic.jl:1588
   [30] isapprox(::CuArray{Float32,2,Nothing}, ::Array{Float32,2}) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/generic.jl:1588
   [31] eval_test(::Expr, ::Expr, ::LineNumberNode, ::Bool) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:246
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:128 [inlined]
   [33] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [34] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:126 [inlined]
   [35] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [36] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:80 [inlined]
   [37] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [38] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:7
  
divpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:129
  Test threw exception
  Expression: pool(:div, CuArray{Int64}(cluster), T.(X)) ≈ T.(y)
  GPU compilation of kernel broadcast(CuArrays.CuKernelContext, CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global}, Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}) failed
  KernelError: passing and using non-bitstype argument
  
  Argument 4 to your kernel function is of type Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}, which is not isbits:
    .args is of type Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}} which is not isbits.
      .2 is of type Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}} which is not isbits.
        .x is of type Array{Float32,2} which is not isbits.
  
  Passing non-isbits types is only allowed if they they are unused by the kernel.
  
  Stacktrace:
   [1] check_invocation(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::LLVM.Function) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/validation.jl:75
   [2] macro expansion at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:184 [inlined]
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [4] codegen(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:183
   [5] compile(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:36
   [6] _cufunction(::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:308
   [7] _cufunction at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:302 [inlined]
   [8] #77 at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:21 [inlined]
   [9] get!(::GPUCompiler.var"#77#78"{Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CUDAnative._cufunction),GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}}, ::Dict{UInt64,Any}, ::UInt64) at ./dict.jl:452
   [10] macro expansion at ./lock.jl:183 [inlined]
   [11] check_cache(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:19
   [12] + at ./int.jl:53 [inlined]
   [13] hash_64_64 at ./hashing.jl:35 [inlined]
   [14] hash_uint64 at ./hashing.jl:62 [inlined]
   [15] hx at ./float.jl:568 [inlined]
   [16] hash at ./float.jl:571 [inlined]
   [17] cached_compilation(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:0
   [18] cached_compilation(::Function, ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:37
   [19] cufunction(::Function, ::Type; name::String, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:296
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:108 [inlined]
   [21] gpu_call(::CuArrays.CuArrayBackend, ::Function, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64; name::String) at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/gpuarrays.jl:32
   [22] #gpu_call#1 at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/device/execution.jl:61 [inlined]
   [23] copyto! at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/broadcast.jl:63 [inlined]
   [24] copyto! at ./broadcast.jl:864 [inlined]
   [25] copy at ./broadcast.jl:840 [inlined]
   [26] materialize at ./broadcast.jl:820 [inlined]
   [27] broadcast_preserving_zero_d at ./broadcast.jl:809 [inlined]
   [28] -(::CuArray{Float32,2,Nothing}, ::Array{Float32,2}) at ./arraymath.jl:39
   [29] isapprox(::CuArray{Float32,2,Nothing}, ::Array{Float32,2}; atol::Int64, rtol::Float32, nans::Bool, norm::typeof(LinearAlgebra.norm)) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/generic.jl:1588
   [30] isapprox(::CuArray{Float32,2,Nothing}, ::Array{Float32,2}) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/generic.jl:1588
   [31] eval_test(::Expr, ::Expr, ::LineNumberNode, ::Bool) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:246
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:129 [inlined]
   [33] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [34] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:126 [inlined]
   [35] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [36] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:80 [inlined]
   [37] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [38] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:7
  
divpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:130
  Test threw exception
  Expression: divpool(cluster, T.(X)) ≈ T.(y)
  GPU compilation of kernel broadcast(CuArrays.CuKernelContext, CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global}, Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}) failed
  KernelError: passing and using non-bitstype argument
  
  Argument 4 to your kernel function is of type Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}, which is not isbits:
    .args is of type Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}} which is not isbits.
      .2 is of type Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}} which is not isbits.
        .x is of type Array{Float32,2} which is not isbits.
  
  Passing non-isbits types is only allowed if they they are unused by the kernel.
  
  Stacktrace:
   [1] check_invocation(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::LLVM.Function) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/validation.jl:75
   [2] macro expansion at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:184 [inlined]
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [4] codegen(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:183
   [5] compile(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:36
   [6] _cufunction(::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:308
   [7] _cufunction at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:302 [inlined]
   [8] #77 at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:21 [inlined]
   [9] get!(::GPUCompiler.var"#77#78"{Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CUDAnative._cufunction),GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}}, ::Dict{UInt64,Any}, ::UInt64) at ./dict.jl:452
   [10] macro expansion at ./lock.jl:183 [inlined]
   [11] check_cache(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:19
   [12] + at ./int.jl:53 [inlined]
   [13] hash_64_64 at ./hashing.jl:35 [inlined]
   [14] hash_uint64 at ./hashing.jl:62 [inlined]
   [15] hx at ./float.jl:568 [inlined]
   [16] hash at ./float.jl:571 [inlined]
   [17] cached_compilation(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:0
   [18] cached_compilation(::Function, ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:37
   [19] cufunction(::Function, ::Type; name::String, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:296
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:108 [inlined]
   [21] gpu_call(::CuArrays.CuArrayBackend, ::Function, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64; name::String) at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/gpuarrays.jl:32
   [22] #gpu_call#1 at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/device/execution.jl:61 [inlined]
   [23] copyto! at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/broadcast.jl:63 [inlined]
   [24] copyto! at ./broadcast.jl:864 [inlined]
   [25] copy at ./broadcast.jl:840 [inlined]
   [26] materialize at ./broadcast.jl:820 [inlined]
   [27] broadcast_preserving_zero_d at ./broadcast.jl:809 [inlined]
   [28] -(::CuArray{Float32,2,Nothing}, ::Array{Float32,2}) at ./arraymath.jl:39
   [29] isapprox(::CuArray{Float32,2,Nothing}, ::Array{Float32,2}; atol::Int64, rtol::Float32, nans::Bool, norm::typeof(LinearAlgebra.norm)) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/generic.jl:1588
   [30] isapprox(::CuArray{Float32,2,Nothing}, ::Array{Float32,2}) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/generic.jl:1588
   [31] eval_test(::Expr, ::Expr, ::LineNumberNode, ::Bool) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:246
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:130 [inlined]
   [33] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [34] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:126 [inlined]
   [35] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [36] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:80 [inlined]
   [37] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [38] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:7
  
divpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:131
  Test threw exception
  Expression: pool(:div, cluster, T.(X)) ≈ T.(y)
  GPU compilation of kernel broadcast(CuArrays.CuKernelContext, CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global}, Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}) failed
  KernelError: passing and using non-bitstype argument
  
  Argument 4 to your kernel function is of type Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}, which is not isbits:
    .args is of type Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}} which is not isbits.
      .2 is of type Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}} which is not isbits.
        .x is of type Array{Float32,2} which is not isbits.
  
  Passing non-isbits types is only allowed if they they are unused by the kernel.
  
  Stacktrace:
   [1] check_invocation(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::LLVM.Function) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/validation.jl:75
   [2] macro expansion at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:184 [inlined]
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [4] codegen(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:183
   [5] compile(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:36
   [6] _cufunction(::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:308
   [7] _cufunction at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:302 [inlined]
   [8] #77 at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:21 [inlined]
   [9] get!(::GPUCompiler.var"#77#78"{Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CUDAnative._cufunction),GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}}, ::Dict{UInt64,Any}, ::UInt64) at ./dict.jl:452
   [10] macro expansion at ./lock.jl:183 [inlined]
   [11] check_cache(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:19
   [12] + at ./int.jl:53 [inlined]
   [13] hash_64_64 at ./hashing.jl:35 [inlined]
   [14] hash_uint64 at ./hashing.jl:62 [inlined]
   [15] hx at ./float.jl:568 [inlined]
   [16] hash at ./float.jl:571 [inlined]
   [17] cached_compilation(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:0
   [18] cached_compilation(::Function, ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:37
   [19] cufunction(::Function, ::Type; name::String, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:296
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:108 [inlined]
   [21] gpu_call(::CuArrays.CuArrayBackend, ::Function, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64; name::String) at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/gpuarrays.jl:32
   [22] #gpu_call#1 at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/device/execution.jl:61 [inlined]
   [23] copyto! at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/broadcast.jl:63 [inlined]
   [24] copyto! at ./broadcast.jl:864 [inlined]
   [25] copy at ./broadcast.jl:840 [inlined]
   [26] materialize at ./broadcast.jl:820 [inlined]
   [27] broadcast_preserving_zero_d at ./broadcast.jl:809 [inlined]
   [28] -(::CuArray{Float32,2,Nothing}, ::Array{Float32,2}) at ./arraymath.jl:39
   [29] isapprox(::CuArray{Float32,2,Nothing}, ::Array{Float32,2}; atol::Int64, rtol::Float32, nans::Bool, norm::typeof(LinearAlgebra.norm)) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/generic.jl:1588
   [30] isapprox(::CuArray{Float32,2,Nothing}, ::Array{Float32,2}) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/generic.jl:1588
   [31] eval_test(::Expr, ::Expr, ::LineNumberNode, ::Bool) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:246
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:131 [inlined]
   [33] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [34] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:126 [inlined]
   [35] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [36] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:80 [inlined]
   [37] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [38] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:7
  
divpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:128
  Test threw exception
  Expression: divpool(CuArray{Int64}(cluster), T.(X)) ≈ T.(y)
  GPU compilation of kernel broadcast(CuArrays.CuKernelContext, CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global}, Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}) failed
  KernelError: passing and using non-bitstype argument
  
  Argument 4 to your kernel function is of type Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}, which is not isbits:
    .args is of type Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}} which is not isbits.
      .2 is of type Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}} which is not isbits.
        .x is of type Array{Float64,2} which is not isbits.
  
  Passing non-isbits types is only allowed if they they are unused by the kernel.
  
  Stacktrace:
   [1] check_invocation(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::LLVM.Function) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/validation.jl:75
   [2] macro expansion at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:184 [inlined]
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [4] codegen(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:183
   [5] compile(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:36
   [6] _cufunction(::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:308
   [7] _cufunction at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:302 [inlined]
   [8] #77 at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:21 [inlined]
   [9] get!(::GPUCompiler.var"#77#78"{Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CUDAnative._cufunction),GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}}, ::Dict{UInt64,Any}, ::UInt64) at ./dict.jl:452
   [10] macro expansion at ./lock.jl:183 [inlined]
   [11] check_cache(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:19
   [12] + at ./int.jl:53 [inlined]
   [13] hash_64_64 at ./hashing.jl:35 [inlined]
   [14] hash_uint64 at ./hashing.jl:62 [inlined]
   [15] hx at ./float.jl:568 [inlined]
   [16] hash at ./float.jl:571 [inlined]
   [17] cached_compilation(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:0
   [18] cached_compilation(::Function, ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:37
   [19] cufunction(::Function, ::Type; name::String, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:296
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:108 [inlined]
   [21] gpu_call(::CuArrays.CuArrayBackend, ::Function, ::Tuple{CuArray{Float64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CuArray{Float64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64; name::String) at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/gpuarrays.jl:32
   [22] #gpu_call#1 at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/device/execution.jl:61 [inlined]
   [23] copyto! at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/broadcast.jl:63 [inlined]
   [24] copyto! at ./broadcast.jl:864 [inlined]
   [25] copy at ./broadcast.jl:840 [inlined]
   [26] materialize at ./broadcast.jl:820 [inlined]
   [27] broadcast_preserving_zero_d at ./broadcast.jl:809 [inlined]
   [28] -(::CuArray{Float64,2,Nothing}, ::Array{Float64,2}) at ./arraymath.jl:39
   [29] isapprox(::CuArray{Float64,2,Nothing}, ::Array{Float64,2}; atol::Int64, rtol::Float64, nans::Bool, norm::typeof(LinearAlgebra.norm)) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/generic.jl:1588
   [30] isapprox(::CuArray{Float64,2,Nothing}, ::Array{Float64,2}) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/generic.jl:1588
   [31] eval_test(::Expr, ::Expr, ::LineNumberNode, ::Bool) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:246
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:128 [inlined]
   [33] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [34] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:126 [inlined]
   [35] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [36] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:80 [inlined]
   [37] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [38] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:7
  
divpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:129
  Test threw exception
  Expression: pool(:div, CuArray{Int64}(cluster), T.(X)) ≈ T.(y)
  GPU compilation of kernel broadcast(CuArrays.CuKernelContext, CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global}, Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}) failed
  KernelError: passing and using non-bitstype argument
  
  Argument 4 to your kernel function is of type Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}, which is not isbits:
    .args is of type Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}} which is not isbits.
      .2 is of type Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}} which is not isbits.
        .x is of type Array{Float64,2} which is not isbits.
  
  Passing non-isbits types is only allowed if they they are unused by the kernel.
  
  Stacktrace:
   [1] check_invocation(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::LLVM.Function) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/validation.jl:75
   [2] macro expansion at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:184 [inlined]
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [4] codegen(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:183
   [5] compile(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:36
   [6] _cufunction(::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:308
   [7] _cufunction at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:302 [inlined]
   [8] #77 at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:21 [inlined]
   [9] get!(::GPUCompiler.var"#77#78"{Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CUDAnative._cufunction),GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}}, ::Dict{UInt64,Any}, ::UInt64) at ./dict.jl:452
   [10] macro expansion at ./lock.jl:183 [inlined]
   [11] check_cache(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:19
   [12] + at ./int.jl:53 [inlined]
   [13] hash_64_64 at ./hashing.jl:35 [inlined]
   [14] hash_uint64 at ./hashing.jl:62 [inlined]
   [15] hx at ./float.jl:568 [inlined]
   [16] hash at ./float.jl:571 [inlined]
   [17] cached_compilation(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:0
   [18] cached_compilation(::Function, ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:37
   [19] cufunction(::Function, ::Type; name::String, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:296
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:108 [inlined]
   [21] gpu_call(::CuArrays.CuArrayBackend, ::Function, ::Tuple{CuArray{Float64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CuArray{Float64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64; name::String) at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/gpuarrays.jl:32
   [22] #gpu_call#1 at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/device/execution.jl:61 [inlined]
   [23] copyto! at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/broadcast.jl:63 [inlined]
   [24] copyto! at ./broadcast.jl:864 [inlined]
   [25] copy at ./broadcast.jl:840 [inlined]
   [26] materialize at ./broadcast.jl:820 [inlined]
   [27] broadcast_preserving_zero_d at ./broadcast.jl:809 [inlined]
   [28] -(::CuArray{Float64,2,Nothing}, ::Array{Float64,2}) at ./arraymath.jl:39
   [29] isapprox(::CuArray{Float64,2,Nothing}, ::Array{Float64,2}; atol::Int64, rtol::Float64, nans::Bool, norm::typeof(LinearAlgebra.norm)) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/generic.jl:1588
   [30] isapprox(::CuArray{Float64,2,Nothing}, ::Array{Float64,2}) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/generic.jl:1588
   [31] eval_test(::Expr, ::Expr, ::LineNumberNode, ::Bool) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:246
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:129 [inlined]
   [33] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [34] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:126 [inlined]
   [35] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [36] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:80 [inlined]
   [37] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [38] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:7
  
divpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:130
  Test threw exception
  Expression: divpool(cluster, T.(X)) ≈ T.(y)
  GPU compilation of kernel broadcast(CuArrays.CuKernelContext, CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global}, Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}) failed
  KernelError: passing and using non-bitstype argument
  
  Argument 4 to your kernel function is of type Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}, which is not isbits:
    .args is of type Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}} which is not isbits.
      .2 is of type Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}} which is not isbits.
        .x is of type Array{Float64,2} which is not isbits.
  
  Passing non-isbits types is only allowed if they they are unused by the kernel.
  
  Stacktrace:
   [1] check_invocation(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::LLVM.Function) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/validation.jl:75
   [2] macro expansion at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:184 [inlined]
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [4] codegen(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:183
   [5] compile(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:36
   [6] _cufunction(::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:308
   [7] _cufunction at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:302 [inlined]
   [8] #77 at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:21 [inlined]
   [9] get!(::GPUCompiler.var"#77#78"{Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CUDAnative._cufunction),GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}}, ::Dict{UInt64,Any}, ::UInt64) at ./dict.jl:452
   [10] macro expansion at ./lock.jl:183 [inlined]
   [11] check_cache(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:19
   [12] + at ./int.jl:53 [inlined]
   [13] hash_64_64 at ./hashing.jl:35 [inlined]
   [14] hash_uint64 at ./hashing.jl:62 [inlined]
   [15] hx at ./float.jl:568 [inlined]
   [16] hash at ./float.jl:571 [inlined]
   [17] cached_compilation(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:0
   [18] cached_compilation(::Function, ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:37
   [19] cufunction(::Function, ::Type; name::String, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:296
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:108 [inlined]
   [21] gpu_call(::CuArrays.CuArrayBackend, ::Function, ::Tuple{CuArray{Float64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CuArray{Float64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64; name::String) at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/gpuarrays.jl:32
   [22] #gpu_call#1 at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/device/execution.jl:61 [inlined]
   [23] copyto! at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/broadcast.jl:63 [inlined]
   [24] copyto! at ./broadcast.jl:864 [inlined]
   [25] copy at ./broadcast.jl:840 [inlined]
   [26] materialize at ./broadcast.jl:820 [inlined]
   [27] broadcast_preserving_zero_d at ./broadcast.jl:809 [inlined]
   [28] -(::CuArray{Float64,2,Nothing}, ::Array{Float64,2}) at ./arraymath.jl:39
   [29] isapprox(::CuArray{Float64,2,Nothing}, ::Array{Float64,2}; atol::Int64, rtol::Float64, nans::Bool, norm::typeof(LinearAlgebra.norm)) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/generic.jl:1588
   [30] isapprox(::CuArray{Float64,2,Nothing}, ::Array{Float64,2}) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/generic.jl:1588
   [31] eval_test(::Expr, ::Expr, ::LineNumberNode, ::Bool) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:246
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:130 [inlined]
   [33] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [34] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:126 [inlined]
   [35] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [36] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:80 [inlined]
   [37] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [38] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:7
  
divpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:131
  Test threw exception
  Expression: pool(:div, cluster, T.(X)) ≈ T.(y)
  GPU compilation of kernel broadcast(CuArrays.CuKernelContext, CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global}, Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}) failed
  KernelError: passing and using non-bitstype argument
  
  Argument 4 to your kernel function is of type Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}, which is not isbits:
    .args is of type Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}} which is not isbits.
      .2 is of type Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}} which is not isbits.
        .x is of type Array{Float64,2} which is not isbits.
  
  Passing non-isbits types is only allowed if they they are unused by the kernel.
  
  Stacktrace:
   [1] check_invocation(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::LLVM.Function) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/validation.jl:75
   [2] macro expansion at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:184 [inlined]
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [4] codegen(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:183
   [5] compile(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:36
   [6] _cufunction(::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:308
   [7] _cufunction at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:302 [inlined]
   [8] #77 at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:21 [inlined]
   [9] get!(::GPUCompiler.var"#77#78"{Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CUDAnative._cufunction),GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}}, ::Dict{UInt64,Any}, ::UInt64) at ./dict.jl:452
   [10] macro expansion at ./lock.jl:183 [inlined]
   [11] check_cache(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:19
   [12] + at ./int.jl:53 [inlined]
   [13] hash_64_64 at ./hashing.jl:35 [inlined]
   [14] hash_uint64 at ./hashing.jl:62 [inlined]
   [15] hx at ./float.jl:568 [inlined]
   [16] hash at ./float.jl:571 [inlined]
   [17] cached_compilation(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:0
   [18] cached_compilation(::Function, ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:37
   [19] cufunction(::Function, ::Type; name::String, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:296
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:108 [inlined]
   [21] gpu_call(::CuArrays.CuArrayBackend, ::Function, ::Tuple{CuArray{Float64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CuArray{Float64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64; name::String) at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/gpuarrays.jl:32
   [22] #gpu_call#1 at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/device/execution.jl:61 [inlined]
   [23] copyto! at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/broadcast.jl:63 [inlined]
   [24] copyto! at ./broadcast.jl:864 [inlined]
   [25] copy at ./broadcast.jl:840 [inlined]
   [26] materialize at ./broadcast.jl:820 [inlined]
   [27] broadcast_preserving_zero_d at ./broadcast.jl:809 [inlined]
   [28] -(::CuArray{Float64,2,Nothing}, ::Array{Float64,2}) at ./arraymath.jl:39
   [29] isapprox(::CuArray{Float64,2,Nothing}, ::Array{Float64,2}; atol::Int64, rtol::Float64, nans::Bool, norm::typeof(LinearAlgebra.norm)) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/generic.jl:1588
   [30] isapprox(::CuArray{Float64,2,Nothing}, ::Array{Float64,2}) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/generic.jl:1588
   [31] eval_test(::Expr, ::Expr, ::LineNumberNode, ::Bool) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:246
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:131 [inlined]
   [33] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [34] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:126 [inlined]
   [35] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [36] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:80 [inlined]
   [37] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [38] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:7
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:33
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_mul!(x, us, xs))
            end), ys) == (∇y_mul,)
  GPU compilation of kernel partial_mapreduce_grid(typeof(identity), typeof(*), Float32, CartesianIndices{1,Tuple{Base.OneTo{Int64}}}, CartesianIndices{1,Tuple{Base.OneTo{Int64}}}, Val{true}, CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global}, Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#183#186"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}) failed
  KernelError: recursion is currently not supported
  
  Try inspecting the generated code with any of the @device_code_... macros.
  
  Stacktrace:
   [1] _nextind_str at strings/string.jl:141
   [2] nextind at strings/string.jl:137
   [3] _nextind_str at strings/string.jl:141
   [4] _split at strings/util.jl:325
   [5] env_override_minlevel at logging.jl:423
   [6] current_logger_for_env at logging.jl:386
   [7] assertscalar at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/indexing.jl:40
   [8] #183 at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/scatter.jl:125
   [9] partial_mapreduce_grid at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/mapreduce.jl:93
  Stacktrace:
   [1] push!(::GPUCompiler.MethodCompileTracer, ::Core.MethodInstance) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/irgen.jl:52
   [2] (::GPUCompiler.var"#hook_emit_function#24"{GPUCompiler.MethodCompileTracer})(::Core.MethodInstance, ::Core.CodeInfo, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/irgen.jl:204
   [3] compile_method_instance(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/irgen.jl:268
   [4] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [5] irgen(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/irgen.jl:326
   [6] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:98 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [9] codegen(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:97
   [10] compile(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:36
   [11] _cufunction(::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#183#186"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:308
   [12] _cufunction at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:302 [inlined]
   [13] #77 at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:21 [inlined]
   [14] get!(::GPUCompiler.var"#77#78"{Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CUDAnative._cufunction),GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#183#186"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}}, ::Dict{UInt64,Any}, ::UInt64) at ./dict.jl:452
   [15] macro expansion at ./lock.jl:183 [inlined]
   [16] check_cache(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#183#186"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:19
   [17] + at ./int.jl:53 [inlined]
   [18] hash_64_64 at ./hashing.jl:35 [inlined]
   [19] hash_uint64 at ./hashing.jl:62 [inlined]
   [20] hx at ./float.jl:568 [inlined]
   [21] hash at ./float.jl:571 [inlined]
   [22] cached_compilation(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#183#186"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:0
   [23] cached_compilation(::Function, ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#183#186"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:37
   [24] cufunction(::Function, ::Type; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:296
   [25] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:291
   [26] macro expansion at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/mapreduce.jl:197 [inlined]
   [27] mapreducedim!(::Function, ::Function, ::CuArray{Float32,1,Nothing}, ::Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#183#186"{Int64,CuArray{Float32,3,Nothing}},Tuple{CuArray{Tuple{Int64,Int64},1,Nothing}}}; init::Float32) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/nvtx/highlevel.jl:83
   [28] _mapreduce(::Function, ::Function, ::CuArray{Tuple{Int64,Int64},1,Nothing}; dims::Function, init::Float32) at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/mapreduce.jl:62
   [29] #mapreduce#31 at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/mapreduce.jl:28 [inlined]
   [30] (::GeometricFlux.var"#181#184"{Float32,CuArray{Float32,2,Nothing},CuArray{Float32,3,Nothing},CuArray{Int64,2,Nothing}})(::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/scatter.jl:125
   [31] #93#back at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49 [inlined]
   [32] #88 at ./none:0 [inlined]
   [33] (::Zygote.var"#36#37"{typeof(∂(#88))})(::Float32) at /home/pkgeval/.julia/packages/Zygote/YeCEW/src/compiler/interface.jl:46
   [34] gradient(::Function, ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/YeCEW/src/compiler/interface.jl:55
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:33
   [36] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113
   [37] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:17
   [38] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113
   [39] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:16
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:34
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_mul!(copy(ys), x, xs))
            end), us) == (2048 * gather(ys, xs),)
  GPU compilation of kernel partial_mapreduce_grid(typeof(identity), typeof(*), Float32, CartesianIndices{1,Tuple{Base.OneTo{Int64}}}, CartesianIndices{1,Tuple{Base.OneTo{Int64}}}, Val{true}, CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global}, Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#183#186"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}) failed
  KernelError: recursion is currently not supported
  
  Try inspecting the generated code with any of the @device_code_... macros.
  
  Stacktrace:
   [1] _nextind_str at strings/string.jl:141
   [2] nextind at strings/string.jl:137
   [3] _nextind_str at strings/string.jl:141
   [4] _split at strings/util.jl:325
   [5] env_override_minlevel at logging.jl:423
   [6] current_logger_for_env at logging.jl:386
   [7] assertscalar at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/indexing.jl:40
   [8] #183 at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/scatter.jl:125
   [9] partial_mapreduce_grid at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/mapreduce.jl:93
  Stacktrace:
   [1] push!(::GPUCompiler.MethodCompileTracer, ::Core.MethodInstance) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/irgen.jl:52
   [2] (::GPUCompiler.var"#hook_emit_function#24"{GPUCompiler.MethodCompileTracer})(::Core.MethodInstance, ::Core.CodeInfo, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/irgen.jl:204
   [3] compile_method_instance(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/irgen.jl:268
   [4] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [5] irgen(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/irgen.jl:326
   [6] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:98 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [9] codegen(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:97
   [10] compile(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:36
   [11] _cufunction(::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#183#186"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:308
   [12] _cufunction at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:302 [inlined]
   [13] #77 at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:21 [inlined]
   [14] get!(::GPUCompiler.var"#77#78"{Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CUDAnative._cufunction),GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#183#186"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}}, ::Dict{UInt64,Any}, ::UInt64) at ./dict.jl:452
   [15] macro expansion at ./lock.jl:183 [inlined]
   [16] check_cache(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#183#186"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:19
   [17] + at ./int.jl:53 [inlined]
   [18] hash_64_64 at ./hashing.jl:35 [inlined]
   [19] hash_uint64 at ./hashing.jl:62 [inlined]
   [20] hx at ./float.jl:568 [inlined]
   [21] hash at ./float.jl:571 [inlined]
   [22] cached_compilation(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#183#186"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:0
   [23] cached_compilation(::Function, ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#183#186"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:37
   [24] cufunction(::Function, ::Type; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:296
   [25] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:291
   [26] macro expansion at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/mapreduce.jl:197 [inlined]
   [27] mapreducedim!(::Function, ::Function, ::CuArray{Float32,1,Nothing}, ::Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#183#186"{Int64,CuArray{Float32,3,Nothing}},Tuple{CuArray{Tuple{Int64,Int64},1,Nothing}}}; init::Float32) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/nvtx/highlevel.jl:83
   [28] _mapreduce(::Function, ::Function, ::CuArray{Tuple{Int64,Int64},1,Nothing}; dims::Function, init::Float32) at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/mapreduce.jl:62
   [29] #mapreduce#31 at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/mapreduce.jl:28 [inlined]
   [30] (::GeometricFlux.var"#181#184"{Float32,CuArray{Float32,2,Nothing},CuArray{Float32,3,Nothing},CuArray{Int64,2,Nothing}})(::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/scatter.jl:125
   [31] #93#back at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49 [inlined]
   [32] #89 at ./none:0 [inlined]
   [33] (::Zygote.var"#36#37"{typeof(∂(#89))})(::Float32) at /home/pkgeval/.julia/packages/Zygote/YeCEW/src/compiler/interface.jl:46
   [34] gradient(::Function, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/YeCEW/src/compiler/interface.jl:55
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:34
   [36] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113
   [37] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:17
   [38] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113
   [39] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:16
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:35
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_mul!(copy(ys), us, x))
            end), xs) == (nothing,)
  GPU compilation of kernel partial_mapreduce_grid(typeof(identity), typeof(*), Float32, CartesianIndices{1,Tuple{Base.OneTo{Int64}}}, CartesianIndices{1,Tuple{Base.OneTo{Int64}}}, Val{true}, CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global}, Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#183#186"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}) failed
  KernelError: recursion is currently not supported
  
  Try inspecting the generated code with any of the @device_code_... macros.
  
  Stacktrace:
   [1] _nextind_str at strings/string.jl:141
   [2] nextind at strings/string.jl:137
   [3] _nextind_str at strings/string.jl:141
   [4] _split at strings/util.jl:325
   [5] env_override_minlevel at logging.jl:423
   [6] current_logger_for_env at logging.jl:386
   [7] assertscalar at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/indexing.jl:40
   [8] #183 at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/scatter.jl:125
   [9] partial_mapreduce_grid at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/mapreduce.jl:93
  Stacktrace:
   [1] push!(::GPUCompiler.MethodCompileTracer, ::Core.MethodInstance) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/irgen.jl:52
   [2] (::GPUCompiler.var"#hook_emit_function#24"{GPUCompiler.MethodCompileTracer})(::Core.MethodInstance, ::Core.CodeInfo, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/irgen.jl:204
   [3] compile_method_instance(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/irgen.jl:268
   [4] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [5] irgen(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/irgen.jl:326
   [6] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:98 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [9] codegen(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:97
   [10] compile(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:36
   [11] _cufunction(::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#183#186"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:308
   [12] _cufunction at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:302 [inlined]
   [13] #77 at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:21 [inlined]
   [14] get!(::GPUCompiler.var"#77#78"{Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CUDAnative._cufunction),GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#183#186"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}}, ::Dict{UInt64,Any}, ::UInt64) at ./dict.jl:452
   [15] macro expansion at ./lock.jl:183 [inlined]
   [16] check_cache(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#183#186"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:19
   [17] + at ./int.jl:53 [inlined]
   [18] hash_64_64 at ./hashing.jl:35 [inlined]
   [19] hash_uint64 at ./hashing.jl:62 [inlined]
   [20] hx at ./float.jl:568 [inlined]
   [21] hash at ./float.jl:571 [inlined]
   [22] cached_compilation(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#183#186"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:0
   [23] cached_compilation(::Function, ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#183#186"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:37
   [24] cufunction(::Function, ::Type; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:296
   [25] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:291
   [26] macro expansion at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/mapreduce.jl:197 [inlined]
   [27] mapreducedim!(::Function, ::Function, ::CuArray{Float32,1,Nothing}, ::Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#183#186"{Int64,CuArray{Float32,3,Nothing}},Tuple{CuArray{Tuple{Int64,Int64},1,Nothing}}}; init::Float32) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/nvtx/highlevel.jl:83
   [28] _mapreduce(::Function, ::Function, ::CuArray{Tuple{Int64,Int64},1,Nothing}; dims::Function, init::Float32) at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/mapreduce.jl:62
   [29] #mapreduce#31 at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/mapreduce.jl:28 [inlined]
   [30] (::GeometricFlux.var"#181#184"{Float32,CuArray{Float32,2,Nothing},CuArray{Float32,3,Nothing},CuArray{Int64,2,Nothing}})(::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/scatter.jl:125
   [31] #93#back at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49 [inlined]
   [32] #90 at ./none:0 [inlined]
   [33] (::typeof(∂(#90)))(::Float32) at /home/pkgeval/.julia/packages/Zygote/YeCEW/src/compiler/interface2.jl:0
   [34] (::Zygote.var"#36#37"{typeof(∂(#90))})(::Float32) at /home/pkgeval/.julia/packages/Zygote/YeCEW/src/compiler/interface.jl:46
   [35] gradient(::Function, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/YeCEW/src/compiler/interface.jl:55
   [36] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:35
   [37] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113
   [38] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:17
   [39] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113
   [40] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:16
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:37
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_div!(x, us, xs))
            end), ys) == (∇y_div,)
  GPU compilation of kernel partial_mapreduce_grid(typeof(identity), typeof(*), Float32, CartesianIndices{1,Tuple{Base.OneTo{Int64}}}, CartesianIndices{1,Tuple{Base.OneTo{Int64}}}, Val{true}, CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global}, Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#191#194"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}) failed
  KernelError: recursion is currently not supported
  
  Try inspecting the generated code with any of the @device_code_... macros.
  
  Stacktrace:
   [1] _nextind_str at strings/string.jl:141
   [2] nextind at strings/string.jl:137
   [3] _nextind_str at strings/string.jl:141
   [4] _split at strings/util.jl:325
   [5] env_override_minlevel at logging.jl:423
   [6] current_logger_for_env at logging.jl:386
   [7] assertscalar at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/indexing.jl:40
   [8] #191 at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/scatter.jl:146
   [9] partial_mapreduce_grid at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/mapreduce.jl:93
  Stacktrace:
   [1] push!(::GPUCompiler.MethodCompileTracer, ::Core.MethodInstance) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/irgen.jl:52
   [2] (::GPUCompiler.var"#hook_emit_function#24"{GPUCompiler.MethodCompileTracer})(::Core.MethodInstance, ::Core.CodeInfo, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/irgen.jl:204
   [3] compile_method_instance(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/irgen.jl:268
   [4] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [5] irgen(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/irgen.jl:326
   [6] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:98 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [9] codegen(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:97
   [10] compile(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:36
   [11] _cufunction(::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#191#194"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:308
   [12] _cufunction at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:302 [inlined]
   [13] #77 at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:21 [inlined]
   [14] get!(::GPUCompiler.var"#77#78"{Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CUDAnative._cufunction),GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#191#194"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}}, ::Dict{UInt64,Any}, ::UInt64) at ./dict.jl:452
   [15] macro expansion at ./lock.jl:183 [inlined]
   [16] check_cache(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#191#194"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:19
   [17] + at ./int.jl:53 [inlined]
   [18] hash_64_64 at ./hashing.jl:35 [inlined]
   [19] hash_uint64 at ./hashing.jl:62 [inlined]
   [20] hx at ./float.jl:568 [inlined]
   [21] hash at ./float.jl:571 [inlined]
   [22] cached_compilation(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#191#194"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:0
   [23] cached_compilation(::Function, ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#191#194"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:37
   [24] cufunction(::Function, ::Type; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:296
   [25] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:291
   [26] macro expansion at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/mapreduce.jl:197 [inlined]
   [27] mapreducedim!(::Function, ::Function, ::CuArray{Float32,1,Nothing}, ::Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#191#194"{Int64,CuArray{Float32,3,Nothing}},Tuple{CuArray{Tuple{Int64,Int64},1,Nothing}}}; init::Float32) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/nvtx/highlevel.jl:83
   [28] _mapreduce(::Function, ::Function, ::CuArray{Tuple{Int64,Int64},1,Nothing}; dims::Function, init::Float32) at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/mapreduce.jl:62
   [29] #mapreduce#31 at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/mapreduce.jl:28 [inlined]
   [30] (::GeometricFlux.var"#189#192"{Float32,CuArray{Float32,2,Nothing},CuArray{Float32,3,Nothing},CuArray{Int64,2,Nothing}})(::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/scatter.jl:146
   [31] #111#back at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49 [inlined]
   [32] #91 at ./none:0 [inlined]
   [33] (::Zygote.var"#36#37"{typeof(∂(#91))})(::Float32) at /home/pkgeval/.julia/packages/Zygote/YeCEW/src/compiler/interface.jl:46
   [34] gradient(::Function, ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/YeCEW/src/compiler/interface.jl:55
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:37
   [36] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113
   [37] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:17
   [38] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113
   [39] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:16
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:38
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_div!(copy(ys), x, xs))
            end), us) == (-(gather(ys, xs)) / 8192,)
  GPU compilation of kernel partial_mapreduce_grid(typeof(identity), typeof(*), Float32, CartesianIndices{1,Tuple{Base.OneTo{Int64}}}, CartesianIndices{1,Tuple{Base.OneTo{Int64}}}, Val{true}, CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global}, Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#191#194"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}) failed
  KernelError: recursion is currently not supported
  
  Try inspecting the generated code with any of the @device_code_... macros.
  
  Stacktrace:
   [1] _nextind_str at strings/string.jl:141
   [2] nextind at strings/string.jl:137
   [3] _nextind_str at strings/string.jl:141
   [4] _split at strings/util.jl:325
   [5] env_override_minlevel at logging.jl:423
   [6] current_logger_for_env at logging.jl:386
   [7] assertscalar at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/indexing.jl:40
   [8] #191 at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/scatter.jl:146
   [9] partial_mapreduce_grid at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/mapreduce.jl:93
  Stacktrace:
   [1] push!(::GPUCompiler.MethodCompileTracer, ::Core.MethodInstance) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/irgen.jl:52
   [2] (::GPUCompiler.var"#hook_emit_function#24"{GPUCompiler.MethodCompileTracer})(::Core.MethodInstance, ::Core.CodeInfo, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/irgen.jl:204
   [3] compile_method_instance(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/irgen.jl:268
   [4] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [5] irgen(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/irgen.jl:326
   [6] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:98 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [9] codegen(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:97
   [10] compile(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:36
   [11] _cufunction(::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#191#194"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:308
   [12] _cufunction at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:302 [inlined]
   [13] #77 at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:21 [inlined]
   [14] get!(::GPUCompiler.var"#77#78"{Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CUDAnative._cufunction),GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#191#194"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}}, ::Dict{UInt64,Any}, ::UInt64) at ./dict.jl:452
   [15] macro expansion at ./lock.jl:183 [inlined]
   [16] check_cache(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#191#194"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:19
   [17] + at ./int.jl:53 [inlined]
   [18] hash_64_64 at ./hashing.jl:35 [inlined]
   [19] hash_uint64 at ./hashing.jl:62 [inlined]
   [20] hx at ./float.jl:568 [inlined]
   [21] hash at ./float.jl:571 [inlined]
   [22] cached_compilation(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#191#194"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:0
   [23] cached_compilation(::Function, ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#191#194"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:37
   [24] cufunction(::Function, ::Type; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:296
   [25] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:291
   [26] macro expansion at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/mapreduce.jl:197 [inlined]
   [27] mapreducedim!(::Function, ::Function, ::CuArray{Float32,1,Nothing}, ::Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#191#194"{Int64,CuArray{Float32,3,Nothing}},Tuple{CuArray{Tuple{Int64,Int64},1,Nothing}}}; init::Float32) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/nvtx/highlevel.jl:83
   [28] _mapreduce(::Function, ::Function, ::CuArray{Tuple{Int64,Int64},1,Nothing}; dims::Function, init::Float32) at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/mapreduce.jl:62
   [29] #mapreduce#31 at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/mapreduce.jl:28 [inlined]
   [30] (::GeometricFlux.var"#189#192"{Float32,CuArray{Float32,2,Nothing},CuArray{Float32,3,Nothing},CuArray{Int64,2,Nothing}})(::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/scatter.jl:146
   [31] #111#back at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49 [inlined]
   [32] #92 at ./none:0 [inlined]
   [33] (::Zygote.var"#36#37"{typeof(∂(#92))})(::Float32) at /home/pkgeval/.julia/packages/Zygote/YeCEW/src/compiler/interface.jl:46
   [34] gradient(::Function, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/YeCEW/src/compiler/interface.jl:55
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:38
   [36] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113
   [37] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:17
   [38] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113
   [39] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:16
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:39
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_div!(copy(ys), us, x))
            end), xs) == (nothing,)
  GPU compilation of kernel partial_mapreduce_grid(typeof(identity), typeof(*), Float32, CartesianIndices{1,Tuple{Base.OneTo{Int64}}}, CartesianIndices{1,Tuple{Base.OneTo{Int64}}}, Val{true}, CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global}, Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#191#194"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}) failed
  KernelError: recursion is currently not supported
  
  Try inspecting the generated code with any of the @device_code_... macros.
  
  Stacktrace:
   [1] _nextind_str at strings/string.jl:141
   [2] nextind at strings/string.jl:137
   [3] _nextind_str at strings/string.jl:141
   [4] _split at strings/util.jl:325
   [5] env_override_minlevel at logging.jl:423
   [6] current_logger_for_env at logging.jl:386
   [7] assertscalar at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/indexing.jl:40
   [8] #191 at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/scatter.jl:146
   [9] partial_mapreduce_grid at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/mapreduce.jl:93
  Stacktrace:
   [1] push!(::GPUCompiler.MethodCompileTracer, ::Core.MethodInstance) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/irgen.jl:52
   [2] (::GPUCompiler.var"#hook_emit_function#24"{GPUCompiler.MethodCompileTracer})(::Core.MethodInstance, ::Core.CodeInfo, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/irgen.jl:204
   [3] compile_method_instance(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/irgen.jl:268
   [4] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [5] irgen(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/irgen.jl:326
   [6] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:98 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [9] codegen(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:97
   [10] compile(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:36
   [11] _cufunction(::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#191#194"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:308
   [12] _cufunction at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:302 [inlined]
   [13] #77 at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:21 [inlined]
   [14] get!(::GPUCompiler.var"#77#78"{Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CUDAnative._cufunction),GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#191#194"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}}, ::Dict{UInt64,Any}, ::UInt64) at ./dict.jl:452
   [15] macro expansion at ./lock.jl:183 [inlined]
   [16] check_cache(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#191#194"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:19
   [17] + at ./int.jl:53 [inlined]
   [18] hash_64_64 at ./hashing.jl:35 [inlined]
   [19] hash_uint64 at ./hashing.jl:62 [inlined]
   [20] hx at ./float.jl:568 [inlined]
   [21] hash at ./float.jl:571 [inlined]
   [22] cached_compilation(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#191#194"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:0
   [23] cached_compilation(::Function, ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#191#194"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:37
   [24] cufunction(::Function, ::Type; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:296
   [25] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:291
   [26] macro expansion at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/mapreduce.jl:197 [inlined]
   [27] mapreducedim!(::Function, ::Function, ::CuArray{Float32,1,Nothing}, ::Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#191#194"{Int64,CuArray{Float32,3,Nothing}},Tuple{CuArray{Tuple{Int64,Int64},1,Nothing}}}; init::Float32) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/nvtx/highlevel.jl:83
   [28] _mapreduce(::Function, ::Function, ::CuArray{Tuple{Int64,Int64},1,Nothing}; dims::Function, init::Float32) at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/mapreduce.jl:62
   [29] #mapreduce#31 at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/mapreduce.jl:28 [inlined]
   [30] (::GeometricFlux.var"#189#192"{Float32,CuArray{Float32,2,Nothing},CuArray{Float32,3,Nothing},CuArray{Int64,2,Nothing}})(::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/scatter.jl:146
   [31] #111#back at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49 [inlined]
   [32] #93 at ./none:0 [inlined]
   [33] (::typeof(∂(#93)))(::Float32) at /home/pkgeval/.julia/packages/Zygote/YeCEW/src/compiler/interface2.jl:0
   [34] (::Zygote.var"#36#37"{typeof(∂(#93))})(::Float32) at /home/pkgeval/.julia/packages/Zygote/YeCEW/src/compiler/interface.jl:46
   [35] gradient(::Function, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/YeCEW/src/compiler/interface.jl:55
   [36] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:39
   [37] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113
   [38] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:17
   [39] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113
   [40] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:16
  
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:59
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(prodpool(x, us))
            end), xs) == (nothing,)
  GPU compilation of kernel partial_mapreduce_grid(typeof(identity), typeof(*), Float32, CartesianIndices{1,Tuple{Base.OneTo{Int64}}}, CartesianIndices{1,Tuple{Base.OneTo{Int64}}}, Val{true}, CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global}, Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#208#211"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}) failed
  KernelError: recursion is currently not supported
  
  Try inspecting the generated code with any of the @device_code_... macros.
  
  Stacktrace:
   [1] _nextind_str at strings/string.jl:141
   [2] nextind at strings/string.jl:137
   [3] _nextind_str at strings/string.jl:141
   [4] _split at strings/util.jl:325
   [5] env_override_minlevel at logging.jl:423
   [6] current_logger_for_env at logging.jl:386
   [7] assertscalar at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/indexing.jl:40
   [8] #208 at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/pool.jl:88
   [9] partial_mapreduce_grid at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/mapreduce.jl:93
  Stacktrace:
   [1] push!(::GPUCompiler.MethodCompileTracer, ::Core.MethodInstance) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/irgen.jl:52
   [2] (::GPUCompiler.var"#hook_emit_function#24"{GPUCompiler.MethodCompileTracer})(::Core.MethodInstance, ::Core.CodeInfo, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/irgen.jl:204
   [3] compile_method_instance(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/irgen.jl:268
   [4] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [5] irgen(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/irgen.jl:326
   [6] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:98 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [9] codegen(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:97
   [10] compile(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:36
   [11] _cufunction(::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#208#211"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:308
   [12] _cufunction at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:302 [inlined]
   [13] #77 at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:21 [inlined]
   [14] get!(::GPUCompiler.var"#77#78"{Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CUDAnative._cufunction),GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#208#211"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}}, ::Dict{UInt64,Any}, ::UInt64) at ./dict.jl:452
   [15] macro expansion at ./lock.jl:183 [inlined]
   [16] check_cache(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#208#211"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:19
   [17] + at ./int.jl:53 [inlined]
   [18] hash_64_64 at ./hashing.jl:35 [inlined]
   [19] hash_uint64 at ./hashing.jl:62 [inlined]
   [20] hx at ./float.jl:568 [inlined]
   [21] hash at ./float.jl:571 [inlined]
   [22] cached_compilation(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#208#211"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:0
   [23] cached_compilation(::Function, ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#208#211"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:37
   [24] cufunction(::Function, ::Type; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:296
   [25] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:291
   [26] macro expansion at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/mapreduce.jl:197 [inlined]
   [27] mapreducedim!(::Function, ::Function, ::CuArray{Float32,1,Nothing}, ::Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#208#211"{Int64,CuArray{Float32,3,Nothing}},Tuple{CuArray{Tuple{Int64,Int64},1,Nothing}}}; init::Float32) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/nvtx/highlevel.jl:83
   [28] _mapreduce(::Function, ::Function, ::CuArray{Tuple{Int64,Int64},1,Nothing}; dims::Function, init::Float32) at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/mapreduce.jl:62
   [29] #mapreduce#31 at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/mapreduce.jl:28 [inlined]
   [30] (::GeometricFlux.var"#206#209"{Float32,CuArray{Int64,2,Nothing},CuArray{Float32,3,Nothing}})(::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/pool.jl:88
   [31] #154#back at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49 [inlined]
   [32] #105 at ./none:0 [inlined]
   [33] (::typeof(∂(#105)))(::Float32) at /home/pkgeval/.julia/packages/Zygote/YeCEW/src/compiler/interface2.jl:0
   [34] (::Zygote.var"#36#37"{typeof(∂(#105))})(::Float32) at /home/pkgeval/.julia/packages/Zygote/YeCEW/src/compiler/interface.jl:46
   [35] gradient(::Function, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/YeCEW/src/compiler/interface.jl:55
   [36] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:59
   [37] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113
   [38] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:47
   [39] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113
   [40] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:16
  
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:60
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(prodpool(xs, x))
            end), us) == (2048 * ones(2, 3, 4),)
  GPU compilation of kernel partial_mapreduce_grid(typeof(identity), typeof(*), Float32, CartesianIndices{1,Tuple{Base.OneTo{Int64}}}, CartesianIndices{1,Tuple{Base.OneTo{Int64}}}, Val{true}, CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global}, Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#208#211"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}) failed
  KernelError: recursion is currently not supported
  
  Try inspecting the generated code with any of the @device_code_... macros.
  
  Stacktrace:
   [1] _nextind_str at strings/string.jl:141
   [2] nextind at strings/string.jl:137
   [3] _nextind_str at strings/string.jl:141
   [4] _split at strings/util.jl:325
   [5] env_override_minlevel at logging.jl:423
   [6] current_logger_for_env at logging.jl:386
   [7] assertscalar at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/indexing.jl:40
   [8] #208 at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/pool.jl:88
   [9] partial_mapreduce_grid at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/mapreduce.jl:93
  Stacktrace:
   [1] push!(::GPUCompiler.MethodCompileTracer, ::Core.MethodInstance) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/irgen.jl:52
   [2] (::GPUCompiler.var"#hook_emit_function#24"{GPUCompiler.MethodCompileTracer})(::Core.MethodInstance, ::Core.CodeInfo, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/irgen.jl:204
   [3] compile_method_instance(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/irgen.jl:268
   [4] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [5] irgen(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/irgen.jl:326
   [6] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:98 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [9] codegen(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:97
   [10] compile(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:36
   [11] _cufunction(::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#208#211"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:308
   [12] _cufunction at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:302 [inlined]
   [13] #77 at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:21 [inlined]
   [14] get!(::GPUCompiler.var"#77#78"{Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CUDAnative._cufunction),GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#208#211"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}}, ::Dict{UInt64,Any}, ::UInt64) at ./dict.jl:452
   [15] macro expansion at ./lock.jl:183 [inlined]
   [16] check_cache(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#208#211"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:19
   [17] + at ./int.jl:53 [inlined]
   [18] hash_64_64 at ./hashing.jl:35 [inlined]
   [19] hash_uint64 at ./hashing.jl:62 [inlined]
   [20] hx at ./float.jl:568 [inlined]
   [21] hash at ./float.jl:571 [inlined]
   [22] cached_compilation(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#208#211"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:0
   [23] cached_compilation(::Function, ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#208#211"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:37
   [24] cufunction(::Function, ::Type; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:296
   [25] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:291
   [26] macro expansion at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/mapreduce.jl:197 [inlined]
   [27] mapreducedim!(::Function, ::Function, ::CuArray{Float32,1,Nothing}, ::Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#208#211"{Int64,CuArray{Float32,3,Nothing}},Tuple{CuArray{Tuple{Int64,Int64},1,Nothing}}}; init::Float32) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/nvtx/highlevel.jl:83
   [28] _mapreduce(::Function, ::Function, ::CuArray{Tuple{Int64,Int64},1,Nothing}; dims::Function, init::Float32) at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/mapreduce.jl:62
   [29] #mapreduce#31 at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/mapreduce.jl:28 [inlined]
   [30] (::GeometricFlux.var"#206#209"{Float32,CuArray{Int64,2,Nothing},CuArray{Float32,3,Nothing}})(::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/pool.jl:88
   [31] #154#back at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49 [inlined]
   [32] #106 at ./none:0 [inlined]
   [33] (::Zygote.var"#36#37"{typeof(∂(#106))})(::Float32) at /home/pkgeval/.julia/packages/Zygote/YeCEW/src/compiler/interface.jl:46
   [34] gradient(::Function, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/YeCEW/src/compiler/interface.jl:55
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:60
   [36] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113
   [37] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:47
   [38] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113
   [39] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:16
  
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:62
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(divpool(x, us))
            end), xs) == (nothing,)
  GPU compilation of kernel partial_mapreduce_grid(typeof(identity), typeof(*), Float32, CartesianIndices{1,Tuple{Base.OneTo{Int64}}}, CartesianIndices{1,Tuple{Base.OneTo{Int64}}}, Val{true}, CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global}, Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#216#219"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}) failed
  KernelError: recursion is currently not supported
  
  Try inspecting the generated code with any of the @device_code_... macros.
  
  Stacktrace:
   [1] _nextind_str at strings/string.jl:141
   [2] nextind at strings/string.jl:137
   [3] _nextind_str at strings/string.jl:141
   [4] _split at strings/util.jl:325
   [5] env_override_minlevel at logging.jl:423
   [6] current_logger_for_env at logging.jl:386
   [7] assertscalar at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/indexing.jl:40
   [8] #216 at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/pool.jl:104
   [9] partial_mapreduce_grid at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/mapreduce.jl:93
  Stacktrace:
   [1] push!(::GPUCompiler.MethodCompileTracer, ::Core.MethodInstance) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/irgen.jl:52
   [2] (::GPUCompiler.var"#hook_emit_function#24"{GPUCompiler.MethodCompileTracer})(::Core.MethodInstance, ::Core.CodeInfo, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/irgen.jl:204
   [3] compile_method_instance(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/irgen.jl:268
   [4] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [5] irgen(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/irgen.jl:326
   [6] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:98 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [9] codegen(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:97
   [10] compile(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:36
   [11] _cufunction(::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#216#219"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:308
   [12] _cufunction at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:302 [inlined]
   [13] #77 at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:21 [inlined]
   [14] get!(::GPUCompiler.var"#77#78"{Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CUDAnative._cufunction),GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#216#219"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}}, ::Dict{UInt64,Any}, ::UInt64) at ./dict.jl:452
   [15] macro expansion at ./lock.jl:183 [inlined]
   [16] check_cache(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#216#219"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:19
   [17] + at ./int.jl:53 [inlined]
   [18] hash_64_64 at ./hashing.jl:35 [inlined]
   [19] hash_uint64 at ./hashing.jl:62 [inlined]
   [20] hx at ./float.jl:568 [inlined]
   [21] hash at ./float.jl:571 [inlined]
   [22] cached_compilation(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#216#219"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:0
   [23] cached_compilation(::Function, ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#216#219"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:37
   [24] cufunction(::Function, ::Type; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:296
   [25] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:291
   [26] macro expansion at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/mapreduce.jl:197 [inlined]
   [27] mapreducedim!(::Function, ::Function, ::CuArray{Float32,1,Nothing}, ::Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#216#219"{Int64,CuArray{Float32,3,Nothing}},Tuple{CuArray{Tuple{Int64,Int64},1,Nothing}}}; init::Float32) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/nvtx/highlevel.jl:83
   [28] _mapreduce(::Function, ::Function, ::CuArray{Tuple{Int64,Int64},1,Nothing}; dims::Function, init::Float32) at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/mapreduce.jl:62
   [29] #mapreduce#31 at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/mapreduce.jl:28 [inlined]
   [30] (::GeometricFlux.var"#214#217"{Float32,CuArray{Int64,2,Nothing},CuArray{Float32,3,Nothing}})(::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/pool.jl:104
   [31] #172#back at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49 [inlined]
   [32] #107 at ./none:0 [inlined]
   [33] (::typeof(∂(#107)))(::Float32) at /home/pkgeval/.julia/packages/Zygote/YeCEW/src/compiler/interface2.jl:0
   [34] (::Zygote.var"#36#37"{typeof(∂(#107))})(::Float32) at /home/pkgeval/.julia/packages/Zygote/YeCEW/src/compiler/interface.jl:46
   [35] gradient(::Function, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/YeCEW/src/compiler/interface.jl:55
   [36] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:62
   [37] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113
   [38] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:47
   [39] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113
   [40] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:16
  
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:63
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(divpool(xs, x))
            end), us) == (-(ones(2, 3, 4)) / 8192,)
  GPU compilation of kernel partial_mapreduce_grid(typeof(identity), typeof(*), Float32, CartesianIndices{1,Tuple{Base.OneTo{Int64}}}, CartesianIndices{1,Tuple{Base.OneTo{Int64}}}, Val{true}, CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global}, Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#216#219"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}) failed
  KernelError: recursion is currently not supported
  
  Try inspecting the generated code with any of the @device_code_... macros.
  
  Stacktrace:
   [1] _nextind_str at strings/string.jl:141
   [2] nextind at strings/string.jl:137
   [3] _nextind_str at strings/string.jl:141
   [4] _split at strings/util.jl:325
   [5] env_override_minlevel at logging.jl:423
   [6] current_logger_for_env at logging.jl:386
   [7] assertscalar at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/indexing.jl:40
   [8] #216 at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/pool.jl:104
   [9] partial_mapreduce_grid at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/mapreduce.jl:93
  Stacktrace:
   [1] push!(::GPUCompiler.MethodCompileTracer, ::Core.MethodInstance) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/irgen.jl:52
   [2] (::GPUCompiler.var"#hook_emit_function#24"{GPUCompiler.MethodCompileTracer})(::Core.MethodInstance, ::Core.CodeInfo, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/irgen.jl:204
   [3] compile_method_instance(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/irgen.jl:268
   [4] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [5] irgen(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/irgen.jl:326
   [6] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:98 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [9] codegen(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:97
   [10] compile(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:36
   [11] _cufunction(::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#216#219"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:308
   [12] _cufunction at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:302 [inlined]
   [13] #77 at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:21 [inlined]
   [14] get!(::GPUCompiler.var"#77#78"{Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CUDAnative._cufunction),GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#216#219"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}}, ::Dict{UInt64,Any}, ::UInt64) at ./dict.jl:452
   [15] macro expansion at ./lock.jl:183 [inlined]
   [16] check_cache(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#216#219"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:19
   [17] + at ./int.jl:53 [inlined]
   [18] hash_64_64 at ./hashing.jl:35 [inlined]
   [19] hash_uint64 at ./hashing.jl:62 [inlined]
   [20] hx at ./float.jl:568 [inlined]
   [21] hash at ./float.jl:571 [inlined]
   [22] cached_compilation(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#216#219"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:0
   [23] cached_compilation(::Function, ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#216#219"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:37
   [24] cufunction(::Function, ::Type; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:296
   [25] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:291
   [26] macro expansion at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/mapreduce.jl:197 [inlined]
   [27] mapreducedim!(::Function, ::Function, ::CuArray{Float32,1,Nothing}, ::Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#216#219"{Int64,CuArray{Float32,3,Nothing}},Tuple{CuArray{Tuple{Int64,Int64},1,Nothing}}}; init::Float32) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/nvtx/highlevel.jl:83
   [28] _mapreduce(::Function, ::Function, ::CuArray{Tuple{Int64,Int64},1,Nothing}; dims::Function, init::Float32) at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/mapreduce.jl:62
   [29] #mapreduce#31 at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/mapreduce.jl:28 [inlined]
   [30] (::GeometricFlux.var"#214#217"{Float32,CuArray{Int64,2,Nothing},CuArray{Float32,3,Nothing}})(::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/pool.jl:104
   [31] #172#back at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49 [inlined]
   [32] #108 at ./none:0 [inlined]
   [33] (::Zygote.var"#36#37"{typeof(∂(#108))})(::Float32) at /home/pkgeval/.julia/packages/Zygote/YeCEW/src/compiler/interface.jl:46
   [34] gradient(::Function, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/YeCEW/src/compiler/interface.jl:55
   [35] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:63
   [36] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113
   [37] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:47
   [38] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113
   [39] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:16
  
Test Summary:                    | Pass  Error  Total
GeometricFlux                    | 1208     18   1226
  msgpass                        |    1             1
  layer                          |   42            42
  pool                           |   82            82
  grad                           |   35            35
  Test InnerProductDecoder layer |    2             2
  Test VariationalEncoder layer  |    1             1
  Test GAE model                 |    1             1
  Test VGAE model                |    1             1
  Test Linear Algebra            |   24            24
  scatter                        |  360           360
  scatter-staticarray            |  312           312
  simplegraphs                   |   32            32
  weightedgraphs                 |   32            32
  metagraphs                     |   38            38
  Test adjlist                   |    4             4
  utils                          |   22            22
  cuda/scatter                   |   60            60
  cuda/pool                      |  104      8    112
    UInt32                       |   12            12
    UInt64                       |   12            12
    Int32                        |   16            16
    Int64                        |   16            16
    Float32                      |   24      4     28
      sumpool                    |    4             4
      subpool                    |    4             4
      maxpool                    |    4             4
      minpool                    |    4             4
      prodpool                   |    4             4
      divpool                    |           4      4
      meanpool                   |    4             4
    Float64                      |   24      4     28
      sumpool                    |    4             4
      subpool                    |    4             4
      maxpool                    |    4             4
      minpool                    |    4             4
      prodpool                   |    4             4
      divpool                    |           4      4
      meanpool                   |    4             4
  cuda/grad                      |   25     10     35
    scatter                      |   15      6     21
    pool                         |   10      4     14
  cuda/conv                      |   29            29
  cuda/msgpass                   |    1             1
ERROR: LoadError: Some tests did not pass: 1208 passed, 0 failed, 18 errored, 0 broken.
in expression starting at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/runtests.jl:51
ERROR: Package GeometricFlux errored during testing
Stacktrace:
 [1] pkgerror(::String, ::Vararg{String,N} where N) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Pkg/src/Types.jl:53
 [2] test(::Pkg.Types.Context, ::Array{Pkg.Types.PackageSpec,1}; coverage::Bool, julia_args::Cmd, test_args::Cmd, test_fn::Nothing) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Pkg/src/Operations.jl:1510
 [3] test(::Pkg.Types.Context, ::Array{Pkg.Types.PackageSpec,1}; coverage::Bool, test_fn::Nothing, julia_args::Cmd, test_args::Cmd, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Pkg/src/API.jl:316
 [4] test(::Pkg.Types.Context, ::Array{Pkg.Types.PackageSpec,1}) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Pkg/src/API.jl:303
 [5] #test#68 at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Pkg/src/API.jl:297 [inlined]
 [6] test at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Pkg/src/API.jl:297 [inlined]
 [7] #test#67 at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Pkg/src/API.jl:296 [inlined]
 [8] test at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Pkg/src/API.jl:296 [inlined]
 [9] test(::String; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Pkg/src/API.jl:295
 [10] test(::String) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Pkg/src/API.jl:295
 [11] top-level scope at none:16
