Julia Version 1.5.0-DEV.278
Commit 3a469f602d (2020-02-17 02:20 UTC)
Platform Info:
  OS: Linux (x86_64-linux-gnu)
  CPU: Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-9.0.1 (ORCJIT, skylake)
Environment:
  JULIA_DEPOT_PATH = ::/usr/local/share/julia
  JULIA_NUM_THREADS = 2

  Resolving package versions...
  Installed GaussianMixtures ─── v0.3.0
  Installed URIParser ────────── v0.4.0
  Installed CMakeWrapper ─────── v0.2.3
  Installed Blosc ────────────── v0.5.1
  Installed JLD ──────────────── v0.9.2
  Installed OrderedCollections ─ v1.1.0
  Installed SpecialFunctions ─── v0.10.0
  Installed StatsBase ────────── v0.32.1
  Installed OpenBLAS_jll ─────── v0.3.7+5
  Installed Missings ─────────── v0.4.3
  Installed Parameters ───────── v0.12.0
  Installed BinDeps ──────────── v1.0.0
  Installed StatsFuns ────────── v0.9.4
  Installed CMake ────────────── v1.2.0
  Installed LegacyStrings ────── v0.4.1
  Installed Clustering ───────── v0.13.3
  Installed Arpack_jll ───────── v3.5.0+2
  Installed PDMats ───────────── v0.9.11
  Installed OpenSpecFun_jll ──── v0.5.3+1
  Installed FillArrays ───────── v0.8.4
  Installed Distances ────────── v0.8.2
  Installed Arpack ───────────── v0.4.0
  Installed HDF5 ─────────────── v0.12.5
  Installed Rmath ────────────── v0.6.0
  Installed SortingAlgorithms ── v0.3.1
  Installed QuadGK ───────────── v2.3.1
  Installed ScikitLearnBase ──── v0.5.0
  Installed Compat ───────────── v2.2.0
  Installed BinaryProvider ───── v0.5.8
  Installed DataAPI ──────────── v1.1.0
  Installed FileIO ───────────── v1.2.2
  Installed Distributions ────── v0.22.4
  Installed NearestNeighbors ─── v0.4.4
  Installed DataStructures ───── v0.17.9
  Installed StaticArrays ─────── v0.12.1
#=#=#                                                                         #######                                                                   10.0%#####################                                                     29.8%########################################                                  55.9%###########################################################               83.0%######################################################################## 100.0%
#=#=#                                                                         ######################################################################## 100.0%
#####                                                                      7.0%######################################################################## 100.0%
   Updating `~/.julia/environments/v1.5/Project.toml`
  [cc18c42c] + GaussianMixtures v0.3.0
   Updating `~/.julia/environments/v1.5/Manifest.toml`
  [7d9fca2a] + Arpack v0.4.0
  [68821587] + Arpack_jll v3.5.0+2
  [9e28174c] + BinDeps v1.0.0
  [b99e7846] + BinaryProvider v0.5.8
  [a74b3585] + Blosc v0.5.1
  [631607c0] + CMake v1.2.0
  [d5fb7624] + CMakeWrapper v0.2.3
  [aaaa29a8] + Clustering v0.13.3
  [34da2185] + Compat v2.2.0
  [9a962f9c] + DataAPI v1.1.0
  [864edb3b] + DataStructures v0.17.9
  [b4f34e82] + Distances v0.8.2
  [31c24e10] + Distributions v0.22.4
  [5789e2e9] + FileIO v1.2.2
  [1a297f60] + FillArrays v0.8.4
  [cc18c42c] + GaussianMixtures v0.3.0
  [f67ccb44] + HDF5 v0.12.5
  [4138dd39] + JLD v0.9.2
  [1b4a561d] + LegacyStrings v0.4.1
  [e1d29d7a] + Missings v0.4.3
  [b8a86587] + NearestNeighbors v0.4.4
  [4536629a] + OpenBLAS_jll v0.3.7+5
  [efe28fd5] + OpenSpecFun_jll v0.5.3+1
  [bac558e1] + OrderedCollections v1.1.0
  [90014a1f] + PDMats v0.9.11
  [d96e819e] + Parameters v0.12.0
  [1fd47b50] + QuadGK v2.3.1
  [79098fc4] + Rmath v0.6.0
  [6e75b9c4] + ScikitLearnBase v0.5.0
  [a2af1166] + SortingAlgorithms v0.3.1
  [276daf66] + SpecialFunctions v0.10.0
  [90137ffa] + StaticArrays v0.12.1
  [2913bbd2] + StatsBase v0.32.1
  [4c63d2b9] + StatsFuns v0.9.4
  [30578b45] + URIParser v0.4.0
  [2a0f44e3] + Base64 
  [ade2ca70] + Dates 
  [8bb1440f] + DelimitedFiles 
  [8ba89e20] + Distributed 
  [b77e0a4c] + InteractiveUtils 
  [76f85450] + LibGit2 
  [8f399da3] + Libdl 
  [37e2e46d] + LinearAlgebra 
  [56ddb016] + Logging 
  [d6f4376e] + Markdown 
  [a63ad114] + Mmap 
  [44cfe95a] + Pkg 
  [de0858da] + Printf 
  [3fa0cd96] + REPL 
  [9a3f8284] + Random 
  [ea8e919c] + SHA 
  [9e88b42a] + Serialization 
  [1a1011a3] + SharedArrays 
  [6462fe0b] + Sockets 
  [2f01184e] + SparseArrays 
  [10745b16] + Statistics 
  [4607b0f0] + SuiteSparse 
  [8dfed614] + Test 
  [cf7118a7] + UUIDs 
  [4ec0a83e] + Unicode 
   Building CMake → `~/.julia/packages/CMake/ULbyn/deps/build.log`
   Building Blosc → `~/.julia/packages/Blosc/lzFr0/deps/build.log`
   Building HDF5 ─→ `~/.julia/packages/HDF5/Zh9on/deps/build.log`
   Building Rmath → `~/.julia/packages/Rmath/BoBag/deps/build.log`
    Testing GaussianMixtures
Status `/tmp/jl_InypzW/Manifest.toml`
  [7d9fca2a] Arpack v0.4.0
  [68821587] Arpack_jll v3.5.0+2
  [9e28174c] BinDeps v1.0.0
  [b99e7846] BinaryProvider v0.5.8
  [a74b3585] Blosc v0.5.1
  [631607c0] CMake v1.2.0
  [d5fb7624] CMakeWrapper v0.2.3
  [aaaa29a8] Clustering v0.13.3
  [34da2185] Compat v2.2.0
  [9a962f9c] DataAPI v1.1.0
  [864edb3b] DataStructures v0.17.9
  [b4f34e82] Distances v0.8.2
  [31c24e10] Distributions v0.22.4
  [5789e2e9] FileIO v1.2.2
  [1a297f60] FillArrays v0.8.4
  [cc18c42c] GaussianMixtures v0.3.0
  [f67ccb44] HDF5 v0.12.5
  [4138dd39] JLD v0.9.2
  [1b4a561d] LegacyStrings v0.4.1
  [e1d29d7a] Missings v0.4.3
  [b8a86587] NearestNeighbors v0.4.4
  [4536629a] OpenBLAS_jll v0.3.7+5
  [efe28fd5] OpenSpecFun_jll v0.5.3+1
  [bac558e1] OrderedCollections v1.1.0
  [90014a1f] PDMats v0.9.11
  [d96e819e] Parameters v0.12.0
  [1fd47b50] QuadGK v2.3.1
  [79098fc4] Rmath v0.6.0
  [6e75b9c4] ScikitLearnBase v0.5.0
  [a2af1166] SortingAlgorithms v0.3.1
  [276daf66] SpecialFunctions v0.10.0
  [90137ffa] StaticArrays v0.12.1
  [2913bbd2] StatsBase v0.32.1
  [4c63d2b9] StatsFuns v0.9.4
  [30578b45] URIParser v0.4.0
  [2a0f44e3] Base64 
  [ade2ca70] Dates 
  [8bb1440f] DelimitedFiles 
  [8ba89e20] Distributed 
  [b77e0a4c] InteractiveUtils 
  [76f85450] LibGit2 
  [8f399da3] Libdl 
  [37e2e46d] LinearAlgebra 
  [56ddb016] Logging 
  [d6f4376e] Markdown 
  [a63ad114] Mmap 
  [44cfe95a] Pkg 
  [de0858da] Printf 
  [3fa0cd96] REPL 
  [9a3f8284] Random 
  [ea8e919c] SHA 
  [9e88b42a] Serialization 
  [1a1011a3] SharedArrays 
  [6462fe0b] Sockets 
  [2f01184e] SparseArrays 
  [10745b16] Statistics 
  [4607b0f0] SuiteSparse 
  [8dfed614] Test 
  [cf7118a7] UUIDs 
  [4ec0a83e] Unicode 
[ Info: Testing Data
(100000, -2.233097831377835e6, [76456.11930745648, 23543.88069254354], [-13287.662801152423 7617.067944595449 -5347.433398896474; 13518.236671350334 -7435.88057948821 5578.070621833112], [[71996.61921176632 -3064.5876161567176 3878.410964197732; -3064.5876161567176 89189.35757221762 -4489.883231839016; 3878.410964197732 -4489.883231839016 76716.91592943041], [28214.39760636968 2911.677586847452 -4395.622488031691; 2911.6775868474515 11365.117548569237 4433.692426710168; -4395.622488031691 4433.692426710168 23528.296581699655]])
┌ Warning: rmprocs: process 1 not removed
└ @ Distributed /workspace/srcdir/usr/share/julia/stdlib/v1.5/Distributed/src/cluster.jl:1030
[ Info: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.319724e+03
      1       8.877313e+02      -4.319927e+02 |        4
      2       8.644972e+02      -2.323412e+01 |        2
      3       8.606920e+02      -3.805206e+00 |        2
      4       8.557842e+02      -4.907853e+00 |        2
      5       8.497260e+02      -6.058195e+00 |        0
      6       8.497260e+02       0.000000e+00 |        0
K-means converged with 6 iterations (objv = 849.725959955128)
┌ Info: K-means with 272 data points using 6 iterations
└ 11.3 data points per parameter
[ Info: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
┌ Info: EM with 272 data points 0 iterations avll -2.084014
└ 5.8 data points per parameter
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:221
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:221
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:221
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:221
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:631 [inlined]
└ @ Core ./broadcast.jl:631
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:230
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:230
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:631 [inlined]
└ @ Core ./broadcast.jl:631
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:631 [inlined]
└ @ Core ./broadcast.jl:631
[ Info: iteration 1, lowerbound -3.885621
[ Info: iteration 2, lowerbound -3.802858
[ Info: iteration 3, lowerbound -3.723143
[ Info: iteration 4, lowerbound -3.621056
[ Info: iteration 5, lowerbound -3.494794
[ Info: iteration 6, lowerbound -3.354786
[ Info: iteration 7, lowerbound -3.227150
[ Info: dropping number of Gaussions to 7
[ Info: iteration 8, lowerbound -3.124430
[ Info: dropping number of Gaussions to 6
[ Info: iteration 9, lowerbound -3.051380
[ Info: dropping number of Gaussions to 5
[ Info: iteration 10, lowerbound -2.994178
[ Info: dropping number of Gaussions to 4
[ Info: iteration 11, lowerbound -2.935996
[ Info: iteration 12, lowerbound -2.883192
[ Info: iteration 13, lowerbound -2.843482
[ Info: iteration 14, lowerbound -2.818363
[ Info: iteration 15, lowerbound -2.809022
[ Info: dropping number of Gaussions to 3
[ Info: iteration 16, lowerbound -2.801229
[ Info: iteration 17, lowerbound -2.791653
[ Info: iteration 18, lowerbound -2.783299
[ Info: iteration 19, lowerbound -2.771388
[ Info: iteration 20, lowerbound -2.754540
[ Info: iteration 21, lowerbound -2.731308
[ Info: iteration 22, lowerbound -2.700562
[ Info: iteration 23, lowerbound -2.662040
[ Info: iteration 24, lowerbound -2.616844
[ Info: iteration 25, lowerbound -2.567562
[ Info: iteration 26, lowerbound -2.517786
[ Info: iteration 27, lowerbound -2.470976
[ Info: iteration 28, lowerbound -2.429184
[ Info: iteration 29, lowerbound -2.392548
[ Info: iteration 30, lowerbound -2.360195
[ Info: iteration 31, lowerbound -2.332576
[ Info: iteration 32, lowerbound -2.313456
[ Info: iteration 33, lowerbound -2.307448
[ Info: dropping number of Gaussions to 2
[ Info: iteration 34, lowerbound -2.302928
[ Info: iteration 35, lowerbound -2.299260
[ Info: iteration 36, lowerbound -2.299256
[ Info: iteration 37, lowerbound -2.299254
[ Info: iteration 38, lowerbound -2.299254
[ Info: iteration 39, lowerbound -2.299253
[ Info: iteration 40, lowerbound -2.299253
[ Info: iteration 41, lowerbound -2.299253
[ Info: iteration 42, lowerbound -2.299253
[ Info: iteration 43, lowerbound -2.299253
[ Info: iteration 44, lowerbound -2.299253
[ Info: iteration 45, lowerbound -2.299253
[ Info: iteration 46, lowerbound -2.299253
[ Info: iteration 47, lowerbound -2.299253
[ Info: iteration 48, lowerbound -2.299253
[ Info: iteration 49, lowerbound -2.299253
[ Info: iteration 50, lowerbound -2.299253
[ Info: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
History[Mon Feb 17 14:31:30 2020: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
, Mon Feb 17 14:31:38 2020: K-means with 272 data points using 6 iterations
11.3 data points per parameter
, Mon Feb 17 14:31:40 2020: EM with 272 data points 0 iterations avll -2.084014
5.8 data points per parameter
, Mon Feb 17 14:31:42 2020: GMM converted to Variational GMM
, Mon Feb 17 14:31:50 2020: iteration 1, lowerbound -3.885621
, Mon Feb 17 14:31:50 2020: iteration 2, lowerbound -3.802858
, Mon Feb 17 14:31:50 2020: iteration 3, lowerbound -3.723143
, Mon Feb 17 14:31:50 2020: iteration 4, lowerbound -3.621056
, Mon Feb 17 14:31:50 2020: iteration 5, lowerbound -3.494794
, Mon Feb 17 14:31:50 2020: iteration 6, lowerbound -3.354786
, Mon Feb 17 14:31:50 2020: iteration 7, lowerbound -3.227150
, Mon Feb 17 14:31:50 2020: dropping number of Gaussions to 7
, Mon Feb 17 14:31:50 2020: iteration 8, lowerbound -3.124430
, Mon Feb 17 14:31:50 2020: dropping number of Gaussions to 6
, Mon Feb 17 14:31:50 2020: iteration 9, lowerbound -3.051380
, Mon Feb 17 14:31:50 2020: dropping number of Gaussions to 5
, Mon Feb 17 14:31:50 2020: iteration 10, lowerbound -2.994178
, Mon Feb 17 14:31:50 2020: dropping number of Gaussions to 4
, Mon Feb 17 14:31:50 2020: iteration 11, lowerbound -2.935996
, Mon Feb 17 14:31:50 2020: iteration 12, lowerbound -2.883192
, Mon Feb 17 14:31:50 2020: iteration 13, lowerbound -2.843482
, Mon Feb 17 14:31:50 2020: iteration 14, lowerbound -2.818363
, Mon Feb 17 14:31:50 2020: iteration 15, lowerbound -2.809022
, Mon Feb 17 14:31:50 2020: dropping number of Gaussions to 3
, Mon Feb 17 14:31:50 2020: iteration 16, lowerbound -2.801229
, Mon Feb 17 14:31:50 2020: iteration 17, lowerbound -2.791653
, Mon Feb 17 14:31:50 2020: iteration 18, lowerbound -2.783299
, Mon Feb 17 14:31:50 2020: iteration 19, lowerbound -2.771388
, Mon Feb 17 14:31:50 2020: iteration 20, lowerbound -2.754540
, Mon Feb 17 14:31:51 2020: iteration 21, lowerbound -2.731308
, Mon Feb 17 14:31:51 2020: iteration 22, lowerbound -2.700562
, Mon Feb 17 14:31:51 2020: iteration 23, lowerbound -2.662040
, Mon Feb 17 14:31:51 2020: iteration 24, lowerbound -2.616844
, Mon Feb 17 14:31:51 2020: iteration 25, lowerbound -2.567562
, Mon Feb 17 14:31:51 2020: iteration 26, lowerbound -2.517786
, Mon Feb 17 14:31:51 2020: iteration 27, lowerbound -2.470976
, Mon Feb 17 14:31:51 2020: iteration 28, lowerbound -2.429184
, Mon Feb 17 14:31:51 2020: iteration 29, lowerbound -2.392548
, Mon Feb 17 14:31:51 2020: iteration 30, lowerbound -2.360195
, Mon Feb 17 14:31:51 2020: iteration 31, lowerbound -2.332576
, Mon Feb 17 14:31:51 2020: iteration 32, lowerbound -2.313456
, Mon Feb 17 14:31:51 2020: iteration 33, lowerbound -2.307448
, Mon Feb 17 14:31:51 2020: dropping number of Gaussions to 2
, Mon Feb 17 14:31:51 2020: iteration 34, lowerbound -2.302928
, Mon Feb 17 14:31:51 2020: iteration 35, lowerbound -2.299260
, Mon Feb 17 14:31:51 2020: iteration 36, lowerbound -2.299256
, Mon Feb 17 14:31:51 2020: iteration 37, lowerbound -2.299254
, Mon Feb 17 14:31:51 2020: iteration 38, lowerbound -2.299254
, Mon Feb 17 14:31:51 2020: iteration 39, lowerbound -2.299253
, Mon Feb 17 14:31:51 2020: iteration 40, lowerbound -2.299253
, Mon Feb 17 14:31:51 2020: iteration 41, lowerbound -2.299253
, Mon Feb 17 14:31:51 2020: iteration 42, lowerbound -2.299253
, Mon Feb 17 14:31:51 2020: iteration 43, lowerbound -2.299253
, Mon Feb 17 14:31:51 2020: iteration 44, lowerbound -2.299253
, Mon Feb 17 14:31:51 2020: iteration 45, lowerbound -2.299253
, Mon Feb 17 14:31:51 2020: iteration 46, lowerbound -2.299253
, Mon Feb 17 14:31:51 2020: iteration 47, lowerbound -2.299253
, Mon Feb 17 14:31:51 2020: iteration 48, lowerbound -2.299253
, Mon Feb 17 14:31:51 2020: iteration 49, lowerbound -2.299253
, Mon Feb 17 14:31:51 2020: iteration 50, lowerbound -2.299253
, Mon Feb 17 14:31:51 2020: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [178.04509227140505, 95.95490772859512]
β = [178.04509227140505, 95.95490772859512]
m = [4.250300732901752 79.28686693894745; 2.0002292573941 53.85198717047565]
ν = [180.04509227140505, 97.95490772859512]
W = LinearAlgebra.UpperTriangular{Float64,Array{Float64,2}}[[0.184041555424076 -0.0076440490471628725; 0.0 0.008581705159527935], [0.375876361828638 -0.008953123834870434; 0.0 0.012748664779337471]]
┌ Warning: Assignment to `x` in soft scope is ambiguous because a global variable by the same name exists: `x` will be treated as a new local. Disambiguate by using `local x` to suppress this warning or `global x` to assign to the existing global variable.
└ @ ~/.julia/packages/GaussianMixtures/RGtTJ/test/train.jl:7
┌ Warning: Assignment to `p` in soft scope is ambiguous because a global variable by the same name exists: `p` will be treated as a new local. Disambiguate by using `local p` to suppress this warning or `global p` to assign to the existing global variable.
└ @ ~/.julia/packages/GaussianMixtures/RGtTJ/test/train.jl:17
Kind: diag, size256
nx: 100000 sum(zeroth order stats): 99999.99999999999
avll from stats: -1.0014059353377953
avll from llpg:  -1.0014059353377958
avll direct:     -1.0014059353377958
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 100000.0
avll from stats: -1.0139835982369685
avll from llpg:  -1.0139835982369685
avll direct:     -1.0139835982369685
sum posterior: 100000.0
┌ Warning: Assignment to `x` in soft scope is ambiguous because a global variable by the same name exists: `x` will be treated as a new local. Disambiguate by using `local x` to suppress this warning or `global x` to assign to the existing global variable.
└ @ ~/.julia/packages/GaussianMixtures/RGtTJ/test/train.jl:26
32×26 Array{Float64,2}:
 -0.0939715   -0.0386582   -0.112703    -0.127534     0.0933965     0.105062    -0.0774536   -0.0844203   -0.00423548   0.118312    -0.0806055   -0.0068157   -0.0198558    0.0994385   -0.206359    -0.033135     -0.0197015     0.0103386    0.0583319   -0.124135     -0.00856955  -0.00699382   0.0409627   -0.144066     0.00155079   0.130831
  0.0299117   -0.176771    -0.00699444   0.0639075   -0.095187     -0.0520803   -0.0671109   -0.122909     0.0506937   -0.0510004   -0.0678451   -0.0126014    0.125446     0.0451399   -0.153438    -0.236862     -0.0471345    -0.139233    -0.0456768   -0.00249334   -0.100113     0.063925     0.0029843   -0.125325     0.171725     0.124192
 -0.155429    -0.0819938    0.013156     0.10286      0.113661      0.0894785    0.133083    -0.104695    -0.0878694    0.044704     0.0509752   -0.0295975    0.144348    -0.134231     0.0744889   -0.0319985     0.00129614   -0.100399    -0.00396908   0.0232852    -0.134424     0.05949      0.21872     -0.120358     0.102592    -0.0709916
  0.0403796   -0.0450284    0.013297    -0.0250344   -0.15363      -0.0617854   -0.0924494    0.0268398   -0.00798516  -0.142289     0.0112656   -0.00875597   0.00668526   0.0881923    0.112372    -0.0206635     0.104289     -0.127868    -0.0164899   -0.0141085    -0.00332734  -0.01651      0.00039165   0.0920577    0.0461382    0.0195582
  0.013751    -0.00922392  -0.029411     0.116733     0.0964169    -0.0796672    0.181943     0.0130308   -0.111137     0.12829      0.136855     0.0715498   -0.0107036    0.102555     0.0383184   -0.0214653     0.102913     -0.0136447    0.0673705    0.0762951     0.129431     0.0514971    0.0209328    0.0381837   -0.114854    -0.0512163
 -0.0457677    0.0424105   -0.0669569    0.110187     0.0398723     0.0583618   -0.0215808    0.0434255   -0.0861106   -0.0812746    0.0110392    0.120146    -0.0149729   -0.174878     0.145697     0.000332755  -0.0565181     0.0236063   -0.00349557   0.203306     -0.246088    -0.0784366    0.0514862    0.00913828  -0.127037     0.0829216
 -0.0909559   -0.00615021   0.0413861   -0.107921    -0.00827581    0.0624088    0.0747226    0.0676805   -0.0470779    0.0338223   -0.235335    -0.0297022    0.0064848    0.0658838   -0.00174556   0.0353713     0.0592426     0.075862     0.134657    -0.112816     -0.0569201    0.246493     0.11685      0.153107     0.0922578   -0.0117204
 -0.0434169   -0.0115728    0.143118     0.100316     0.113345     -0.0430649   -0.0908832   -0.156529    -0.0733979   -0.098866     0.0714245    0.19463     -0.0188904    0.0756574   -0.108849    -0.0802982     0.0733555     0.144256     0.12515     -0.158629      0.0296789   -0.039695    -0.0803523    0.014288    -0.040557     0.0112057
 -0.0384111   -0.24122     -0.0409835   -0.0258563   -0.176352      0.06145     -0.236063     0.023272    -0.00856967  -0.00352617   0.0871486   -0.142644     0.0239218   -0.0413995    0.0583014    0.131356      0.00504666   -0.158692     0.135906    -0.0234521    -0.0494766   -0.0594842   -0.038054    -0.00430506  -0.0127612   -0.0816161
 -0.131149     0.122768    -0.0410981    0.00985497   0.000885133  -0.13707      0.0255056    0.0109401   -0.0814714    0.0191165   -0.0625961   -0.114999     0.175887    -0.170792    -0.0344746   -0.0991265    -0.000806971   0.260914    -0.147839    -0.155027     -0.145174     0.0260092   -0.0461501   -0.197026    -0.237595    -0.0450547
  0.216761     0.0640207    0.0459604    0.060565    -0.0655059     0.0362752    0.0380405    0.101619    -0.00813235  -0.00585778  -0.159536    -0.0320968   -0.0325679    0.0495148   -0.0330549    0.106423     -0.0862782     0.0746718   -0.0070905    0.116106     -0.217463    -0.0204792   -0.00582822  -0.138751    -0.0639382    0.056273
  0.0197418    0.0417275    0.181978     0.0682995   -0.119202     -0.0243881    0.0497543    0.0479944   -0.0639988    0.250102     0.00197134  -0.0211917    0.0112517   -0.134914     0.0638371   -0.0689741     0.195328     -0.0634506    0.0414388   -0.141337     -0.0147764   -0.100161    -0.0879943   -0.00845317   0.0726583   -0.099821
 -0.0093006    0.008782     0.0970838   -0.0355472    0.0183441     0.0539274   -0.107095    -0.146322    -0.044109    -0.0281418    0.210456     0.191119    -0.12839     -0.00762731  -0.134247     0.0496779    -0.00613335   -0.0104916    0.0315904   -0.0675823    -0.0355475    0.144193     0.159033     0.0102925   -0.168689     0.0283744
 -0.162748    -0.115578    -0.0459951    0.233095    -0.152405     -0.166596    -0.00789151  -0.102947    -0.0542819    0.0265906    0.0999557   -0.19674     -0.057491    -0.0155729   -0.0948743    0.0548837    -0.157878      0.00292544   0.178089    -0.184698      0.0820504    0.03696      0.0368042   -0.0632134   -0.0737085   -0.0623959
  0.0513795    0.097443    -0.0284203    0.15734     -0.100715      0.0801147   -0.0759165   -0.0604509    0.091038     0.0547438    0.00713369   0.0755045    0.138777     0.0110617   -0.0142473   -0.157776     -0.0578785     0.0701284    0.0373575   -0.0256555     0.108323     0.0579864   -0.0702406   -0.062141     0.0991656   -0.00819043
 -0.0208432   -0.0256212   -0.155733     0.05839     -0.0470228     0.016749     0.057511     0.0924277   -0.0780711    0.0763955    0.063171    -0.0885001   -0.075405     0.166959    -0.0942452    0.0403347     0.077419     -0.0446766   -0.0294616   -0.218142     -0.0883892    0.0397065    0.107145    -0.175756     0.236202    -0.0308521
  0.100454    -0.113361     0.156023     0.0560522   -0.0203343    -0.0369709    0.0686888   -0.160544    -0.0476336   -0.0426484   -0.0634232   -0.00894327  -0.0308464    0.18765      0.0331815   -0.00456605    0.0424725     0.0668224    0.0537179    0.0307743    -0.07799      0.0265503    0.135766    -0.0799362    0.168956    -0.0951996
  0.016283     0.0790629    0.136324     0.0752621   -0.107367      0.0383559   -0.0123785   -0.0242279   -0.0253155   -0.163907     0.120533     0.0589445   -0.00968818   0.175509     0.001703     0.0430885     0.0521578     0.067564     0.0517426    0.0704505    -0.0563276    0.0915765    0.0661477   -0.100449     0.0566418    0.0420336
 -0.117044     0.0619085    0.00865152  -0.10807     -0.161427      0.189743    -0.00643555   0.138366     0.00381978   0.0708033    0.00520588  -0.129269    -0.13052      0.0988381    0.131068     0.0153761    -0.0105909     0.0373661    0.0737004    0.000537288   0.107279    -0.0796968   -0.0413163   -0.112894    -0.063286     0.0599532
  0.00145955  -0.0288659    0.0452935    0.122417    -0.0141575     0.0377821   -0.0128899    0.0709681    0.0422017   -0.0364638    0.125853     0.123699     0.0602814    0.151638    -0.126556    -0.103658      0.109405      0.0668366    0.0496609   -0.094327     -0.169216    -0.203896     0.0477994   -0.194745     0.163858    -0.0975566
  0.147575    -0.0307757    0.0256278    0.2535      -0.0759089    -0.096766     0.0916832   -0.0269108   -0.0544492    0.134401     0.0215319    0.0608344   -0.0945538   -0.00208418   0.0436063   -0.0338198     0.0579367     0.0157839   -0.069659    -0.0994546    -0.047368    -0.108494    -0.0228673    0.0166206   -0.0979746    0.300792
  0.00197441   0.186116    -0.0424717   -0.0786516   -0.0470229     0.0421812   -0.0102986    0.0474685    0.0584788    0.0812538   -0.0526745    0.236413    -0.0606137    0.00463339  -0.00181194   0.00826013    0.0614373    -0.156571    -0.0861928    0.0437272     0.076321    -0.0599623   -0.124218     0.0349648   -0.0168129   -0.104634
  0.147012    -0.014739    -0.154226    -0.0453111    0.04923      -0.0068123   -0.0401559   -0.0991419    0.162417    -0.0185734   -0.103263    -0.0328668    0.142046    -0.00257241  -0.0492162    0.0312535     0.0170193    -0.101794    -0.0135498   -0.106961      0.183032     0.135254    -0.0408118   -0.066159    -0.0910518   -0.0419639
 -0.00574156  -0.0386336    0.0451807   -0.00059913   0.134037     -0.0281566    0.0824246   -0.00535663  -0.126894     0.0851987    0.0816338   -0.114873    -0.175836     0.0373885    0.0627659    0.00172247    0.0239787    -0.00390856   0.194713    -0.103995     -0.0792771   -0.0307985    0.0385561    0.117384    -0.0324811   -0.00816369
  0.0769312   -0.0988236   -0.0439651    0.0821949    0.165753      0.0241948    0.019461     0.17208      0.0458536    0.0394927    0.0426428    0.0165406   -0.0630433   -0.0476483   -0.063428    -0.097274     -0.0243495     0.0259213   -0.1262       0.0400744    -0.0439319    0.0988176   -0.0421386   -0.0452952   -0.187133    -0.0197412
  0.118971     0.0153      -0.0868851   -0.0251672    0.113194      0.00969714  -0.147355    -0.130169    -0.052344     0.0206989   -0.0368593   -0.0809909    0.0902658    0.0485284   -0.00247018   0.0756446     0.150413      0.03005     -0.171698    -0.0122692     0.0181858    0.0696822    0.027717     0.0213101    0.0362895   -0.0839497
 -0.170065    -0.0488612   -0.0769582    0.18805      0.104415     -0.00268156   0.128465     0.0315279    0.105187    -0.0383483   -0.187218     0.11963      0.0548261   -0.148603     0.0293844    0.191138     -0.0607743     0.0256526   -0.077342     0.00131282   -0.110601    -0.0847285   -0.0402249   -0.0282771   -0.0954483   -0.181142
  0.0440685    0.167271     0.114896    -0.250455     0.0773533    -0.0134862   -0.0264444    0.0161394    0.00897747  -0.121811     0.032765     0.0945292    0.0517719   -0.0593027    0.0349206   -0.0114389    -0.29417      -0.0115821   -0.0419985   -0.19093       0.0601925   -0.03615      0.120483     0.0116413   -0.0917942   -0.0819109
 -0.0349866   -0.111727     0.0410447   -0.073974     0.0182135     0.0726201   -0.0252495    0.0183957    0.0456936    0.0429392    0.164762    -0.0192889    0.224371     0.139016     0.0794052   -0.0788823    -0.0431136    -0.0432847   -0.183253     0.0136998    -0.0202063   -0.0412545   -0.0635073    0.038203     0.21429      0.0299071
  0.00913042   0.250444    -0.222462    -0.0340419   -0.136514     -0.0102039   -0.0754346   -0.0302774    0.0919546    0.0552744   -0.0724017   -0.0565939    0.0803755   -0.00428648   0.0956095   -0.00132479    0.0924928     0.136089     0.00646499  -0.0703787     0.012259    -0.0270471   -0.00509296   0.13426      0.121863     0.0420421
  0.0243878   -0.0307681    0.0527954   -0.0620307   -0.017428     -0.012122    -0.0210281   -0.198546     0.0619851    0.112996    -0.0421325   -0.0909305    0.0905294    0.129527    -0.117669    -0.060963     -0.035245      0.0609502    0.0150346   -0.000615651  -0.106995     0.13855     -0.0920629   -0.0931844   -0.00832312  -0.00136724
 -0.102882     0.229935     0.0231617    0.0729978    0.0535284    -0.102831    -0.157861     0.131802    -0.0542365   -0.16114     -0.0164122    0.106437    -0.0667607   -0.167986     0.264369    -0.119837     -0.00157018    0.0453321   -0.104225     0.0479886    -0.060897    -0.0921184   -0.0581659   -0.0447576   -0.035436    -0.167039kind diag, method split
┌ Info: 0: avll = 
└   tll[1] = -1.3992461709652484
[ Info: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.399328
[ Info: iteration 2, average log likelihood -1.399240
[ Info: iteration 3, average log likelihood -1.398486
[ Info: iteration 4, average log likelihood -1.389593
[ Info: iteration 5, average log likelihood -1.369762
[ Info: iteration 6, average log likelihood -1.363437
[ Info: iteration 7, average log likelihood -1.361773
[ Info: iteration 8, average log likelihood -1.360472
[ Info: iteration 9, average log likelihood -1.359254
[ Info: iteration 10, average log likelihood -1.358068
[ Info: iteration 11, average log likelihood -1.356939
[ Info: iteration 12, average log likelihood -1.355973
[ Info: iteration 13, average log likelihood -1.355268
[ Info: iteration 14, average log likelihood -1.354769
[ Info: iteration 15, average log likelihood -1.354435
[ Info: iteration 16, average log likelihood -1.354212
[ Info: iteration 17, average log likelihood -1.354053
[ Info: iteration 18, average log likelihood -1.353932
[ Info: iteration 19, average log likelihood -1.353846
[ Info: iteration 20, average log likelihood -1.353793
[ Info: iteration 21, average log likelihood -1.353760
[ Info: iteration 22, average log likelihood -1.353742
[ Info: iteration 23, average log likelihood -1.353732
[ Info: iteration 24, average log likelihood -1.353727
[ Info: iteration 25, average log likelihood -1.353724
[ Info: iteration 26, average log likelihood -1.353722
[ Info: iteration 27, average log likelihood -1.353721
[ Info: iteration 28, average log likelihood -1.353720
[ Info: iteration 29, average log likelihood -1.353719
[ Info: iteration 30, average log likelihood -1.353719
[ Info: iteration 31, average log likelihood -1.353719
[ Info: iteration 32, average log likelihood -1.353719
[ Info: iteration 33, average log likelihood -1.353719
[ Info: iteration 34, average log likelihood -1.353719
[ Info: iteration 35, average log likelihood -1.353718
[ Info: iteration 36, average log likelihood -1.353718
[ Info: iteration 37, average log likelihood -1.353718
[ Info: iteration 38, average log likelihood -1.353718
[ Info: iteration 39, average log likelihood -1.353718
[ Info: iteration 40, average log likelihood -1.353718
[ Info: iteration 41, average log likelihood -1.353718
[ Info: iteration 42, average log likelihood -1.353718
[ Info: iteration 43, average log likelihood -1.353718
[ Info: iteration 44, average log likelihood -1.353718
[ Info: iteration 45, average log likelihood -1.353718
[ Info: iteration 46, average log likelihood -1.353718
[ Info: iteration 47, average log likelihood -1.353718
[ Info: iteration 48, average log likelihood -1.353718
[ Info: iteration 49, average log likelihood -1.353718
[ Info: iteration 50, average log likelihood -1.353718
┌ Info: EM with 100000 data points 50 iterations avll -1.353718
└ 952.4 data points per parameter
┌ Info: 1
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.3993282894209302
│     -1.3992403936378666
│      ⋮
└     -1.3537183814156097
[ Info: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.353824
[ Info: iteration 2, average log likelihood -1.353716
[ Info: iteration 3, average log likelihood -1.353299
[ Info: iteration 4, average log likelihood -1.349137
[ Info: iteration 5, average log likelihood -1.335369
[ Info: iteration 6, average log likelihood -1.325289
[ Info: iteration 7, average log likelihood -1.322034
[ Info: iteration 8, average log likelihood -1.320420
[ Info: iteration 9, average log likelihood -1.319137
[ Info: iteration 10, average log likelihood -1.318004
[ Info: iteration 11, average log likelihood -1.317179
[ Info: iteration 12, average log likelihood -1.316650
[ Info: iteration 13, average log likelihood -1.316371
[ Info: iteration 14, average log likelihood -1.316220
[ Info: iteration 15, average log likelihood -1.316123
[ Info: iteration 16, average log likelihood -1.316049
[ Info: iteration 17, average log likelihood -1.315987
[ Info: iteration 18, average log likelihood -1.315927
[ Info: iteration 19, average log likelihood -1.315864
[ Info: iteration 20, average log likelihood -1.315792
[ Info: iteration 21, average log likelihood -1.315701
[ Info: iteration 22, average log likelihood -1.315574
[ Info: iteration 23, average log likelihood -1.315359
[ Info: iteration 24, average log likelihood -1.314999
[ Info: iteration 25, average log likelihood -1.314534
[ Info: iteration 26, average log likelihood -1.314047
[ Info: iteration 27, average log likelihood -1.313576
[ Info: iteration 28, average log likelihood -1.313173
[ Info: iteration 29, average log likelihood -1.312880
[ Info: iteration 30, average log likelihood -1.312684
[ Info: iteration 31, average log likelihood -1.312543
[ Info: iteration 32, average log likelihood -1.312421
[ Info: iteration 33, average log likelihood -1.312294
[ Info: iteration 34, average log likelihood -1.312131
[ Info: iteration 35, average log likelihood -1.311885
[ Info: iteration 36, average log likelihood -1.311523
[ Info: iteration 37, average log likelihood -1.311122
[ Info: iteration 38, average log likelihood -1.310789
[ Info: iteration 39, average log likelihood -1.310527
[ Info: iteration 40, average log likelihood -1.310332
[ Info: iteration 41, average log likelihood -1.310192
[ Info: iteration 42, average log likelihood -1.310094
[ Info: iteration 43, average log likelihood -1.310029
[ Info: iteration 44, average log likelihood -1.309986
[ Info: iteration 45, average log likelihood -1.309958
[ Info: iteration 46, average log likelihood -1.309939
[ Info: iteration 47, average log likelihood -1.309927
[ Info: iteration 48, average log likelihood -1.309919
[ Info: iteration 49, average log likelihood -1.309913
[ Info: iteration 50, average log likelihood -1.309909
┌ Info: EM with 100000 data points 50 iterations avll -1.309909
└ 473.9 data points per parameter
┌ Info: 2
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.3538239834761507
│     -1.3537157948227831
│      ⋮
└     -1.309908932321457
[ Info: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.310099
[ Info: iteration 2, average log likelihood -1.309904
[ Info: iteration 3, average log likelihood -1.309349
[ Info: iteration 4, average log likelihood -1.304961
[ Info: iteration 5, average log likelihood -1.291262
[ Info: iteration 6, average log likelihood -1.280160
[ Info: iteration 7, average log likelihood -1.275921
[ Info: iteration 8, average log likelihood -1.273753
[ Info: iteration 9, average log likelihood -1.272250
[ Info: iteration 10, average log likelihood -1.270885
[ Info: iteration 11, average log likelihood -1.269344
[ Info: iteration 12, average log likelihood -1.267605
[ Info: iteration 13, average log likelihood -1.265951
[ Info: iteration 14, average log likelihood -1.264641
[ Info: iteration 15, average log likelihood -1.263706
[ Info: iteration 16, average log likelihood -1.263025
[ Info: iteration 17, average log likelihood -1.262462
[ Info: iteration 18, average log likelihood -1.261937
[ Info: iteration 19, average log likelihood -1.261408
[ Info: iteration 20, average log likelihood -1.260833
[ Info: iteration 21, average log likelihood -1.260159
[ Info: iteration 22, average log likelihood -1.259362
[ Info: iteration 23, average log likelihood -1.258429
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     4
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 24, average log likelihood -1.257385
[ Info: iteration 25, average log likelihood -1.268944
[ Info: iteration 26, average log likelihood -1.263056
[ Info: iteration 27, average log likelihood -1.261413
[ Info: iteration 28, average log likelihood -1.260625
[ Info: iteration 29, average log likelihood -1.260183
[ Info: iteration 30, average log likelihood -1.259924
[ Info: iteration 31, average log likelihood -1.259752
[ Info: iteration 32, average log likelihood -1.259605
[ Info: iteration 33, average log likelihood -1.259441
[ Info: iteration 34, average log likelihood -1.259221
[ Info: iteration 35, average log likelihood -1.258918
[ Info: iteration 36, average log likelihood -1.258577
[ Info: iteration 37, average log likelihood -1.258288
[ Info: iteration 38, average log likelihood -1.258092
[ Info: iteration 39, average log likelihood -1.257973
[ Info: iteration 40, average log likelihood -1.257897
[ Info: iteration 41, average log likelihood -1.257842
[ Info: iteration 42, average log likelihood -1.257795
[ Info: iteration 43, average log likelihood -1.257753
[ Info: iteration 44, average log likelihood -1.257715
[ Info: iteration 45, average log likelihood -1.257677
[ Info: iteration 46, average log likelihood -1.257640
[ Info: iteration 47, average log likelihood -1.257601
[ Info: iteration 48, average log likelihood -1.257559
[ Info: iteration 49, average log likelihood -1.257513
[ Info: iteration 50, average log likelihood -1.257462
┌ Info: EM with 100000 data points 50 iterations avll -1.257462
└ 236.4 data points per parameter
┌ Info: 3
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.310099343653067
│     -1.3099038753086727
│      ⋮
└     -1.257462187340289
[ Info: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.257607
[ Info: iteration 2, average log likelihood -1.257336
[ Info: iteration 3, average log likelihood -1.256600
[ Info: iteration 4, average log likelihood -1.248852
[ Info: iteration 5, average log likelihood -1.221590
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      3
│     13
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.189124
[ Info: iteration 7, average log likelihood -1.186247
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.164209
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      4
│     13
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.161378
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.172972
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 11, average log likelihood -1.166416
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 12, average log likelihood -1.165941
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      4
│     13
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 13, average log likelihood -1.162801
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 14, average log likelihood -1.174538
[ Info: iteration 15, average log likelihood -1.168448
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 16, average log likelihood -1.156960
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      4
│     13
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 17, average log likelihood -1.157662
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 18, average log likelihood -1.170212
[ Info: iteration 19, average log likelihood -1.165034
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 20, average log likelihood -1.153950
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      4
│      7
│     13
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 21, average log likelihood -1.154775
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 22, average log likelihood -1.177888
[ Info: iteration 23, average log likelihood -1.168471
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 24, average log likelihood -1.156309
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      4
│     13
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 25, average log likelihood -1.157239
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 26, average log likelihood -1.169931
[ Info: iteration 27, average log likelihood -1.164892
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 28, average log likelihood -1.153745
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      4
│      7
│     13
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 29, average log likelihood -1.154692
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 30, average log likelihood -1.177835
[ Info: iteration 31, average log likelihood -1.168422
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 32, average log likelihood -1.156171
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      4
│     13
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 33, average log likelihood -1.157158
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 34, average log likelihood -1.169821
[ Info: iteration 35, average log likelihood -1.164754
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 36, average log likelihood -1.153529
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      4
│      7
│     13
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 37, average log likelihood -1.154444
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 38, average log likelihood -1.177832
[ Info: iteration 39, average log likelihood -1.168406
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 40, average log likelihood -1.156117
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      4
│     13
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 41, average log likelihood -1.157121
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 42, average log likelihood -1.169766
[ Info: iteration 43, average log likelihood -1.164677
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 44, average log likelihood -1.153416
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      4
│      7
│     13
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 45, average log likelihood -1.154305
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 46, average log likelihood -1.177833
[ Info: iteration 47, average log likelihood -1.168399
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 48, average log likelihood -1.156092
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      4
│     13
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 49, average log likelihood -1.157102
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 50, average log likelihood -1.169739
┌ Info: EM with 100000 data points 50 iterations avll -1.169739
└ 118.1 data points per parameter
┌ Info: 4
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.2576073148758151
│     -1.2573358957478509
│      ⋮
└     -1.1697389619739151
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.164955
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     5
│     6
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 2, average log likelihood -1.153207
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      7
│     13
│     14
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 3, average log likelihood -1.151702
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     5
│     6
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.132094
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     13
│     14
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.110081
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      5
│      6
│     25
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.075173
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      8
│     12
│     13
│     14
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.069846
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      5
│      6
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.059901
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     13
│     14
│     26
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.055726
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      5
│      6
│      8
│     12
│     25
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.042046
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     13
│     14
│     16
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 11, average log likelihood -1.076351
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      5
│      6
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 12, average log likelihood -1.061420
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      8
│     12
│     13
│     14
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 13, average log likelihood -1.047617
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      5
│      6
│      7
│     16
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 14, average log likelihood -1.036235
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│     13
│     14
│     26
│     27
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 15, average log likelihood -1.067998
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     5
│     6
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 16, average log likelihood -1.067858
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     12
│     13
│     14
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 17, average log likelihood -1.052356
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      5
│      6
│      8
│     16
│     25
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 18, average log likelihood -1.043159
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      7
│     13
│     14
│     26
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 19, average log likelihood -1.064951
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     5
│     6
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 20, average log likelihood -1.066334
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      8
│     12
│     13
│     14
│     26
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 21, average log likelihood -1.047614
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      5
│      6
│     16
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 22, average log likelihood -1.055359
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      7
│     13
│     14
│     26
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 23, average log likelihood -1.055613
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      5
│      6
│      8
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 24, average log likelihood -1.063217
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     12
│     13
│     14
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 25, average log likelihood -1.059035
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      5
│      6
│      8
│     16
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 26, average log likelihood -1.045874
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│     13
│     14
│     26
│     27
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 27, average log likelihood -1.062131
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     5
│     6
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 28, average log likelihood -1.053590
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      8
│     13
│     14
│     16
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 29, average log likelihood -1.051939
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      5
│      6
│     12
│     25
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 30, average log likelihood -1.053347
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     13
│     14
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 31, average log likelihood -1.067613
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      5
│      6
│      8
│     16
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 32, average log likelihood -1.038437
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      7
│     13
│     14
│     26
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 33, average log likelihood -1.054257
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      5
│      6
│      8
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 34, average log likelihood -1.059837
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│     12
│     13
│     14
│     16
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 35, average log likelihood -1.060295
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      5
│      6
│     27
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 36, average log likelihood -1.057376
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      8
│     13
│     14
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 37, average log likelihood -1.059448
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      5
│      6
│      7
│     16
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 38, average log likelihood -1.040366
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│     12
│     13
│     14
│     26
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 39, average log likelihood -1.068963
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      5
│      6
│      8
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 40, average log likelihood -1.057948
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     13
│     14
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 41, average log likelihood -1.062324
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      5
│      6
│      8
│     12
│     16
│     25
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 42, average log likelihood -1.038398
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      7
│     13
│     14
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 43, average log likelihood -1.064026
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     5
│     6
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 44, average log likelihood -1.056559
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      8
│     13
│     14
│     26
│     27
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 45, average log likelihood -1.041790
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      5
│      6
│     12
│     16
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 46, average log likelihood -1.055838
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     13
│     14
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 47, average log likelihood -1.068509
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      5
│      6
│      7
│      8
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 48, average log likelihood -1.038464
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│     13
│     14
│     16
│     25
│     26
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 49, average log likelihood -1.057838
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     5
│     6
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 50, average log likelihood -1.075244
┌ Info: EM with 100000 data points 50 iterations avll -1.075244
└ 59.0 data points per parameter
┌ Info: 5
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.1649551967386171
│     -1.1532071212401045
│      ⋮
└     -1.0752440528743155
┌ Info: Total log likelihood: 
│   tll =
│    251-element Array{Float64,1}:
│     -1.3992461709652484
│     -1.3993282894209302
│     -1.3992403936378666
│     -1.3984864401317951
│      ⋮
│     -1.0384641501051077
│     -1.0578378550820666
└     -1.0752440528743155
32×26 Array{Float64,2}:
 -0.0149796    -0.207027    -0.0634286  -0.000574541  -0.17319      0.0150345    -0.171649    -0.126734    -0.0128579    0.0287443    0.0967869   -0.116119     0.0792783     0.0487413    0.065928    0.132714     -0.0126307   -0.158535     -0.591694    -0.0227337   -0.0295266   -0.064252    -0.20256      0.00971131  -0.0813066  -0.0731163
 -0.0496138    -0.241194    -0.013066   -0.0683776    -0.184786     0.133183     -0.269089     0.116878    -0.00441125  -0.00771292   0.0751579   -0.138357     0.0448172    -0.196274     0.0390247   0.125309      0.0212493   -0.088756      0.8473      -0.0255599   -0.0682832   -0.0532705    0.130295    -0.0367947    0.0869143  -0.0929858
  0.0502746    -0.0424281    0.021616   -0.0698931    -0.148471    -0.0501328    -0.105878     0.0207921   -0.00902415  -0.135778    -0.0361701    0.00378948   0.0383095     0.0619509    0.119286   -0.0200684     0.0846101   -0.164325     -0.0117169   -0.00968455  -0.00405794  -0.0201158    0.00127896   0.12854      0.0500243   0.00635119
  0.192705      0.0609514    0.0452051   0.0649247    -0.0490497    0.0298024     0.0323747    0.0942858   -0.0103826   -0.0117505   -0.144557    -0.043905    -0.00854214    0.0331151   -0.0207894   0.0880044    -0.0726445    0.0708503    -0.0120189    0.11017     -0.208937    -0.0193692   -0.0168014   -0.118316    -0.033215    0.0451385
 -0.1846        0.192034    -0.0758988   0.177243      0.0997588   -0.00236883    0.129095     0.0244794   -0.0190332   -0.0189535   -0.202757     0.145355     0.0555578    -0.11071      0.0310059   0.212834     -0.0600128   -0.0335708    -0.0501944    0.00182189  -0.114602    -0.0846492   -0.639723    -0.0359348   -0.097099   -0.15887
 -0.131415     -0.307353    -0.0727541   0.18704       0.118097     0.00153011    0.107439     0.0990419    0.183155    -0.0319915   -0.17577      0.124808     0.0360681    -0.141732     0.0340886   0.145617     -0.0593693    0.106662     -0.0709575    0.00234255  -0.111436    -0.0844511    0.832227    -0.0430309   -0.0963691  -0.22225
  0.00326426   -0.0157696    0.0557492  -0.0216234     0.0813561   -0.017602     -0.00294736  -0.130768    -0.0917075    0.10599      0.0165311   -0.106109     0.037669      0.0923099    0.0359572  -0.0279952    -0.0188762    0.119291      0.101231    -0.0465697   -0.0888577    0.151436     0.0223874    0.0296735   -0.0221049   0.0307033
  0.00589061   -0.0324419    0.0364794  -0.0226956     0.158917    -0.0415098     0.117796     0.00529478  -0.12479      0.0685653    0.0759311   -0.112129    -0.170634      0.00744415   0.0625874   0.00820253    0.0262533   -0.0386626     0.211099    -0.109552    -0.0785285   -0.0550408    0.0393689    0.12167     -0.0201356  -0.0109527
 -0.0228318     0.0859441   -0.0808068   0.0467827    -0.130227     0.0093981     0.0517846    0.0792222   -0.147869     0.0972702    0.10064     -0.0302656   -0.139014      0.197529    -0.238146    0.0478096     0.0768347    0.0246824    -0.00782707  -0.239284    -0.103397     0.0399364    0.12093     -0.949793     0.240321   -0.181443
 -0.0195635    -0.127737    -0.194713    0.065954      0.0244277    0.00432271    0.037137     0.0984852   -0.116847     0.0734816    0.0131527   -0.0900372   -0.0112204     0.165618     0.0223468   0.0242432     0.0746005   -0.0947932    -0.0364125   -0.200749    -0.113599     0.0235409    0.0946743    0.649081     0.233517    0.181189
  0.102424     -0.1216       0.164195    0.0311596    -0.0288623   -0.0937429     0.0834446   -0.157053    -0.048588    -0.0586134   -0.0658127   -0.00981969  -0.0247888     0.195475     0.0433067  -0.00980241    0.0636874    0.0758812     0.0551357    0.0304787   -0.0630972    0.0124371    0.123401    -0.109924     0.180553   -0.100352
  0.0301828    -0.0368709    0.0499318  -0.0544271    -0.014497     0.00726834   -0.017394    -0.186512     0.048777     0.111348    -0.0442607   -0.0813345    0.0852526     0.120752    -0.111116   -0.059432     -0.0399062    0.0791687     0.040064    -0.0162374   -0.0999763    0.134877    -0.0881539    0.0232368    0.0132664  -0.00671411
  0.00102138    0.254188    -0.216469   -0.0339081    -0.111487    -0.00933126   -0.0636003   -0.801871     0.115151     0.0866901   -0.0448201    0.114698     0.0866806    -0.114174    -0.0108397   0.000477939   0.220927     0.143252     -0.0488136    0.00217998   0.032392    -0.0357767   -0.0324979    0.054139     0.321852    0.0426472
  0.0176242     0.242187    -0.244697   -0.0192491    -0.141611    -0.0164231    -0.0752402    0.831826     0.109274     0.0695263   -0.0947412   -0.124782     0.0705325     0.133384     0.193139   -0.00211843   -0.0302246    0.12317       0.0492238   -0.14933     -0.0193253   -0.0269401    0.0263467    0.224604    -0.101444    0.0421428
 -0.112949     -0.040786    -0.0317889   0.0942981     0.0905475    0.0400623     0.0378563   -0.0244008   -0.117918    -0.0296081    0.0353775    0.0772554    0.0583361    -0.150877     0.115885   -0.0267768    -0.0396437   -0.0393356    -0.0225557    0.120861    -0.198949    -0.00237307   0.117679    -0.0573538   -0.0259778  -0.00510793
  0.00584651    0.020553     0.167977    0.0343074    -0.0921715   -0.00582557    0.0408151   -0.00591661  -0.0546861    0.225347     0.00345029  -0.0294256    0.0395324    -0.0983287    0.0446795  -0.0636863     0.170442    -0.060768      0.0499494   -0.128977    -0.033359    -0.0735941   -0.105393    -0.00494721   0.12994    -0.07418
  0.0547691     0.0391419   -0.0831963  -0.0685333     0.00659858   0.0241425    -0.00160441  -0.013226     0.0720145    0.0252303   -0.049535     0.0976035    0.0441208     0.00779484  -0.0204486  -0.00889882    0.0480072   -0.139809     -0.0658006   -0.0289349    0.116404     0.0270124   -0.0777192    0.025745    -0.0728476  -0.0633865
 -0.129275      0.111296    -0.0395237   0.0101776    -0.0126871   -0.142086      0.00269461   0.003315    -0.104031     0.0222743   -0.0519597   -0.117307     0.159503     -0.163178    -0.0433183  -0.090373     -0.00343768   0.262822     -0.13187     -0.154586    -0.160941     0.0202059   -0.050504    -0.188685    -0.227988   -0.0399116
  0.0324493    -0.177464    -0.0170826   0.0693929    -0.0765914   -0.0531393    -0.0886214   -0.13441      0.0503381   -0.0667336   -0.0527137   -0.0267403    0.0989715    -0.0105605   -0.164096   -0.23034      -0.0674484   -0.140139     -0.0433396   -0.0209542   -0.119256    -0.00628065   0.00508404  -0.122286     0.166878    0.139978
 -0.0565108     0.0979058    0.100289    0.0421771     0.0274636   -0.0385148    -0.128823    -0.0091095   -0.0654483   -0.109592     0.096436     0.143819    -0.0911421    -0.0866817    0.075414   -0.0329882    -0.00239564   0.0261973    -0.0388985   -0.0441652   -0.0271287    0.0569102    0.0442883    0.00164816  -0.081359   -0.0514162
 -0.00708287    0.0362031    0.155835    0.131919      0.0401949   -0.171563     -0.0911132   -0.120829    -0.116471    -0.0721699    0.0716465    0.197711     0.0208034     0.00922724  -0.107426   -0.0248809    -0.747484     0.152693      0.0707795   -0.158715     0.0210904   -0.0363385   -0.0809765    0.00969807  -0.0867169   0.017949
 -0.0807489    -0.0539135    0.139655    0.101808      0.190466     0.0504433    -0.0910028   -0.204079    -0.0428527   -0.127116     0.0721736    0.191945    -0.0931832     0.17845     -0.124514   -0.316543      1.04901      0.139663      0.142021    -0.169254     0.0380097   -0.0400571   -0.0788241    0.0203263   -0.0597602   0.0174405
 -0.114563      0.0657488    0.0121606  -0.101087     -0.150996     0.190116     -0.0143021    0.134908    -0.015685     0.0808337    0.00730513  -0.130112    -0.107548      0.0899385    0.143812    0.0394284    -0.00122612   0.0392061     0.0840922    0.00342871   0.107556    -0.0800472   -0.0418129   -0.113996    -0.0657221   0.0789259
  0.000247768   0.044353     0.0187394  -0.167805      0.0446365    0.0223396    -0.0411422   -0.0210703   -0.00410225   0.00798846  -0.0134093    0.0463759    0.0160961     0.0274321   -0.0505963  -0.0618509    -0.135782    -0.0279222    -0.00244821  -0.141438     0.0194162   -0.049297     0.0585892   -0.0534203   -0.0651026   0.0560631
 -0.0232109    -0.0618529    0.0351564  -0.0555081     0.00582092   0.000452313  -0.0330729    0.076813     0.0636694    0.0461261    0.118659    -0.0335834    0.192301      0.0978413    0.0418375  -0.074081     -0.0707535   -0.0369017    -0.184761    -0.013734    -0.00447265  -0.0160149   -0.0665648    0.0374453    0.159336    0.0362611
  0.0511346     0.116934    -0.0597347   0.188338     -0.122084     0.0948076    -0.147874    -0.0969994    0.109465     0.0528854   -0.0317034    0.130288     0.137938     -0.0390253   -0.0186974  -0.114259     -0.0462454    0.0995624     0.100815    -0.02424      0.151415     0.0781525   -0.0706376   -0.044704     0.0912379   0.00120141
  0.0293504     0.00581662  -0.0310096   0.115062      0.0962933   -0.078258      0.178977     0.0141157   -0.114545     0.129198     0.12635      0.0839361   -0.00621419    0.100028     0.0452631  -0.024821      0.0974675   -0.0646913     0.0642068    0.0821158    0.12797      0.0202358    0.0387494    0.0364899   -0.118564   -0.0233262
  0.0151371    -0.0598534    0.046044    0.122945     -0.0123416    0.0407613    -0.0153534    0.0735168    0.0468279   -0.0359969    0.118005     0.125179     0.0498048     0.144276    -0.102691    0.0566434     0.115206     0.0615954     0.0512241   -0.10327     -0.165829    -0.19594      0.0405933   -0.20557      0.168733   -0.0687299
 -0.139746     -0.107755    -0.0632566   0.21877      -0.133688    -0.166707     -0.0222831   -0.0953949   -0.04268      0.0422904    0.112852    -0.188905    -0.0551183    -0.00960811  -0.0714831   0.0523589    -0.174464     0.000741167   0.174839    -0.185973     0.0890482    0.0366946    0.0425841   -0.0824415   -0.0847859  -0.0590753
  0.121187      0.0129721   -0.0924941  -0.0252059     0.142824    -0.00385363   -0.144525    -0.126308    -0.0606408    0.0200363   -0.0535307   -0.0798081    0.0841197     0.037754    -0.0230314   0.0743069     0.149281     0.0203874    -0.18131     -0.0436329    0.0255916    0.0638811    0.0470442    0.0181462    0.0217357  -0.0838949
  0.0804472    -0.00730844   0.0472123   0.144378     -0.00923707   0.00271293    0.0243911    0.0418906   -0.00972655  -0.0195231    0.103573     0.0552239   -0.0465906     0.0366661   -0.0111786  -0.0311743     0.0153599    0.0056623    -0.0535089    0.0504058   -0.0592067    0.0348326   -0.00160351  -0.0695911   -0.0553681   0.0907504
 -0.0677172    -0.0347112    0.0468785  -0.0472962     0.00853253   0.0504993     0.0688357    0.0660804   -0.0167385    0.043809    -0.201225    -0.0138044   -0.000303874   0.0240833    0.0135693   0.0323398     0.0428123    0.0639488     0.0856698   -0.0916127   -0.0386944    0.252979     0.127985     0.120345     0.0602923  -0.00881236[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      8
│     12
│     13
│     14
│     26
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 1, average log likelihood -1.055064
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      5
│      6
│      8
│     12
│      ⋮
│     16
│     26
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 2, average log likelihood -1.034727
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      8
│     12
│     13
│     14
│     25
│     26
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 3, average log likelihood -1.039778
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      5
│      6
│      8
│     12
│      ⋮
│     26
│     27
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.034307
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      7
│      8
│     12
│     13
│     14
│     26
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.047328
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      5
│      6
│      8
│     12
│      ⋮
│     25
│     26
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.029909
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      8
│     12
│     13
│     14
│     26
│     27
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.047303
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      5
│      6
│      8
│     12
│      ⋮
│     16
│     26
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.036688
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      7
│      8
│     12
│     13
│     14
│     25
│     26
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.036944
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      5
│      6
│      8
│     12
│      ⋮
│     26
│     27
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.037903
┌ Info: EM with 100000 data points 10 iterations avll -1.037903
└ 59.0 data points per parameter
kind diag, method kmeans
[ Info: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.677275e+05
      1       6.657738e+05      -2.019538e+05 |       32
      2       6.352688e+05      -3.050500e+04 |       32
      3       6.183069e+05      -1.696188e+04 |       32
      4       6.063042e+05      -1.200267e+04 |       32
      5       5.986862e+05      -7.618063e+03 |       32
      6       5.951819e+05      -3.504256e+03 |       32
      7       5.931061e+05      -2.075851e+03 |       32
      8       5.917056e+05      -1.400430e+03 |       32
      9       5.904921e+05      -1.213522e+03 |       32
     10       5.894299e+05      -1.062251e+03 |       32
     11       5.886642e+05      -7.656569e+02 |       32
     12       5.881056e+05      -5.585712e+02 |       32
     13       5.876622e+05      -4.434539e+02 |       32
     14       5.872216e+05      -4.405699e+02 |       32
     15       5.867709e+05      -4.506833e+02 |       32
     16       5.863857e+05      -3.851919e+02 |       32
     17       5.861691e+05      -2.166750e+02 |       32
     18       5.860795e+05      -8.960946e+01 |       32
     19       5.860492e+05      -3.028495e+01 |       32
     20       5.860306e+05      -1.856805e+01 |       32
     21       5.860211e+05      -9.484738e+00 |       28
     22       5.860154e+05      -5.750780e+00 |       30
     23       5.860110e+05      -4.354462e+00 |       27
     24       5.860088e+05      -2.210186e+00 |       15
     25       5.860077e+05      -1.075674e+00 |       20
     26       5.860065e+05      -1.251435e+00 |       18
     27       5.860054e+05      -1.096931e+00 |       16
     28       5.860045e+05      -8.798251e-01 |       14
     29       5.860038e+05      -7.384849e-01 |       11
     30       5.860030e+05      -7.297035e-01 |       10
     31       5.860024e+05      -6.650965e-01 |       13
     32       5.860014e+05      -1.012281e+00 |       15
     33       5.860002e+05      -1.176409e+00 |       15
     34       5.859990e+05      -1.146846e+00 |       13
     35       5.859981e+05      -9.275475e-01 |       14
     36       5.859974e+05      -7.280396e-01 |       13
     37       5.859967e+05      -7.013196e-01 |        8
     38       5.859962e+05      -4.732862e-01 |        6
     39       5.859961e+05      -1.327380e-01 |        4
     40       5.859960e+05      -7.361082e-02 |        0
     41       5.859960e+05       0.000000e+00 |        0
K-means converged with 41 iterations (objv = 585995.9902083913)
┌ Info: K-means with 32000 data points using 41 iterations
└ 37.0 data points per parameter
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.304870
[ Info: iteration 2, average log likelihood -1.276962
[ Info: iteration 3, average log likelihood -1.250072
[ Info: iteration 4, average log likelihood -1.218064
[ Info: iteration 5, average log likelihood -1.172442
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.114220
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      2
│     20
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.070187
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.078580
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     21
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.054164
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      5
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.053252
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      2
│      6
│     20
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 11, average log likelihood -1.035097
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     15
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 12, average log likelihood -1.038275
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     21
│     25
│     29
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 13, average log likelihood -1.021168
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     20
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 14, average log likelihood -1.053388
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      2
│      5
│     15
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 15, average log likelihood -1.027880
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      6
│     29
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 16, average log likelihood -1.036619
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     20
│     21
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 17, average log likelihood -1.041033
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 18, average log likelihood -1.061214
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      2
│     15
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 19, average log likelihood -1.025168
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      6
│     20
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 20, average log likelihood -1.037467
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 21, average log likelihood -1.038952
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     21
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 22, average log likelihood -1.030297
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      2
│      5
│     15
│     20
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 23, average log likelihood -1.018025
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      6
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 24, average log likelihood -1.058562
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 25, average log likelihood -1.035860
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     20
│     21
│     22
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 26, average log likelihood -1.028700
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      2
│     15
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 27, average log likelihood -1.048122
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      6
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 28, average log likelihood -1.034284
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      5
│     20
│     21
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 29, average log likelihood -1.017594
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     22
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 30, average log likelihood -1.065210
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      2
│     15
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 31, average log likelihood -1.043460
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      6
│     20
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 32, average log likelihood -1.033290
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 33, average log likelihood -1.037190
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     21
│     22
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 34, average log likelihood -1.029443
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      2
│      5
│     15
│     20
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 35, average log likelihood -1.033168
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      6
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 36, average log likelihood -1.050860
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 37, average log likelihood -1.031039
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     20
│     21
│     22
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 38, average log likelihood -1.027529
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      2
│     15
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 39, average log likelihood -1.048841
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      6
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 40, average log likelihood -1.035110
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     20
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 41, average log likelihood -1.018925
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      5
│     21
│     22
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 42, average log likelihood -1.037179
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      2
│     15
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 43, average log likelihood -1.053797
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      6
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 44, average log likelihood -1.040378
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     20
│     21
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 45, average log likelihood -1.024215
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     22
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 46, average log likelihood -1.056361
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      2
│     15
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 47, average log likelihood -1.038554
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      6
│     20
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 48, average log likelihood -1.030128
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      5
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 49, average log likelihood -1.034957
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     21
│     22
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 50, average log likelihood -1.041671
┌ Info: EM with 100000 data points 50 iterations avll -1.041671
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.058445     0.0144391   -0.0729099   0.104309     0.0576142    0.00454994  -0.0211848    0.0387192   -0.0924855   -0.0893098    0.0150895     0.136037    -0.0151802   -0.1747       0.146254    -0.0154109    -0.0559421     0.0212002   -0.00596584   0.190239     -0.264083    -0.0811042    0.0355395     0.00606118  -0.115362    0.0473422
  0.0153099    0.0659846    0.123905    0.0859683   -0.101361     0.0320579   -0.0122593   -0.0327139   -0.0204108   -0.149681     0.114238      0.052101    -0.00798934   0.0928519   -0.022006     0.0442202     0.0527689     0.0250886    0.0437049    0.0868887    -0.0593182    0.117554     0.0570955    -0.100516     0.0723513   0.0347287
  0.163247     0.0320022    0.0167844   0.0940085   -0.00856138   0.0219638    0.0518112    0.0770456   -0.00968685  -0.0189119   -0.148452     -0.0156707   -0.00865002   0.0390232   -0.0106451    0.1016       -0.0719878     0.0697202    0.00100921   0.0778536    -0.190429    -0.0343337   -0.0221875    -0.0891254   -0.048751    0.00635041
  0.134824    -0.0312787    0.0193009   0.203791    -0.0464753   -0.0852412    0.101605    -0.0306672   -0.0556723    0.133503     0.0229727     0.0611825   -0.0820843    0.0273686    0.0359551   -0.0549677     0.0521589    -0.0503977   -0.0288049   -0.014458      0.00403844  -0.0524543   -0.0207508     0.0220739   -0.113071    0.217129
 -0.00722997  -0.00140799  -0.0704102   0.095542     0.0781304   -0.0515041    0.142251     0.00439686  -0.0990869    0.124075     0.0905843     0.0533957   -0.0155885    0.108726     0.030177    -0.0375324     0.113392     -0.0907892    0.0704669    0.132315      0.161535     0.0122217    0.0463492     0.00219506  -0.0870106   0.00849317
  0.158411    -0.0318887   -0.149039   -0.0842883    0.0475069    0.0143527   -0.0508516   -0.0873552    0.134908    -0.0117424   -0.102433     -0.0351964    0.12175     -0.00761404  -0.0430342    0.0286232     0.0365425    -0.18601     -0.0121947   -0.115964      0.17483      0.123209    -0.0304376    -0.0257678   -0.184498   -0.0538445
 -0.0177459    0.134203    -0.0463012  -0.0772987   -0.022104     0.0485209    0.0334343    0.0271062    0.0263593    0.0510105   -0.0585646     0.240158    -0.0443778    0.00213791  -0.0199358   -0.000532093   0.0617049    -0.140577    -0.0716627    0.0400939     0.0923927   -0.0432598   -0.118045      0.0758597   -0.0201418  -0.105789
  0.00992949  -0.0564755    0.041128    0.123056    -0.0108089    0.0351451   -0.00768139   0.0801352    0.0408376   -0.0358578    0.122064      0.124455     0.0491195    0.142092    -0.100351     0.0540249     0.115274      0.0640886    0.0521139   -0.0943427    -0.163856    -0.193381     0.0403941    -0.197828     0.15681    -0.0709831
  0.0848239   -0.103222     0.136354    0.010919    -0.0186123   -0.0498075    0.0766051   -0.15914     -0.0320352   -0.0105309   -0.0620026    -0.0214922   -0.00729749   0.17836      0.00781103  -0.0183101     0.042326      0.0764922    0.0549944    0.0209369    -0.0691694    0.0383691    0.0862102    -0.085832     0.158082   -0.085305
 -0.0443358   -0.00954727   0.148533    0.117311     0.115024    -0.0605783   -0.0910912   -0.161734    -0.0805335   -0.0995891    0.07193       0.194849    -0.0362435    0.0930301   -0.115768    -0.170752      0.148889      0.146296     0.106028    -0.163906      0.0294396   -0.0384935   -0.0798892     0.0149206   -0.0733452   0.0176846
 -0.0346159   -0.219586    -0.0401279  -0.0315387   -0.17729      0.0679185   -0.21214     -0.0101448   -0.00650282   0.0124729    0.0838828    -0.124492     0.0608564   -0.0679662    0.0529073    0.130242      0.00169078   -0.122531     0.0876922   -0.0236402    -0.0488113   -0.0578309   -0.044519     -0.0145359   -0.0063723  -0.0823095
 -0.0211826   -0.0196342   -0.136415    0.0555783   -0.0543648    0.0067904    0.0458778    0.0881735   -0.132464     0.0863494    0.0580543    -0.0609077   -0.0781258    0.183653    -0.112871     0.0368003     0.0759964    -0.0355271   -0.0210445   -0.221533     -0.1094       0.0327844    0.108584     -0.185978     0.237103   -0.00941507
 -0.141611    -0.108017    -0.0674532   0.2121      -0.138196    -0.165818    -0.0327155   -0.103699    -0.0464462    0.0393304    0.114759     -0.1916      -0.0533735   -0.00470687  -0.0689201    0.0568025    -0.184929     -0.0132184    0.189537    -0.186887      0.091703     0.0378665    0.0449497    -0.0823679   -0.0776433  -0.0618564
  0.0923912    0.687626    -0.0419035   0.0901751    0.175937     0.0250185    0.029146     0.166588     0.136081     0.0383905    0.0501398     0.0800176   -0.0789728   -0.0351538   -0.0152937   -0.0174093    -0.0316342     0.0409788   -0.100805     0.231395     -0.0596091    0.168168    -0.0575045    -0.0440101   -0.204522   -0.106921
 -0.228929     0.0104035   -0.0641224   0.176208     0.130632    -0.00297255   0.130374     0.0572821    0.0561852   -0.00972393  -0.181384      0.129402     0.0664065   -0.142661     0.0376705    0.172549     -0.0535828    -0.00662144  -0.0510626   -0.00781894   -0.104712    -0.0743836   -0.0229826    -0.0192511   -0.091091   -0.20904
  0.0494245   -0.0415495    0.0192957  -0.0690977   -0.149718    -0.0506705   -0.103765     0.0167677   -0.0098822   -0.133075    -0.0334847     0.00105743   0.0377382    0.0635856    0.112989    -0.0234466     0.0816152    -0.159867    -0.011972    -0.00921897   -0.00444615  -0.0154963    0.000914664   0.129897     0.0468179   0.010934
 -0.15859     -0.075982     0.0171842   0.0758401    0.111947     0.0684164    0.0969049   -0.102148    -0.132093     0.0340261    0.0413432     0.00209499   0.13906     -0.137918     0.0693945   -0.0351825    -0.00549886   -0.0974522   -0.0243132    0.0274011    -0.122017     0.0688096    0.208471     -0.127615     0.0903717  -0.0843982
 -0.133698     0.108753    -0.0412227   0.00749864  -0.0115822   -0.141272     0.00324051  -0.00123037  -0.106223     0.0254364   -0.0501114    -0.116743     0.164075    -0.160146    -0.0436074   -0.0895836    -0.00260937    0.269312    -0.129446    -0.15721      -0.161005     0.0246105   -0.0523833    -0.188473    -0.227747   -0.0430231
  0.121608     0.0118326   -0.0907123  -0.0252459    0.142872    -0.00368857  -0.145165    -0.126306    -0.0604117    0.0201694   -0.0542177    -0.0799651    0.085249     0.038352    -0.0247481    0.0740886     0.149022      0.0220174   -0.180494    -0.0439486     0.0246458    0.0636758    0.0479567     0.0185129    0.021316   -0.0836498
  0.0421114    0.0973858   -0.0277383   0.118235    -0.085935     0.0697918   -0.109889    -0.0615229    0.0785046    0.054242    -0.0271677     0.0965919    0.137317    -0.0168465   -0.0173812   -0.102043     -0.0467956     0.0830389    0.0488365   -0.027517      0.0994467    0.0857245   -0.0666404    -0.0453749    0.0967919  -0.0110876
  0.019028     0.0459748    0.182259    0.0467859   -0.125369    -0.0238147    0.0501398    0.039321    -0.0565353    0.26414      0.000303146  -0.0298436    0.0295283   -0.132554     0.0567233   -0.0676372     0.187197     -0.0617741    0.0482771   -0.116377     -0.0423982   -0.0938776   -0.16045       0.0176595    0.113303   -0.049547
 -0.028385    -0.0965328    0.0346652  -0.0663813    0.0144923    0.0140946   -0.0284176    0.0757442    0.0640647    0.0438585    0.15345      -0.0120664    0.223188     0.126347     0.0804452   -0.0603007    -0.0467908    -0.0437227   -0.193261     0.0128769    -0.00765623  -0.0406864   -0.0621093     0.0501712    0.193937    0.0267843
  0.0324292   -0.176318    -0.017144    0.0699738   -0.0745383   -0.0523994   -0.0852827   -0.129063     0.0465387   -0.0676921   -0.048599     -0.0242644    0.099164    -0.0102796   -0.165757    -0.231017     -0.0668718    -0.139431    -0.0419814   -0.0192348    -0.118548    -0.0033927    0.00564817   -0.119586     0.166201    0.140004
 -0.018459    -0.00863011   0.101471    0.001023     0.0159304    0.0350386   -0.10668     -0.161966    -0.0935766   -0.0489672    0.205102      0.181535    -0.134824    -0.00514676  -0.12131      0.036976      0.00306747   -0.00480899   0.040143    -0.144426      0.0231307    0.164499     0.157264      0.0492617   -0.149597    0.0480042
 -0.141901    -0.0353756    0.0684246  -0.103244     0.00316498   0.0515962    0.0805996    0.129614    -0.0187347    0.0274909   -0.200913     -0.00553153  -8.54397e-5   0.0275975    0.0108137    0.069372      0.0542865     0.0756601    0.124393    -0.128834     -0.0166216    0.327087     0.291466      0.228225     0.0875378   0.000688402
  0.0566366   -0.659961    -0.0464715   0.0777934    0.136219     0.0183613    0.00394639   0.178055    -0.0814234    0.0655093    0.0421642     0.0125885   -0.0657174   -0.0436469   -0.0401455   -0.134061     -0.0243616     0.0114372   -0.139321    -0.13807      -0.0777675   -0.00202962  -0.0380472    -0.0704005   -0.172341    0.0444908
 -0.0112501    0.0673739    0.0332082  -0.217803     0.063128     0.0401075   -0.0403928   -0.0185689    0.00197045  -0.00980499  -0.0103282     0.0529652    0.0384102    0.0160146   -0.0704073   -0.0605986    -0.184494     -0.0161874    0.00442058  -0.179957      0.0300482   -0.0460759    0.0839667    -0.0459994   -0.0647416   0.00518272
 -0.0918441    0.227642     0.10087     0.0817677    0.0312293   -0.101341    -0.155333     0.121018    -0.046435    -0.16936     -0.0265225     0.108006    -0.0651917   -0.161954     0.262283    -0.110369      0.00013098    0.0468165   -0.108728     0.0471525    -0.0608234   -0.0479268   -0.055579     -0.0579104   -0.0349457  -0.166354
  0.00778901   0.24119     -0.226087   -0.0270277   -0.123435    -0.0147241   -0.0705458   -0.0252657    0.111087     0.0780551   -0.0717833    -0.011045     0.078895     0.0128816    0.0827824   -0.00100885    0.107196      0.133794    -0.00144038  -0.0750341     0.00547905  -0.0296862   -0.00890856    0.141028     0.119436    0.042008
 -0.00314434  -0.0226534    0.0379313  -0.0377794    0.159035    -0.0632632    0.141175    -0.021973    -0.121456     0.0776102    0.0821948    -0.155425    -0.250351     0.0280275    0.0533134    0.0028928     0.0356844     0.041203     0.249605    -0.120776     -0.0807754   -0.031637     0.0373185     0.125822    -0.0141547   0.0380613
  0.0255426   -0.0275132    0.0681387  -0.0605591   -0.00462216   0.0127681   -0.0256524   -0.194978     0.0291958    0.0979365   -0.0863644    -0.077357     0.0752944    0.122559    -0.08608     -0.0587602    -0.0176094     0.0818756    0.0561361   -0.0559937    -0.124511     0.167116    -0.187083     -0.0409046    0.0095673   0.0123442
 -0.11447      0.06763      0.0119044  -0.103104    -0.147169     0.190737    -0.0128949    0.131924    -0.0171692    0.0811374    0.00743073   -0.130505    -0.110245     0.088703     0.1422       0.0396964    -0.000689075   0.0382199    0.0875277   -0.000981833   0.107703    -0.0801138   -0.0410811    -0.113247    -0.0667438   0.0739533[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      2
│     15
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 1, average log likelihood -1.041920
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      2
│      6
│     15
│     20
│     25
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 2, average log likelihood -0.999535
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      2
│      6
│     15
│     20
│     21
│     22
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 3, average log likelihood -0.986476
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      2
│     15
│     20
│     25
│     29
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.013499
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      2
│      6
│     15
│     20
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.016683
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      2
│      6
│     15
│     20
│     22
│     25
│     30
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -0.981644
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      2
│     15
│     20
│     21
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.009567
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      2
│      6
│     15
│     20
│     25
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.013651
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      2
│      6
│     15
│     20
│     22
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -0.993320
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      2
│     15
│     20
│     21
│     25
│     29
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -0.998578
┌ Info: EM with 100000 data points 10 iterations avll -0.998578
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.0495806    0.00543256  -0.0690611    -0.0442088     0.0460733    0.0632515   0.0246848   -0.0353195   -0.0519903     0.0763362    0.214802    -0.0656135    0.0434785    0.0700375    -0.0781066  -0.00468184   0.11675     -0.0150599   0.264014     0.129281     0.135788    0.0627703    0.057446    -0.089348    -0.24341    -0.039074
  0.219941     0.130711    -0.0496401    -0.0106227     0.208271     0.0748505   0.12438     -0.214039    -0.0602084    -0.0145478    0.00543903   0.0673165    0.0934594   -0.060342     -0.116152   -0.0126768    0.0692102    0.154295   -0.135021    -0.19222      0.0526848  -0.104251     0.0130839    0.00652188   0.102685    0.142506
  0.114812     0.0158059   -0.0320145    -0.0548631     0.0436885    0.0143753  -0.0671168    0.00829679   0.140977     -0.148126    -0.123501     0.0205112    0.0543766   -0.00315852    0.131498    0.0430671    0.0897661    0.0648912   0.0861958    0.107672     0.216932    0.0447734    0.0329646   -0.0470815   -0.0919887  -0.0709407
 -0.0120313    0.114687    -0.0919491     0.137492     -0.0896535   -0.0386285   0.0743106   -0.0532068   -0.05836       0.209674     0.22373     -0.199822     0.187703    -0.0861901     0.102295   -0.189091     0.0340169   -0.0113034   0.05309      0.0419862    0.0225686   0.154361    -0.0686431    0.0512343   -0.191599    0.0316225
  0.0223784    0.0852447   -0.110048      0.0268501     0.00235453   0.0574282   0.245061    -0.239428    -0.0764694     0.0404881   -0.0738961   -0.0919661    0.122906     0.000526194  -0.0302136   0.0949196   -0.132853     0.0389366   0.0774712   -0.0819421    0.0245027   0.0248939    0.135788     0.0971337    0.045831    0.0854224
 -0.0264916   -0.127479     0.079403     -0.0208733     0.0578481    0.0136162  -0.163099     0.0703771    0.0104977     0.0649293   -0.110982    -0.0649104   -0.14184      0.066461     -0.152321    0.00231596   0.0419531   -0.102984   -0.0296556   -0.060062     0.131507    0.0396054   -0.171353     0.0965279   -0.0223151   0.0255695
 -0.0542964    0.108096    -0.113582     -0.108879      0.00771403  -0.0984695   0.117685    -0.0201291   -0.132391     -0.0885971   -0.0479138    0.0861845   -0.0409209   -0.0676588    -0.153694    0.0795829   -0.102124    -0.089449    0.104611    -0.0019766    0.0615881   0.169333     0.0773513    0.0596952   -0.0304367  -0.108947
  0.0270925   -0.272768    -0.0392416     0.0874515     0.0600024    0.0260173   0.0234433    0.178186    -0.119316     -0.152469    -0.0625041   -0.0793905   -0.065525     0.0828315    -0.0422521   0.0976191    0.133787    -0.109011    0.095404    -0.0135001   -0.107615    0.0159282    0.00689593  -0.082234     0.190065   -0.158291
 -0.0822845   -0.0102209    0.0506026     0.11776       0.211233    -0.115276    0.0993518    0.123627    -0.0449795    -0.0665269    0.215293    -0.0157379   -0.0583985   -0.0411124    -0.219688    0.0311066   -0.0156759   -0.0290039   0.0295174    0.0680007    0.14753     0.113493     0.00962267  -0.0263995    0.177542    0.0841029
  0.10674     -0.169545     0.0290584     0.0447754     0.0877682   -0.0182435   0.0394549    0.111775    -0.0832595    -0.00790848   0.0204793    0.0497888   -0.0063718   -0.0429984    -0.0323787   0.183751    -0.0152106   -0.02276    -0.0893114   -0.0859305    0.201954   -0.078611    -0.0513401   -0.0444991    0.146691    0.0173052
  0.00692546  -0.0944627    0.128579      0.0941872     0.123778    -0.110949    0.0521859   -0.127609     0.19983       0.112159    -0.0390248    0.00477704   0.013057     0.0761178     0.0494784  -0.158241    -0.0462674    0.0201888   0.00137528   0.109566    -0.183931    0.0595689    0.0722387    0.0622741    0.0366538   0.0261125
 -0.176528     0.150697     0.0769155     0.0393645    -0.0800894    0.135833   -0.00665956   0.0429486   -0.117209     -0.045568     0.0125287    0.0019064    0.0403751   -0.318681      0.210911    0.0430873   -0.0995528   -0.0665044  -0.0811011   -0.0563269    0.0153951   0.0372174    0.04539     -0.00405565   0.144566    0.0339916
 -0.0043128    0.0777318    0.0221877    -0.218496     -0.115603     0.325537   -0.023298    -0.0425494   -0.0657363     0.106345     0.0130462   -0.152028     0.0125868    0.001659      0.110965    0.0759511   -0.216353     0.150997    0.00861649  -0.159163     0.0391155  -0.12168      0.110914     0.154733     0.0179621   0.0358443
 -0.042706     0.00141853   0.183373     -0.041099      0.0493611   -0.0667392  -0.0181336    0.0900382    0.155163      0.160387     0.0222267   -0.136321     0.121544     0.12831      -0.0282696  -0.00632503   0.015603     0.0537452   0.0198836    0.0274177   -0.10031    -0.0361422    0.0273603   -0.12531     -0.0363883  -0.0885528
 -0.185701     0.0298107    0.165006      0.162482      0.170507     0.0700287   0.0834623    0.083856    -0.0348445    -0.0222988    0.145604    -0.14762     -0.122168     0.0449462     0.0288387  -0.0946304   -0.0326131   -0.0194216   0.196861     0.107188    -0.0338278   0.029138     0.0596793   -0.103448     0.0190902  -0.0712285
 -0.194666    -0.095248     0.0348845     0.000800556   0.0168792   -0.0414511  -0.205198    -0.0432248   -0.0180635     0.00632975   0.176156    -0.121613    -0.0573026    0.132747      0.020888    0.110445     0.177388     0.0911628  -0.010659    -0.0818005    0.128531   -0.00216914   0.0151342   -0.0791388    0.0157553   0.0132568
 -0.0206696   -0.0355957    0.143485     -0.0898085     0.0729009    0.169983   -0.0833402    0.148824    -0.0382751    -0.00878199   0.0896783   -0.0234416   -0.0280027   -0.0706734    -0.0890799  -0.0766482   -0.0942383   -0.0593799  -0.145179    -0.0609087    0.157055   -0.0509729   -0.00416419   0.14758     -0.0353301   0.122608
 -0.00404039  -0.00233797   0.0375294     0.0877549    -0.108334     0.184555   -0.0159768   -0.0466339   -0.0231352     0.151149     0.0264702   -0.0823411    0.0249759    0.0313432     0.0841938  -0.207808    -0.026616    -0.0144483   0.0172962   -0.0360003    0.0297224   0.0809591    0.0038999   -0.00291023   0.0676476  -0.153462
 -0.110854    -0.0523594    0.0421398     0.0486306    -0.0417455    0.0427965  -0.0498724   -0.120673     0.0340247     0.0676308   -0.0056219    0.0994265    0.0344354    0.216463      0.0577645  -0.0136213    0.118172    -0.0469627  -0.0662934    0.168668    -0.0972381   0.112972     0.123097    -0.0508787   -0.0729828  -0.0274901
  0.0605032   -0.121885     0.329343     -0.123645     -0.0917357    0.0540375  -0.127908     0.0504872    0.0476592     0.0604786    0.0615028    0.0654376   -0.0978685   -0.102429     -0.151601    0.154706     0.0542057    0.0730499   0.0912515    0.0728891    0.0567437   0.0400301   -0.019001    -0.072922     0.0371952  -0.142283
 -0.0184228   -0.00376289  -0.215666     -0.0337312     0.166525     0.0215502  -0.130154    -0.109604    -0.124335      0.0148678   -0.0725809   -0.0369092    0.0513376    0.0289286     0.11415     0.0428708   -0.0512616    0.112332   -0.0285212    0.160552    -0.0184128  -0.0811944    0.0879261   -0.0144724    0.0232601   0.128767
  0.200693     0.0505012    0.0687472    -0.114719      0.0298144   -0.0249851  -0.0928941   -0.0957627    0.187553     -0.0640886    0.0756297   -0.0453449   -0.00222145   0.189575     -0.0723928  -0.0545233   -0.0231246    0.0839091   0.0183175   -0.00134889   0.267257    0.0547691   -0.075549     0.240363    -0.0319565   0.178932
  0.0421517   -0.0495715   -0.0111605    -0.18232      -0.136869     0.0594247  -0.116165    -0.153557     0.116122     -0.0234292    0.0304661   -0.0368401   -0.113968    -0.12004       0.205192   -0.0348937   -0.00762175   0.0893558   0.0482145   -0.12253     -0.047801   -0.13555      0.0868998   -0.163206    -0.180522   -0.0750627
 -0.0483891   -0.27076      0.162638      0.0297575     0.0103718   -0.0657252   0.0305163   -0.00133364   0.0349145    -0.142818    -0.255264    -0.0905062   -0.0583539   -0.03613       0.13256    -0.114028    -0.099693     0.168172    0.145743     0.0773859   -0.282562   -0.176465    -0.151424    -0.0329144    0.232674   -0.151494
  0.137487     0.0934545    0.0751113    -0.020689     -0.140324    -0.123541   -0.106062    -0.161766    -0.0206166     0.0480604   -0.263174     0.0509201    0.106531    -0.0276861     0.179441    0.202553     0.136187    -0.129431    0.0561332   -0.226835     0.0159653   0.00675338   0.0735682    0.149703    -0.167826   -0.0293234
 -0.0851695    0.0492494   -0.0467202     0.0272655    -0.198386     0.0830024   0.152221     0.0438911   -0.185161      0.0335258   -0.187945     0.0500208    0.022681     0.0743547     0.0240837  -0.0558776   -0.138043    -0.0277318  -0.0151067   -0.0860386    0.012033   -0.0732973   -0.0338713    0.145776     0.0538163   0.146888
 -0.0383955    0.0732353   -0.0275591    -0.0469757    -0.0623934   -0.0945248  -0.0836309    0.0392535   -0.022793      0.0332636   -0.122462     0.0189022    0.139004    -0.123989     -0.0849366  -0.00363999   0.110944     0.132055    0.0443764   -0.143996     0.124892   -0.172903     0.163215     0.0839016    0.0725473  -0.0681074
 -0.0475709   -0.149026    -0.0395255     0.0458455     0.202714    -0.198193   -0.0815097    0.0105871   -0.0396451     0.00186517   0.0734127    0.0334574   -0.0425724   -0.141605      0.0598129   0.0134741    0.0941468    0.0459     -0.0791651   -0.00740429   0.084863    0.0299141   -0.259094     0.0322903    0.0820551  -0.138132
 -0.111589     0.0654094   -0.104599      0.0436782    -0.116626    -0.0415015  -0.117217    -0.121966     0.090212      0.0941564    0.0659244   -0.0916962   -0.0228762   -0.058993     -0.0332127  -0.0590568    0.0856564    0.052815   -0.0823999   -0.0208302   -0.0624969  -0.142437    -0.138046     0.0842616   -0.249567   -0.00467813
 -0.0773565    0.0247967    0.0235066    -0.0358299    -0.0539645    0.101092   -0.0148227    0.0023918   -0.051592      0.0579365   -0.120097     0.0627056    0.0399862   -0.0393437     0.155619    0.187257     0.0300305    0.199919    0.106722    -0.120166    -0.0147187  -0.00160075   0.0817075    0.0170009    0.0605807  -0.046511
  0.0961533    0.0338059    0.000121898   0.122897      0.12017     -0.0109465  -0.152975     0.205951    -0.0688462    -0.0814887    0.0781504    0.0923535    0.144551     0.0747575    -0.0387262  -0.0461928   -0.0170845   -0.0308426  -0.00663383  -0.156072    -0.109157   -0.118718    -0.0314514    0.0773865   -0.0895647   0.085296
  0.159978     0.140929     0.0993532    -0.0762343     0.128647    -0.0973482  -0.0238052    0.104197    -0.000481557   0.0546019    0.242665     0.0139244   -0.150026     0.0412355    -0.0787318  -0.0487127   -0.179658    -0.103389   -0.158862    -0.104589    -0.0898642   0.139009     0.049485    -0.0738833   -0.155938   -0.0740667kind full, method split
┌ Info: 0: avll = 
└   tll[1] = -1.4257455750876222
[ Info: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.425764
[ Info: iteration 2, average log likelihood -1.425701
[ Info: iteration 3, average log likelihood -1.425657
[ Info: iteration 4, average log likelihood -1.425610
[ Info: iteration 5, average log likelihood -1.425555
[ Info: iteration 6, average log likelihood -1.425492
[ Info: iteration 7, average log likelihood -1.425419
[ Info: iteration 8, average log likelihood -1.425327
[ Info: iteration 9, average log likelihood -1.425196
[ Info: iteration 10, average log likelihood -1.424966
[ Info: iteration 11, average log likelihood -1.424525
[ Info: iteration 12, average log likelihood -1.423738
[ Info: iteration 13, average log likelihood -1.422622
[ Info: iteration 14, average log likelihood -1.421522
[ Info: iteration 15, average log likelihood -1.420802
[ Info: iteration 16, average log likelihood -1.420458
[ Info: iteration 17, average log likelihood -1.420318
[ Info: iteration 18, average log likelihood -1.420262
[ Info: iteration 19, average log likelihood -1.420239
[ Info: iteration 20, average log likelihood -1.420230
[ Info: iteration 21, average log likelihood -1.420226
[ Info: iteration 22, average log likelihood -1.420224
[ Info: iteration 23, average log likelihood -1.420223
[ Info: iteration 24, average log likelihood -1.420222
[ Info: iteration 25, average log likelihood -1.420222
[ Info: iteration 26, average log likelihood -1.420221
[ Info: iteration 27, average log likelihood -1.420221
[ Info: iteration 28, average log likelihood -1.420221
[ Info: iteration 29, average log likelihood -1.420221
[ Info: iteration 30, average log likelihood -1.420221
[ Info: iteration 31, average log likelihood -1.420220
[ Info: iteration 32, average log likelihood -1.420220
[ Info: iteration 33, average log likelihood -1.420220
[ Info: iteration 34, average log likelihood -1.420220
[ Info: iteration 35, average log likelihood -1.420220
[ Info: iteration 36, average log likelihood -1.420220
[ Info: iteration 37, average log likelihood -1.420220
[ Info: iteration 38, average log likelihood -1.420220
[ Info: iteration 39, average log likelihood -1.420220
[ Info: iteration 40, average log likelihood -1.420219
[ Info: iteration 41, average log likelihood -1.420219
[ Info: iteration 42, average log likelihood -1.420219
[ Info: iteration 43, average log likelihood -1.420219
[ Info: iteration 44, average log likelihood -1.420219
[ Info: iteration 45, average log likelihood -1.420219
[ Info: iteration 46, average log likelihood -1.420219
[ Info: iteration 47, average log likelihood -1.420219
[ Info: iteration 48, average log likelihood -1.420219
[ Info: iteration 49, average log likelihood -1.420219
[ Info: iteration 50, average log likelihood -1.420219
┌ Info: EM with 100000 data points 50 iterations avll -1.420219
└ 952.4 data points per parameter
┌ Info: 1
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4257635936924904
│     -1.425701311469807
│      ⋮
└     -1.4202191116809095
[ Info: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.420234
[ Info: iteration 2, average log likelihood -1.420151
[ Info: iteration 3, average log likelihood -1.420060
[ Info: iteration 4, average log likelihood -1.419932
[ Info: iteration 5, average log likelihood -1.419763
[ Info: iteration 6, average log likelihood -1.419571
[ Info: iteration 7, average log likelihood -1.419394
[ Info: iteration 8, average log likelihood -1.419257
[ Info: iteration 9, average log likelihood -1.419159
[ Info: iteration 10, average log likelihood -1.419088
[ Info: iteration 11, average log likelihood -1.419032
[ Info: iteration 12, average log likelihood -1.418986
[ Info: iteration 13, average log likelihood -1.418947
[ Info: iteration 14, average log likelihood -1.418916
[ Info: iteration 15, average log likelihood -1.418891
[ Info: iteration 16, average log likelihood -1.418872
[ Info: iteration 17, average log likelihood -1.418856
[ Info: iteration 18, average log likelihood -1.418843
[ Info: iteration 19, average log likelihood -1.418832
[ Info: iteration 20, average log likelihood -1.418822
[ Info: iteration 21, average log likelihood -1.418813
[ Info: iteration 22, average log likelihood -1.418805
[ Info: iteration 23, average log likelihood -1.418798
[ Info: iteration 24, average log likelihood -1.418792
[ Info: iteration 25, average log likelihood -1.418786
[ Info: iteration 26, average log likelihood -1.418780
[ Info: iteration 27, average log likelihood -1.418775
[ Info: iteration 28, average log likelihood -1.418770
[ Info: iteration 29, average log likelihood -1.418766
[ Info: iteration 30, average log likelihood -1.418763
[ Info: iteration 31, average log likelihood -1.418759
[ Info: iteration 32, average log likelihood -1.418756
[ Info: iteration 33, average log likelihood -1.418753
[ Info: iteration 34, average log likelihood -1.418751
[ Info: iteration 35, average log likelihood -1.418749
[ Info: iteration 36, average log likelihood -1.418747
[ Info: iteration 37, average log likelihood -1.418745
[ Info: iteration 38, average log likelihood -1.418743
[ Info: iteration 39, average log likelihood -1.418742
[ Info: iteration 40, average log likelihood -1.418740
[ Info: iteration 41, average log likelihood -1.418739
[ Info: iteration 42, average log likelihood -1.418738
[ Info: iteration 43, average log likelihood -1.418737
[ Info: iteration 44, average log likelihood -1.418736
[ Info: iteration 45, average log likelihood -1.418735
[ Info: iteration 46, average log likelihood -1.418735
[ Info: iteration 47, average log likelihood -1.418734
[ Info: iteration 48, average log likelihood -1.418733
[ Info: iteration 49, average log likelihood -1.418733
[ Info: iteration 50, average log likelihood -1.418732
┌ Info: EM with 100000 data points 50 iterations avll -1.418732
└ 473.9 data points per parameter
┌ Info: 2
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4202341005420007
│     -1.4201513637355077
│      ⋮
└     -1.4187319909829021
[ Info: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.418742
[ Info: iteration 2, average log likelihood -1.418686
[ Info: iteration 3, average log likelihood -1.418638
[ Info: iteration 4, average log likelihood -1.418581
[ Info: iteration 5, average log likelihood -1.418511
[ Info: iteration 6, average log likelihood -1.418426
[ Info: iteration 7, average log likelihood -1.418330
[ Info: iteration 8, average log likelihood -1.418228
[ Info: iteration 9, average log likelihood -1.418129
[ Info: iteration 10, average log likelihood -1.418039
[ Info: iteration 11, average log likelihood -1.417961
[ Info: iteration 12, average log likelihood -1.417894
[ Info: iteration 13, average log likelihood -1.417837
[ Info: iteration 14, average log likelihood -1.417789
[ Info: iteration 15, average log likelihood -1.417749
[ Info: iteration 16, average log likelihood -1.417714
[ Info: iteration 17, average log likelihood -1.417684
[ Info: iteration 18, average log likelihood -1.417659
[ Info: iteration 19, average log likelihood -1.417636
[ Info: iteration 20, average log likelihood -1.417616
[ Info: iteration 21, average log likelihood -1.417597
[ Info: iteration 22, average log likelihood -1.417579
[ Info: iteration 23, average log likelihood -1.417562
[ Info: iteration 24, average log likelihood -1.417546
[ Info: iteration 25, average log likelihood -1.417529
[ Info: iteration 26, average log likelihood -1.417513
[ Info: iteration 27, average log likelihood -1.417497
[ Info: iteration 28, average log likelihood -1.417481
[ Info: iteration 29, average log likelihood -1.417465
[ Info: iteration 30, average log likelihood -1.417450
[ Info: iteration 31, average log likelihood -1.417434
[ Info: iteration 32, average log likelihood -1.417419
[ Info: iteration 33, average log likelihood -1.417404
[ Info: iteration 34, average log likelihood -1.417389
[ Info: iteration 35, average log likelihood -1.417375
[ Info: iteration 36, average log likelihood -1.417360
[ Info: iteration 37, average log likelihood -1.417347
[ Info: iteration 38, average log likelihood -1.417333
[ Info: iteration 39, average log likelihood -1.417320
[ Info: iteration 40, average log likelihood -1.417307
[ Info: iteration 41, average log likelihood -1.417294
[ Info: iteration 42, average log likelihood -1.417282
[ Info: iteration 43, average log likelihood -1.417269
[ Info: iteration 44, average log likelihood -1.417257
[ Info: iteration 45, average log likelihood -1.417244
[ Info: iteration 46, average log likelihood -1.417232
[ Info: iteration 47, average log likelihood -1.417219
[ Info: iteration 48, average log likelihood -1.417207
[ Info: iteration 49, average log likelihood -1.417194
[ Info: iteration 50, average log likelihood -1.417181
┌ Info: EM with 100000 data points 50 iterations avll -1.417181
└ 236.4 data points per parameter
┌ Info: 3
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4187423394866931
│     -1.4186864679545241
│      ⋮
└     -1.4171810197246648
[ Info: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.417177
[ Info: iteration 2, average log likelihood -1.417110
[ Info: iteration 3, average log likelihood -1.417047
[ Info: iteration 4, average log likelihood -1.416974
[ Info: iteration 5, average log likelihood -1.416887
[ Info: iteration 6, average log likelihood -1.416783
[ Info: iteration 7, average log likelihood -1.416666
[ Info: iteration 8, average log likelihood -1.416539
[ Info: iteration 9, average log likelihood -1.416409
[ Info: iteration 10, average log likelihood -1.416281
[ Info: iteration 11, average log likelihood -1.416162
[ Info: iteration 12, average log likelihood -1.416055
[ Info: iteration 13, average log likelihood -1.415962
[ Info: iteration 14, average log likelihood -1.415882
[ Info: iteration 15, average log likelihood -1.415815
[ Info: iteration 16, average log likelihood -1.415759
[ Info: iteration 17, average log likelihood -1.415712
[ Info: iteration 18, average log likelihood -1.415672
[ Info: iteration 19, average log likelihood -1.415638
[ Info: iteration 20, average log likelihood -1.415608
[ Info: iteration 21, average log likelihood -1.415582
[ Info: iteration 22, average log likelihood -1.415558
[ Info: iteration 23, average log likelihood -1.415537
[ Info: iteration 24, average log likelihood -1.415518
[ Info: iteration 25, average log likelihood -1.415500
[ Info: iteration 26, average log likelihood -1.415484
[ Info: iteration 27, average log likelihood -1.415469
[ Info: iteration 28, average log likelihood -1.415454
[ Info: iteration 29, average log likelihood -1.415441
[ Info: iteration 30, average log likelihood -1.415428
[ Info: iteration 31, average log likelihood -1.415416
[ Info: iteration 32, average log likelihood -1.415404
[ Info: iteration 33, average log likelihood -1.415393
[ Info: iteration 34, average log likelihood -1.415382
[ Info: iteration 35, average log likelihood -1.415372
[ Info: iteration 36, average log likelihood -1.415362
[ Info: iteration 37, average log likelihood -1.415352
[ Info: iteration 38, average log likelihood -1.415343
[ Info: iteration 39, average log likelihood -1.415333
[ Info: iteration 40, average log likelihood -1.415324
[ Info: iteration 41, average log likelihood -1.415315
[ Info: iteration 42, average log likelihood -1.415306
[ Info: iteration 43, average log likelihood -1.415298
[ Info: iteration 44, average log likelihood -1.415289
[ Info: iteration 45, average log likelihood -1.415281
[ Info: iteration 46, average log likelihood -1.415272
[ Info: iteration 47, average log likelihood -1.415264
[ Info: iteration 48, average log likelihood -1.415255
[ Info: iteration 49, average log likelihood -1.415247
[ Info: iteration 50, average log likelihood -1.415239
┌ Info: EM with 100000 data points 50 iterations avll -1.415239
└ 118.1 data points per parameter
┌ Info: 4
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4171767825721477
│     -1.417110349024508
│      ⋮
└     -1.4152388044894235
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.415239
[ Info: iteration 2, average log likelihood -1.415176
[ Info: iteration 3, average log likelihood -1.415114
[ Info: iteration 4, average log likelihood -1.415042
[ Info: iteration 5, average log likelihood -1.414952
[ Info: iteration 6, average log likelihood -1.414840
[ Info: iteration 7, average log likelihood -1.414704
[ Info: iteration 8, average log likelihood -1.414550
[ Info: iteration 9, average log likelihood -1.414386
[ Info: iteration 10, average log likelihood -1.414220
[ Info: iteration 11, average log likelihood -1.414063
[ Info: iteration 12, average log likelihood -1.413919
[ Info: iteration 13, average log likelihood -1.413790
[ Info: iteration 14, average log likelihood -1.413676
[ Info: iteration 15, average log likelihood -1.413573
[ Info: iteration 16, average log likelihood -1.413482
[ Info: iteration 17, average log likelihood -1.413400
[ Info: iteration 18, average log likelihood -1.413326
[ Info: iteration 19, average log likelihood -1.413259
[ Info: iteration 20, average log likelihood -1.413197
[ Info: iteration 21, average log likelihood -1.413141
[ Info: iteration 22, average log likelihood -1.413088
[ Info: iteration 23, average log likelihood -1.413039
[ Info: iteration 24, average log likelihood -1.412994
[ Info: iteration 25, average log likelihood -1.412951
[ Info: iteration 26, average log likelihood -1.412911
[ Info: iteration 27, average log likelihood -1.412873
[ Info: iteration 28, average log likelihood -1.412837
[ Info: iteration 29, average log likelihood -1.412803
[ Info: iteration 30, average log likelihood -1.412770
[ Info: iteration 31, average log likelihood -1.412740
[ Info: iteration 32, average log likelihood -1.412711
[ Info: iteration 33, average log likelihood -1.412684
[ Info: iteration 34, average log likelihood -1.412658
[ Info: iteration 35, average log likelihood -1.412634
[ Info: iteration 36, average log likelihood -1.412611
[ Info: iteration 37, average log likelihood -1.412589
[ Info: iteration 38, average log likelihood -1.412568
[ Info: iteration 39, average log likelihood -1.412549
[ Info: iteration 40, average log likelihood -1.412530
[ Info: iteration 41, average log likelihood -1.412513
[ Info: iteration 42, average log likelihood -1.412496
[ Info: iteration 43, average log likelihood -1.412480
[ Info: iteration 44, average log likelihood -1.412465
[ Info: iteration 45, average log likelihood -1.412451
[ Info: iteration 46, average log likelihood -1.412437
[ Info: iteration 47, average log likelihood -1.412424
[ Info: iteration 48, average log likelihood -1.412411
[ Info: iteration 49, average log likelihood -1.412399
[ Info: iteration 50, average log likelihood -1.412387
┌ Info: EM with 100000 data points 50 iterations avll -1.412387
└ 59.0 data points per parameter
┌ Info: 5
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4152389137082388
│     -1.4151756511454943
│      ⋮
└     -1.4123866936607261
┌ Info: Total log likelihood: 
│   tll =
│    251-element Array{Float64,1}:
│     -1.4257455750876222
│     -1.4257635936924904
│     -1.425701311469807
│     -1.42565735515504
│      ⋮
│     -1.4124109221073764
│     -1.4123985972055035
└     -1.4123866936607261
32×26 Array{Float64,2}:
  0.405197    -0.2582      0.245902    0.264272     0.174407      0.33881     0.554907    0.0141395   0.16173    -0.216343     0.548132   -0.0895677   0.224817    0.182562   -0.158801   -0.621709    -0.321822      0.51806      0.017528    0.0953166   -0.0916028  -0.624818   -0.445958     0.295915   -0.340241    0.0237567
 -0.0579396   -0.826654    0.23973     0.374462    -0.254863     -0.0196067   0.231952   -0.12224    -0.147435   -0.338697     0.16606    -0.141574   -0.0647635   0.037702   -0.507894   -0.276665     0.513489      0.566698     0.311749   -0.232073    -0.0653411   0.528626   -0.138991    -0.0551852  -0.848796   -0.604961
 -0.222571    -0.36087     0.45782    -0.17434      0.16214       0.0643348   0.118916    0.286596   -0.141331   -0.168557     0.433253   -0.168298   -0.34218     0.349345    0.658344    0.371378    -0.226688      0.451119     0.139853    0.620769     0.0615739   0.0640973  -0.359788     0.573441   -0.095478   -0.330553
  0.118548    -0.102165    0.877529    0.202958     0.559415     -0.278798   -0.0194609   0.141605    0.205155   -0.0132925    0.0412421  -0.230641   -0.406687   -0.056827    0.0500603  -0.00475149  -0.176959      0.499594     0.362725   -0.0138975    0.250005    0.331527    0.358991    -0.383022   -0.16494    -0.0617722
 -0.228983    -0.576332    0.297491   -0.364375     0.0760702    -0.294228   -0.346697    0.317581   -0.922306    0.738682    -0.138083    1.01761    -0.417667   -0.2624     -0.0884081  -0.471025     0.0816251    -0.0720344    0.0462829  -0.162035     0.191278   -0.360924    0.0584508   -0.208438   -0.339289    0.0142251
 -0.427095    -0.257598    0.467686   -0.00114194   0.152473     -0.311649    0.180222   -0.273879   -0.465081   -0.040123     0.254743    0.0539744  -0.272211    0.2032      0.122504   -0.756087    -0.160638      0.256713    -0.199658   -0.00549537   0.373261   -0.872849    0.106867    -0.208955   -0.0889692  -0.0545535
 -0.296161    -0.153462    0.121928   -0.166753     0.190185      0.315056    0.40199     0.930058   -0.391623   -0.0343315   -0.894177   -0.129524   -0.0653539   0.289928   -0.401709   -0.123117     0.499238      0.0624251   -0.0169825  -0.826434    -0.615445   -0.57164     0.0990933   -0.276279   -0.203002    0.126204
 -0.685695    -0.31323     0.428139    0.784993    -0.170462     -0.619962    0.15528     0.677305   -0.395947   -0.320794    -0.105503   -0.438533    0.16499     0.278276   -0.185837   -0.140009     0.101676     -0.448811    -0.0759257  -0.364416     0.0988258   0.330571   -0.0789311   -0.559646   -0.287658   -0.206075
  0.116253     0.0658262  -0.137071    0.0778184   -0.00431173    0.13177     0.0256164  -0.226619   -0.0149128  -0.180716     0.0902365  -0.0852054   0.24822    -0.166653   -0.29197    -0.104094     0.0506102    -0.00796231   0.0406239   0.0806806    0.134122    0.0689777  -0.00978221   0.0293914   0.0695792  -0.0866775
 -0.064231    -0.178144    0.0680454   0.11449     -0.00179319   -0.216469   -0.0858535   0.263953   -0.170034   -0.00463482  -0.204221    0.0344367  -0.285998   -0.121792   -0.0679402   0.0844355   -0.000872215   0.150338    -0.242095    0.00661461  -0.0214601   0.0941695  -0.0475583   -0.113856    0.0916842  -0.0752719
 -0.0666399    0.391203    0.0114345  -0.247825    -0.550115     -0.191981    0.198109    0.108054    0.138286    0.329442     0.200581    0.147387   -0.0838804   0.335349    0.0994885   0.0330822   -0.337249     -0.205243    -0.115116   -0.185622    -0.398509   -0.262406    0.139008    -0.0377905   0.207       0.00957361
 -0.217235     0.0761148   0.139983   -0.179849     0.350721      0.190973   -0.148688   -0.0116352   0.0881638   0.330905     0.0419884  -0.0765834   0.0982283   0.241354    0.54889     0.172394     0.0263434    -0.0337907    0.342393   -0.0800555   -0.0473542  -0.146859    0.100932     0.172322   -0.270514    0.243268
  0.0157694   -0.529522   -0.615044   -0.0250406   -0.125603      0.395873   -0.0153829  -0.175249    0.0831762  -0.130016    -0.147704    0.481937    0.53036    -0.268736    0.0168031  -0.128775     0.673526     -0.306613    -0.537096    0.224817     0.228933    0.552272   -0.254373     0.590709    0.423242   -0.127086
  0.177767    -0.726565    0.042481   -0.661801     0.00558009   -0.140793   -0.546697   -0.156315   -0.218895    0.196757     0.418029   -0.257233    0.283607    0.800352   -0.301232    0.0834713   -0.0843227    -0.549499    -0.685659   -0.38549      0.398926    0.222279    0.45133     -0.612742    0.163646   -0.330584
  0.332411     0.52096    -0.371146    0.136322    -0.0412679     0.194634    0.12113    -0.180058    0.800192   -0.30174      0.0837595  -0.488233    0.348109    0.0862973  -0.0711418   0.624259    -0.153703      0.0943218    0.234811    0.168356    -0.245982    0.717408   -0.05842      0.111932    0.285433    0.127597
  0.120092     0.466178   -0.115224    0.115434     0.0757442    -0.0362804  -0.41061    -0.621111    0.369499   -0.186147     0.305427   -0.0531328   0.457987    0.148815   -0.0508222   0.0626407   -0.603979     -0.493603     0.133334    0.485272     1.28563     0.359973   -0.0825514    0.267005    0.289085    0.24225
  0.571779    -0.361678   -0.363126    0.253942    -0.751744     -0.431656    0.0932529   0.127837    0.0769796  -0.544869     0.170182    0.143464   -0.214948   -0.274169   -0.811069   -0.00588597  -0.333622      0.097924    -0.695275    0.158777    -0.294793    0.248257   -0.252373    -0.163582    0.397665   -0.580077
 -0.205315     0.791367   -0.0299687   0.305382    -0.610364     -0.727911   -0.0539631   0.0982699  -0.0695893  -0.0398635   -0.365854    0.113758   -0.45692    -0.273383   -0.759198   -0.178362    -0.328851      0.181639    -0.28296     0.09442     -0.043861   -0.384723    0.214805     0.0438358   0.244909   -0.360956
  0.79016      0.189207   -0.29463    -0.460684    -0.000347889  -0.131187   -0.403142   -0.580836    0.140863    0.149264    -0.0727349   0.565802   -0.593933   -0.63901     0.346212    0.161898     0.275091      0.278293    -0.362312    0.359185     0.151478   -0.108977    0.156618    -0.196854    0.365817   -0.0742796
  0.100371    -0.125143   -0.110546   -0.266035     0.240643      0.470489    0.167674    0.350151   -0.287522    0.205904    -0.331187    0.0204074  -0.76465     0.013185    0.421009    0.199334    -0.155121      0.167621    -0.386357   -0.198079     0.0950311   0.278083    0.270518     0.247406    0.589072   -0.249468
 -0.116602     0.37161    -0.134531    0.2082      -0.167019     -0.393332    0.133247   -0.363259   -0.476361    0.100261    -0.214575   -0.0773367   0.354306   -0.588322    0.136636    0.454172     0.207946      0.0915979    0.62361     0.222427     0.184401   -0.314899    0.250401     0.263296   -1.22277     0.76886
 -0.532116     0.12917    -0.373574    0.104769     0.128549     -0.180237   -0.258912   -0.470799   -0.0721119   0.116604    -0.0776351  -0.25996     0.316166   -0.586167   -0.567588    0.0305604    0.521969     -0.458813    -0.276408   -0.553518     0.110474   -0.164144   -0.100758    -0.180367    0.376719    0.266746
  0.260445     0.0403356   0.0538856   0.0502751   -0.271474      0.463541   -0.368444   -0.225948    0.299245   -0.0351375   -0.546688    0.188929    0.65491    -0.186339   -0.0323351  -0.692043     0.590492      0.301192     0.718835   -0.314406    -0.314813   -0.463907   -0.0819601   -0.439454   -0.209363   -0.0344955
  0.0226814    0.328408    0.214334    0.130717    -0.00085226    0.303222    0.018906    0.203775    0.637922   -0.337106    -0.55777    -0.565624   -0.033426   -0.38219    -0.279462    0.179607     0.4568        0.516676    -0.234015    0.133955    -0.153797    0.361593   -0.0224225   -0.129218    0.794776   -0.447505
 -0.200006     0.365389   -0.622293   -0.435748     0.100336      0.755587    0.0374898  -0.388227   -0.256397    0.499698     0.403356   -0.175865    0.333212   -0.0250485   0.0255789   0.0264755   -0.303584     -0.278923     0.280438    0.422875    -0.550956   -0.727265   -0.157736     0.641578    0.157727    0.262028
  0.17047     -0.253009   -0.782641    0.277649    -0.256822      0.012118   -0.238469    0.247414   -0.436741    0.508973     0.38461     0.339311   -0.0890806  -0.0224584   0.296489    0.491728    -0.0284882    -0.445771     0.490897    0.317408    -0.532595    0.409474   -0.0499175    0.233404   -0.380212   -0.0132455
  0.00728245  -0.268266    0.306414   -0.383978     0.389         0.240003    0.250678   -0.0949688  -0.251239   -0.245735     0.112951    0.0386792   0.620014    0.251357   -0.0200044  -0.0285916   -0.208653     -0.243104     0.158842   -0.584842     0.491556    0.159691    0.135675     0.257999   -0.599236    0.569675
 -0.0683685    0.0311212   0.235521   -0.421913     0.586401      0.53516    -0.105605   -0.224188    0.337279    0.569415     0.100416    0.0625033   0.502182    0.289858    0.987452    0.112423     0.407146     -0.251865     0.415522   -0.462239     0.259849    0.152284   -0.172336     0.129886   -0.257993    0.873708
 -0.0332312   -0.0271277   0.215458   -0.0464891   -0.243504     -0.167007   -0.0669677   0.299554   -0.177839    0.341257     0.204858    0.126641   -0.344152    0.695789    0.228107    0.177028    -0.751887     -0.185348    -0.354393   -0.149805     0.017573    0.0300335  -0.0101979    0.152309    0.213351   -0.130851
  0.108034    -0.207185   -0.157459    0.301818     0.107496      0.202043    0.0483034  -0.189051    0.141948   -0.324972    -0.0185416  -0.227642    0.3267     -0.443143   -0.174793   -0.164588     0.489886      0.117192     0.0795939   0.0584958    0.218904    0.12645    -0.1529       0.0841438   0.026227   -0.0520899
 -0.290044     0.772081    0.368842    0.380056     0.443178     -0.498046   -0.22144    -0.409825    1.23828    -0.837654     0.219119   -0.560219   -0.156364   -0.472481    1.02564     0.262901     0.121786      0.70176      0.284495    0.159368    -0.26875    -0.304887   -0.174685    -0.250275   -0.427299   -0.0192152
 -0.0820852    0.840492    0.575796   -0.350331     0.582969      0.0877717  -0.368101    0.0589918   0.27779     0.320562    -0.088054   -0.229157   -0.134191    0.622748    0.513088    0.0869052   -0.550889     -0.0118813    0.671121    0.00911413   0.181363   -0.356297    0.735214    -0.380322   -0.198885    0.176132[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.412375
[ Info: iteration 2, average log likelihood -1.412364
[ Info: iteration 3, average log likelihood -1.412353
[ Info: iteration 4, average log likelihood -1.412343
[ Info: iteration 5, average log likelihood -1.412332
[ Info: iteration 6, average log likelihood -1.412322
[ Info: iteration 7, average log likelihood -1.412313
[ Info: iteration 8, average log likelihood -1.412303
[ Info: iteration 9, average log likelihood -1.412294
[ Info: iteration 10, average log likelihood -1.412285
┌ Info: EM with 100000 data points 10 iterations avll -1.412285
└ 59.0 data points per parameter
kind full, method kmeans
[ Info: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.383339e+05
      1       7.045405e+05      -2.337934e+05 |       32
      2       6.916339e+05      -1.290660e+04 |       32
      3       6.868683e+05      -4.765613e+03 |       32
      4       6.844438e+05      -2.424425e+03 |       32
      5       6.829161e+05      -1.527792e+03 |       32
      6       6.817414e+05      -1.174662e+03 |       32
      7       6.808662e+05      -8.751845e+02 |       32
      8       6.801472e+05      -7.190505e+02 |       32
      9       6.795061e+05      -6.411023e+02 |       32
     10       6.789688e+05      -5.372710e+02 |       32
     11       6.784953e+05      -4.735117e+02 |       32
     12       6.780844e+05      -4.108688e+02 |       32
     13       6.777220e+05      -3.624355e+02 |       32
     14       6.774036e+05      -3.183743e+02 |       32
     15       6.771217e+05      -2.818525e+02 |       32
     16       6.768525e+05      -2.692603e+02 |       32
     17       6.766181e+05      -2.343470e+02 |       32
     18       6.764093e+05      -2.088378e+02 |       32
     19       6.762378e+05      -1.714938e+02 |       32
     20       6.760648e+05      -1.730118e+02 |       32
     21       6.759037e+05      -1.610837e+02 |       32
     22       6.757430e+05      -1.607179e+02 |       32
     23       6.755951e+05      -1.478836e+02 |       32
     24       6.754674e+05      -1.277132e+02 |       32
     25       6.753454e+05      -1.219998e+02 |       32
     26       6.752272e+05      -1.181863e+02 |       32
     27       6.751152e+05      -1.120272e+02 |       32
     28       6.750116e+05      -1.035499e+02 |       32
     29       6.749116e+05      -1.000242e+02 |       32
     30       6.748201e+05      -9.146857e+01 |       32
     31       6.747402e+05      -7.990792e+01 |       32
     32       6.746588e+05      -8.145988e+01 |       32
     33       6.745792e+05      -7.952811e+01 |       32
     34       6.745014e+05      -7.781629e+01 |       32
     35       6.744422e+05      -5.925129e+01 |       32
     36       6.743872e+05      -5.494950e+01 |       32
     37       6.743377e+05      -4.949504e+01 |       32
     38       6.742889e+05      -4.885171e+01 |       32
     39       6.742479e+05      -4.099378e+01 |       32
     40       6.742125e+05      -3.533775e+01 |       32
     41       6.741803e+05      -3.227652e+01 |       32
     42       6.741420e+05      -3.828764e+01 |       32
     43       6.740954e+05      -4.662235e+01 |       32
     44       6.740473e+05      -4.803469e+01 |       32
     45       6.740062e+05      -4.116793e+01 |       32
     46       6.739620e+05      -4.414496e+01 |       32
     47       6.739178e+05      -4.415987e+01 |       32
     48       6.738771e+05      -4.076927e+01 |       32
     49       6.738422e+05      -3.486344e+01 |       32
     50       6.738095e+05      -3.274427e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 673809.4729734883)
┌ Info: K-means with 32000 data points using 50 iterations
└ 37.0 data points per parameter
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.424023
[ Info: iteration 2, average log likelihood -1.419048
[ Info: iteration 3, average log likelihood -1.417791
[ Info: iteration 4, average log likelihood -1.416914
[ Info: iteration 5, average log likelihood -1.415973
[ Info: iteration 6, average log likelihood -1.415011
[ Info: iteration 7, average log likelihood -1.414246
[ Info: iteration 8, average log likelihood -1.413770
[ Info: iteration 9, average log likelihood -1.413496
[ Info: iteration 10, average log likelihood -1.413324
[ Info: iteration 11, average log likelihood -1.413200
[ Info: iteration 12, average log likelihood -1.413101
[ Info: iteration 13, average log likelihood -1.413019
[ Info: iteration 14, average log likelihood -1.412947
[ Info: iteration 15, average log likelihood -1.412884
[ Info: iteration 16, average log likelihood -1.412827
[ Info: iteration 17, average log likelihood -1.412775
[ Info: iteration 18, average log likelihood -1.412728
[ Info: iteration 19, average log likelihood -1.412685
[ Info: iteration 20, average log likelihood -1.412644
[ Info: iteration 21, average log likelihood -1.412607
[ Info: iteration 22, average log likelihood -1.412572
[ Info: iteration 23, average log likelihood -1.412539
[ Info: iteration 24, average log likelihood -1.412509
[ Info: iteration 25, average log likelihood -1.412480
[ Info: iteration 26, average log likelihood -1.412452
[ Info: iteration 27, average log likelihood -1.412426
[ Info: iteration 28, average log likelihood -1.412402
[ Info: iteration 29, average log likelihood -1.412378
[ Info: iteration 30, average log likelihood -1.412355
[ Info: iteration 31, average log likelihood -1.412334
[ Info: iteration 32, average log likelihood -1.412313
[ Info: iteration 33, average log likelihood -1.412293
[ Info: iteration 34, average log likelihood -1.412273
[ Info: iteration 35, average log likelihood -1.412255
[ Info: iteration 36, average log likelihood -1.412237
[ Info: iteration 37, average log likelihood -1.412220
[ Info: iteration 38, average log likelihood -1.412203
[ Info: iteration 39, average log likelihood -1.412187
[ Info: iteration 40, average log likelihood -1.412172
[ Info: iteration 41, average log likelihood -1.412158
[ Info: iteration 42, average log likelihood -1.412144
[ Info: iteration 43, average log likelihood -1.412131
[ Info: iteration 44, average log likelihood -1.412119
[ Info: iteration 45, average log likelihood -1.412107
[ Info: iteration 46, average log likelihood -1.412096
[ Info: iteration 47, average log likelihood -1.412085
[ Info: iteration 48, average log likelihood -1.412075
[ Info: iteration 49, average log likelihood -1.412065
[ Info: iteration 50, average log likelihood -1.412056
┌ Info: EM with 100000 data points 50 iterations avll -1.412056
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
  0.0614163   0.725468    0.184573     0.198892     0.0739876    0.122373     0.199627     0.0232342   1.08242     -0.48565    -0.0866024  -0.641096     0.0223724  -0.175505     0.173788    0.603463    0.167756    0.543233     0.189824     0.140705    -0.424092    0.382743   -0.0301768    -0.062924     0.260894   -0.0723651
  0.0163657   0.087007   -0.176098     0.17239     -0.0330264    0.0338692    0.00345579  -0.167038    0.0558682   -0.158694    0.196153   -0.161435     0.443445   -0.00354108  -0.219783   -0.036759   -0.0567874  -0.133764     0.185241     0.0411009    0.125348    0.0992896  -0.0549947    -0.018232    -0.0411242   0.0399049
 -0.0450406  -0.345182    0.416875     0.0707144    0.396618     0.2207      -0.0722547    0.227164   -0.00677681   0.0496792  -0.0101486  -0.073712     0.0150468   0.078871     0.232352    0.143735    0.0573596   0.327091     0.507052    -0.059424     0.0503232   0.162036   -0.0371889     0.17581     -0.550137    0.0825683
  0.019875   -0.0254016  -0.00298173  -0.050153    -0.0885617   -0.032836    -0.00161211   0.125687   -0.0644461    0.0228384  -0.20055     0.114417    -0.260104   -0.154051    -0.0634793   0.0217329   0.0735011   0.101046    -0.261926    -0.0351494   -0.040981    0.0247557   0.0469351    -0.0567033    0.247322   -0.084711
  0.372862    0.573636   -0.506123    -0.602538     0.328142     0.950359    -0.061563    -0.505116    0.257885     0.793865    0.0666468   0.274351    -0.0221277  -0.459001     0.214163   -0.186615   -0.363325   -0.0279439    0.460134     0.194191    -0.177545   -0.259437    0.227855      0.650364     0.673263    0.118036
 -0.607732    0.121089   -0.518951     0.258529     0.0740489   -0.331561    -1.09301     -0.288197    0.0271849    0.0504923  -0.208767   -0.627714    -0.086785   -0.281785    -0.0472752   0.105562    0.826708   -0.520628    -0.186686    -0.0466264   -0.11818     0.179589   -0.0419834    -0.467046     0.455097   -0.250632
 -0.395721    0.0855245   0.661534     0.0490961   -0.021512    -0.626807    -0.229832     0.232261    0.0100078    0.0153199   0.21389    -0.367872    -0.270305    0.534444     0.220662    0.315572   -0.562998    0.0442294   -0.345199    -0.139039     0.277489    0.13366     0.247399     -0.118195     0.191848   -0.125079
  0.260583    0.08406     0.108683    -0.520578    -0.293815    -0.282073    -0.157655    -0.0128537   0.435568     0.211358   -0.0556468  -0.16845      0.19173    -0.0069234   -0.530388   -0.283125   -0.250386   -0.782857    -0.846325    -0.868971    -0.112216   -0.0873149   0.506201     -1.022        0.159476   -0.0200241
 -0.0470426  -1.34862    -0.140206    -0.91696      0.184298     0.00336518  -0.609348     0.0381142  -0.951077     0.419677    0.68647     0.0583066    0.290564    1.29117     -0.404809    0.145219    0.288148   -0.498608    -0.466275    -0.145552     0.590317    0.233666    0.417135     -0.479208     0.229044   -0.568831
 -0.0799813   0.0506712   0.801282     0.158255     0.290367    -0.152195    -0.163959    -0.379691    0.494708    -0.231462   -0.465323   -0.0922689    0.168142   -0.21541      0.125726   -0.705153    0.17883     0.68339      0.374397    -0.277079     0.176285   -0.360935    0.0744868    -0.59204     -0.180526   -0.017798
  0.332378   -0.20318     0.0225738    0.324793    -0.305339    -0.205502     0.0979713    0.186672    0.0788158   -0.770865   -0.191157   -0.1377      -0.460057    0.0201156   -0.640796   -0.27593    -0.627613    0.41578     -0.368962     0.436546     0.166843    0.268521    0.0251046    -0.0107189    0.493105   -0.810155
 -0.0270876  -0.130655   -0.185104     0.170395    -0.0938983   -0.0645861    0.164053    -0.382712    0.191147    -0.292131   -0.516383   -0.200684     0.593642   -0.71301     -0.542084   -0.16491     0.622232   -0.10706     -0.849863    -0.362069     0.359792    0.331698   -0.484013      0.375485     0.358901    0.061784
  0.816967    0.0232167  -0.217729    -0.307984    -0.0573879   -0.688068    -0.612137    -0.645778    0.204207    -0.105748    0.0137446   0.647202    -0.698318   -0.47796      0.480511    0.336884    0.268509    0.453185    -0.177089     0.34179      0.21733     0.0133902   0.291921     -0.496364     0.103107   -0.155618
 -0.768618    0.200077    0.176723     0.0603605    0.230916     0.105544     1.03924      0.41626    -0.304873    -0.411034   -0.256465   -0.710683     0.548221    0.536486    -0.498775   -0.31301     0.309616   -0.0352813    0.0121393   -0.700981    -0.200803   -0.437926    0.215451      0.341695    -0.334488    0.225211
 -0.590858   -0.173578   -0.420184     0.187921     0.0885201    0.168326     0.0201796   -0.060034   -0.545777    -0.147687    0.216356    0.00881974  -0.228697   -0.153364     0.419337    0.468287    0.257774    0.281119     0.455824     0.968273     0.0883955  -0.0994701  -0.356504      0.864759    -0.0741797   0.0182887
  0.0624805   0.205994   -0.0761962    0.108288    -0.0255228   -0.193865    -0.0545759   -0.260278   -0.265843     0.0517032   0.0260768  -0.178595     0.516337   -0.245162     0.1624      0.400803    0.0402826  -0.125012     0.89667      0.153846     0.185538   -0.0822398   0.229715      0.043848    -1.26456     0.668798
  0.417004   -0.191089   -0.817906     0.20209     -0.482921    -0.207322     0.0989106    0.0852012  -0.0253006    0.184311    0.566805    0.158813    -0.1027     -0.204483    -0.250991    0.360757   -0.122754   -0.333458    -0.378696     0.149411    -0.652798    0.425696   -0.274586      0.305107     0.211199   -0.242515
 -0.374503   -0.114661    1.35364      0.135863     0.660257    -0.586589    -0.190203     0.572385   -0.153952     0.621541    0.414134    0.325828    -0.703057   -0.022225     0.111835   -0.508767   -0.386945   -0.325442     0.760247     0.0384076    0.692762    0.106559    0.223778     -0.529603    -0.158037    0.459058
  0.324861   -0.233842   -0.331773     0.116969    -0.378111     0.638586    -0.191356    -0.186536    0.110932    -0.217396   -0.0620329   0.0392768    0.885178    0.0164417   -0.307496   -0.628835    0.50195     0.21231      0.548939    -0.0605384   -0.456608   -0.330114   -0.383018     -0.1461      -0.227402   -0.174452
  0.0764246   0.641778   -0.23144     -0.0330345    0.157081     0.162318    -0.352399    -0.41367     0.665256    -0.287257    0.284749   -0.248062     0.563755    0.316527     0.0416196   0.269635   -0.309376   -0.566214     0.234413     0.401764     0.864985    0.33172    -0.107939      0.097853     0.308325    0.287003
  0.231993   -0.156272    0.464844    -0.258452    -0.00525497   0.00707618   0.76266     -0.0640675   0.206562    -0.130974    0.694322    0.0907695   -0.320886    0.247931     0.188862   -0.309848   -0.470295    0.448618    -0.243705     0.141457    -0.0373995  -0.595718   -0.388041      0.223857    -0.0775835  -0.0713967
 -0.192303    0.345708    0.0218978   -0.338192     0.173566     0.146452    -0.0151181   -0.12345     0.0953325    0.512912    0.104659   -0.134976    -0.112177    0.366953     0.60453     0.178849   -0.186464   -0.139148     0.17232     -0.0675001   -0.35571    -0.316235    0.275503      0.170982    -0.0651312   0.141903
 -0.173431    0.0113404  -0.219158    -0.331843    -0.449178    -0.126312    -0.152063     0.850332   -0.657128     0.740489    0.224687    0.653214    -0.43356     0.70349      0.269043    0.505002   -0.421952   -0.592647     0.444709    -0.0737935   -0.519428   -0.245197    0.0584216    -0.185723    -0.194833   -0.119214
 -0.18628     1.04471    -0.212082     0.194445    -0.632363    -0.657349     0.124353     0.0190635  -0.137962     0.260755   -0.37518     0.350507    -0.237837   -0.483708    -0.570171   -0.157271   -0.139012    0.24383     -0.174975     0.0565563   -0.237249   -0.523823    0.105947      0.219966     0.181947   -0.0517521
 -0.0127262  -0.155942   -0.00537767  -0.22663      0.0435224    0.294552    -0.0420849    0.830841   -0.314566     0.124978   -1.068       0.263176    -0.487398   -0.0597779   -0.120914    0.0347709   0.335065    0.170325    -0.120368    -0.508146    -0.22572    -0.213731    0.16327      -0.466107     0.177865   -0.00298978
  0.340486   -0.100971   -0.170317    -0.222236     0.221466    -0.063541    -0.168436    -0.455563   -0.622099     0.0644597   0.0411305   0.266019     0.126271    0.153214     0.178176   -0.0986595  -0.44016    -0.231034    -0.450188    -0.228476     0.993627    0.147748    0.360762      0.42312     -0.0526957   0.434244
 -0.389716   -0.685013    0.455269     0.754891    -0.237526    -0.670294     0.365293     0.440589   -0.377998    -0.479215   -0.0518553  -0.292272    -0.0880739   0.107457    -0.40051    -0.0396739   0.4192      0.00865507  -0.208846    -0.277017     0.0350038   0.432974   -0.170555     -0.637815    -0.584675   -0.449201
  0.199364    0.0346197   0.858761    -0.204737     0.313114     0.558365    -0.254507     0.137439    0.0750653    0.374043    0.0419681  -0.0601924   -0.283187    0.742975     0.716489    0.0192277  -0.444238    0.405871     0.121967    -0.00454556   0.118881    0.161861   -0.000319823   0.364581    -0.35363     0.0597229
 -0.112411   -0.0930345  -0.198959     0.297461    -0.0784129   -0.115309     0.0157044   -0.107747   -0.0177707   -0.0529269   0.300351   -0.07657      0.145575   -0.0665909   -0.116254   -0.0319653  -0.097474   -0.179055    -0.167928    -0.00218223   0.133142    0.0767284  -0.0950492     0.104565     0.294069   -0.186349
 -0.350366   -0.0893496   0.0455072   -0.416988     0.424922     0.419782    -0.00531191  -0.139921    0.0950113    0.533292    0.169862    0.072286     0.699977    0.135847     0.583253    0.135157    0.389164   -0.43275      0.377323    -0.523942     0.0481125  -0.125758   -0.142113      0.115423    -0.196984    0.810365
 -0.368222   -0.538869    0.160542     0.00847041   0.0849835   -0.190225    -0.0469914   -0.0183522  -1.03158      0.34315     0.0377444   0.428535    -0.165697   -0.229071    -0.243883   -0.655705    0.0278263   0.0241019   -0.00766849  -0.109672     0.0192268  -0.570037    0.0756404     0.00950981  -0.356814   -0.0726209
  0.575666   -0.651725   -0.128424    -0.14934      0.422152     1.15998      0.0891551   -0.0718135   0.0952895   -0.100975    0.0394864  -0.0579311   -0.055595   -0.531147     0.0602413   0.108544    0.671141    0.2816      -0.17606      0.155455     0.192315    0.45947     0.0599306     0.0911609    0.339085   -0.301894[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.412047
[ Info: iteration 2, average log likelihood -1.412039
[ Info: iteration 3, average log likelihood -1.412031
[ Info: iteration 4, average log likelihood -1.412024
[ Info: iteration 5, average log likelihood -1.412016
[ Info: iteration 6, average log likelihood -1.412010
[ Info: iteration 7, average log likelihood -1.412003
[ Info: iteration 8, average log likelihood -1.411997
[ Info: iteration 9, average log likelihood -1.411991
[ Info: iteration 10, average log likelihood -1.411985
┌ Info: EM with 100000 data points 10 iterations avll -1.411985
└ 59.0 data points per parameter
[ Info: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.678561e+05
      1       2.230230e+04      -1.455538e+05 |        2
      2       7.823675e+03      -1.447862e+04 |        0
      3       7.823675e+03       0.000000e+00 |        0
K-means converged with 3 iterations (objv = 7823.67549422947)
┌ Info: K-means with 900 data points using 3 iterations
└ 150.0 data points per parameter
[ Info: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
[ Info: iteration 1, average log likelihood -2.043155
[ Info: iteration 2, average log likelihood -2.043154
[ Info: iteration 3, average log likelihood -2.043154
[ Info: iteration 4, average log likelihood -2.043154
[ Info: iteration 5, average log likelihood -2.043154
[ Info: iteration 6, average log likelihood -2.043154
[ Info: iteration 7, average log likelihood -2.043154
[ Info: iteration 8, average log likelihood -2.043154
[ Info: iteration 9, average log likelihood -2.043154
[ Info: iteration 10, average log likelihood -2.043154
┌ Info: EM with 900 data points 10 iterations avll -2.043154
└ 81.8 data points per parameter
    Testing GaussianMixtures tests passed 
