Julia Version 1.5.0-DEV.275
Commit 2eba23f58f (2020-02-17 02:20 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-9.0.1 (ORCJIT, skylake)
Environment:
  JULIA_DEPOT_PATH = ::/usr/local/share/julia
  JULIA_NUM_THREADS = 2

  Resolving package versions...
  Installed OpenSpecFun_jll ──── v0.5.3+1
  Installed OrderedCollections ─ v1.1.0
  Installed StatsFuns ────────── v0.9.4
  Installed Rmath ────────────── v0.6.0
  Installed GaussianMixtures ─── v0.3.0
  Installed LegacyStrings ────── v0.4.1
  Installed FillArrays ───────── v0.8.4
  Installed OpenBLAS_jll ─────── v0.3.7+5
  Installed QuadGK ───────────── v2.3.1
  Installed CMakeWrapper ─────── v0.2.3
  Installed JLD ──────────────── v0.9.2
  Installed Missings ─────────── v0.4.3
  Installed DataStructures ───── v0.17.9
  Installed PDMats ───────────── v0.9.11
  Installed Distributions ────── v0.22.4
  Installed SortingAlgorithms ── v0.3.1
  Installed DataAPI ──────────── v1.1.0
  Installed CMake ────────────── v1.2.0
  Installed Parameters ───────── v0.12.0
  Installed HDF5 ─────────────── v0.12.5
  Installed ScikitLearnBase ──── v0.5.0
  Installed Blosc ────────────── v0.5.1
  Installed Distances ────────── v0.8.2
  Installed URIParser ────────── v0.4.0
  Installed Arpack_jll ───────── v3.5.0+2
  Installed Compat ───────────── v2.2.0
  Installed Arpack ───────────── v0.4.0
  Installed BinaryProvider ───── v0.5.8
  Installed StatsBase ────────── v0.32.1
  Installed StaticArrays ─────── v0.12.1
  Installed FileIO ───────────── v1.2.2
  Installed BinDeps ──────────── v1.0.0
  Installed SpecialFunctions ─── v0.10.0
  Installed Clustering ───────── v0.13.3
  Installed NearestNeighbors ─── v0.4.4
#=#=#                                                                         ##O#- #                                                                       ##O=#  #                                                                      #=#=-#  #                                                                     -#O#- #   #                                                                   -=#=#   #   #                                                                 -=O#- #  #    #                                                               -=O=#  #   #   #                                                              -=O=-#  #    #   #                                                            ########################################                                  56.7%#############################################                             63.8%######################################################################## 100.0%
#=#=#                                                                         ##O#- #                                                                       ##O=#  #                                                                      #=#=-#  #                                                                     -#O#- #   #                                                                   -=#=#   #   #                                                                 -=O#- #  #    #                                                               -=O=#  #   #   #                                                              -=O=-#  #    #   #                                                            -=O=- #   #   #     #                                                         -=O=-   #   #   #     #                                                       -=O=-    #    #    #    #                                                     -=O=-      #   #     #    #                                                   -=O=-        #   #     #     #                                                -=O=-         #     #    #     #                                              -=O=-           #     #    #     #                                            -=O=-              #    #     #     #                                                                                                                    0.6%#                                                                          2.2%###                                                                        4.7%#######                                                                   10.3%##############                                                            19.6%###############                                                           21.7%###############                                                           21.7%###################                                                       27.1%####################                                                      27.8%#####################                                                     29.3%#########################                                                 35.7%#############################                                             41.3%####################################                                      50.2%###############################################                           66.1%###########################################################               82.4%####################################################################      95.1%####################################################################      95.2%######################################################################## 100.0%
#=#=#                                                                         ##O#- #                                                                       ##O=#  #                                                                      #=#=-#  #                                                                     -#O#- #   #                                                                   -=#=#   #   #                                                                 -=O#- #  #    #                                                               -=O=#  #   #   #                                                              -=O=-#  #    #   #                                                            -=O=- #   #   #     #                                                         -=O=-   #   #   #     #                                                       -=O=-    #    #    #    #                                                     -=O=-      #   #     #    #                                                   -=O=-        #   #     #     #                                                -=O=-         #     #    #     #                                              #######                                                                   10.8%###############################################################           87.5%######################################################################    98.5%######################################################################## 100.0%
   Updating `~/.julia/environments/v1.5/Project.toml`
  [cc18c42c] + GaussianMixtures v0.3.0
   Updating `~/.julia/environments/v1.5/Manifest.toml`
  [7d9fca2a] + Arpack v0.4.0
  [68821587] + Arpack_jll v3.5.0+2
  [9e28174c] + BinDeps v1.0.0
  [b99e7846] + BinaryProvider v0.5.8
  [a74b3585] + Blosc v0.5.1
  [631607c0] + CMake v1.2.0
  [d5fb7624] + CMakeWrapper v0.2.3
  [aaaa29a8] + Clustering v0.13.3
  [34da2185] + Compat v2.2.0
  [9a962f9c] + DataAPI v1.1.0
  [864edb3b] + DataStructures v0.17.9
  [b4f34e82] + Distances v0.8.2
  [31c24e10] + Distributions v0.22.4
  [5789e2e9] + FileIO v1.2.2
  [1a297f60] + FillArrays v0.8.4
  [cc18c42c] + GaussianMixtures v0.3.0
  [f67ccb44] + HDF5 v0.12.5
  [4138dd39] + JLD v0.9.2
  [1b4a561d] + LegacyStrings v0.4.1
  [e1d29d7a] + Missings v0.4.3
  [b8a86587] + NearestNeighbors v0.4.4
  [4536629a] + OpenBLAS_jll v0.3.7+5
  [efe28fd5] + OpenSpecFun_jll v0.5.3+1
  [bac558e1] + OrderedCollections v1.1.0
  [90014a1f] + PDMats v0.9.11
  [d96e819e] + Parameters v0.12.0
  [1fd47b50] + QuadGK v2.3.1
  [79098fc4] + Rmath v0.6.0
  [6e75b9c4] + ScikitLearnBase v0.5.0
  [a2af1166] + SortingAlgorithms v0.3.1
  [276daf66] + SpecialFunctions v0.10.0
  [90137ffa] + StaticArrays v0.12.1
  [2913bbd2] + StatsBase v0.32.1
  [4c63d2b9] + StatsFuns v0.9.4
  [30578b45] + URIParser v0.4.0
  [2a0f44e3] + Base64 
  [ade2ca70] + Dates 
  [8bb1440f] + DelimitedFiles 
  [8ba89e20] + Distributed 
  [b77e0a4c] + InteractiveUtils 
  [76f85450] + LibGit2 
  [8f399da3] + Libdl 
  [37e2e46d] + LinearAlgebra 
  [56ddb016] + Logging 
  [d6f4376e] + Markdown 
  [a63ad114] + Mmap 
  [44cfe95a] + Pkg 
  [de0858da] + Printf 
  [3fa0cd96] + REPL 
  [9a3f8284] + Random 
  [ea8e919c] + SHA 
  [9e88b42a] + Serialization 
  [1a1011a3] + SharedArrays 
  [6462fe0b] + Sockets 
  [2f01184e] + SparseArrays 
  [10745b16] + Statistics 
  [4607b0f0] + SuiteSparse 
  [8dfed614] + Test 
  [cf7118a7] + UUIDs 
  [4ec0a83e] + Unicode 
   Building Rmath → `~/.julia/packages/Rmath/BoBag/deps/build.log`
   Building CMake → `~/.julia/packages/CMake/ULbyn/deps/build.log`
   Building Blosc → `~/.julia/packages/Blosc/lzFr0/deps/build.log`
   Building HDF5 ─→ `~/.julia/packages/HDF5/Zh9on/deps/build.log`
    Testing GaussianMixtures
Status `/tmp/jl_IzNrgO/Manifest.toml`
  [7d9fca2a] Arpack v0.4.0
  [68821587] Arpack_jll v3.5.0+2
  [9e28174c] BinDeps v1.0.0
  [b99e7846] BinaryProvider v0.5.8
  [a74b3585] Blosc v0.5.1
  [631607c0] CMake v1.2.0
  [d5fb7624] CMakeWrapper v0.2.3
  [aaaa29a8] Clustering v0.13.3
  [34da2185] Compat v2.2.0
  [9a962f9c] DataAPI v1.1.0
  [864edb3b] DataStructures v0.17.9
  [b4f34e82] Distances v0.8.2
  [31c24e10] Distributions v0.22.4
  [5789e2e9] FileIO v1.2.2
  [1a297f60] FillArrays v0.8.4
  [cc18c42c] GaussianMixtures v0.3.0
  [f67ccb44] HDF5 v0.12.5
  [4138dd39] JLD v0.9.2
  [1b4a561d] LegacyStrings v0.4.1
  [e1d29d7a] Missings v0.4.3
  [b8a86587] NearestNeighbors v0.4.4
  [4536629a] OpenBLAS_jll v0.3.7+5
  [efe28fd5] OpenSpecFun_jll v0.5.3+1
  [bac558e1] OrderedCollections v1.1.0
  [90014a1f] PDMats v0.9.11
  [d96e819e] Parameters v0.12.0
  [1fd47b50] QuadGK v2.3.1
  [79098fc4] Rmath v0.6.0
  [6e75b9c4] ScikitLearnBase v0.5.0
  [a2af1166] SortingAlgorithms v0.3.1
  [276daf66] SpecialFunctions v0.10.0
  [90137ffa] StaticArrays v0.12.1
  [2913bbd2] StatsBase v0.32.1
  [4c63d2b9] StatsFuns v0.9.4
  [30578b45] URIParser v0.4.0
  [2a0f44e3] Base64 
  [ade2ca70] Dates 
  [8bb1440f] DelimitedFiles 
  [8ba89e20] Distributed 
  [b77e0a4c] InteractiveUtils 
  [76f85450] LibGit2 
  [8f399da3] Libdl 
  [37e2e46d] LinearAlgebra 
  [56ddb016] Logging 
  [d6f4376e] Markdown 
  [a63ad114] Mmap 
  [44cfe95a] Pkg 
  [de0858da] Printf 
  [3fa0cd96] REPL 
  [9a3f8284] Random 
  [ea8e919c] SHA 
  [9e88b42a] Serialization 
  [1a1011a3] SharedArrays 
  [6462fe0b] Sockets 
  [2f01184e] SparseArrays 
  [10745b16] Statistics 
  [4607b0f0] SuiteSparse 
  [8dfed614] Test 
  [cf7118a7] UUIDs 
  [4ec0a83e] Unicode 
[ Info: Testing Data
(100000, -2.372085133338156e6, [68033.25458065662, 31966.74541934338], [-7302.731936528363 7083.35977536539 4007.3300020552074; 7384.510936856761 -6831.959750301925 -3591.7778178006047], [[79763.61031614765 5258.862453869277 -239.33971817511676; 5258.862453869277 26930.058733040078 7771.919861502004; -239.33971817511664 7771.919861502004 66453.2381651614], [20531.77436064985 -5755.421331574532 95.33513249391218; -5755.421331574532 72371.72425440857 -7527.846668612404; 95.33513249391211 -7527.846668612404 33384.299664367]])
┌ Warning: rmprocs: process 1 not removed
└ @ Distributed /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Distributed/src/cluster.jl:1030
[ Info: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.537782e+03
      1       1.212142e+03      -3.256401e+02 |        7
      2       1.145618e+03      -6.652415e+01 |        5
      3       1.085492e+03      -6.012538e+01 |        4
      4       1.080592e+03      -4.899871e+00 |        2
      5       1.063888e+03      -1.670471e+01 |        0
      6       1.063888e+03       0.000000e+00 |        0
K-means converged with 6 iterations (objv = 1063.887729162458)
┌ Info: K-means with 272 data points using 6 iterations
└ 11.3 data points per parameter
[ Info: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
┌ Info: EM with 272 data points 0 iterations avll -2.069539
└ 5.8 data points per parameter
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:221
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:221
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:221
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:221
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:631 [inlined]
└ @ Core ./broadcast.jl:631
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:230
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:230
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:631 [inlined]
└ @ Core ./broadcast.jl:631
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:631 [inlined]
└ @ Core ./broadcast.jl:631
[ Info: iteration 1, lowerbound -3.679098
[ Info: iteration 2, lowerbound -3.550306
[ Info: iteration 3, lowerbound -3.434019
[ Info: iteration 4, lowerbound -3.321508
[ Info: dropping number of Gaussions to 7
[ Info: iteration 5, lowerbound -3.217613
[ Info: iteration 6, lowerbound -3.135162
[ Info: iteration 7, lowerbound -3.090230
[ Info: dropping number of Gaussions to 5
[ Info: iteration 8, lowerbound -3.051456
[ Info: iteration 9, lowerbound -3.017690
[ Info: dropping number of Gaussions to 4
[ Info: iteration 10, lowerbound -2.977333
[ Info: iteration 11, lowerbound -2.921105
[ Info: iteration 12, lowerbound -2.852756
[ Info: iteration 13, lowerbound -2.774931
[ Info: iteration 14, lowerbound -2.697085
[ Info: iteration 15, lowerbound -2.623520
[ Info: iteration 16, lowerbound -2.551562
[ Info: iteration 17, lowerbound -2.480357
[ Info: iteration 18, lowerbound -2.417796
[ Info: iteration 19, lowerbound -2.374133
[ Info: dropping number of Gaussions to 3
[ Info: iteration 20, lowerbound -2.335844
[ Info: iteration 21, lowerbound -2.312393
[ Info: iteration 22, lowerbound -2.307741
[ Info: dropping number of Gaussions to 2
[ Info: iteration 23, lowerbound -2.302920
[ Info: iteration 24, lowerbound -2.299260
[ Info: iteration 25, lowerbound -2.299256
[ Info: iteration 26, lowerbound -2.299254
[ Info: iteration 27, lowerbound -2.299254
[ Info: iteration 28, lowerbound -2.299253
[ Info: iteration 29, lowerbound -2.299253
[ Info: iteration 30, lowerbound -2.299253
[ Info: iteration 31, lowerbound -2.299253
[ Info: iteration 32, lowerbound -2.299253
[ Info: iteration 33, lowerbound -2.299253
[ Info: iteration 34, lowerbound -2.299253
[ Info: iteration 35, lowerbound -2.299253
[ Info: iteration 36, lowerbound -2.299253
[ Info: iteration 37, lowerbound -2.299253
[ Info: iteration 38, lowerbound -2.299253
[ Info: iteration 39, lowerbound -2.299253
[ Info: iteration 40, lowerbound -2.299253
[ Info: iteration 41, lowerbound -2.299253
[ Info: iteration 42, lowerbound -2.299253
[ Info: iteration 43, lowerbound -2.299253
[ Info: iteration 44, lowerbound -2.299253
[ Info: iteration 45, lowerbound -2.299253
[ Info: iteration 46, lowerbound -2.299253
[ Info: iteration 47, lowerbound -2.299253
[ Info: iteration 48, lowerbound -2.299253
[ Info: iteration 49, lowerbound -2.299253
[ Info: iteration 50, lowerbound -2.299253
[ Info: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
History[Mon Feb 17 23:54:13 2020: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
, Mon Feb 17 23:54:18 2020: K-means with 272 data points using 6 iterations
11.3 data points per parameter
, Mon Feb 17 23:54:19 2020: EM with 272 data points 0 iterations avll -2.069539
5.8 data points per parameter
, Mon Feb 17 23:54:21 2020: GMM converted to Variational GMM
, Mon Feb 17 23:54:26 2020: iteration 1, lowerbound -3.679098
, Mon Feb 17 23:54:26 2020: iteration 2, lowerbound -3.550306
, Mon Feb 17 23:54:26 2020: iteration 3, lowerbound -3.434019
, Mon Feb 17 23:54:26 2020: iteration 4, lowerbound -3.321508
, Mon Feb 17 23:54:26 2020: dropping number of Gaussions to 7
, Mon Feb 17 23:54:26 2020: iteration 5, lowerbound -3.217613
, Mon Feb 17 23:54:26 2020: iteration 6, lowerbound -3.135162
, Mon Feb 17 23:54:26 2020: iteration 7, lowerbound -3.090230
, Mon Feb 17 23:54:26 2020: dropping number of Gaussions to 5
, Mon Feb 17 23:54:26 2020: iteration 8, lowerbound -3.051456
, Mon Feb 17 23:54:26 2020: iteration 9, lowerbound -3.017690
, Mon Feb 17 23:54:26 2020: dropping number of Gaussions to 4
, Mon Feb 17 23:54:26 2020: iteration 10, lowerbound -2.977333
, Mon Feb 17 23:54:26 2020: iteration 11, lowerbound -2.921105
, Mon Feb 17 23:54:26 2020: iteration 12, lowerbound -2.852756
, Mon Feb 17 23:54:26 2020: iteration 13, lowerbound -2.774931
, Mon Feb 17 23:54:26 2020: iteration 14, lowerbound -2.697085
, Mon Feb 17 23:54:26 2020: iteration 15, lowerbound -2.623520
, Mon Feb 17 23:54:26 2020: iteration 16, lowerbound -2.551562
, Mon Feb 17 23:54:26 2020: iteration 17, lowerbound -2.480357
, Mon Feb 17 23:54:26 2020: iteration 18, lowerbound -2.417796
, Mon Feb 17 23:54:26 2020: iteration 19, lowerbound -2.374133
, Mon Feb 17 23:54:26 2020: dropping number of Gaussions to 3
, Mon Feb 17 23:54:26 2020: iteration 20, lowerbound -2.335844
, Mon Feb 17 23:54:26 2020: iteration 21, lowerbound -2.312393
, Mon Feb 17 23:54:26 2020: iteration 22, lowerbound -2.307741
, Mon Feb 17 23:54:26 2020: dropping number of Gaussions to 2
, Mon Feb 17 23:54:26 2020: iteration 23, lowerbound -2.302920
, Mon Feb 17 23:54:26 2020: iteration 24, lowerbound -2.299260
, Mon Feb 17 23:54:26 2020: iteration 25, lowerbound -2.299256
, Mon Feb 17 23:54:26 2020: iteration 26, lowerbound -2.299254
, Mon Feb 17 23:54:26 2020: iteration 27, lowerbound -2.299254
, Mon Feb 17 23:54:26 2020: iteration 28, lowerbound -2.299253
, Mon Feb 17 23:54:26 2020: iteration 29, lowerbound -2.299253
, Mon Feb 17 23:54:26 2020: iteration 30, lowerbound -2.299253
, Mon Feb 17 23:54:26 2020: iteration 31, lowerbound -2.299253
, Mon Feb 17 23:54:26 2020: iteration 32, lowerbound -2.299253
, Mon Feb 17 23:54:26 2020: iteration 33, lowerbound -2.299253
, Mon Feb 17 23:54:26 2020: iteration 34, lowerbound -2.299253
, Mon Feb 17 23:54:26 2020: iteration 35, lowerbound -2.299253
, Mon Feb 17 23:54:26 2020: iteration 36, lowerbound -2.299253
, Mon Feb 17 23:54:26 2020: iteration 37, lowerbound -2.299253
, Mon Feb 17 23:54:26 2020: iteration 38, lowerbound -2.299253
, Mon Feb 17 23:54:26 2020: iteration 39, lowerbound -2.299253
, Mon Feb 17 23:54:26 2020: iteration 40, lowerbound -2.299253
, Mon Feb 17 23:54:26 2020: iteration 41, lowerbound -2.299253
, Mon Feb 17 23:54:26 2020: iteration 42, lowerbound -2.299253
, Mon Feb 17 23:54:26 2020: iteration 43, lowerbound -2.299253
, Mon Feb 17 23:54:26 2020: iteration 44, lowerbound -2.299253
, Mon Feb 17 23:54:26 2020: iteration 45, lowerbound -2.299253
, Mon Feb 17 23:54:26 2020: iteration 46, lowerbound -2.299253
, Mon Feb 17 23:54:26 2020: iteration 47, lowerbound -2.299253
, Mon Feb 17 23:54:26 2020: iteration 48, lowerbound -2.299253
, Mon Feb 17 23:54:26 2020: iteration 49, lowerbound -2.299253
, Mon Feb 17 23:54:26 2020: iteration 50, lowerbound -2.299253
, Mon Feb 17 23:54:26 2020: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [95.95490777397984, 178.04509222602005]
β = [95.95490777397984, 178.04509222602005]
m = [2.000229257775318 53.85198717246103; 4.25030073326986 79.28686694436115]
ν = [97.95490777397984, 180.04509222602005]
W = LinearAlgebra.UpperTriangular{Float64,Array{Float64,2}}[[0.37587636119492945 -0.008953123827346981; 0.0 0.012748664777409676], [0.1840415554748349 -0.0076440490423279025; 0.0 0.008581705166332857]]
┌ Warning: Assignment to `x` in soft scope is ambiguous because a global variable by the same name exists: `x` will be treated as a new local. Disambiguate by using `local x` to suppress this warning or `global x` to assign to the existing global variable.
└ @ ~/.julia/packages/GaussianMixtures/RGtTJ/test/train.jl:7
┌ Warning: Assignment to `p` in soft scope is ambiguous because a global variable by the same name exists: `p` will be treated as a new local. Disambiguate by using `local p` to suppress this warning or `global p` to assign to the existing global variable.
└ @ ~/.julia/packages/GaussianMixtures/RGtTJ/test/train.jl:17
Kind: diag, size256
nx: 100000 sum(zeroth order stats): 100000.00000000007
avll from stats: -0.990675886429602
avll from llpg:  -0.9906758864296021
avll direct:     -0.9906758864296022
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 100000.0
avll from stats: -0.9897980765617957
avll from llpg:  -0.9897980765617957
avll direct:     -0.9897980765617957
sum posterior: 100000.0
┌ Warning: Assignment to `x` in soft scope is ambiguous because a global variable by the same name exists: `x` will be treated as a new local. Disambiguate by using `local x` to suppress this warning or `global x` to assign to the existing global variable.
└ @ ~/.julia/packages/GaussianMixtures/RGtTJ/test/train.jl:26
32×26 Array{Float64,2}:
  0.0466618    0.014304     -0.177644     0.274886     -0.0509157   -0.0821829    0.0372664  -0.024146    0.190876     0.129193    -0.094703    0.118855    -0.0428193  -0.0842238    -0.012877    -0.0522535   0.0658413    0.0342453    -0.277623     0.137909     0.0239835    0.105498     0.0116261    0.0522232   -0.0615631    0.0999011
 -0.00467199   0.251323      0.10757      0.00305031   -0.0685395    0.1291      -0.16362    -0.140375    0.0591236    0.0820252   -0.133759   -0.00423353  -0.120742   -0.0994656    -0.075031    -0.156357   -0.155723    -0.0326662     0.0511648    0.0966127   -0.0729233   -0.0697495   -0.0884303    0.0881406    0.0621249   -0.0490139
 -0.0798004    0.127774     -0.0955923   -0.0935455     0.126296     0.069613    -0.088138   -0.0285093   0.0406416   -0.0137596    0.043901    0.0869717    0.0117224  -0.109513      0.122442     0.0496169   0.0440782    0.0894859    -0.043025     0.111094    -0.0201624    0.0840612   -0.164337     0.0340314   -0.0552969   -0.0720706
 -0.0367565    0.137748      0.0365742    0.123281     -0.051087    -0.127876     0.0527927   0.0505952  -0.0259161   -0.102736    -0.0516663  -0.0472091   -0.0143808   0.140978      0.0793966    0.0437883   0.0129175   -0.0392355     0.00775824  -0.0206628    0.0248995   -0.0116848   -0.104561     0.0783253    0.0322616   -0.0961477
  0.114119     0.0817567    -0.0763252    0.139555     -0.122307     0.0295372   -0.0216159  -0.210001   -0.0878992    0.135942     0.0127343   0.0742672    0.0771215   0.142816     -0.0538813   -0.0457898   0.173073    -0.129446     -0.0318682    0.144327     0.0734615   -0.0987555   -0.0353422    0.0697905   -0.00248225  -0.0478093
  0.0852382    0.0323105     0.136018    -0.175209      0.115857    -0.0557091   -0.188252    0.11112    -0.0288551    0.08867      0.051949   -0.0531083    0.123614   -0.018922      0.223196    -0.165316   -0.0204382    0.0063883    -0.0234086   -0.0108888    0.0955657    0.109341    -0.0355766   -0.0200672   -0.157407     0.0793787
 -0.0394241    0.00196593    0.226835    -0.034612     -0.05311      0.0224476   -0.0957481  -0.0173496  -0.0921965   -0.0423926   -0.194731   -0.0505017   -0.0276144  -0.0267988    -0.0686878   -0.101894   -0.129243    -0.0375566    -0.0331141   -0.0208982    0.037831     0.121345     0.0339746   -0.0703047   -0.137501     0.187387
 -0.115719    -0.0525424    -0.126336    -0.0783196    -0.0813973   -0.0142563    0.0596905   0.0508184  -0.116761    -0.134999     0.113432   -0.0922428    0.0923686  -0.119546     -0.0751833    0.185824   -0.0387476   -0.0849872    -0.0593244    0.0495643    0.074596    -0.104575     0.0264969    0.0397263    0.123872     0.0873964
 -0.0373248    0.1736       -0.065832     0.0735974    -9.29081e-5   0.0850754    0.0307266  -0.114054   -0.0374371    0.00765015  -0.166129    0.150961    -0.0691383   0.0314345     0.00795223  -0.126082   -0.154042    -0.00552837   -0.0426431   -0.0180411   -0.0698549    0.0392987    0.107037     0.16049      0.0118891   -0.132247
 -0.0320756   -0.027188     -0.0706732   -0.0552461     0.0874554   -0.101493     0.0881087  -0.138076   -0.00105762  -0.138468     0.114776   -0.0840058   -0.032149   -0.0493977     0.0244955   -0.0156052   0.061493     0.0530697    -0.0894815   -0.0436689   -0.113333    -0.0348868   -0.0962409   -0.0537558   -0.00999224  -0.0564858
  0.120766    -0.16878      -0.0885365    0.0422602     0.13536      0.0216881    0.0721828   0.0709587  -0.0287516   -0.0793749   -0.0265758  -0.0947398   -0.123246   -0.0880305    -0.143938    -0.0953965  -0.0288125   -0.00276572   -0.156081    -0.0667195    0.00821032   0.0733311   -0.0538618    0.0630135    0.00207776  -0.109397
 -0.0957748    0.0257394     0.0144735    0.00922195    0.00900993  -0.0279089   -0.260336   -0.112112    0.0344175    0.0914411   -0.0805621   0.0545309   -0.0642623  -0.0631969    -0.039719    -0.113791   -0.166569    -0.13037       0.101722    -0.0926198   -0.178683    -0.00392656  -0.0707041   -0.0496783   -0.00695562  -0.0614149
  0.00337289  -0.0352078     0.173487     0.0296233    -0.0623323   -0.0415513    0.0483925  -0.0118133   0.0219138    0.107991     0.0724056  -0.151239    -0.147848    0.182384      0.0210191    0.0590077  -0.103972    -0.0097149    -0.100539    -0.0238006   -0.233155    -0.041571     0.0221277   -0.0965779    0.00256195   0.214991
  0.102169     0.0728629    -0.0404765   -0.0842542     0.205559    -0.0426324   -0.0115226   0.0957531   0.127691    -0.0300825    0.0493737  -0.04087      0.156957   -0.0398015     0.0435946    0.118762    0.141207    -0.00991567   -0.0011601    0.00147192  -0.0768982    0.12142      0.0530615   -0.138548     0.0429083   -0.023891
  0.196269     0.0157796     0.0897266   -0.118072      0.161362    -0.00374179   0.0676374  -0.0780217  -0.00838504  -0.123637    -0.0205343   0.0382658    0.178617   -0.0347692    -0.0438502   -0.284745   -0.0567018   -0.0318928     0.0284589   -0.00703263  -0.0629029   -0.0900544   -0.0646808    0.0769835    0.0235331    0.130901
  0.00700547   0.0498798    -0.111773    -0.086569      0.270814    -0.0419055    0.0156336   0.122447   -0.152015    -0.026009     0.0134117  -0.0299711    0.0688977   0.0359813     0.0889606   -0.0545458   0.107736     0.119193      0.00674644   0.215061    -0.0718038    0.102475    -0.0160962   -0.115495     0.0241878   -0.0168915
  0.0634996   -0.139954      0.0624546    0.043641     -0.146333    -0.0357031    0.0820006  -0.0753465   0.0750615    0.125664     0.134738   -0.0536068    0.100592   -0.0902365    -0.0143818    0.0967194  -0.0718667   -0.171073     -0.0259136   -0.0723018    0.0774334    0.0703266    0.0466123    0.056276    -0.138232     0.00260377
 -0.122494    -0.225182     -0.189334    -0.115555     -0.0335615    0.0511149   -0.0401366  -0.0145164  -0.00474958  -0.0805732    0.047157   -0.0376097   -0.112045    0.0781623    -0.0104258   -0.0263653  -0.147395    -0.145863     -0.00487515  -0.0051832    0.0192959   -0.0237156    0.0883528    0.0867756   -0.181689     0.134786
 -0.0716921   -0.115915      0.0466738   -0.12354      -0.0518549   -0.2287       0.114481   -0.104181    0.0621048    0.0917668    0.0386813   0.0326509    0.0197073   0.0233352    -0.0440877   -0.19563     0.165644    -0.130462     -0.0696677   -0.0974413    0.00507223   0.201841     0.133833    -0.193411     0.101444    -0.187365
  0.157598    -0.0531941    -0.11985      0.159333     -0.0545583   -0.0369763   -0.0122867  -0.0994116   0.0403908   -0.0622496    0.0959798   0.00095346  -0.0164433  -0.0305373     0.0536491    0.0474266   0.0941789   -0.105668     -0.0206126   -0.0541635   -0.095476    -0.16641      0.0939449    0.144207     0.0626806    0.104483
 -0.00568931   0.0946847     0.00658872   0.115237     -0.0161169   -0.124553     0.0747613   0.20356    -0.0653665    0.0393218    0.0687227  -0.0807764    0.0850265  -0.00856322   -0.0116089    0.0543251  -0.0427228    0.083841      0.0421994   -0.233334    -0.115668    -0.0311785    0.0139016   -0.125393     0.119642    -0.125903
 -0.0122665   -0.279202      0.187172    -0.0434866    -0.0295909   -0.00665648  -0.156795    0.0640134  -0.0138695    0.0242306   -0.0354491   0.031756    -0.0865576  -0.005096     -0.00171537   0.0103055  -0.00833466  -0.100809      0.011784     0.0295362   -0.191835     0.0570633    0.161183     0.00700692   0.0495818   -0.0478906
 -0.166553     0.0087389    -0.116722    -0.188473      0.20656      0.0938038   -0.136289   -0.045378    0.002769    -0.0607829   -0.0663501  -0.10813     -0.0302945   0.101748      0.0341819    0.152369   -0.158298     0.177872      0.010566    -0.0337858   -0.113584    -0.136854     0.0276875   -0.0507183   -0.0357922    0.17694
 -0.0556335   -0.0728037    -0.0168641   -0.113495     -0.108558    -0.00122087  -0.0377927   0.0703682  -0.00976313  -0.00455223   0.0935104  -0.108591     0.0373153   0.129665     -0.0719889   -0.0223623  -0.029592     0.102548      0.101961    -0.114099    -0.039097     0.0658424    0.0464016    0.151835     0.0615789   -0.0414783
  0.0891012   -0.000433695  -0.0324042   -0.0829569     0.0256558    0.0439199    0.103915   -0.0661422   9.74555e-5   0.10339     -0.0189742   0.0356102   -0.0138256  -0.0682349     0.130835    -0.0730686  -0.00278172  -0.000253278  -0.0930533    0.0293539    0.0801801   -0.138062     0.140362     0.149351     0.0561902    0.0120885
 -0.141729     0.0253351    -0.0355842   -0.117419      0.0212265   -0.0796753    0.0135362   0.122715   -0.193414    -0.011023    -0.149467   -0.103009    -0.188789   -0.105249      0.0518662   -0.0181964   0.00241204   0.0911688     0.0916771    0.0221003    0.149296     0.122768     0.0682104   -0.14735      0.0621161   -0.0684063
  0.0644398   -0.044245     -0.00962418  -0.130009     -0.00932204   0.116206     0.0610225   0.0357251  -0.091862     0.0570289    0.0796201   0.10799      0.0710108   0.135652     -0.133306    -0.104454   -0.185626    -0.0308956    -0.168175     0.0135585    0.227951     0.0605982    0.0109994    0.00698219   0.0414011    0.0299732
 -0.108725    -0.0484537     0.0870634   -0.0702055     0.00940121   0.132246     0.112026   -0.111618    0.0120592    0.00065415   0.0731607  -0.0595431    0.0446073  -0.124755     -0.0538559   -0.0756074   0.0272548    0.12478       0.00759608  -0.0458568   -0.184776    -0.0494082   -0.013517    -0.0294959    0.0414277   -0.0457219
  0.105226    -0.0829826     0.135145    -0.000410079  -0.0715851    0.0584655    0.0536251   0.0300665   0.0296212    0.0029063    0.0213106  -0.0713906    0.0948024  -0.237749     -0.0141165    0.0312618  -0.0193896    0.149044      0.0340154   -0.123342    -0.0540001    0.153248    -0.102512    -0.112356    -0.127085     0.175968
  0.085661     0.0645478     0.0207318    0.0362718     0.134789     0.132127     0.184145   -0.0291751  -0.0512816   -0.130369    -0.14417    -0.0867136   -0.15133     0.000611758  -0.124507    -0.0939374   0.107399    -0.132477     -0.0867437   -0.0907228    0.0607407    0.0703708    0.0510404   -0.169658     0.0202858    0.118165
  0.0219843   -0.0593228     0.017512     0.0766938     0.0539154    0.22252      0.0428731  -0.164561    0.0148696   -0.0246712   -0.059385    0.0462342   -0.0979252   0.0837806     0.129003     0.0116463  -0.115645    -0.0949579    -0.129031    -0.0175408   -0.0433965   -0.117302     0.00115647  -0.0117406    0.038072    -0.050433
  0.112928    -0.0717866     0.285835     0.095335     -0.0739433    0.200562     0.256786    0.182066   -0.11029     -0.11565     -0.115984   -0.0670528    0.0173478  -0.0583898    -0.0980455    0.0676353   0.084439    -0.0288469    -0.0275166   -0.0229116    0.176394    -0.00995681  -0.2093      -0.180462    -0.037512     0.182952kind diag, method split
┌ Info: 0: avll = 
└   tll[1] = -1.4386644140428544
[ Info: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.438738
[ Info: iteration 2, average log likelihood -1.438637
[ Info: iteration 3, average log likelihood -1.437320
[ Info: iteration 4, average log likelihood -1.425828
[ Info: iteration 5, average log likelihood -1.409301
[ Info: iteration 6, average log likelihood -1.405522
[ Info: iteration 7, average log likelihood -1.404807
[ Info: iteration 8, average log likelihood -1.404504
[ Info: iteration 9, average log likelihood -1.404338
[ Info: iteration 10, average log likelihood -1.404224
[ Info: iteration 11, average log likelihood -1.404125
[ Info: iteration 12, average log likelihood -1.404026
[ Info: iteration 13, average log likelihood -1.403922
[ Info: iteration 14, average log likelihood -1.403807
[ Info: iteration 15, average log likelihood -1.403668
[ Info: iteration 16, average log likelihood -1.403482
[ Info: iteration 17, average log likelihood -1.403243
[ Info: iteration 18, average log likelihood -1.402981
[ Info: iteration 19, average log likelihood -1.402645
[ Info: iteration 20, average log likelihood -1.401660
[ Info: iteration 21, average log likelihood -1.399939
[ Info: iteration 22, average log likelihood -1.399439
[ Info: iteration 23, average log likelihood -1.399228
[ Info: iteration 24, average log likelihood -1.399101
[ Info: iteration 25, average log likelihood -1.399017
[ Info: iteration 26, average log likelihood -1.398959
[ Info: iteration 27, average log likelihood -1.398917
[ Info: iteration 28, average log likelihood -1.398887
[ Info: iteration 29, average log likelihood -1.398865
[ Info: iteration 30, average log likelihood -1.398848
[ Info: iteration 31, average log likelihood -1.398836
[ Info: iteration 32, average log likelihood -1.398827
[ Info: iteration 33, average log likelihood -1.398820
[ Info: iteration 34, average log likelihood -1.398814
[ Info: iteration 35, average log likelihood -1.398809
[ Info: iteration 36, average log likelihood -1.398806
[ Info: iteration 37, average log likelihood -1.398803
[ Info: iteration 38, average log likelihood -1.398800
[ Info: iteration 39, average log likelihood -1.398798
[ Info: iteration 40, average log likelihood -1.398796
[ Info: iteration 41, average log likelihood -1.398795
[ Info: iteration 42, average log likelihood -1.398794
[ Info: iteration 43, average log likelihood -1.398793
[ Info: iteration 44, average log likelihood -1.398792
[ Info: iteration 45, average log likelihood -1.398791
[ Info: iteration 46, average log likelihood -1.398791
[ Info: iteration 47, average log likelihood -1.398790
[ Info: iteration 48, average log likelihood -1.398790
[ Info: iteration 49, average log likelihood -1.398789
[ Info: iteration 50, average log likelihood -1.398789
┌ Info: EM with 100000 data points 50 iterations avll -1.398789
└ 952.4 data points per parameter
┌ Info: 1
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4387380208225444
│     -1.4386366875644991
│      ⋮
└     -1.3987889798408886
[ Info: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.398905
[ Info: iteration 2, average log likelihood -1.398783
[ Info: iteration 3, average log likelihood -1.398210
[ Info: iteration 4, average log likelihood -1.392334
[ Info: iteration 5, average log likelihood -1.374076
[ Info: iteration 6, average log likelihood -1.363504
[ Info: iteration 7, average log likelihood -1.360463
[ Info: iteration 8, average log likelihood -1.358942
[ Info: iteration 9, average log likelihood -1.357966
[ Info: iteration 10, average log likelihood -1.357290
[ Info: iteration 11, average log likelihood -1.356857
[ Info: iteration 12, average log likelihood -1.356591
[ Info: iteration 13, average log likelihood -1.356425
[ Info: iteration 14, average log likelihood -1.356314
[ Info: iteration 15, average log likelihood -1.356236
[ Info: iteration 16, average log likelihood -1.356179
[ Info: iteration 17, average log likelihood -1.356136
[ Info: iteration 18, average log likelihood -1.356103
[ Info: iteration 19, average log likelihood -1.356075
[ Info: iteration 20, average log likelihood -1.356051
[ Info: iteration 21, average log likelihood -1.356029
[ Info: iteration 22, average log likelihood -1.356010
[ Info: iteration 23, average log likelihood -1.355993
[ Info: iteration 24, average log likelihood -1.355976
[ Info: iteration 25, average log likelihood -1.355960
[ Info: iteration 26, average log likelihood -1.355944
[ Info: iteration 27, average log likelihood -1.355929
[ Info: iteration 28, average log likelihood -1.355914
[ Info: iteration 29, average log likelihood -1.355899
[ Info: iteration 30, average log likelihood -1.355884
[ Info: iteration 31, average log likelihood -1.355867
[ Info: iteration 32, average log likelihood -1.355849
[ Info: iteration 33, average log likelihood -1.355829
[ Info: iteration 34, average log likelihood -1.355806
[ Info: iteration 35, average log likelihood -1.355778
[ Info: iteration 36, average log likelihood -1.355743
[ Info: iteration 37, average log likelihood -1.355697
[ Info: iteration 38, average log likelihood -1.355635
[ Info: iteration 39, average log likelihood -1.355552
[ Info: iteration 40, average log likelihood -1.355439
[ Info: iteration 41, average log likelihood -1.355291
[ Info: iteration 42, average log likelihood -1.355092
[ Info: iteration 43, average log likelihood -1.354834
[ Info: iteration 44, average log likelihood -1.354529
[ Info: iteration 45, average log likelihood -1.354197
[ Info: iteration 46, average log likelihood -1.353876
[ Info: iteration 47, average log likelihood -1.353591
[ Info: iteration 48, average log likelihood -1.353348
[ Info: iteration 49, average log likelihood -1.353157
[ Info: iteration 50, average log likelihood -1.353026
┌ Info: EM with 100000 data points 50 iterations avll -1.353026
└ 473.9 data points per parameter
┌ Info: 2
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.3989053709079329
│     -1.398782726394514
│      ⋮
└     -1.3530260380387245
[ Info: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.353140
[ Info: iteration 2, average log likelihood -1.352890
[ Info: iteration 3, average log likelihood -1.352188
[ Info: iteration 4, average log likelihood -1.346530
[ Info: iteration 5, average log likelihood -1.330423
[ Info: iteration 6, average log likelihood -1.315423
[ Info: iteration 7, average log likelihood -1.309253
[ Info: iteration 8, average log likelihood -1.306578
[ Info: iteration 9, average log likelihood -1.305034
[ Info: iteration 10, average log likelihood -1.304075
[ Info: iteration 11, average log likelihood -1.303432
[ Info: iteration 12, average log likelihood -1.303027
[ Info: iteration 13, average log likelihood -1.302767
[ Info: iteration 14, average log likelihood -1.302569
[ Info: iteration 15, average log likelihood -1.302386
[ Info: iteration 16, average log likelihood -1.302184
[ Info: iteration 17, average log likelihood -1.301918
[ Info: iteration 18, average log likelihood -1.301559
[ Info: iteration 19, average log likelihood -1.301143
[ Info: iteration 20, average log likelihood -1.300797
[ Info: iteration 21, average log likelihood -1.300561
[ Info: iteration 22, average log likelihood -1.300395
[ Info: iteration 23, average log likelihood -1.300269
[ Info: iteration 24, average log likelihood -1.300170
[ Info: iteration 25, average log likelihood -1.300089
[ Info: iteration 26, average log likelihood -1.300027
[ Info: iteration 27, average log likelihood -1.299977
[ Info: iteration 28, average log likelihood -1.299936
[ Info: iteration 29, average log likelihood -1.299900
[ Info: iteration 30, average log likelihood -1.299865
[ Info: iteration 31, average log likelihood -1.299826
[ Info: iteration 32, average log likelihood -1.299775
[ Info: iteration 33, average log likelihood -1.299701
[ Info: iteration 34, average log likelihood -1.299586
[ Info: iteration 35, average log likelihood -1.299401
[ Info: iteration 36, average log likelihood -1.299112
[ Info: iteration 37, average log likelihood -1.298694
[ Info: iteration 38, average log likelihood -1.298162
[ Info: iteration 39, average log likelihood -1.297536
[ Info: iteration 40, average log likelihood -1.296765
[ Info: iteration 41, average log likelihood -1.295727
[ Info: iteration 42, average log likelihood -1.294507
[ Info: iteration 43, average log likelihood -1.293472
[ Info: iteration 44, average log likelihood -1.292740
[ Info: iteration 45, average log likelihood -1.292154
[ Info: iteration 46, average log likelihood -1.291642
[ Info: iteration 47, average log likelihood -1.291183
[ Info: iteration 48, average log likelihood -1.290773
[ Info: iteration 49, average log likelihood -1.290415
[ Info: iteration 50, average log likelihood -1.290115
┌ Info: EM with 100000 data points 50 iterations avll -1.290115
└ 236.4 data points per parameter
┌ Info: 3
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.3531398523962248
│     -1.35289010051429
│      ⋮
└     -1.290115317525449
[ Info: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.290127
[ Info: iteration 2, average log likelihood -1.289676
[ Info: iteration 3, average log likelihood -1.288531
[ Info: iteration 4, average log likelihood -1.276098
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     7
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.237070
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     1
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.226299
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.219635
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     3
│     4
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.204261
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     1
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.218011
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.220250
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 11, average log likelihood -1.204601
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     1
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 12, average log likelihood -1.191666
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     3
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 13, average log likelihood -1.200998
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     4
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 14, average log likelihood -1.202641
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     1
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 15, average log likelihood -1.202689
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 16, average log likelihood -1.204751
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     3
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 17, average log likelihood -1.195731
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     1
│     4
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 18, average log likelihood -1.198645
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 19, average log likelihood -1.213491
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 20, average log likelihood -1.197722
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     1
│     3
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 21, average log likelihood -1.190617
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     4
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 22, average log likelihood -1.209549
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 23, average log likelihood -1.206238
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     1
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 24, average log likelihood -1.192395
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     3
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 25, average log likelihood -1.201482
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     4
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 26, average log likelihood -1.202225
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     1
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 27, average log likelihood -1.201234
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 28, average log likelihood -1.203288
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     3
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 29, average log likelihood -1.194270
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     1
│     4
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 30, average log likelihood -1.197414
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 31, average log likelihood -1.212408
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 32, average log likelihood -1.196333
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     1
│     3
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 33, average log likelihood -1.189309
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     4
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 34, average log likelihood -1.208647
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 35, average log likelihood -1.205836
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     1
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 36, average log likelihood -1.191439
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     3
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 37, average log likelihood -1.200430
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     4
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 38, average log likelihood -1.201693
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     1
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 39, average log likelihood -1.200990
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 40, average log likelihood -1.202386
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     3
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 41, average log likelihood -1.193421
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     1
│     4
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 42, average log likelihood -1.196966
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 43, average log likelihood -1.212030
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 44, average log likelihood -1.195515
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     1
│     3
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 45, average log likelihood -1.188539
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     4
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 46, average log likelihood -1.208002
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 47, average log likelihood -1.205228
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     1
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 48, average log likelihood -1.190685
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     3
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 49, average log likelihood -1.199808
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     4
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 50, average log likelihood -1.200968
┌ Info: EM with 100000 data points 50 iterations avll -1.200968
└ 118.1 data points per parameter
┌ Info: 4
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.2901270702579908
│     -1.2896755539654783
│      ⋮
└     -1.200967566833777
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      1
│      2
│     13
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 1, average log likelihood -1.200458
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      1
│      2
│     13
│     14
│     15
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 2, average log likelihood -1.185380
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│     13
│     14
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 3, average log likelihood -1.185888
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      1
│      2
│      8
│     13
│     14
│     15
│     16
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.158089
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      1
│      2
│      7
│     13
│     14
│     26
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.126432
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     16
│     20
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.104131
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      1
│      2
│      7
│     13
│      ⋮
│     26
│     29
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.099149
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     14
│     15
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.102833
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      1
│      2
│      7
│     13
│     14
│     20
│     26
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.098151
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      1
│      2
│     13
│     14
│      ⋮
│     18
│     29
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.097900
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     13
│     14
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 11, average log likelihood -1.094145
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      1
│      2
│     13
│     14
│      ⋮
│     18
│     20
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 12, average log likelihood -1.102911
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      1
│      2
│     13
│     14
│     26
│     29
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 13, average log likelihood -1.111139
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     14
│     15
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 14, average log likelihood -1.092966
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      1
│      2
│      8
│     13
│      ⋮
│     20
│     26
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 15, average log likelihood -1.085028
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     15
│     16
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 16, average log likelihood -1.108673
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      1
│      2
│     13
│     14
│     26
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 17, average log likelihood -1.103722
┌ Warning: Variances had to be floored 
│   ind =
│    14-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     20
│     29
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 18, average log likelihood -1.068438
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      1
│      2
│     13
│     14
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 19, average log likelihood -1.130763
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      1
│      2
│     13
│     14
│     15
│     16
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 20, average log likelihood -1.101191
┌ Warning: Variances had to be floored 
│   ind =
│    12-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     26
│     29
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 21, average log likelihood -1.071140
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      1
│      2
│      8
│     13
│     14
│     15
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 22, average log likelihood -1.116164
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     14
│     26
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 23, average log likelihood -1.099669
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      1
│      2
│     13
│     14
│      ⋮
│     20
│     29
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 24, average log likelihood -1.085106
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     13
│     14
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 25, average log likelihood -1.103323
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      1
│      2
│     13
│     14
│     15
│     16
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 26, average log likelihood -1.110488
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      1
│      2
│     13
│     14
│      ⋮
│     26
│     29
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 27, average log likelihood -1.083992
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     14
│     15
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 28, average log likelihood -1.106369
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      1
│      2
│      8
│     13
│     14
│     26
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 29, average log likelihood -1.094466
┌ Warning: Variances had to be floored 
│   ind =
│    12-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     18
│     20
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 30, average log likelihood -1.077341
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      1
│      2
│     13
│     14
│     22
│     26
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 31, average log likelihood -1.111378
┌ Warning: Variances had to be floored 
│   ind =
│    12-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     16
│     29
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 32, average log likelihood -1.085788
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      1
│      2
│     13
│     14
│     18
│     20
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 33, average log likelihood -1.106476
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      1
│      2
│     13
│     14
│     15
│     16
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 34, average log likelihood -1.116506
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     26
│     29
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 35, average log likelihood -1.081027
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      1
│      2
│      8
│     13
│      ⋮
│     16
│     18
│     20
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 36, average log likelihood -1.085745
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     22
│     26
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 37, average log likelihood -1.109124
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      1
│      2
│     13
│     14
│     15
│     16
│     29
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 38, average log likelihood -1.104200
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     18
│     20
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 39, average log likelihood -1.079832
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      1
│      2
│     13
│     14
│     15
│     16
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 40, average log likelihood -1.125036
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      1
│      2
│     13
│     14
│     26
│     29
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 41, average log likelihood -1.093299
┌ Warning: Variances had to be floored 
│   ind =
│    12-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     18
│     20
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 42, average log likelihood -1.075250
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      1
│      2
│      8
│     13
│     14
│     26
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 43, average log likelihood -1.118260
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     15
│     16
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 44, average log likelihood -1.097243
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      1
│      2
│     13
│     14
│     18
│     20
│     26
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 45, average log likelihood -1.092833
┌ Warning: Variances had to be floored 
│   ind =
│    12-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     16
│     29
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 46, average log likelihood -1.093139
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      1
│      2
│     13
│     14
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 47, average log likelihood -1.112400
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      1
│      2
│     13
│     14
│      ⋮
│     20
│     22
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 48, average log likelihood -1.083059
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     26
│     29
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 49, average log likelihood -1.104025
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      1
│      2
│      8
│     13
│     14
│     15
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 50, average log likelihood -1.104332
┌ Info: EM with 100000 data points 50 iterations avll -1.104332
└ 59.0 data points per parameter
┌ Info: 5
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.20045798289887
│     -1.1853796713448705
│      ⋮
└     -1.1043323710459878
┌ Info: Total log likelihood: 
│   tll =
│    251-element Array{Float64,1}:
│     -1.4386644140428544
│     -1.4387380208225444
│     -1.4386366875644991
│     -1.4373204301344993
│      ⋮
│     -1.0830587030334065
│     -1.1040254011344606
└     -1.1043323710459878
32×26 Array{Float64,2}:
 -0.0365706    0.137065    -0.00684973   0.1319      -0.0177294   -0.12612      0.0479178    0.122862     0.0145822   -0.454932    -0.0631898   -0.0298986    0.0332564    0.143657      0.0842277    0.0543932    0.0129764    0.00371631   0.188912    -0.0739575    0.0230199   -0.0124635   -0.0919625     0.307771     0.00489783  -0.10693
 -0.0368853    0.137551     0.0857943    0.113325    -0.0843783   -0.127737     0.0494588   -0.00438978  -0.0519311    0.232243    -0.0503627   -0.0533004   -0.0341922    0.135649      0.067602     0.0251403    0.0127996   -0.133278    -0.181574     0.0381295    0.0351506    0.00924598  -0.103967     -0.0976585    0.0575413   -0.059491
 -0.0225166    0.0886964   -0.024824     0.0155957    0.141301     0.227759    -0.417932     0.00680092  -0.0666539   -0.18817     -0.0750479   -0.0813721   -0.146377     0.00498522   -0.0912482   -0.0585539    0.103512    -0.124028    -0.0941276   -0.117905     0.248633     0.0728271    0.0382241    -0.271106    -0.0612608    0.117933
  0.261422     0.0316503    0.0320256    0.0490983    0.128779     0.0352265    0.745517    -0.146169    -0.0295953   -0.0879685   -0.288849    -0.0704196   -0.163444     0.000678733  -0.145849    -0.184273     0.122203    -0.0913803   -0.0871446   -0.102172    -0.0650222    0.0696719    0.065843     -0.0297657    0.0702595    0.0643318
 -0.163445     0.0213134   -0.0205099   -0.119518     0.0231812   -0.145603    -0.00390661   0.121606    -0.186564    -0.00493349  -0.117135     0.0450947   -0.199887    -0.104994      0.0492337   -0.0810599   -0.00352083   0.0817964    0.10781      0.00789281  -0.645719     0.176978     0.069839     -0.143062     0.0866412   -0.0641749
 -0.133066     0.0199776    0.00582197  -0.118148     0.0252294    0.00329289   0.0387495    0.159173    -0.18584     -0.0229119   -0.151246    -0.322841    -0.186936    -0.0918743     0.0498309   -0.0446004    0.0159954    0.0844735    0.0781098    0.063888     1.3534       0.150426     0.064487     -0.12992      0.0917127   -0.0518057
  0.028404    -0.0552604    0.0461212    0.0667177    0.0510863    0.194972     0.041348    -0.183524     0.00993553  -0.0250531   -0.0659468    0.0423157   -0.0937106    0.0790229     0.115974    -0.00096888  -0.120297    -0.0797715   -0.129642    -0.0172458   -0.00952187  -0.127584     0.000541899  -0.0255588    0.0217595   -0.0548005
  0.0891187    0.0362574   -0.0337398   -0.0751752    0.01259      0.0412839    0.103059    -0.0696081    0.0272084    0.0851946   -0.0231793    0.0118233   -0.0163982   -0.0653882     0.122204    -0.0741108   -0.00294502  -0.0403543   -0.0873721    0.0564738    0.090577    -0.158593     0.140523      0.147554     0.059527     0.0143556
  0.0190498    0.0308381   -0.115102    -0.0524483    0.274244    -0.0304924    0.0164259    0.122716    -0.148471    -0.0305496    0.0119409   -0.0336001    0.0553885    0.0334793     0.0916167   -0.0466369    0.103033     0.117883     0.00177131   0.223701    -0.0765993    0.0988686   -0.0244958    -0.0849006    0.0321457   -0.0114049
  0.0886516    0.0407671    0.138644    -0.169808     0.117243    -0.124901    -0.203954     0.146172    -0.0428863    0.08226      0.0584748   -0.0494891    0.124264    -0.0605161     0.177129    -0.170041     0.0339074    0.0209427   -0.0461583   -0.0148546    0.119424     0.106658    -0.0260416    -0.0275594   -0.1034       0.0899626
 -0.101305    -0.0737267   -0.0704711   -0.109805    -0.133882     0.00199574  -0.0368538    0.0753704   -0.0012576    0.00712414   0.0922342   -0.121007    -0.0169695    0.0884262    -0.089772    -0.0320316   -0.0510958    0.114559     0.190492    -0.141869    -0.0695847   -0.00662089   0.0624271    -1.18124      0.204468    -0.0167375
 -0.00643182  -0.0700155    0.0444717   -0.11623     -0.089078    -0.0229355   -0.0405497    0.022138    -0.00693421  -0.0127624    0.0952537   -0.108517     0.0621287    0.159157     -0.107222     0.00915641  -0.0319881    0.0842819    0.0569508   -0.102265    -0.00726094   0.165653     0.0381497     1.48393     -0.0816224   -0.0575497
 -0.0708279   -0.0371017    0.0682774   -0.123518    -0.207343    -0.117562     0.0708205    0.10357      0.0632041    0.0693971   -0.233043     0.128587     0.0097726   -0.0540881    -0.0233798   -0.304256     0.207259    -0.129306    -0.0778943   -0.0907781   -0.00110935   0.316795     0.139398     -0.19318      0.0135008   -0.186703
 -0.0732619   -0.190024     0.0275101   -0.123776     0.103453    -0.242028     0.185121    -0.301725     0.0637072    0.105663     0.371722    -0.0384111    0.00417234   0.091637     -0.0653064   -0.0843957    0.118971    -0.128414    -0.0714327   -0.116273     0.0082575    0.0628255    0.150865     -0.187926     0.222984    -0.188648
 -0.0161339   -0.0446193    0.0112956   -0.131101    -0.00922865   0.114808     0.0772162   -0.0191519   -0.0664457    0.0392287    0.105526     0.0997835   -1.52331      0.13538      -0.167338    -0.0961889   -0.221759    -0.0375377   -0.177655     0.00290883   0.227787     0.0656587    0.0588789     0.066472     0.055086     0.0311898
  0.18948     -0.0205018   -0.0284238   -0.130018    -0.010259     0.111405     0.0422123    0.0471345   -0.110563     0.0603667    0.0874082    0.108118     1.56136      0.140853     -0.125779    -0.120006    -0.165508    -0.0313863   -0.153941     0.0457313    0.227791     0.0634238   -0.0268323    -0.0815761    0.0308653    0.032223
 -0.0299682    0.0309973    0.2163      -0.0542976   -0.052891     0.0648806   -0.0951676   -0.013475    -0.0899513   -0.031729    -0.233924    -0.0508346   -0.0244612   -0.0269858    -0.0354345   -0.116583    -0.1264      -0.0119476   -0.0403638   -0.0301836    0.0568038    0.143877     0.0308507    -0.0617066   -0.138093     0.183163
 -0.0108277   -0.231198     0.185312    -0.0487647   -0.0299768   -0.0291217   -0.171539     0.072745    -0.0175505    0.0236453   -0.030079     0.0306926   -0.136194    -0.00849876   -0.0378447   -0.00935652  -0.00395858  -0.107985     0.00787392   0.0254957   -0.20719      0.0660597    0.162931      0.0210383    0.0546902   -0.0484281
 -0.062589    -0.0482876    0.00602075  -0.0286002   -0.064252    -0.0424317    0.0421047    0.024585    -0.0455066   -0.0299739    0.103295    -0.123771    -0.0170881    0.00370498   -0.0441196    0.127575    -0.0648724   -0.048246    -0.0944722    0.0332409   -0.075761    -0.082884     0.0293942    -0.0746983    0.0612834    0.138041
  0.113709     0.0890088   -0.0688847    0.169937    -0.10852      0.0126502   -0.00844639  -0.195184    -0.0829297    0.135973     0.0267921    0.0725219    0.0599848    0.137742     -0.0520077   -0.0189343    0.169281    -0.128465    -0.0348534    0.140836     0.0669363   -0.0798494   -0.0375742     0.0650799   -0.00356552  -0.0434699
 -0.0734067    0.00205481  -0.0576263   -0.0923372    0.153176     0.0223263   -0.160252    -0.0289906    0.0743859   -0.00951004  -0.027697    -0.035896     0.0225452    0.0263111     0.0110184    0.0500295   -0.0894037    0.0341596    0.0389915   -0.0313984   -0.141245    -0.0113002    0.0151615    -0.0892607   -0.0113064    0.03261
 -0.0118005    0.210735     0.0937164    0.00894037  -0.0464867    0.0682565   -0.13009     -0.132041     0.116746     0.07934     -0.127615    -0.0029186   -0.100107    -0.0853794    -0.068815    -0.125814    -0.155891    -0.0351953    0.0624474    0.0567417   -0.083519    -0.0536086   -0.0676093     0.107394     0.0433524   -0.0617944
  0.00236804  -0.0482913    0.108293    -0.0347877   -0.0274493    0.0999235    0.0288997   -0.0515527    0.0349817    0.00916617   0.0439476   -0.0665056    0.0660899   -0.170181     -0.0262726   -0.0351983   -0.00777541   0.172944     0.0180602   -0.0765836   -0.114672     0.0589009   -0.0433993    -0.0694315   -0.0110083    0.0468373
 -0.0711727    0.156107    -0.0936038   -0.0856832    0.0973258    0.0691828   -0.0806332   -0.0288053    0.0354699   -0.0116452    0.0406022    0.0873049    0.0150978   -0.116767      0.184235     0.0441017    0.0349972    0.0644775   -0.0495764    0.104733    -0.022257     0.0402937   -0.148068      0.0271689   -0.0462528   -0.0755723
  0.115639    -0.0731486    0.290493     0.0938354   -0.0712007    0.202758     0.257291     0.180546    -0.131945    -0.136363    -0.105216    -0.0807314    0.0175537   -0.0606928    -0.0953964    0.0588536    0.0592102   -0.0318322   -0.0158238   -0.0174051    0.174464     0.00250109  -0.208874     -0.184623    -0.0424448    0.146498
 -0.0398434    0.172621    -0.0508846    0.0727322    0.0122909    0.0857216    0.0287535   -0.109673    -0.036623     0.00998273  -0.154227     0.139144    -0.0656218    0.0278014     0.00695137  -0.117622    -0.138447    -0.0405352   -0.051036    -0.0272788   -0.0631435    0.0481344    0.106147      0.152879     0.0107785   -0.123329
 -0.0849591   -0.203577    -0.190915    -0.14113      0.0177218    0.0941165   -0.0425486   -0.011588     0.00302449  -0.0732461    0.0377283   -0.0383322   -0.091422     0.0760686    -0.00867382  -0.00903411  -0.147794    -0.127896     0.0381214    0.00174827   0.0236994   -0.0234733    0.0644261     0.0838775   -0.184059     0.14814
  0.16064     -0.0393855   -0.115292     0.134732     2.52211e-5  -0.0151964    0.016168    -0.095367     0.0310346   -0.0346748    0.0911487    0.00201378  -0.00326405  -0.0246646     0.058708     0.0468698    0.110256    -0.0959363    0.00620903  -0.0707484   -0.11126     -0.161841     0.0834621     0.121592     0.0619746    0.0900237
  0.0622947   -0.12479      0.00635886   0.0455361   -0.0780085   -0.0360548    0.0830476   -0.0689991    0.0811589    0.131206     0.138108    -0.0764753    0.11249     -0.0853099    -0.00938331   0.104427    -0.0661917   -0.176456    -0.0204803   -0.0658407    0.0896271    0.0854958    0.0440286     0.0504924   -0.150975     0.0519409
  0.193736     0.00962274   0.0775431   -0.0876589    0.161877    -0.00509184   0.0626588   -0.0613752   -0.014621    -0.116757    -0.0291103    0.0306698    0.174634    -0.0302894    -0.0302827   -0.30019     -0.047535    -0.0166579    0.0328507   -0.0123924   -0.0624342   -0.084095    -0.0807004     0.0674179    0.0372068    0.133053
  0.0233863   -0.00407914  -0.119944     0.0877132    0.0230156   -0.0728737    0.0625667   -0.070051     0.0935652    0.015112     0.00417473   0.0436075   -0.0398501   -0.0820572     0.0055476   -0.0168788    0.0602708    0.0509429   -0.193276     0.0615594   -0.0299126    0.031986    -0.0405152    -0.00549344  -0.00968683   0.0184392
  0.0463786   -0.018933    -0.0370495    0.0722957    0.0537305   -0.0330949    0.0836289    0.122058    -0.00493856  -0.0322163    0.039372    -0.0802748   -0.00153113  -0.0591544    -0.0801457   -0.0212576   -0.0343965    0.0493336   -0.0562979   -0.150468    -0.0662647    0.0202882   -0.0260021    -0.046667     0.0270635   -0.107182[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     20
│     26
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 1, average log likelihood -1.088230
┌ Warning: Variances had to be floored 
│   ind =
│    14-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     26
│     29
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 2, average log likelihood -1.058142
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     22
│     26
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 3, average log likelihood -1.066683
┌ Warning: Variances had to be floored 
│   ind =
│    14-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     26
│     29
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.070319
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     20
│     26
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.074854
┌ Warning: Variances had to be floored 
│   ind =
│    16-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     26
│     29
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.048704
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     20
│     26
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.087750
┌ Warning: Variances had to be floored 
│   ind =
│    14-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     26
│     29
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.057809
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     22
│     26
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.066118
┌ Warning: Variances had to be floored 
│   ind =
│    14-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     26
│     29
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.070303
┌ Info: EM with 100000 data points 10 iterations avll -1.070303
└ 59.0 data points per parameter
kind diag, method kmeans
[ Info: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.432545e+05
      1       7.209858e+05      -2.222687e+05 |       32
      2       6.817616e+05      -3.922423e+04 |       32
      3       6.638373e+05      -1.792431e+04 |       32
      4       6.551989e+05      -8.638355e+03 |       32
      5       6.501119e+05      -5.086998e+03 |       32
      6       6.471086e+05      -3.003299e+03 |       32
      7       6.455526e+05      -1.556054e+03 |       32
      8       6.444745e+05      -1.078041e+03 |       32
      9       6.435639e+05      -9.106671e+02 |       32
     10       6.427701e+05      -7.937538e+02 |       32
     11       6.423454e+05      -4.246590e+02 |       32
     12       6.421760e+05      -1.694725e+02 |       32
     13       6.420135e+05      -1.624492e+02 |       32
     14       6.417508e+05      -2.627557e+02 |       32
     15       6.413870e+05      -3.637251e+02 |       32
     16       6.409643e+05      -4.227239e+02 |       32
     17       6.405930e+05      -3.713490e+02 |       32
     18       6.403497e+05      -2.432192e+02 |       32
     19       6.402336e+05      -1.161715e+02 |       32
     20       6.401771e+05      -5.647115e+01 |       32
     21       6.401433e+05      -3.376732e+01 |       32
     22       6.401140e+05      -2.931550e+01 |       32
     23       6.400861e+05      -2.793300e+01 |       32
     24       6.400554e+05      -3.071803e+01 |       32
     25       6.400316e+05      -2.374645e+01 |       31
     26       6.400072e+05      -2.446375e+01 |       29
     27       6.399833e+05      -2.386128e+01 |       30
     28       6.399594e+05      -2.391376e+01 |       30
     29       6.399368e+05      -2.258863e+01 |       30
     30       6.399077e+05      -2.908710e+01 |       32
     31       6.398734e+05      -3.431672e+01 |       32
     32       6.398465e+05      -2.685473e+01 |       31
     33       6.398230e+05      -2.349369e+01 |       32
     34       6.398004e+05      -2.269138e+01 |       31
     35       6.397768e+05      -2.352640e+01 |       31
     36       6.397539e+05      -2.293870e+01 |       31
     37       6.397311e+05      -2.278098e+01 |       30
     38       6.397133e+05      -1.779214e+01 |       29
     39       6.397025e+05      -1.076607e+01 |       27
     40       6.396963e+05      -6.226635e+00 |       30
     41       6.396888e+05      -7.557501e+00 |       27
     42       6.396809e+05      -7.902924e+00 |       26
     43       6.396727e+05      -8.186818e+00 |       28
     44       6.396646e+05      -8.024797e+00 |       29
     45       6.396549e+05      -9.745682e+00 |       27
     46       6.396466e+05      -8.351197e+00 |       28
     47       6.396382e+05      -8.382597e+00 |       27
     48       6.396307e+05      -7.444399e+00 |       28
     49       6.396228e+05      -7.894658e+00 |       28
     50       6.396170e+05      -5.793227e+00 |       26
K-means terminated without convergence after 50 iterations (objv = 639617.0355173391)
┌ Info: K-means with 32000 data points using 50 iterations
└ 37.0 data points per parameter
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.344539
[ Info: iteration 2, average log likelihood -1.314429
[ Info: iteration 3, average log likelihood -1.279127
[ Info: iteration 4, average log likelihood -1.239517
[ Info: iteration 5, average log likelihood -1.204548
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     20
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.156932
[ Info: iteration 7, average log likelihood -1.139663
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      1
│      7
│     10
│     18
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.079930
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      9
│     12
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.112008
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.108103
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      4
│     20
│     27
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 11, average log likelihood -1.072955
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      1
│      2
│      3
│      7
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 12, average log likelihood -1.063326
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      9
│     18
│     19
│     21
│     24
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 13, average log likelihood -1.086531
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     12
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 14, average log likelihood -1.116817
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      4
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 15, average log likelihood -1.083869
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      1
│     20
│     21
│     26
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 16, average log likelihood -1.060927
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      2
│      3
│      7
│     12
│     24
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 17, average log likelihood -1.084633
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     19
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 18, average log likelihood -1.108615
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      1
│      4
│      9
│     21
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 19, average log likelihood -1.076469
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 20, average log likelihood -1.114700
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      7
│     18
│     20
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 21, average log likelihood -1.065957
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      1
│      3
│     21
│     26
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 22, average log likelihood -1.059259
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      2
│      4
│      9
│     12
│     19
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 23, average log likelihood -1.092284
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 24, average log likelihood -1.120042
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      7
│     21
│     26
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 25, average log likelihood -1.072740
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      1
│      3
│     18
│     20
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 26, average log likelihood -1.087537
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     12
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 27, average log likelihood -1.095059
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      2
│      4
│     19
│     21
│     26
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 28, average log likelihood -1.054586
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      1
│      7
│      9
│     24
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 29, average log likelihood -1.091742
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      3
│     20
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 30, average log likelihood -1.117185
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     12
│     18
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 31, average log likelihood -1.106199
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      1
│      7
│     21
│     26
│     29
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 32, average log likelihood -1.053492
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      2
│      3
│      9
│     19
│     20
│     24
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 33, average log likelihood -1.081792
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      4
│     18
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 34, average log likelihood -1.133388
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 35, average log likelihood -1.108489
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      1
│     21
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 36, average log likelihood -1.071428
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      7
│     12
│     18
│     20
│     24
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 37, average log likelihood -1.075140
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      3
│      4
│     19
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 38, average log likelihood -1.097197
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      1
│      9
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 39, average log likelihood -1.101055
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     18
│     21
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 40, average log likelihood -1.078150
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      2
│      3
│      7
│     12
│     20
│     24
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 41, average log likelihood -1.069627
[ Info: iteration 42, average log likelihood -1.115024
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      1
│     18
│     19
│     21
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 43, average log likelihood -1.042825
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      2
│      4
│      7
│      9
│     20
│     26
│     27
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 44, average log likelihood -1.061812
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      3
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 45, average log likelihood -1.137932
[ Info: iteration 46, average log likelihood -1.107607
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     18
│     21
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 47, average log likelihood -1.064103
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      2
│      3
│      4
│      7
│     26
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 48, average log likelihood -1.066497
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     20
│     24
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 49, average log likelihood -1.111043
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     12
│     18
│     21
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 50, average log likelihood -1.081678
┌ Info: EM with 100000 data points 50 iterations avll -1.081678
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
  0.00208323   0.22507       0.0939327    -0.0167121   -0.0577441    0.0938163    -0.116653    -0.146859    0.130238     0.070814    -0.104469     0.0182025  -0.112202   -0.0618718    -0.080111    -0.153166    -0.202601    -0.0315016     0.000225854   0.102533    -0.0387192   -0.0535333   -0.0714208    0.149413    0.0659463   -0.0449898
  0.0849714   -0.169987     -0.0156187     0.0412573   -0.0647847   -0.0344658     0.0799179   -0.0926859   0.0788275    0.116003     0.125334    -0.0683332   0.119244   -0.117545     -0.0196007    0.0820262   -0.0870232   -0.168805     -0.0172865    -0.0670397    0.0814815    0.0762557    0.0414768    0.0535271  -0.203392     0.123432
 -0.0296689    0.111982      0.064745      0.109604    -0.0505775   -0.109569      0.0578929    0.0621253   0.00519176  -0.0552448   -0.0102223   -0.0489774   0.015161    0.101197      0.0593413    0.0571662   -0.00109267  -0.0854267    -0.00763087   -0.0426382    0.0366413    0.0186284   -0.0675672    0.0910093   0.0365261   -0.099365
  0.179672     0.0251212     0.0822953    -0.0678185    0.156404    -0.00926969    0.070914    -0.0453025  -0.0250141   -0.114472    -0.0223827    0.02067     0.168594    0.00454815   -0.0324318   -0.309272    -0.0264629   -0.0307419     0.041598     -0.0135851   -0.0717344   -0.085202    -0.100204     0.0549005   0.064783     0.116476
 -0.0516553   -0.0716799    -0.0113716    -0.113426    -0.111791    -0.00992697   -0.0385291    0.0495949  -0.00311925  -0.0022472    0.0937284   -0.113883    0.022313    0.12093      -0.0986211   -0.0102117   -0.0471324    0.0994522     0.12482      -0.122524    -0.0394204    0.0806231    0.0488826    0.128774    0.0609719   -0.0357051
 -0.0296114    0.0308917     0.218106     -0.0533116   -0.0529808    0.0634649    -0.0981561   -0.0127234  -0.0876932   -0.0356169   -0.232936    -0.0509716  -0.0258719  -0.0262597    -0.0346247   -0.113438    -0.12547     -0.0127227    -0.0415722    -0.0299535    0.0519633    0.142359     0.0328373   -0.0626808  -0.13512      0.188122
  0.123602    -0.0694266     0.131528     -0.0101471   -0.0988251    0.0469795     0.0137931    0.0126536   0.0474165    0.0108226    0.00934076  -0.0778493   0.0903115  -0.253684     -0.0134483    0.0221814   -0.0268453    0.150328      0.0334469    -0.124055    -0.0505479    0.13207     -0.0696995   -0.118328   -0.126183     0.181037
 -0.0862499   -0.204453     -0.190045     -0.139253     0.0163476    0.0946334    -0.0433541   -0.0122097   0.00173405  -0.0737316    0.0348869   -0.0386481  -0.089332    0.0796526    -0.0081332   -0.00796547  -0.145852    -0.129093      0.0381904    -0.00599667   0.0227564   -0.0234257    0.0648366    0.0829668  -0.184513     0.152046
 -0.154071     0.0110137    -0.118379     -0.183184     0.170532     0.0871787    -0.127248    -0.0377843   0.0452807   -0.0612412   -0.0465997   -0.114719   -0.0147675   0.0952363     0.0220565    0.130889    -0.161561     0.209842      0.0287862     0.0234609   -0.120242    -0.143778     0.0425178   -0.0308643  -0.0361111    0.157638
  0.116055    -0.151359     -0.084749      0.0763398    0.12279      0.0695626     0.0692746    0.0619704   0.00712155  -0.0932988   -0.029159    -0.078837   -0.106601   -0.0961045    -0.143241    -0.0875913   -0.0651318    0.0240131    -0.153627     -0.0595026    0.00696264   0.0770226   -0.0575711    0.0825524  -0.0248855   -0.120271
  0.114987     0.0613968     0.000997193   0.0329215    0.134869     0.133324      0.156333    -0.0670778  -0.0490551   -0.135576    -0.179965    -0.0761416  -0.154445    0.00323203   -0.117899    -0.11852      0.111437    -0.107618     -0.0894436    -0.108547     0.0942959    0.0712701    0.0512638   -0.154852    0.00535757   0.0946335
  0.0941435   -0.0357206    -0.0105346    -0.130559    -0.00945908   0.111598      0.0577868    0.0142423  -0.089636     0.0499201    0.0981872    0.103292    0.0991194   0.138015     -0.148536    -0.106495    -0.170481    -0.0338166    -0.16383       0.02844      0.227998     0.064126     0.0147909   -0.0134343   0.0404961    0.0312727
 -0.102714    -0.0369767    -0.138662     -0.0764259   -0.0668764   -0.00894272    0.0568197    0.0481904  -0.102382    -0.130968     0.136376    -0.0975173   0.0893821  -0.121639     -0.0828113    0.186959    -0.040371    -0.0803511    -0.0558893     0.0643248    0.0575383   -0.103378     0.0236613   -0.0517935   0.120905     0.0935327
  0.0189554    0.0324881    -0.116047     -0.0484808    0.274067    -0.031488      0.0143875    0.122213   -0.148642    -0.0290844    0.0127035   -0.0363726   0.0540834   0.0364508     0.0918397   -0.0443104    0.101974     0.116262      0.00418119    0.223888    -0.076024     0.100168    -0.0233242   -0.0869779   0.0292221   -0.0117474
 -0.0877192   -0.000728725  -0.00299805    0.0111577    0.00783826  -0.0286468    -0.25803     -0.133428    0.0335251    0.0913504   -0.072082     0.0577189  -0.0501085  -0.0328952    -0.0398114   -0.106945    -0.197159    -0.144499      0.0778725    -0.108037    -0.190999    -0.00527178  -0.0826821   -0.0962547  -0.00275551  -0.0530484
  0.0893134    0.0409432     0.13896      -0.16963      0.117152    -0.12939      -0.203599     0.148499   -0.0413319    0.0818573    0.0593254   -0.0492894   0.124061   -0.0558864     0.177577    -0.166281     0.0356462    0.019687     -0.0453243    -0.0147736    0.118088     0.106846    -0.0282009   -0.0262917  -0.101095     0.0913894
 -0.103959    -0.0407783     0.0897403    -0.0647777    0.0269861    0.147062      0.0564695   -0.106666    0.0294945    0.00786858   0.0742967   -0.0595197   0.0443681  -0.123499     -0.0390866   -0.0758228    0.022465     0.217266      0.00398423   -0.0407369   -0.179118     0.00224474  -0.021821    -0.0351128   0.0712742   -0.0787282
 -0.152152     0.0288011    -0.00374072   -0.119161     0.0246806   -0.0785126     0.0170272    0.1375     -0.190784    -0.0119837   -0.127998    -0.103381   -0.19644    -0.0975025     0.0517215   -0.067683     0.00206878   0.0888316     0.0919885     0.0291599    0.167352     0.159094     0.0687965   -0.138672    0.0868636   -0.0614189
 -0.0235189    0.148542      0.121884      0.00934454  -0.0757181    0.0538976    -0.0838066   -0.0671287   0.0330523    0.115278    -0.0671348   -0.0755638  -0.134625   -0.0177982    -0.0572076   -0.105818    -0.100003    -0.0443788     0.0438979    -0.0183799   -0.116015    -0.0662593   -0.00824422  -0.0376116   0.0142747   -0.00520328
  0.018201    -0.0229288     0.0225465     0.0756812    0.0366184    0.165318      0.0489879   -0.110299    0.00865265  -0.0238344   -0.0526496    0.0163188  -0.062659    0.0814359     0.10205      2.23559e-5  -0.102339    -0.067155     -0.0978118    -0.0376272   -0.0123754   -0.121515     0.00111931  -0.0480926   0.0177548   -0.0481164
 -0.00772291  -0.230532      0.185144     -0.047601    -0.0302038   -0.0318526    -0.171226     0.0722558  -0.018247     0.0245195   -0.0308352    0.0307615  -0.13541    -0.00673046   -0.033814    -0.00791165  -0.00511843  -0.109415      0.00714538    0.0263244   -0.206326     0.0646996    0.163384     0.0193174   0.0545166   -0.0489956
  0.115783    -0.0733986     0.287405      0.0942328   -0.0713615    0.202404      0.256805     0.180361   -0.131537    -0.134089    -0.105859    -0.0808623   0.0173114  -0.0603788    -0.0953392    0.0594458    0.0578721   -0.032241     -0.0179229    -0.0177502    0.172516     0.00478815  -0.207936    -0.182874   -0.0423953    0.148392
  0.158883    -0.0453894    -0.120524      0.14391     -0.0244728    0.000855306  -0.00438213  -0.109689    0.0307945   -0.0375761    0.0997427    0.0157451  -0.0109622  -0.0206281     0.0512876    0.0409696    0.108592    -0.0991331     0.0162437    -0.0723517   -0.133228    -0.165142     0.0916076    0.141559    0.0671058    0.11225
 -0.00231578  -0.0606328     0.210167      0.0501166   -0.0358605   -0.0844694    -0.0121871    0.0022215   0.0265388    0.0862951    0.121222    -0.158642   -0.121258    0.178626      0.0403174    0.0778755   -0.0710075   -0.000146052  -0.166711      0.00103687  -0.266283    -0.0998054    0.0274253   -0.0870177   0.0109323    0.422795
 -0.0726092    0.15519      -0.0949239    -0.0906474    0.0983959    0.0689351    -0.076777    -0.0293997   0.0364847   -0.0138583    0.0416163    0.0876634   0.0157929  -0.118273      0.188042     0.0417494    0.032007     0.063715     -0.0528131     0.108246    -0.021853     0.0388107   -0.149203     0.0339094  -0.0448416   -0.0797308
  0.118839     0.0899574    -0.0717099     0.17024     -0.107716     0.0184483    -0.00978983  -0.202372   -0.0841256    0.137879     0.0231646    0.0692378   0.0619284   0.13954      -0.0510965   -0.0208888    0.165941    -0.127148     -0.0330958     0.141505     0.0648982   -0.0887254   -0.0383619    0.0694676  -0.00254158  -0.0453087
  0.0865037    0.0379247    -0.0326238    -0.0695039    0.00610057   0.0370654     0.102615    -0.0664572   0.0263833    0.079967    -0.0263822    0.0104349  -0.0178482  -0.060929      0.12119     -0.0701551   -0.00259836  -0.0468375    -0.0863178     0.0601657    0.0870653   -0.155831     0.134414     0.148121    0.059056     0.0174427
  0.0159086    0.127953     -0.0351127     0.0214619    0.10075      0.0251562     0.0130126   -0.0208109   0.0465166   -0.00528954  -0.0732776    0.0655927   0.0326594   0.000326838   0.0216096   -0.0132711   -0.0113669   -0.0361928    -0.0259531    -0.0067344   -0.0621976    0.0798513    0.0919648    0.0283875   0.0224871   -0.0955477
 -0.0616734   -0.141245      0.0742585    -0.101071    -0.0457195   -0.201291      0.130277    -0.076299    0.0575842    0.0981381    0.094577     0.0127248  -0.0109032   0.0476737    -0.0396306   -0.154082     0.127057    -0.119151     -0.0880644    -0.0875456   -0.0332415    0.183947     0.139017    -0.181411    0.103284    -0.188851
 -0.00399693  -0.0274181    -0.0645811    -0.0587925    0.11006     -0.0963053     0.0897895   -0.123277    0.00835953  -0.133792     0.104634    -0.0470091  -0.0385951  -0.0464853     0.0115235   -0.0104141    0.056828     0.0562638    -0.0903275    -0.0507578   -0.105746    -0.0195742   -0.0728896   -0.076704    0.0186357   -0.0783473
 -0.00307148   0.0849391     0.0175284     0.102381    -0.0170559   -0.174236      0.0777947    0.180615   -0.0226594    0.05207      0.0760144   -0.0949543   0.0501404  -0.0314555    -0.00739884   0.0825707   -0.0245212    0.118626      0.0300277    -0.287215    -0.196439    -0.0297637    0.0259425   -0.119499    0.0650663   -0.114535
  0.0375002    0.0114675    -0.156846      0.200954    -0.0516703   -0.0656644     0.044918    -0.0232337   0.17165      0.131608    -0.0645341    0.114482   -0.038758   -0.107685     -0.00226027  -0.020396     0.0626794    0.0485286    -0.277418      0.136206     0.0264796    0.0697096    0.00907615   0.0417693  -0.0160309    0.0903156[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      2
│      7
│      9
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 1, average log likelihood -1.087274
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      1
│      2
│      3
│      4
│      7
│      9
│     26
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 2, average log likelihood -1.036160
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      2
│      7
│      9
│     18
│      ⋮
│     26
│     29
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 3, average log likelihood -1.024794
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      1
│      2
│      3
│      4
│      ⋮
│     20
│     21
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.034408
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      1
│      2
│      7
│      9
│     26
│     27
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.034680
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      2
│      3
│      4
│      7
│      ⋮
│     26
│     29
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.015048
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      1
│      2
│      7
│      9
│     19
│     20
│     21
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.051310
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      1
│      2
│      3
│      4
│      ⋮
│     26
│     27
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.026273
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      2
│      7
│      9
│     12
│      ⋮
│     26
│     29
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.021403
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      1
│      2
│      3
│      4
│      ⋮
│     20
│     21
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.043996
┌ Info: EM with 100000 data points 10 iterations avll -1.043996
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
  0.0478182    -0.0913508    -0.199468     0.0138301    0.155271      0.00230762   0.00262858   0.152899    -0.162941     0.0316173    -0.0289744    0.169421    -0.13138      0.107922    -0.15085      0.133414     0.149738     -0.131656      0.104722     0.14383     -0.0328013    -0.124403    -0.0291188   -0.133957     0.0281887    0.0625098
  0.0391533     0.0289697    -0.170744    -0.0690646   -0.106232     -0.0608829    0.261405     0.0744952   -0.0978103   -0.029335     -0.0255999   -0.0630043   -0.181226    -0.0205161    0.001372    -0.0520964    0.0812317    -0.0300368    -0.112925     0.0965305    0.0102397    -0.0562628    0.064267    -0.119537     0.108259    -0.0900101
 -0.129064      0.0831315     0.124361     0.0310757   -0.0783217    -0.0167439   -0.135105    -0.157947     0.0653986    0.173844     -0.108926    -0.0486167    0.0826198   -0.115828     0.185071    -0.0564498   -0.188075     -0.0203764     0.0539892    0.163447     0.0058885    -0.0256246   -0.0504809   -0.0708358    0.116684    -0.0273158
 -0.0652253     0.049891     -0.0265092   -0.186462     0.0973445     0.0266406    0.110816     0.0517012    0.213774     0.0859029    -0.112971    -0.127594    -0.068765     0.0544536   -0.113607     0.184965     0.0200126    -0.000747369   0.00344834  -0.0188281    0.138271     -0.179829     0.0481668    0.134821     0.189457    -0.0553417
 -0.0858584    -0.000949888  -0.0503611    0.0502717    0.0430865    -0.125011    -0.0787999   -0.150642    -0.0781435    0.19979       0.0452291    0.208079    -0.00906202  -0.0276808    0.0841482   -0.017249    -0.0688919    -0.0592784     0.0289622    0.0690616   -0.0218354     0.143903     0.072699     0.0829674   -0.0218122   -0.00187942
 -0.0374742     0.165197      0.0682743   -0.0784434   -0.0276696    -0.0177417    0.105203     0.077099    -0.103252    -0.145008      0.0142649    0.224738    -0.0514963   -0.0472423   -0.0267635   -0.0583653   -0.00921435    0.0304062     0.0752068    0.00290471  -0.0756093    -0.236014    -0.00896686   0.054394     0.0663686    0.07737
  0.09553      -0.00752296   -0.00283929   0.149142    -0.00178434    0.0987277   -0.0562726    0.107872     0.0488455   -0.0663642     0.041948     0.139748    -0.140245    -0.109827    -0.057504    -0.0672823    0.0333329    -0.12274      -0.068404    -0.026891    -0.0939235     0.0337733   -0.209714     0.0597094    0.0472206   -0.0434036
 -0.0644936     0.250659     -0.0912649   -0.0851322    0.0773412    -0.0214459    0.157734    -0.015589    -0.152213     0.160916      0.0443879    0.0965886    0.0926498    0.00550185   0.0170227    0.012905    -0.0194217     0.169074      0.0536961    0.0436043    0.13238       0.275034     0.0100345    0.0822322   -0.00328225   0.0121413
  0.000730974   0.2315       -0.0540236    0.145269     0.126712     -0.0959441   -0.00931779   0.0726875    0.0281088    0.00061749   -0.121391     0.0269707   -0.183828     0.00350327  -0.177111     0.0821571   -0.0301283     0.263178      0.102308     0.142953    -0.216193      0.0168542   -0.113767     0.0151059   -0.0887441   -0.0515342
 -0.0193497    -0.0707152    -0.0946475   -0.0224805   -0.0291566    -0.112782     0.042616     0.0354274    0.0691233    0.0130228     0.0780205   -0.0294564    0.0081803    0.0488454    0.0451972   -0.0734461   -0.182101      0.084486      0.0732172    0.0118094   -0.0391238     0.0493016   -0.0676771   -0.0755303   -0.00364751   0.12084
  0.0147142     0.0285599     0.0832066   -0.0219391    0.190666      0.0670407    0.0110745   -0.12451     -0.0608962    0.0736415    -0.0512186    0.0462073   -0.102897     0.124541     0.079333    -0.0321757   -0.211707     -0.0698166    -0.122993     0.0278606   -0.0570044    -0.0389606   -0.0177579    0.0195735    0.151679     0.0197234
  0.0484589    -0.0691526     0.0865314    0.00781787   0.0458841     0.064317    -0.0929872    0.0366861   -0.121555     0.163268      0.0515027    0.0938396   -0.0470996    0.0191886   -0.0292164   -0.0505923   -0.052742      0.103151     -0.0349295    0.0802474    0.074882     -0.0416331   -0.0537739   -0.011909     0.182588     0.119908
 -0.239364      0.0922488     0.123873    -0.114308    -0.0334156     0.0467107    0.023917     0.116741     0.0767972   -0.000435345  -0.00918834  -0.179071    -0.0146844    0.00190571  -0.00649231  -0.0347467    0.0409044    -0.113397      0.0953081    0.242297     0.0522721    -0.00775193  -0.179729     0.088901     0.0278443   -0.144905
 -0.0490576     0.0265346    -0.105395     0.0991215   -0.0237317    -0.106763     0.00543457   0.0362148    0.115143    -0.0130942    -0.146794    -0.0929969    0.0939255   -0.0999074   -0.0214742    0.0528679   -0.148251      0.0862931     0.0304878    0.0449165   -0.0717599     0.0284062   -0.0322127   -0.0931761   -0.0278481   -0.23561
  0.026888      0.0660428     0.0246997   -0.267821    -0.135228      0.180518     0.0351332   -0.0414624   -0.105748     0.0850881    -0.0393445    0.0412913    0.0101473   -0.012966     0.0676625   -0.0855299    4.98898e-5    0.141797     -0.0509117    0.0851231   -0.0872749    -0.0102325    0.0597509   -0.0556117    0.180422    -0.0853542
 -0.0632793    -0.0856726     0.0978085    0.0789777   -0.153854     -0.00265358  -0.0999432    0.0761272   -0.0938177   -0.0395576     0.0781659   -0.0149926    0.116615    -0.232912     0.0246184   -0.00733501  -0.216377      0.0805162     0.00899352  -0.041415    -0.000186817   0.0635682    0.0262477   -0.10457      0.0265517   -0.0936098
 -0.0959974     0.1908        0.121301     0.079717     0.0341372     0.1205       0.0563876    0.00628356  -0.111071     0.0353419    -0.0997722   -0.0165797    0.0697214    0.0229768    0.0799741   -0.112283     0.0301316     0.20075      -0.143699     1.14331e-5   0.104937      0.152176    -0.0642949    0.14088      0.0802982    0.00807467
 -0.257108      0.114662      0.205395     0.149361     0.11802      -0.124883     0.130583     0.0280286   -0.179235     0.0430418    -0.00782482  -0.0261574    0.0806044    0.133243     0.0296565   -0.0988278    0.0125748    -0.0891647     0.0972027   -0.0361178   -0.050093      0.0388585   -0.120012    -0.0781285   -0.10903     -0.104028
  0.0855421     0.025679     -0.0330832    0.0873592    0.0441533    -0.00403286   0.0271426    0.0267413   -0.150312    -0.0574507    -0.128296     0.0443052   -0.126559    -0.171297     0.0880994    0.0303288    0.137074     -0.0513094     0.160286     0.137272    -0.0129031    -0.0319621   -0.0710052   -0.0092246    0.0128627    0.105619
  0.00482698    0.0591571     0.127922    -0.0882496   -0.00030173    0.0682548   -0.114625    -0.0799652   -0.0922059   -0.139246      0.0065951   -0.0554033   -0.0202766    0.129841    -0.0572622   -0.0890865   -0.0473884     0.0876998    -0.0228208   -0.133152     0.0914388     0.106342     0.0141849   -0.0523925    0.0656015   -0.0554162
  0.109581      0.146559      0.0650585    0.0106919    0.264513      0.150927    -0.224991    -0.00514381  -0.176293     0.0371173    -0.0414716    0.0603779   -0.0186895   -0.00062238   0.0168545    0.0217531   -0.0789149     0.198109     -0.166077     0.121772     0.0644369    -0.174091     0.0633002    0.00979448  -0.00893349   0.0604749
 -0.138793     -0.050195      0.13971     -0.0260295    0.093067     -0.0331034    0.0326872   -0.0777668    0.00383962  -0.0455249     0.0929777    0.0743961   -0.106829     0.182289    -0.00141696  -0.071121     0.100029     -0.144766     -0.0394948   -0.0687711    0.0202847     0.0448584   -0.0146056    0.242825    -0.0435005    0.0818194
 -0.0284577     0.0485011    -0.0563753   -0.0831331    0.160635      0.0667717    0.0983697    0.0700309    0.121418     0.200568     -0.0768229    0.0371412   -0.0278743   -0.118023    -0.0925487    0.0665119   -0.000734055  -0.0100634     0.262667     0.174915    -0.11284       0.133925     0.128823    -0.123142    -0.032423     0.201498
 -0.0859668    -0.167915      0.0941476   -0.122864     0.0160846    -0.0532983    0.00699065   0.0479951    0.0114805    0.30592       0.0516338   -0.0837381   -0.0183814   -0.0935169    0.0420301   -0.0278736    0.0102656    -0.0658801     0.0355675    0.0256488    0.0713082    -0.0396066   -0.0775807    0.0870091   -0.114927     0.146839
 -0.0338838     0.0541993    -0.177653    -0.0996631   -0.00740315   -0.181425    -0.188263    -0.0488325    0.0213935    0.11144       0.042463     0.110606    -0.0897733   -0.0624291    0.0969664    0.0288144    0.135003      0.0548112     0.00635819  -0.039483    -0.00782417    0.00737793   0.0343066    0.0775502   -0.167684     0.0493776
  0.0477867    -0.0785589    -0.0249778   -0.170529    -0.173587      0.172758     0.10944      0.0142374   -0.0432826    0.228221      0.0389064    0.0845793    0.0214469    0.0989444    0.054254    -0.0688249    0.000798886   0.04417      -0.106558    -0.04299     -0.0636131    -0.0339135   -0.0229688   -0.00595574   0.143211     0.0358234
 -0.105507     -0.0446841    -0.134659     0.00972478  -0.103628     -0.108073     0.0547826   -0.196938    -0.0516379   -0.0339308    -0.0786402    0.0571536   -0.0409038    0.00742432   0.0960327   -0.0139479   -0.0470283    -0.0361145    -0.279169     0.0355494    0.0231384    -0.0106123   -0.0812123   -0.107597    -0.117848     0.0376269
  0.0894368     0.0425511    -0.178136     0.0226109    0.0812581    -0.133284    -0.0284669   -0.28631     -0.166072    -0.0248665     0.0768992    0.00789931   0.063984     0.0119716    0.0121586   -0.0519343    0.0945496     0.0508839     0.0449174   -0.211452     0.0907295    -0.114618     0.13574     -0.00430078   0.0930984    0.0790435
 -0.184632      0.0617601     0.185032    -0.0222897   -0.154225      0.168668    -0.0268246   -0.00568287   0.0371329   -0.0174849    -0.105687     0.00409469   0.0369132    0.0115675   -0.0672707    0.0581569    0.04389      -0.0228655     0.0646701   -0.0352498    0.10717       0.153361     0.0907905   -0.00822076  -0.0648544   -0.0385368
 -0.126777     -0.00159606   -0.00312776   0.0386328    0.000985432  -0.1297      -0.0966829   -0.0195433   -0.0876095   -0.0435281    -0.0296659   -0.0645194    0.0778314   -0.0258344    0.0156744    0.16354      0.0903445    -0.000502599   0.0684707    0.155808    -0.0541697     0.0219757    0.0460517   -0.114748     0.103434    -0.00286648
 -0.0973441     0.06111      -0.0650479   -0.0304628    0.0447003     0.0187568    0.320704    -0.0425012    0.255781    -0.0716501    -0.180993     0.104278     0.132294    -0.0963197   -0.0450053    0.0635934    0.197952      0.146307     -0.07871      0.125139    -0.118526     -0.0624469    0.0208881    0.0767034   -0.0091995    0.118546
  0.0739117    -0.183832      0.0122025    0.114683    -0.0635085     0.0611358   -0.129967    -0.17565     -0.0832189    0.0509089    -0.00862978  -0.0946253    0.0405735    0.0161826    0.0192466   -0.137012    -0.136398      0.154522      0.0176831   -0.201135     0.0342414    -0.0157427    0.169649     0.126306     0.0151257    0.196053kind full, method split
┌ Info: 0: avll = 
└   tll[1] = -1.4176741518563039
[ Info: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.417693
[ Info: iteration 2, average log likelihood -1.417634
[ Info: iteration 3, average log likelihood -1.417597
[ Info: iteration 4, average log likelihood -1.417556
[ Info: iteration 5, average log likelihood -1.417505
[ Info: iteration 6, average log likelihood -1.417432
[ Info: iteration 7, average log likelihood -1.417312
[ Info: iteration 8, average log likelihood -1.417077
[ Info: iteration 9, average log likelihood -1.416591
[ Info: iteration 10, average log likelihood -1.415684
[ Info: iteration 11, average log likelihood -1.414420
[ Info: iteration 12, average log likelihood -1.413280
[ Info: iteration 13, average log likelihood -1.412637
[ Info: iteration 14, average log likelihood -1.412371
[ Info: iteration 15, average log likelihood -1.412272
[ Info: iteration 16, average log likelihood -1.412235
[ Info: iteration 17, average log likelihood -1.412220
[ Info: iteration 18, average log likelihood -1.412214
[ Info: iteration 19, average log likelihood -1.412211
[ Info: iteration 20, average log likelihood -1.412210
[ Info: iteration 21, average log likelihood -1.412209
[ Info: iteration 22, average log likelihood -1.412208
[ Info: iteration 23, average log likelihood -1.412208
[ Info: iteration 24, average log likelihood -1.412208
[ Info: iteration 25, average log likelihood -1.412208
[ Info: iteration 26, average log likelihood -1.412208
[ Info: iteration 27, average log likelihood -1.412207
[ Info: iteration 28, average log likelihood -1.412207
[ Info: iteration 29, average log likelihood -1.412207
[ Info: iteration 30, average log likelihood -1.412207
[ Info: iteration 31, average log likelihood -1.412207
[ Info: iteration 32, average log likelihood -1.412207
[ Info: iteration 33, average log likelihood -1.412207
[ Info: iteration 34, average log likelihood -1.412207
[ Info: iteration 35, average log likelihood -1.412207
[ Info: iteration 36, average log likelihood -1.412207
[ Info: iteration 37, average log likelihood -1.412207
[ Info: iteration 38, average log likelihood -1.412207
[ Info: iteration 39, average log likelihood -1.412207
[ Info: iteration 40, average log likelihood -1.412207
[ Info: iteration 41, average log likelihood -1.412207
[ Info: iteration 42, average log likelihood -1.412207
[ Info: iteration 43, average log likelihood -1.412207
[ Info: iteration 44, average log likelihood -1.412206
[ Info: iteration 45, average log likelihood -1.412206
[ Info: iteration 46, average log likelihood -1.412206
[ Info: iteration 47, average log likelihood -1.412206
[ Info: iteration 48, average log likelihood -1.412206
[ Info: iteration 49, average log likelihood -1.412206
[ Info: iteration 50, average log likelihood -1.412206
┌ Info: EM with 100000 data points 50 iterations avll -1.412206
└ 952.4 data points per parameter
┌ Info: 1
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.417692511167983
│     -1.4176340886990593
│      ⋮
└     -1.4122064086927666
[ Info: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.412221
[ Info: iteration 2, average log likelihood -1.412154
[ Info: iteration 3, average log likelihood -1.412099
[ Info: iteration 4, average log likelihood -1.412032
[ Info: iteration 5, average log likelihood -1.411947
[ Info: iteration 6, average log likelihood -1.411840
[ Info: iteration 7, average log likelihood -1.411712
[ Info: iteration 8, average log likelihood -1.411573
[ Info: iteration 9, average log likelihood -1.411437
[ Info: iteration 10, average log likelihood -1.411318
[ Info: iteration 11, average log likelihood -1.411225
[ Info: iteration 12, average log likelihood -1.411159
[ Info: iteration 13, average log likelihood -1.411113
[ Info: iteration 14, average log likelihood -1.411081
[ Info: iteration 15, average log likelihood -1.411058
[ Info: iteration 16, average log likelihood -1.411038
[ Info: iteration 17, average log likelihood -1.411022
[ Info: iteration 18, average log likelihood -1.411007
[ Info: iteration 19, average log likelihood -1.410993
[ Info: iteration 20, average log likelihood -1.410980
[ Info: iteration 21, average log likelihood -1.410969
[ Info: iteration 22, average log likelihood -1.410958
[ Info: iteration 23, average log likelihood -1.410949
[ Info: iteration 24, average log likelihood -1.410940
[ Info: iteration 25, average log likelihood -1.410932
[ Info: iteration 26, average log likelihood -1.410925
[ Info: iteration 27, average log likelihood -1.410919
[ Info: iteration 28, average log likelihood -1.410913
[ Info: iteration 29, average log likelihood -1.410908
[ Info: iteration 30, average log likelihood -1.410902
[ Info: iteration 31, average log likelihood -1.410898
[ Info: iteration 32, average log likelihood -1.410893
[ Info: iteration 33, average log likelihood -1.410888
[ Info: iteration 34, average log likelihood -1.410884
[ Info: iteration 35, average log likelihood -1.410879
[ Info: iteration 36, average log likelihood -1.410875
[ Info: iteration 37, average log likelihood -1.410871
[ Info: iteration 38, average log likelihood -1.410866
[ Info: iteration 39, average log likelihood -1.410862
[ Info: iteration 40, average log likelihood -1.410858
[ Info: iteration 41, average log likelihood -1.410853
[ Info: iteration 42, average log likelihood -1.410849
[ Info: iteration 43, average log likelihood -1.410845
[ Info: iteration 44, average log likelihood -1.410841
[ Info: iteration 45, average log likelihood -1.410837
[ Info: iteration 46, average log likelihood -1.410833
[ Info: iteration 47, average log likelihood -1.410829
[ Info: iteration 48, average log likelihood -1.410825
[ Info: iteration 49, average log likelihood -1.410821
[ Info: iteration 50, average log likelihood -1.410817
┌ Info: EM with 100000 data points 50 iterations avll -1.410817
└ 473.9 data points per parameter
┌ Info: 2
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4122212408277264
│     -1.412154378354494
│      ⋮
└     -1.4108174862860574
[ Info: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.410829
[ Info: iteration 2, average log likelihood -1.410761
[ Info: iteration 3, average log likelihood -1.410710
[ Info: iteration 4, average log likelihood -1.410653
[ Info: iteration 5, average log likelihood -1.410584
[ Info: iteration 6, average log likelihood -1.410501
[ Info: iteration 7, average log likelihood -1.410405
[ Info: iteration 8, average log likelihood -1.410301
[ Info: iteration 9, average log likelihood -1.410196
[ Info: iteration 10, average log likelihood -1.410097
[ Info: iteration 11, average log likelihood -1.410009
[ Info: iteration 12, average log likelihood -1.409934
[ Info: iteration 13, average log likelihood -1.409872
[ Info: iteration 14, average log likelihood -1.409820
[ Info: iteration 15, average log likelihood -1.409776
[ Info: iteration 16, average log likelihood -1.409738
[ Info: iteration 17, average log likelihood -1.409704
[ Info: iteration 18, average log likelihood -1.409673
[ Info: iteration 19, average log likelihood -1.409645
[ Info: iteration 20, average log likelihood -1.409620
[ Info: iteration 21, average log likelihood -1.409597
[ Info: iteration 22, average log likelihood -1.409576
[ Info: iteration 23, average log likelihood -1.409557
[ Info: iteration 24, average log likelihood -1.409539
[ Info: iteration 25, average log likelihood -1.409523
[ Info: iteration 26, average log likelihood -1.409508
[ Info: iteration 27, average log likelihood -1.409495
[ Info: iteration 28, average log likelihood -1.409482
[ Info: iteration 29, average log likelihood -1.409470
[ Info: iteration 30, average log likelihood -1.409458
[ Info: iteration 31, average log likelihood -1.409448
[ Info: iteration 32, average log likelihood -1.409437
[ Info: iteration 33, average log likelihood -1.409427
[ Info: iteration 34, average log likelihood -1.409418
[ Info: iteration 35, average log likelihood -1.409409
[ Info: iteration 36, average log likelihood -1.409400
[ Info: iteration 37, average log likelihood -1.409391
[ Info: iteration 38, average log likelihood -1.409383
[ Info: iteration 39, average log likelihood -1.409375
[ Info: iteration 40, average log likelihood -1.409367
[ Info: iteration 41, average log likelihood -1.409360
[ Info: iteration 42, average log likelihood -1.409352
[ Info: iteration 43, average log likelihood -1.409345
[ Info: iteration 44, average log likelihood -1.409338
[ Info: iteration 45, average log likelihood -1.409331
[ Info: iteration 46, average log likelihood -1.409324
[ Info: iteration 47, average log likelihood -1.409317
[ Info: iteration 48, average log likelihood -1.409311
[ Info: iteration 49, average log likelihood -1.409304
[ Info: iteration 50, average log likelihood -1.409298
┌ Info: EM with 100000 data points 50 iterations avll -1.409298
└ 236.4 data points per parameter
┌ Info: 3
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4108287114390097
│     -1.4107613243097725
│      ⋮
└     -1.4092976623242814
[ Info: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.409299
[ Info: iteration 2, average log likelihood -1.409245
[ Info: iteration 3, average log likelihood -1.409196
[ Info: iteration 4, average log likelihood -1.409142
[ Info: iteration 5, average log likelihood -1.409077
[ Info: iteration 6, average log likelihood -1.409000
[ Info: iteration 7, average log likelihood -1.408910
[ Info: iteration 8, average log likelihood -1.408807
[ Info: iteration 9, average log likelihood -1.408696
[ Info: iteration 10, average log likelihood -1.408582
[ Info: iteration 11, average log likelihood -1.408470
[ Info: iteration 12, average log likelihood -1.408365
[ Info: iteration 13, average log likelihood -1.408270
[ Info: iteration 14, average log likelihood -1.408186
[ Info: iteration 15, average log likelihood -1.408114
[ Info: iteration 16, average log likelihood -1.408054
[ Info: iteration 17, average log likelihood -1.408003
[ Info: iteration 18, average log likelihood -1.407961
[ Info: iteration 19, average log likelihood -1.407926
[ Info: iteration 20, average log likelihood -1.407896
[ Info: iteration 21, average log likelihood -1.407870
[ Info: iteration 22, average log likelihood -1.407848
[ Info: iteration 23, average log likelihood -1.407828
[ Info: iteration 24, average log likelihood -1.407810
[ Info: iteration 25, average log likelihood -1.407793
[ Info: iteration 26, average log likelihood -1.407778
[ Info: iteration 27, average log likelihood -1.407763
[ Info: iteration 28, average log likelihood -1.407749
[ Info: iteration 29, average log likelihood -1.407735
[ Info: iteration 30, average log likelihood -1.407721
[ Info: iteration 31, average log likelihood -1.407708
[ Info: iteration 32, average log likelihood -1.407695
[ Info: iteration 33, average log likelihood -1.407682
[ Info: iteration 34, average log likelihood -1.407669
[ Info: iteration 35, average log likelihood -1.407656
[ Info: iteration 36, average log likelihood -1.407644
[ Info: iteration 37, average log likelihood -1.407631
[ Info: iteration 38, average log likelihood -1.407619
[ Info: iteration 39, average log likelihood -1.407606
[ Info: iteration 40, average log likelihood -1.407594
[ Info: iteration 41, average log likelihood -1.407581
[ Info: iteration 42, average log likelihood -1.407569
[ Info: iteration 43, average log likelihood -1.407557
[ Info: iteration 44, average log likelihood -1.407544
[ Info: iteration 45, average log likelihood -1.407532
[ Info: iteration 46, average log likelihood -1.407520
[ Info: iteration 47, average log likelihood -1.407509
[ Info: iteration 48, average log likelihood -1.407497
[ Info: iteration 49, average log likelihood -1.407485
[ Info: iteration 50, average log likelihood -1.407474
┌ Info: EM with 100000 data points 50 iterations avll -1.407474
└ 118.1 data points per parameter
┌ Info: 4
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.409299217523814
│     -1.409244565600529
│      ⋮
└     -1.4074739162370928
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.407471
[ Info: iteration 2, average log likelihood -1.407394
[ Info: iteration 3, average log likelihood -1.407319
[ Info: iteration 4, average log likelihood -1.407228
[ Info: iteration 5, average log likelihood -1.407111
[ Info: iteration 6, average log likelihood -1.406965
[ Info: iteration 7, average log likelihood -1.406793
[ Info: iteration 8, average log likelihood -1.406606
[ Info: iteration 9, average log likelihood -1.406415
[ Info: iteration 10, average log likelihood -1.406230
[ Info: iteration 11, average log likelihood -1.406057
[ Info: iteration 12, average log likelihood -1.405899
[ Info: iteration 13, average log likelihood -1.405758
[ Info: iteration 14, average log likelihood -1.405634
[ Info: iteration 15, average log likelihood -1.405527
[ Info: iteration 16, average log likelihood -1.405434
[ Info: iteration 17, average log likelihood -1.405354
[ Info: iteration 18, average log likelihood -1.405285
[ Info: iteration 19, average log likelihood -1.405225
[ Info: iteration 20, average log likelihood -1.405172
[ Info: iteration 21, average log likelihood -1.405125
[ Info: iteration 22, average log likelihood -1.405083
[ Info: iteration 23, average log likelihood -1.405045
[ Info: iteration 24, average log likelihood -1.405010
[ Info: iteration 25, average log likelihood -1.404979
[ Info: iteration 26, average log likelihood -1.404949
[ Info: iteration 27, average log likelihood -1.404922
[ Info: iteration 28, average log likelihood -1.404897
[ Info: iteration 29, average log likelihood -1.404872
[ Info: iteration 30, average log likelihood -1.404850
[ Info: iteration 31, average log likelihood -1.404828
[ Info: iteration 32, average log likelihood -1.404807
[ Info: iteration 33, average log likelihood -1.404787
[ Info: iteration 34, average log likelihood -1.404768
[ Info: iteration 35, average log likelihood -1.404750
[ Info: iteration 36, average log likelihood -1.404732
[ Info: iteration 37, average log likelihood -1.404715
[ Info: iteration 38, average log likelihood -1.404698
[ Info: iteration 39, average log likelihood -1.404682
[ Info: iteration 40, average log likelihood -1.404666
[ Info: iteration 41, average log likelihood -1.404651
[ Info: iteration 42, average log likelihood -1.404636
[ Info: iteration 43, average log likelihood -1.404621
[ Info: iteration 44, average log likelihood -1.404607
[ Info: iteration 45, average log likelihood -1.404594
[ Info: iteration 46, average log likelihood -1.404580
[ Info: iteration 47, average log likelihood -1.404567
[ Info: iteration 48, average log likelihood -1.404555
[ Info: iteration 49, average log likelihood -1.404542
[ Info: iteration 50, average log likelihood -1.404530
┌ Info: EM with 100000 data points 50 iterations avll -1.404530
└ 59.0 data points per parameter
┌ Info: 5
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4074713975249997
│     -1.4073940757080168
│      ⋮
└     -1.4045302445619616
┌ Info: Total log likelihood: 
│   tll =
│    251-element Array{Float64,1}:
│     -1.4176741518563039
│     -1.417692511167983
│     -1.4176340886990593
│     -1.417596865850308
│      ⋮
│     -1.4045547211540292
│     -1.4045423550693505
└     -1.4045302445619616
32×26 Array{Float64,2}:
 -0.276324     0.24136      0.158125     0.0531765  -0.103975   -0.00477225   -0.306471    -0.200751      0.121278    -0.100975   -0.186034     -0.0334569   0.0738882   0.184013     0.213112    0.0705548  -0.0785082   -0.0496696     0.0580144    -0.0782286  -0.0553378   -0.0995985   -0.0961098    0.182681     0.253468    -0.342353
 -0.34372      0.167927     0.0540344   -0.0645094  -0.0199789   0.145596      0.190214    -0.10544       0.0792941    0.052671   -0.00555498   -0.139715   -0.171695   -0.193018    -0.285765    0.0537945   0.305127    -0.142808      0.071343     -0.282289   -0.0417957   -0.00549226  -0.169412    -0.0258876    0.00274349   0.456637
  0.0784294   -0.0954821   -0.166371    -0.185106    0.294476    0.000481126  -0.01222     -0.226567     -0.0314429   -0.118867    0.175003      0.18458    -0.302305    0.145471     0.0750396   0.109728   -0.166659     0.225198     -0.231099      0.236088   -0.10596     -0.139492     0.161927     0.274196    -0.0657961    0.0628433
  0.243626    -0.182683    -0.047501     0.22035    -0.015652   -0.135793      0.0397841    0.405972     -0.254473     0.155057    0.0240884    -0.0516756   0.210307   -0.126376     0.0777081  -0.14789     0.0335024    0.114412      0.0690699     0.1523      0.119895     0.164399     0.0758713   -0.171265    -0.041397    -0.0252105
 -0.517956     0.343283     0.21761     -0.564081    0.192813   -0.146315      0.102384    -0.419867      0.539072     0.377731   -0.33555       0.285867    0.156824    0.375728    -0.690869   -0.102423    0.3878       0.169494      0.677083     -0.113202   -0.105169     0.05045      0.0014637    0.0191426   -0.182       -0.106856
  0.384865     0.267524    -0.127702    -0.7292      0.0509724   0.837529     -0.0254922   -0.429712      0.160575     0.391278    0.344339      0.44301    -0.232709    0.15187     -0.625234   -0.439736    0.148105     0.490472     -0.491786     -0.580114   -0.192227    -0.110434    -0.00278257   0.248374     0.116793    -0.275968
 -0.238225    -0.0398336    0.00543609  -0.336401   -0.111652    0.549577     -0.148199    -0.317331      0.169436     0.179184    0.234472      0.0721571  -0.0511268  -0.048008     0.278923   -0.0419376  -0.264152    -0.0126441     0.397683      0.35802    -0.387431    -0.194193    -0.00230327  -0.68279      0.00589327  -0.508526
 -0.280951    -0.717126     0.497962     0.326631   -0.187645    0.0494448     0.4072      -0.185206      0.342267     0.314311    0.323368      0.342544    0.0611067   0.141523    -0.13681    -0.0728389  -0.0699044   -0.0803593     0.000304261   0.675254   -0.0225302   -0.253926     0.00137587   0.328162     0.0608612   -0.332698
 -0.276418     0.0806819    0.390458     0.913668   -0.458644   -0.169056     -0.420595    -0.0733931    -0.568306     0.120183   -0.521081     -0.494931    0.234628   -0.286236    -0.173691   -0.0307827  -0.035907     0.0219072    -0.0529826    -0.318282   -0.192913     0.128589    -0.092105     0.0609389    0.0272108   -0.310192
  0.0855923    0.258025     0.303857     0.217096   -0.843318    0.505196      0.0590955    0.67476       0.134147     0.354694   -0.107267      0.106369    0.330942   -0.279033    -0.447229   -0.0685856   0.587537    -0.0194116     0.242796     -0.689692    0.234469    -0.152449    -0.157282    -0.385113    -0.0257212    0.191985
 -0.0701558   -0.65361     -0.140898     0.187322    0.539703    0.0249247     0.735427     0.488077      0.0952062   -0.304185    0.241678      0.0570286   0.182794   -0.0138612   -0.425425    0.19032     0.089777     0.18158       0.053746      0.109672    0.235771     0.244318     0.0832898   -0.19701     -0.0505186    0.321783
 -0.166368     0.224005    -0.0246992    0.40604     0.464863    0.426455      0.518572     0.223577     -0.103498     0.655196   -0.221086      0.357917    0.0694884  -0.31223     -0.992231    0.186814    0.00300557  -0.176507     -0.0518048     0.112354   -0.382252     0.217625     0.158052    -0.250075     0.663993     0.256728
  0.227915    -0.191517    -0.0402998   -0.0885487   0.171112   -0.399929     -0.82452      0.0517643    -0.277624    -0.30279     0.230276     -0.119565   -0.52336     0.0144386    0.609792   -0.333701    0.183356    -0.127627      0.160386      0.0986307  -0.201866    -0.317864    -0.128479     0.178204    -0.432655     0.0898169
 -0.304917     0.163165    -0.0508882    0.515983   -0.0381342  -0.157116      0.485743     0.191545     -0.370994    -0.335919   -0.000676647  -0.227763   -0.311774   -0.0770946    0.651152    0.227679    0.236699    -0.0812948    -0.290969      0.220702    0.545056     0.0132279   -0.202001    -0.027984    -0.139574     0.751942
  0.250451     0.372936    -0.0319523   -0.461388   -0.127414    0.108744      0.362676    -0.0721274    -0.171862     0.232262   -0.346202      0.208651   -0.0766228  -0.00377099   0.377695   -0.248152   -0.483099    -0.1762       -0.392891      0.193768    0.0603166    0.288387    -0.0443868   -0.383852     0.00413838   0.0673038
  0.24594      0.509419    -0.435304    -0.303304    0.130265   -0.384564     -0.0631835    0.000560031  -0.427165     0.0433927  -0.230026     -0.470241    0.0787648  -0.0493398    0.271002   -0.174019   -0.360904     0.430349     -0.246027     -0.291957   -0.0848235    0.805718    -0.170772     0.349155     0.055384    -0.129487
 -0.230679     0.185456     0.604062    -0.282731   -0.15327     0.542986     -0.278911    -0.143995     -0.127604     0.220587   -0.0342936    -0.147073   -1.08458     0.267409     0.0611378  -0.368588   -0.236783    -0.163089      0.00511332    0.0860894  -0.307432    -0.459829     0.218851     0.172096    -0.285356     0.603364
  0.202043     0.54547      0.518515    -0.239754   -0.250684    0.596493      0.139527     0.223732      0.045987     0.16408    -0.0405849     0.203951   -0.0246462   1.01085     -0.356187    0.340818   -0.0788815    0.295723     -0.197456      0.0598295  -0.922474    -0.232531     0.594932    -0.293393    -0.221536     0.362121
  0.770228    -0.593061     0.124278     0.172339    0.153484   -0.266464     -0.185511    -0.365688     -0.481096     0.381937    0.513642     -0.0143081  -0.0509302   0.748421    -0.571014    0.198059   -0.61237      0.786642      0.166754      0.389121    0.0299105   -0.632369     0.250572     0.408668    -0.224779     0.418523
  0.688932    -0.235772    -0.162333    -0.0717482  -0.359731    0.00267392   -0.100194     0.547799     -0.747112    -0.642994   -0.0610384     0.603652   -0.358533    0.447481     0.442786    0.143892   -0.195489    -0.000390086  -0.404789      0.185347    0.609203    -0.593021     0.172039     0.0128689    0.252251     0.221809
 -0.37043      0.498493    -0.365871    -0.0682382  -0.0970399  -0.557867     -0.76967     -0.251161      0.00281868  -0.2319      0.485775     -0.362231    0.337092   -0.148923     0.548638    0.257218    0.450518     0.279321     -0.21496      -0.0238204   0.52233     -0.404724    -0.277468     0.550998    -0.424709    -0.165635
 -0.00120689  -0.0759069   -0.208385    -0.235413    0.260051   -0.521872     -0.0125      -0.222732      0.851417    -0.834111   -0.215527      0.411635    0.348814   -0.407589     0.238793   -0.0513324   0.20456      0.663524     -0.492105     -0.395839   -0.162216    -0.0189829   -0.502954     0.790237     0.466689    -0.257591
 -0.479824     0.0217658   -0.121117     0.297726    0.647499   -1.1433       -0.301179     0.0619968    -0.0370347   -0.184174   -0.418585     -0.0873164  -0.293066    0.289058     0.921825    0.357413   -0.750353    -0.489913      0.378809      0.907924   -0.345441     0.339027     0.192117    -0.00552059   0.623681    -0.0127055
 -0.445585     0.0394392   -0.208354    -0.012479    0.75015     0.191021      0.0311893   -0.765791      0.375366    -0.44806     0.387635      0.0501695  -0.281307    0.7129       0.430427    0.689274   -0.346728     0.131533     -0.0311626     0.631367   -0.11969     -0.285787     0.430974     0.661573     0.104509     0.0205048
  0.0864897    0.168081    -0.122772     0.0954216  -0.329609   -0.0447688     0.301364     0.146383     -0.597393     0.540609   -0.0480021    -0.305442    0.0648832   0.0218692   -0.27581     0.0873633   0.0836457   -0.0356494     0.0830074     0.0980459   9.69589e-5   0.46674     -0.00873174  -0.947157    -1.00791      0.334682
 -0.0437593    0.0258119    0.023031    -0.287453    0.0505983  -0.13085      -0.00351982  -0.379677     -0.128271    -0.018038    0.0410267    -0.0136705  -0.181337   -0.00635725   0.236133   -0.187116   -0.263205     0.22782      -0.265099      0.34606    -0.0153649    0.0379627   -0.120575     0.295397    -0.150476    -0.214251
 -0.843435     0.0511109   -0.0681667   -0.0634768   0.346461    0.636759      0.0537678   -0.52055       0.198931    -0.427175    0.14721      -0.346802   -0.388388   -0.029112     0.0461483  -0.336005    0.0893191   -0.282514      0.674263     -0.821204    0.0286621   -0.160697     0.13786      0.0623314    0.809915    -0.242218
 -0.310006     0.146823    -0.0521287    0.517203    0.202169    0.131779     -0.174708     0.317255      0.148216    -0.0605442  -0.304509      0.114025    0.382931    0.0148562   -0.416018    0.173387    0.539534    -0.0807087     0.642159     -0.530517   -0.383073     0.201058    -0.29148     -0.386273     0.540025    -0.0240282
 -0.0663872   -0.473274    -0.761073     0.105873    0.251492   -0.610037      0.0578348   -0.355198      0.110781     0.390826   -0.0618294    -0.336035    0.544457   -1.14685     -0.0116653   0.318431   -0.247408    -0.423242      0.852381      0.34936     0.629088     0.521584     0.0358505   -0.187393     0.361109    -0.0756261
  0.0660594   -0.12693     -0.301817    -0.745769   -0.32167    -0.12382      -0.0444419    0.704331     -0.381489    -0.401095   -0.648892     -0.249715    0.100484   -1.12591     -0.15394    -0.40262     0.0229409   -0.466396     -0.0408046    -0.938474    0.0717343    0.476729    -0.166402    -0.750269     0.43056      0.243473
  0.618646    -0.253204     0.24182      0.144923    0.216921   -0.491403     -0.460025     0.406877     -0.0736583    0.458646   -0.245755      0.798409    0.203242   -0.372111    -0.332004   -0.33501     0.490252    -0.234592     -0.131225      0.329725    0.304456     0.268447    -0.0420958    0.300157    -0.431584    -0.147743
  0.427502     0.00829211  -0.141493     0.0803997  -0.286826   -0.551474      0.662228     0.514391      0.431179     0.339708   -0.156783     -0.0324652   0.730425   -0.419588     0.20694     0.122656   -0.114039    -0.0633674    -0.261579      0.0223626   0.74892     -0.353837     0.0898392   -0.109315     0.963373    -0.427576[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.404518
[ Info: iteration 2, average log likelihood -1.404507
[ Info: iteration 3, average log likelihood -1.404495
[ Info: iteration 4, average log likelihood -1.404484
[ Info: iteration 5, average log likelihood -1.404473
[ Info: iteration 6, average log likelihood -1.404461
[ Info: iteration 7, average log likelihood -1.404450
[ Info: iteration 8, average log likelihood -1.404440
[ Info: iteration 9, average log likelihood -1.404429
[ Info: iteration 10, average log likelihood -1.404418
┌ Info: EM with 100000 data points 10 iterations avll -1.404418
└ 59.0 data points per parameter
kind full, method kmeans
[ Info: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.840828e+05
      1       6.933381e+05      -1.907447e+05 |       32
      2       6.815585e+05      -1.177960e+04 |       32
      3       6.764539e+05      -5.104624e+03 |       32
      4       6.737191e+05      -2.734743e+03 |       32
      5       6.719417e+05      -1.777490e+03 |       32
      6       6.707187e+05      -1.222944e+03 |       32
      7       6.698041e+05      -9.146153e+02 |       32
      8       6.690562e+05      -7.479217e+02 |       32
      9       6.684659e+05      -5.902690e+02 |       32
     10       6.680089e+05      -4.570069e+02 |       32
     11       6.676442e+05      -3.647142e+02 |       32
     12       6.673343e+05      -3.098725e+02 |       32
     13       6.670683e+05      -2.660581e+02 |       32
     14       6.668090e+05      -2.592187e+02 |       32
     15       6.665706e+05      -2.384079e+02 |       32
     16       6.663660e+05      -2.046186e+02 |       32
     17       6.661706e+05      -1.954086e+02 |       32
     18       6.659961e+05      -1.745225e+02 |       32
     19       6.658387e+05      -1.573447e+02 |       32
     20       6.657061e+05      -1.326842e+02 |       32
     21       6.655820e+05      -1.240526e+02 |       32
     22       6.654651e+05      -1.169078e+02 |       32
     23       6.653638e+05      -1.012425e+02 |       32
     24       6.652710e+05      -9.289530e+01 |       32
     25       6.651769e+05      -9.403463e+01 |       32
     26       6.650861e+05      -9.078463e+01 |       32
     27       6.650050e+05      -8.117855e+01 |       32
     28       6.649369e+05      -6.806280e+01 |       32
     29       6.648748e+05      -6.210266e+01 |       32
     30       6.648215e+05      -5.328140e+01 |       32
     31       6.647744e+05      -4.713498e+01 |       32
     32       6.647293e+05      -4.502495e+01 |       32
     33       6.646847e+05      -4.460407e+01 |       32
     34       6.646424e+05      -4.231726e+01 |       32
     35       6.646026e+05      -3.985798e+01 |       32
     36       6.645694e+05      -3.313303e+01 |       32
     37       6.645416e+05      -2.787381e+01 |       32
     38       6.645149e+05      -2.664510e+01 |       32
     39       6.644899e+05      -2.500393e+01 |       32
     40       6.644634e+05      -2.654273e+01 |       32
     41       6.644325e+05      -3.088152e+01 |       32
     42       6.644030e+05      -2.951119e+01 |       32
     43       6.643812e+05      -2.173976e+01 |       32
     44       6.643619e+05      -1.933686e+01 |       32
     45       6.643445e+05      -1.744922e+01 |       32
     46       6.643301e+05      -1.430333e+01 |       32
     47       6.643176e+05      -1.250071e+01 |       32
     48       6.643015e+05      -1.615925e+01 |       32
     49       6.642849e+05      -1.658150e+01 |       32
     50       6.642700e+05      -1.491462e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 664269.9925537985)
┌ Info: K-means with 32000 data points using 50 iterations
└ 37.0 data points per parameter
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.416832
[ Info: iteration 2, average log likelihood -1.411698
[ Info: iteration 3, average log likelihood -1.410307
[ Info: iteration 4, average log likelihood -1.409254
[ Info: iteration 5, average log likelihood -1.408140
[ Info: iteration 6, average log likelihood -1.407123
[ Info: iteration 7, average log likelihood -1.406431
[ Info: iteration 8, average log likelihood -1.406042
[ Info: iteration 9, average log likelihood -1.405817
[ Info: iteration 10, average log likelihood -1.405667
[ Info: iteration 11, average log likelihood -1.405554
[ Info: iteration 12, average log likelihood -1.405461
[ Info: iteration 13, average log likelihood -1.405382
[ Info: iteration 14, average log likelihood -1.405312
[ Info: iteration 15, average log likelihood -1.405250
[ Info: iteration 16, average log likelihood -1.405193
[ Info: iteration 17, average log likelihood -1.405142
[ Info: iteration 18, average log likelihood -1.405094
[ Info: iteration 19, average log likelihood -1.405051
[ Info: iteration 20, average log likelihood -1.405010
[ Info: iteration 21, average log likelihood -1.404971
[ Info: iteration 22, average log likelihood -1.404935
[ Info: iteration 23, average log likelihood -1.404901
[ Info: iteration 24, average log likelihood -1.404869
[ Info: iteration 25, average log likelihood -1.404839
[ Info: iteration 26, average log likelihood -1.404810
[ Info: iteration 27, average log likelihood -1.404782
[ Info: iteration 28, average log likelihood -1.404756
[ Info: iteration 29, average log likelihood -1.404731
[ Info: iteration 30, average log likelihood -1.404707
[ Info: iteration 31, average log likelihood -1.404684
[ Info: iteration 32, average log likelihood -1.404663
[ Info: iteration 33, average log likelihood -1.404642
[ Info: iteration 34, average log likelihood -1.404622
[ Info: iteration 35, average log likelihood -1.404603
[ Info: iteration 36, average log likelihood -1.404585
[ Info: iteration 37, average log likelihood -1.404568
[ Info: iteration 38, average log likelihood -1.404551
[ Info: iteration 39, average log likelihood -1.404536
[ Info: iteration 40, average log likelihood -1.404520
[ Info: iteration 41, average log likelihood -1.404506
[ Info: iteration 42, average log likelihood -1.404491
[ Info: iteration 43, average log likelihood -1.404478
[ Info: iteration 44, average log likelihood -1.404464
[ Info: iteration 45, average log likelihood -1.404452
[ Info: iteration 46, average log likelihood -1.404439
[ Info: iteration 47, average log likelihood -1.404427
[ Info: iteration 48, average log likelihood -1.404415
[ Info: iteration 49, average log likelihood -1.404404
[ Info: iteration 50, average log likelihood -1.404393
┌ Info: EM with 100000 data points 50 iterations avll -1.404393
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.341152   -0.429511   -0.395654     0.469915     0.0745363   -0.645619    0.325731   -0.11134    -0.0139502   0.272585      0.0985663  -0.507459    0.513867   -0.743835   -0.0723055   0.375237    -0.0070229  -0.245729     0.632746     0.319894     0.418596    0.551414   -0.0154529   -0.204709    0.183488     0.174421
 -0.040859    0.038095   -0.157314    -0.345948     0.279792    -1.00374    -0.15057    -0.149361   -0.648891   -0.48631      -0.447173   -0.353258   -0.160101   -0.0349403   0.777638    0.0128565   -0.576079    0.105544    -0.547612     0.506328     0.357211    0.157508   -0.269162     0.552624    6.73416e-5  -0.0790458
 -0.859903    0.0350296  -0.153702    -0.23354      0.297287     0.464153   -0.18909    -0.365764    0.443915   -0.684451     -0.0077423  -0.109144   -0.0811194  -0.111625    0.423075   -0.629392     0.117854   -0.311713     0.508828    -1.03853      0.175777   -0.0889813   0.05118      0.148556    0.829221    -0.36349
  0.346216   -0.108462   -0.166094    -0.302377     0.372755    -0.467159   -0.79       -0.165646   -0.0396369   0.336587     -0.498731    0.238557    0.486124   -0.425493   -0.122499   -0.179481    -0.193291   -0.330106     0.672116     0.365829    -0.100664    0.347848    0.355705    -0.316609    0.133687    -0.755719
 -0.289387   -0.0606817  -0.419171    -0.0840279    0.967046    -0.1514      0.139363   -0.380969    0.408875   -0.504924      0.351749    0.0254914  -0.429048    0.63829     0.689331    0.675086    -0.495711    0.083695     0.012943     0.745834    -0.0675245  -0.0904322   0.50501      0.45325     0.29785      0.187854
  0.114251    0.382369   -0.0987926    0.634904    -0.483671     0.156056    0.0907028   0.727604   -0.23475    -0.209857     -0.0792379   0.211745    0.183222    0.216641    0.0239625   0.4429       0.882197    0.120015    -0.0253618   -0.228261     0.618482   -0.197346   -0.0638036   -0.2767      0.0982473    0.643422
  0.566424   -0.407017    0.0787871    0.146804    -0.00454784  -0.42345     0.19926     0.194143   -0.566153    0.182426      0.0641432   0.198479   -0.10596    -0.0634549   0.320862   -0.451793    -0.37486     0.00355783  -0.553277     0.409006     0.284063    0.0515925  -0.0622043    0.0588834  -0.233286     0.125066
  0.0735657   0.0423084  -0.224472    -0.00767644   0.107293    -0.324103   -0.966687    0.104897   -0.282214   -0.538969      0.613633   -0.130728   -0.378361   -0.083844    0.692864    0.0048702    0.408392    0.0127344    0.0354563    0.154054     0.0647682  -0.601522   -0.0786285    0.316435   -0.401382     0.0683915
 -0.10413    -0.474252    0.486503     0.171396     0.0643503    0.154809   -0.154703   -0.440632    0.582648    0.221034      0.273918    0.4026     -0.0667947   0.329441    0.0753542  -0.0577484   -0.106438    0.0137635   -0.152172     0.691114    -0.0812871  -0.801879    0.00395355   0.60079    -0.00621671  -0.368698
 -0.385946   -0.320092    0.161417     0.709144     0.275611     0.777607    0.29704     0.0388777   0.0598682   0.0785991    -0.0267525   0.048958   -0.205823   -0.0997083  -0.855142    0.22349      0.374562   -0.189978     0.580018    -0.315941    -0.45935    -0.0288342  -0.144866    -0.404382    0.606472    -0.0387986
  0.207857   -0.108083    0.0618167    0.376632     0.805056    -0.639715   -0.110362    0.615173    0.0376142   0.280338     -0.334236    0.330339   -0.112414   -0.364548   -0.550636    0.0270791    0.743456   -0.37432      0.097989     0.0145986    0.212275    0.273602   -0.165945     0.0857751  -0.393811     0.525695
  0.315621   -0.0139648   0.381372     0.0302122   -0.736764     0.163832   -0.155241    0.297177   -0.211809    0.650032     -0.140746    0.133734    0.259971   -0.574674   -0.589753   -0.81203      0.499267    0.0534829    0.0519328   -0.576607    -0.103791    0.213347   -0.379237    -0.412699   -0.293102    -0.367628
 -0.564332   -0.019883    0.00721705   0.0174546   -0.122877    -0.0747357  -0.223311   -0.412128    0.0272151   0.0590404     0.063488   -0.163354   -0.370254    0.0793511   0.782232    0.0528702   -0.352127   -0.253543     0.474754     0.328306    -0.245314   -0.0609089  -0.343237    -0.391271   -0.022185    -0.343224
 -0.412347    0.36804     0.293234     0.307006    -0.245836     0.605047    0.272153    0.129319   -0.405098   -0.145937      0.0770643  -0.6224     -0.533635    0.456596    0.352276   -0.00031899  -0.0643989   0.22766     -0.448583     0.00882524  -0.119822   -0.0180859   0.0939502   -0.128201   -0.723691     0.705051
  0.606934    0.194866    0.496567    -0.0998195   -0.0851894    0.300321   -0.157281    0.328771   -0.119829    0.45303      -0.0974288   0.563976   -0.1581      0.96317    -0.230869    0.144203    -0.170755    0.0876079   -0.468384     0.325963    -1.02656     0.0539057   0.619945    -0.204892   -0.346449     0.32138
 -0.191368    0.109606   -0.228409     0.0447663    0.101471     0.126487    0.760868    0.200532    0.146636    0.187288     -0.366514   -0.204421    0.448949   -0.649951   -0.0332062  -0.259765    -0.216883   -0.147325    -0.445226    -0.414354     0.202651    0.521023   -0.201775    -0.139163    0.596428    -0.049797
  0.400667    0.106218   -0.131362    -0.754499     0.233152     0.475649    0.0428466  -0.435225    0.136041    0.172519      0.357316    0.388567   -0.379362    0.341945   -0.411201   -0.19961      0.0419769   0.479442    -0.360793    -0.353718    -0.14487    -0.216949   -0.0463141    0.323638    0.0083477   -0.0101152
  0.662991   -0.109903   -0.0400814    0.0350557   -0.391088    -0.52205     0.68338     0.532574    0.449554    0.257854     -0.140942    0.0837694   0.613625   -0.216431    0.120528    0.34362     -0.0781925  -0.0826813   -0.221938     0.144425     0.615692   -0.569692    0.159185     0.0271674   1.09044     -0.45119
 -0.255864    0.98392    -0.466818     0.00376573   0.496544     0.475055   -0.0319142  -0.152949   -0.410416    0.323196     -0.191658    0.0216704  -0.102789   -0.205883   -0.767143    0.178448    -0.386336    0.324126    -0.140391    -0.149385    -0.494184    0.410069    0.297335    -0.0199737   0.485557     0.104781
 -0.0588957   0.111421    0.069148    -0.0842721   -0.0426597    0.122383    0.0511252  -0.200495   -0.166955    0.144401      0.0253909   0.0609983  -0.116249    0.117178   -0.0525345  -0.05687     -0.0691232   0.08835      0.0198571    0.0913422   -0.151304    0.0727454  -0.0851512   -0.0750792  -0.130857    -0.0394606
 -0.10488     0.236705    0.342571     0.768725    -0.417368    -0.144557   -0.5303      0.0732552  -0.515413    0.148688     -0.628407   -0.442311    0.216909   -0.0792427  -0.0705979   0.0991921   -0.0862023   0.0892224   -0.158476    -0.346221    -0.118488   -0.0144246  -0.0634856    0.220199    0.177346    -0.385142
  0.221634   -0.0873111  -0.145579     0.0453132    0.150036    -0.0502909  -0.0661868   0.166457   -0.212417    0.000953949   0.084609   -0.0864223  -0.029269    0.0955218   0.164107    0.00049007  -0.116196    0.194486     0.0251959    0.130101    -0.0895211   0.120367    0.126352    -0.0313256   0.0137018   -0.0688072
 -0.866411    0.11994     0.00876474   0.0666073    0.395971     0.0850602  -0.0772776  -0.704471    0.0302671  -0.510322     -0.0483095  -0.0642777  -0.126554    0.381811   -0.0990491   0.263834     0.175463    0.0379197    0.161316     0.102564    -0.177527    0.260915   -0.0118483    0.542377    0.0334894   -0.0310801
  0.0181253  -0.0824109  -0.2599      -0.197658     0.24884     -0.644314   -0.0404084  -0.204631    0.893235   -0.870333     -0.131083    0.455291    0.422367   -0.457971    0.232328    0.0817557    0.294164    0.796183    -0.52769     -0.402161    -0.210504   -0.0544998  -0.578402     0.798614    0.402999    -0.311959
  0.0387256   0.547578   -0.0296505   -0.50648     -0.251257     0.36907     0.194486   -0.227416   -0.177609   -0.0706301    -0.331265    0.515505   -0.296296    0.0299387   0.363416    0.0736534   -0.287197   -0.0990971   -0.394732     0.53772      0.124221   -0.142316    0.190533    -0.367725   -0.0504303    0.15496
 -0.206239    0.0942346   0.152533     0.00292655  -0.100057     0.0129175   0.0325728  -0.0334306   0.110497    0.0803987    -0.0738844   0.0340201  -0.0302615  -0.175261   -0.126527   -0.0345488    0.190086   -0.114422     0.00956226  -0.133561     0.0601626  -0.104605   -0.133173     0.0535759   0.0214218    0.190777
 -0.171164   -0.673548   -0.0041276    0.0708353    0.105304     0.231138    0.772319    0.237814    0.162316    0.139542      0.377059    0.408518    0.298426   -0.0765342  -0.284905    0.0455881   -0.0619437   0.108182     0.0813194    0.489865     0.219403    0.182055    0.14683     -0.331339    0.0409642   -0.0435407
  0.273279    0.0216369  -0.187608    -0.61063     -0.309183    -0.0191888  -0.228981    0.661332   -0.528481   -0.417753     -0.577914   -0.282023   -0.13326    -0.814056   -0.11148    -0.27535     -0.0567716  -0.373091     0.110535    -0.976173     0.0269099   0.316805   -0.234694    -0.699842    0.170592     0.543375
  0.244674   -0.880243    0.196459     0.254733    -0.0583108   -0.318485   -0.307534   -0.287873   -0.513687   -0.100489      0.458016    0.0516096  -0.0670662   0.705203   -0.566214    0.232384    -0.370055    0.403525     0.621057     0.267387    -0.134698   -0.546927    0.385273     0.488973   -0.326105     0.37093
  0.0361876   0.640405   -0.40465     -0.482412    -0.190722    -0.558174   -0.19774    -0.163244    0.0925678   0.126773     -0.0138724  -0.407496    0.311006   -0.15915     0.572694   -0.248581     0.0855943   0.285471    -0.132597    -0.282503     0.209516    0.500297   -0.296716     0.305233   -0.30393     -0.100572
 -0.646754    0.508387    0.363844    -0.3497      -0.0660384    0.114743    0.334479   -0.0788171   0.656835    0.530754     -0.368318    0.343281    0.199496    0.63024    -0.693657   -0.0152907    0.547095    0.243734     0.771015    -0.133151    -0.187979    0.0488923  -0.0497536   -0.08069    -0.230033    -0.158264
 -0.2111      0.297677    0.405256    -0.546783    -0.209919     0.692473    0.0703669  -0.169467    0.32403     0.205454      0.204268   -0.453707   -0.404888    0.178281   -0.251713   -0.00223186  -0.317981   -0.151137     0.496303    -0.214961    -0.407955   -0.567886    0.395365    -0.21016     0.233152     0.264121[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.404382
[ Info: iteration 2, average log likelihood -1.404371
[ Info: iteration 3, average log likelihood -1.404360
[ Info: iteration 4, average log likelihood -1.404350
[ Info: iteration 5, average log likelihood -1.404340
[ Info: iteration 6, average log likelihood -1.404330
[ Info: iteration 7, average log likelihood -1.404321
[ Info: iteration 8, average log likelihood -1.404311
[ Info: iteration 9, average log likelihood -1.404302
[ Info: iteration 10, average log likelihood -1.404293
┌ Info: EM with 100000 data points 10 iterations avll -1.404293
└ 59.0 data points per parameter
[ Info: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.678561e+05
      1       2.230230e+04      -1.455538e+05 |        2
      2       7.823675e+03      -1.447862e+04 |        0
      3       7.823675e+03       0.000000e+00 |        0
K-means converged with 3 iterations (objv = 7823.67549422947)
┌ Info: K-means with 900 data points using 3 iterations
└ 150.0 data points per parameter
[ Info: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
[ Info: iteration 1, average log likelihood -2.043155
[ Info: iteration 2, average log likelihood -2.043154
[ Info: iteration 3, average log likelihood -2.043154
[ Info: iteration 4, average log likelihood -2.043154
[ Info: iteration 5, average log likelihood -2.043154
[ Info: iteration 6, average log likelihood -2.043154
[ Info: iteration 7, average log likelihood -2.043154
[ Info: iteration 8, average log likelihood -2.043154
[ Info: iteration 9, average log likelihood -2.043154
[ Info: iteration 10, average log likelihood -2.043154
┌ Info: EM with 900 data points 10 iterations avll -2.043154
└ 81.8 data points per parameter
    Testing GaussianMixtures tests passed 
