Julia Version 1.6.0-DEV.2
Commit 4c3f57fb55 (2020-05-08 16:58 UTC)
Platform Info:
  OS: Linux (x86_64-linux-gnu)
  CPU: Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-9.0.1 (ORCJIT, skylake)
Environment:
  JULIA_DEPOT_PATH = ::/usr/local/share/julia
  JULIA_NUM_THREADS = 2

  Resolving package versions...
  Installed CodecZlib ──────────────────── v0.7.0
  Installed NaNMath ────────────────────── v0.3.3
  Installed CompilerSupportLibraries_jll ─ v0.3.3+0
  Installed CUDAapi ────────────────────── v4.0.0
  Installed DataAPI ────────────────────── v1.3.0
  Installed DataStructures ─────────────── v0.17.15
  Installed StaticArrays ───────────────── v0.12.3
  Installed GeometricFlux ──────────────── v0.5.0
  Installed AbstractFFTs ───────────────── v0.5.0
  Installed Zlib_jll ───────────────────── v1.2.11+9
  Installed Adapt ──────────────────────── v1.0.1
  Installed ColorTypes ─────────────────── v0.10.3
  Installed GPUCompiler ────────────────── v0.2.0
  Installed Juno ───────────────────────── v0.8.1
  Installed ArrayLayouts ───────────────── v0.2.6
  Installed OrderedCollections ─────────── v1.2.0
  Installed StatsBase ──────────────────── v0.33.0
  Installed TranscodingStreams ─────────── v0.9.5
  Installed CUDAdrv ────────────────────── v6.3.0
  Installed ZipFile ────────────────────── v0.9.1
  Installed Reexport ───────────────────── v0.2.0
  Installed DiffResults ────────────────── v1.0.2
  Installed CommonSubexpressions ───────── v0.2.0
  Installed SpecialFunctions ───────────── v0.10.0
  Installed MacroTools ─────────────────── v0.5.5
  Installed Flux ───────────────────────── v0.10.4
  Installed Missings ───────────────────── v0.4.3
  Installed TimerOutputs ───────────────── v0.5.5
  Installed ExprTools ──────────────────── v0.1.1
  Installed IRTools ────────────────────── v0.3.2
  Installed Colors ─────────────────────── v0.12.0
  Installed NNlib ──────────────────────── v0.6.6
  Installed ForwardDiff ────────────────── v0.10.10
  Installed CUDAnative ─────────────────── v3.1.0
  Installed SortingAlgorithms ──────────── v0.3.1
  Installed ZygoteRules ────────────────── v0.2.0
  Installed CEnum ──────────────────────── v0.3.0
  Installed Cthulhu ────────────────────── v1.0.2
  Installed FillArrays ─────────────────── v0.8.8
  Installed OpenSpecFun_jll ────────────── v0.5.3+3
  Installed AbstractTrees ──────────────── v0.3.3
  Installed Zygote ─────────────────────── v0.4.20
  Installed LLVM ───────────────────────── v1.4.1
  Installed CuArrays ───────────────────── v2.2.0
  Installed Media ──────────────────────── v0.5.0
  Installed CodeTracking ───────────────── v0.5.11
  Installed FixedPointNumbers ──────────── v0.8.0
  Installed DiffRules ──────────────────── v1.0.1
  Installed BinaryProvider ─────────────── v0.5.9
  Installed Requires ───────────────────── v1.0.1
  Installed GPUArrays ──────────────────── v3.3.0
Updating `~/.julia/environments/v1.6/Project.toml`
  [7e08b658] + GeometricFlux v0.5.0
Updating `~/.julia/environments/v1.6/Manifest.toml`
  [621f4979] + AbstractFFTs v0.5.0
  [1520ce14] + AbstractTrees v0.3.3
  [79e6a3ab] + Adapt v1.0.1
  [4c555306] + ArrayLayouts v0.2.6
  [b99e7846] + BinaryProvider v0.5.9
  [fa961155] + CEnum v0.3.0
  [3895d2a7] + CUDAapi v4.0.0
  [c5f51814] + CUDAdrv v6.3.0
  [be33ccc6] + CUDAnative v3.1.0
  [da1fd8a2] + CodeTracking v0.5.11
  [944b1d66] + CodecZlib v0.7.0
  [3da002f7] + ColorTypes v0.10.3
  [5ae59095] + Colors v0.12.0
  [bbf7d656] + CommonSubexpressions v0.2.0
  [e66e0078] + CompilerSupportLibraries_jll v0.3.3+0
  [f68482b8] + Cthulhu v1.0.2
  [3a865a2d] + CuArrays v2.2.0
  [9a962f9c] + DataAPI v1.3.0
  [864edb3b] + DataStructures v0.17.15
  [163ba53b] + DiffResults v1.0.2
  [b552c78f] + DiffRules v1.0.1
  [e2ba6199] + ExprTools v0.1.1
  [1a297f60] + FillArrays v0.8.8
  [53c48c17] + FixedPointNumbers v0.8.0
  [587475ba] + Flux v0.10.4
  [f6369f11] + ForwardDiff v0.10.10
  [0c68f7d7] + GPUArrays v3.3.0
  [61eb1bfa] + GPUCompiler v0.2.0
  [7e08b658] + GeometricFlux v0.5.0
  [7869d1d1] + IRTools v0.3.2
  [e5e0dc1b] + Juno v0.8.1
  [929cbde3] + LLVM v1.4.1
  [1914dd2f] + MacroTools v0.5.5
  [e89f7d12] + Media v0.5.0
  [e1d29d7a] + Missings v0.4.3
  [872c559c] + NNlib v0.6.6
  [77ba4419] + NaNMath v0.3.3
  [efe28fd5] + OpenSpecFun_jll v0.5.3+3
  [bac558e1] + OrderedCollections v1.2.0
  [189a3867] + Reexport v0.2.0
  [ae029012] + Requires v1.0.1
  [a2af1166] + SortingAlgorithms v0.3.1
  [276daf66] + SpecialFunctions v0.10.0
  [90137ffa] + StaticArrays v0.12.3
  [2913bbd2] + StatsBase v0.33.0
  [a759f4b9] + TimerOutputs v0.5.5
  [3bb67fe8] + TranscodingStreams v0.9.5
  [a5390f91] + ZipFile v0.9.1
  [83775a58] + Zlib_jll v1.2.11+9
  [e88e6eb3] + Zygote v0.4.20
  [700de1a5] + ZygoteRules v0.2.0
  [2a0f44e3] + Base64
  [ade2ca70] + Dates
  [8bb1440f] + DelimitedFiles
  [8ba89e20] + Distributed
  [9fa8497b] + Future
  [b77e0a4c] + InteractiveUtils
  [76f85450] + LibGit2
  [8f399da3] + Libdl
  [37e2e46d] + LinearAlgebra
  [56ddb016] + Logging
  [d6f4376e] + Markdown
  [a63ad114] + Mmap
  [44cfe95a] + Pkg
  [de0858da] + Printf
  [9abbd945] + Profile
  [3fa0cd96] + REPL
  [9a3f8284] + Random
  [ea8e919c] + SHA
  [9e88b42a] + Serialization
  [6462fe0b] + Sockets
  [2f01184e] + SparseArrays
  [10745b16] + Statistics
  [8dfed614] + Test
  [cf7118a7] + UUIDs
  [4ec0a83e] + Unicode
   Building NNlib → `~/.julia/packages/NNlib/FAI3o/deps/build.log`
    Testing GeometricFlux
Status `/tmp/jl_yjGG6P/Project.toml`
  [3895d2a7] CUDAapi v4.0.0
  [be33ccc6] CUDAnative v3.1.0
  [3a865a2d] CuArrays v2.2.0
  [864edb3b] DataStructures v0.17.15
  [1a297f60] FillArrays v0.8.8
  [587475ba] Flux v0.10.4
  [7e08b658] GeometricFlux v0.5.0
  [7869d1d1] IRTools v0.3.2
  [093fc24a] LightGraphs v1.3.2
  [626554b9] MetaGraphs v0.6.5
  [ae029012] Requires v1.0.1
  [47aef6b3] SimpleWeightedGraphs v1.1.1
  [90137ffa] StaticArrays v0.12.3
  [e88e6eb3] Zygote v0.4.20
  [700de1a5] ZygoteRules v0.2.0
  [37e2e46d] LinearAlgebra
  [9a3f8284] Random
  [2f01184e] SparseArrays
  [10745b16] Statistics
  [8dfed614] Test
Status `/tmp/jl_yjGG6P/Manifest.toml`
  [621f4979] AbstractFFTs v0.5.0
  [1520ce14] AbstractTrees v0.3.3
  [79e6a3ab] Adapt v1.0.1
  [ec485272] ArnoldiMethod v0.0.4
  [4c555306] ArrayLayouts v0.2.6
  [b99e7846] BinaryProvider v0.5.9
  [fa961155] CEnum v0.3.0
  [3895d2a7] CUDAapi v4.0.0
  [c5f51814] CUDAdrv v6.3.0
  [be33ccc6] CUDAnative v3.1.0
  [da1fd8a2] CodeTracking v0.5.11
  [944b1d66] CodecZlib v0.7.0
  [3da002f7] ColorTypes v0.10.3
  [5ae59095] Colors v0.12.0
  [bbf7d656] CommonSubexpressions v0.2.0
  [e66e0078] CompilerSupportLibraries_jll v0.3.3+0
  [f68482b8] Cthulhu v1.0.2
  [3a865a2d] CuArrays v2.2.0
  [9a962f9c] DataAPI v1.3.0
  [864edb3b] DataStructures v0.17.15
  [163ba53b] DiffResults v1.0.2
  [b552c78f] DiffRules v1.0.1
  [e2ba6199] ExprTools v0.1.1
  [5789e2e9] FileIO v1.3.0
  [1a297f60] FillArrays v0.8.8
  [53c48c17] FixedPointNumbers v0.8.0
  [587475ba] Flux v0.10.4
  [f6369f11] ForwardDiff v0.10.10
  [0c68f7d7] GPUArrays v3.3.0
  [61eb1bfa] GPUCompiler v0.2.0
  [7e08b658] GeometricFlux v0.5.0
  [7869d1d1] IRTools v0.3.2
  [d25df0c9] Inflate v0.1.2
  [033835bb] JLD2 v0.1.13
  [e5e0dc1b] Juno v0.8.1
  [929cbde3] LLVM v1.4.1
  [093fc24a] LightGraphs v1.3.2
  [1914dd2f] MacroTools v0.5.5
  [e89f7d12] Media v0.5.0
  [626554b9] MetaGraphs v0.6.5
  [e1d29d7a] Missings v0.4.3
  [872c559c] NNlib v0.6.6
  [77ba4419] NaNMath v0.3.3
  [efe28fd5] OpenSpecFun_jll v0.5.3+3
  [bac558e1] OrderedCollections v1.2.0
  [189a3867] Reexport v0.2.0
  [ae029012] Requires v1.0.1
  [699a6c99] SimpleTraits v0.9.2
  [47aef6b3] SimpleWeightedGraphs v1.1.1
  [a2af1166] SortingAlgorithms v0.3.1
  [276daf66] SpecialFunctions v0.10.0
  [90137ffa] StaticArrays v0.12.3
  [2913bbd2] StatsBase v0.33.0
  [a759f4b9] TimerOutputs v0.5.5
  [3bb67fe8] TranscodingStreams v0.9.5
  [a5390f91] ZipFile v0.9.1
  [83775a58] Zlib_jll v1.2.11+9
  [e88e6eb3] Zygote v0.4.20
  [700de1a5] ZygoteRules v0.2.0
  [2a0f44e3] Base64
  [ade2ca70] Dates
  [8bb1440f] DelimitedFiles
  [8ba89e20] Distributed
  [9fa8497b] Future
  [b77e0a4c] InteractiveUtils
  [76f85450] LibGit2
  [8f399da3] Libdl
  [37e2e46d] LinearAlgebra
  [56ddb016] Logging
  [d6f4376e] Markdown
  [a63ad114] Mmap
  [44cfe95a] Pkg
  [de0858da] Printf
  [9abbd945] Profile
  [3fa0cd96] REPL
  [9a3f8284] Random
  [ea8e919c] SHA
  [9e88b42a] Serialization
  [1a1011a3] SharedArrays
  [6462fe0b] Sockets
  [2f01184e] SparseArrays
  [10745b16] Statistics
  [8dfed614] Test
  [cf7118a7] UUIDs
  [4ec0a83e] Unicode
WARNING: could not import Compiler.just_construct_ssa into Wrap
┌ Warning: Package GeometricFlux does not have LightGraphs in its dependencies:
│ - If you have GeometricFlux checked out for development and have
│   added LightGraphs as a dependency but haven't updated your primary
│   environment's manifest file, try `Pkg.resolve()`.
│ - Otherwise you may need to report an issue with GeometricFlux
└ Loading LightGraphs into GeometricFlux from project dependency, future warnings for GeometricFlux are suppressed.
┌ Warning: Performing scalar operations on GPU arrays: This is very slow, consider disallowing these operations with `allowscalar(false)`
└ @ GPUArrays ~/.julia/packages/GPUArrays/OXvxB/src/host/indexing.jl:43
divpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:128
  Test threw exception
  Expression: divpool(CuArray{Int64}(cluster), T.(X)) ≈ T.(y)
  GPU compilation of kernel broadcast(CuArrays.CuKernelContext, CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global}, Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}) failed
  KernelError: passing and using non-bitstype argument
  
  Argument 4 to your kernel function is of type Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}, which is not isbits:
    .args is of type Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}} which is not isbits.
      .2 is of type Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}} which is not isbits.
        .x is of type Array{Float32,2} which is not isbits.
  
  
  Stacktrace:
   [1] check_invocation(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::LLVM.Function) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/validation.jl:68
   [2] macro expansion at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:184 [inlined]
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [4] codegen(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:183
   [5] compile(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:36
   [6] _cufunction(::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:308
   [7] _cufunction at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:302 [inlined]
   [8] #77 at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:21 [inlined]
   [9] get!(::GPUCompiler.var"#77#78"{Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CUDAnative._cufunction),GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}}, ::Dict{UInt64,Any}, ::UInt64) at ./dict.jl:450
   [10] macro expansion at ./lock.jl:183 [inlined]
   [11] check_cache(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:19
   [12] + at ./int.jl:86 [inlined]
   [13] hash_64_64 at ./hashing.jl:35 [inlined]
   [14] hash_uint64 at ./hashing.jl:62 [inlined]
   [15] hx at ./float.jl:568 [inlined]
   [16] hash at ./float.jl:571 [inlined]
   [17] cached_compilation(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:0
   [18] cached_compilation(::Function, ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:33
   [19] cufunction(::Function, ::Type{T} where T; name::String, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:296
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:108 [inlined]
   [21] gpu_call(::CuArrays.CuArrayBackend, ::Function, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64; name::String) at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/gpuarrays.jl:32
   [22] #gpu_call#1 at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/device/execution.jl:61 [inlined]
   [23] copyto! at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/broadcast.jl:63 [inlined]
   [24] copyto! at ./broadcast.jl:886 [inlined]
   [25] copy at ./broadcast.jl:862 [inlined]
   [26] materialize at ./broadcast.jl:837 [inlined]
   [27] broadcast_preserving_zero_d at ./broadcast.jl:826 [inlined]
   [28] -(::CuArray{Float32,2,Nothing}, ::Array{Float32,2}) at ./arraymath.jl:39
   [29] isapprox(::CuArray{Float32,2,Nothing}, ::Array{Float32,2}; atol::Int64, rtol::Float32, nans::Bool, norm::typeof(LinearAlgebra.norm)) at /workspace/srcdir/usr/share/julia/stdlib/v1.6/LinearAlgebra/src/generic.jl:1635
   [30] isapprox(::CuArray{Float32,2,Nothing}, ::Array{Float32,2}) at /workspace/srcdir/usr/share/julia/stdlib/v1.6/LinearAlgebra/src/generic.jl:1635
   [31] eval_test(::Expr, ::Expr, ::LineNumberNode, ::Bool) at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:246
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:128 [inlined]
   [33] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114 [inlined]
   [34] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:126 [inlined]
   [35] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114 [inlined]
   [36] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:80 [inlined]
   [37] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114 [inlined]
   [38] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:7
  
divpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:129
  Test threw exception
  Expression: pool(:div, CuArray{Int64}(cluster), T.(X)) ≈ T.(y)
  GPU compilation of kernel broadcast(CuArrays.CuKernelContext, CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global}, Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}) failed
  KernelError: passing and using non-bitstype argument
  
  Argument 4 to your kernel function is of type Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}, which is not isbits:
    .args is of type Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}} which is not isbits.
      .2 is of type Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}} which is not isbits.
        .x is of type Array{Float32,2} which is not isbits.
  
  
  Stacktrace:
   [1] check_invocation(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::LLVM.Function) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/validation.jl:68
   [2] macro expansion at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:184 [inlined]
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [4] codegen(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:183
   [5] compile(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:36
   [6] _cufunction(::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:308
   [7] _cufunction at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:302 [inlined]
   [8] #77 at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:21 [inlined]
   [9] get!(::GPUCompiler.var"#77#78"{Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CUDAnative._cufunction),GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}}, ::Dict{UInt64,Any}, ::UInt64) at ./dict.jl:450
   [10] macro expansion at ./lock.jl:183 [inlined]
   [11] check_cache(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:19
   [12] + at ./int.jl:86 [inlined]
   [13] hash_64_64 at ./hashing.jl:35 [inlined]
   [14] hash_uint64 at ./hashing.jl:62 [inlined]
   [15] hx at ./float.jl:568 [inlined]
   [16] hash at ./float.jl:571 [inlined]
   [17] cached_compilation(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:0
   [18] cached_compilation(::Function, ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:33
   [19] cufunction(::Function, ::Type{T} where T; name::String, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:296
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:108 [inlined]
   [21] gpu_call(::CuArrays.CuArrayBackend, ::Function, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64; name::String) at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/gpuarrays.jl:32
   [22] #gpu_call#1 at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/device/execution.jl:61 [inlined]
   [23] copyto! at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/broadcast.jl:63 [inlined]
   [24] copyto! at ./broadcast.jl:886 [inlined]
   [25] copy at ./broadcast.jl:862 [inlined]
   [26] materialize at ./broadcast.jl:837 [inlined]
   [27] broadcast_preserving_zero_d at ./broadcast.jl:826 [inlined]
   [28] -(::CuArray{Float32,2,Nothing}, ::Array{Float32,2}) at ./arraymath.jl:39
   [29] isapprox(::CuArray{Float32,2,Nothing}, ::Array{Float32,2}; atol::Int64, rtol::Float32, nans::Bool, norm::typeof(LinearAlgebra.norm)) at /workspace/srcdir/usr/share/julia/stdlib/v1.6/LinearAlgebra/src/generic.jl:1635
   [30] isapprox(::CuArray{Float32,2,Nothing}, ::Array{Float32,2}) at /workspace/srcdir/usr/share/julia/stdlib/v1.6/LinearAlgebra/src/generic.jl:1635
   [31] eval_test(::Expr, ::Expr, ::LineNumberNode, ::Bool) at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:246
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:129 [inlined]
   [33] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114 [inlined]
   [34] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:126 [inlined]
   [35] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114 [inlined]
   [36] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:80 [inlined]
   [37] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114 [inlined]
   [38] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:7
  
divpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:130
  Test threw exception
  Expression: divpool(cluster, T.(X)) ≈ T.(y)
  GPU compilation of kernel broadcast(CuArrays.CuKernelContext, CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global}, Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}) failed
  KernelError: passing and using non-bitstype argument
  
  Argument 4 to your kernel function is of type Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}, which is not isbits:
    .args is of type Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}} which is not isbits.
      .2 is of type Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}} which is not isbits.
        .x is of type Array{Float32,2} which is not isbits.
  
  
  Stacktrace:
   [1] check_invocation(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::LLVM.Function) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/validation.jl:68
   [2] macro expansion at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:184 [inlined]
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [4] codegen(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:183
   [5] compile(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:36
   [6] _cufunction(::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:308
   [7] _cufunction at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:302 [inlined]
   [8] #77 at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:21 [inlined]
   [9] get!(::GPUCompiler.var"#77#78"{Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CUDAnative._cufunction),GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}}, ::Dict{UInt64,Any}, ::UInt64) at ./dict.jl:450
   [10] macro expansion at ./lock.jl:183 [inlined]
   [11] check_cache(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:19
   [12] + at ./int.jl:86 [inlined]
   [13] hash_64_64 at ./hashing.jl:35 [inlined]
   [14] hash_uint64 at ./hashing.jl:62 [inlined]
   [15] hx at ./float.jl:568 [inlined]
   [16] hash at ./float.jl:571 [inlined]
   [17] cached_compilation(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:0
   [18] cached_compilation(::Function, ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:33
   [19] cufunction(::Function, ::Type{T} where T; name::String, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:296
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:108 [inlined]
   [21] gpu_call(::CuArrays.CuArrayBackend, ::Function, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64; name::String) at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/gpuarrays.jl:32
   [22] #gpu_call#1 at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/device/execution.jl:61 [inlined]
   [23] copyto! at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/broadcast.jl:63 [inlined]
   [24] copyto! at ./broadcast.jl:886 [inlined]
   [25] copy at ./broadcast.jl:862 [inlined]
   [26] materialize at ./broadcast.jl:837 [inlined]
   [27] broadcast_preserving_zero_d at ./broadcast.jl:826 [inlined]
   [28] -(::CuArray{Float32,2,Nothing}, ::Array{Float32,2}) at ./arraymath.jl:39
   [29] isapprox(::CuArray{Float32,2,Nothing}, ::Array{Float32,2}; atol::Int64, rtol::Float32, nans::Bool, norm::typeof(LinearAlgebra.norm)) at /workspace/srcdir/usr/share/julia/stdlib/v1.6/LinearAlgebra/src/generic.jl:1635
   [30] isapprox(::CuArray{Float32,2,Nothing}, ::Array{Float32,2}) at /workspace/srcdir/usr/share/julia/stdlib/v1.6/LinearAlgebra/src/generic.jl:1635
   [31] eval_test(::Expr, ::Expr, ::LineNumberNode, ::Bool) at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:246
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:130 [inlined]
   [33] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114 [inlined]
   [34] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:126 [inlined]
   [35] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114 [inlined]
   [36] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:80 [inlined]
   [37] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114 [inlined]
   [38] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:7
  
divpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:131
  Test threw exception
  Expression: pool(:div, cluster, T.(X)) ≈ T.(y)
  GPU compilation of kernel broadcast(CuArrays.CuKernelContext, CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global}, Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}) failed
  KernelError: passing and using non-bitstype argument
  
  Argument 4 to your kernel function is of type Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}, which is not isbits:
    .args is of type Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}} which is not isbits.
      .2 is of type Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}} which is not isbits.
        .x is of type Array{Float32,2} which is not isbits.
  
  
  Stacktrace:
   [1] check_invocation(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::LLVM.Function) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/validation.jl:68
   [2] macro expansion at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:184 [inlined]
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [4] codegen(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:183
   [5] compile(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:36
   [6] _cufunction(::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:308
   [7] _cufunction at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:302 [inlined]
   [8] #77 at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:21 [inlined]
   [9] get!(::GPUCompiler.var"#77#78"{Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CUDAnative._cufunction),GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}}, ::Dict{UInt64,Any}, ::UInt64) at ./dict.jl:450
   [10] macro expansion at ./lock.jl:183 [inlined]
   [11] check_cache(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:19
   [12] + at ./int.jl:86 [inlined]
   [13] hash_64_64 at ./hashing.jl:35 [inlined]
   [14] hash_uint64 at ./hashing.jl:62 [inlined]
   [15] hx at ./float.jl:568 [inlined]
   [16] hash at ./float.jl:571 [inlined]
   [17] cached_compilation(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:0
   [18] cached_compilation(::Function, ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:33
   [19] cufunction(::Function, ::Type{T} where T; name::String, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:296
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:108 [inlined]
   [21] gpu_call(::CuArrays.CuArrayBackend, ::Function, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64; name::String) at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/gpuarrays.jl:32
   [22] #gpu_call#1 at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/device/execution.jl:61 [inlined]
   [23] copyto! at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/broadcast.jl:63 [inlined]
   [24] copyto! at ./broadcast.jl:886 [inlined]
   [25] copy at ./broadcast.jl:862 [inlined]
   [26] materialize at ./broadcast.jl:837 [inlined]
   [27] broadcast_preserving_zero_d at ./broadcast.jl:826 [inlined]
   [28] -(::CuArray{Float32,2,Nothing}, ::Array{Float32,2}) at ./arraymath.jl:39
   [29] isapprox(::CuArray{Float32,2,Nothing}, ::Array{Float32,2}; atol::Int64, rtol::Float32, nans::Bool, norm::typeof(LinearAlgebra.norm)) at /workspace/srcdir/usr/share/julia/stdlib/v1.6/LinearAlgebra/src/generic.jl:1635
   [30] isapprox(::CuArray{Float32,2,Nothing}, ::Array{Float32,2}) at /workspace/srcdir/usr/share/julia/stdlib/v1.6/LinearAlgebra/src/generic.jl:1635
   [31] eval_test(::Expr, ::Expr, ::LineNumberNode, ::Bool) at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:246
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:131 [inlined]
   [33] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114 [inlined]
   [34] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:126 [inlined]
   [35] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114 [inlined]
   [36] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:80 [inlined]
   [37] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114 [inlined]
   [38] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:7
  
divpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:128
  Test threw exception
  Expression: divpool(CuArray{Int64}(cluster), T.(X)) ≈ T.(y)
  GPU compilation of kernel broadcast(CuArrays.CuKernelContext, CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global}, Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}) failed
  KernelError: passing and using non-bitstype argument
  
  Argument 4 to your kernel function is of type Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}, which is not isbits:
    .args is of type Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}} which is not isbits.
      .2 is of type Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}} which is not isbits.
        .x is of type Array{Float64,2} which is not isbits.
  
  
  Stacktrace:
   [1] check_invocation(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::LLVM.Function) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/validation.jl:68
   [2] macro expansion at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:184 [inlined]
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [4] codegen(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:183
   [5] compile(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:36
   [6] _cufunction(::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:308
   [7] _cufunction at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:302 [inlined]
   [8] #77 at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:21 [inlined]
   [9] get!(::GPUCompiler.var"#77#78"{Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CUDAnative._cufunction),GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}}, ::Dict{UInt64,Any}, ::UInt64) at ./dict.jl:450
   [10] macro expansion at ./lock.jl:183 [inlined]
   [11] check_cache(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:19
   [12] + at ./int.jl:86 [inlined]
   [13] hash_64_64 at ./hashing.jl:35 [inlined]
   [14] hash_uint64 at ./hashing.jl:62 [inlined]
   [15] hx at ./float.jl:568 [inlined]
   [16] hash at ./float.jl:571 [inlined]
   [17] cached_compilation(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:0
   [18] cached_compilation(::Function, ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:33
   [19] cufunction(::Function, ::Type{T} where T; name::String, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:296
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:108 [inlined]
   [21] gpu_call(::CuArrays.CuArrayBackend, ::Function, ::Tuple{CuArray{Float64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CuArray{Float64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64; name::String) at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/gpuarrays.jl:32
   [22] #gpu_call#1 at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/device/execution.jl:61 [inlined]
   [23] copyto! at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/broadcast.jl:63 [inlined]
   [24] copyto! at ./broadcast.jl:886 [inlined]
   [25] copy at ./broadcast.jl:862 [inlined]
   [26] materialize at ./broadcast.jl:837 [inlined]
   [27] broadcast_preserving_zero_d at ./broadcast.jl:826 [inlined]
   [28] -(::CuArray{Float64,2,Nothing}, ::Array{Float64,2}) at ./arraymath.jl:39
   [29] isapprox(::CuArray{Float64,2,Nothing}, ::Array{Float64,2}; atol::Int64, rtol::Float64, nans::Bool, norm::typeof(LinearAlgebra.norm)) at /workspace/srcdir/usr/share/julia/stdlib/v1.6/LinearAlgebra/src/generic.jl:1635
   [30] isapprox(::CuArray{Float64,2,Nothing}, ::Array{Float64,2}) at /workspace/srcdir/usr/share/julia/stdlib/v1.6/LinearAlgebra/src/generic.jl:1635
   [31] eval_test(::Expr, ::Expr, ::LineNumberNode, ::Bool) at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:246
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:128 [inlined]
   [33] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114 [inlined]
   [34] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:126 [inlined]
   [35] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114 [inlined]
   [36] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:80 [inlined]
   [37] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114 [inlined]
   [38] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:7
  
divpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:129
  Test threw exception
  Expression: pool(:div, CuArray{Int64}(cluster), T.(X)) ≈ T.(y)
  GPU compilation of kernel broadcast(CuArrays.CuKernelContext, CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global}, Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}) failed
  KernelError: passing and using non-bitstype argument
  
  Argument 4 to your kernel function is of type Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}, which is not isbits:
    .args is of type Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}} which is not isbits.
      .2 is of type Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}} which is not isbits.
        .x is of type Array{Float64,2} which is not isbits.
  
  
  Stacktrace:
   [1] check_invocation(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::LLVM.Function) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/validation.jl:68
   [2] macro expansion at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:184 [inlined]
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [4] codegen(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:183
   [5] compile(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:36
   [6] _cufunction(::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:308
   [7] _cufunction at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:302 [inlined]
   [8] #77 at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:21 [inlined]
   [9] get!(::GPUCompiler.var"#77#78"{Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CUDAnative._cufunction),GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}}, ::Dict{UInt64,Any}, ::UInt64) at ./dict.jl:450
   [10] macro expansion at ./lock.jl:183 [inlined]
   [11] check_cache(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:19
   [12] + at ./int.jl:86 [inlined]
   [13] hash_64_64 at ./hashing.jl:35 [inlined]
   [14] hash_uint64 at ./hashing.jl:62 [inlined]
   [15] hx at ./float.jl:568 [inlined]
   [16] hash at ./float.jl:571 [inlined]
   [17] cached_compilation(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:0
   [18] cached_compilation(::Function, ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:33
   [19] cufunction(::Function, ::Type{T} where T; name::String, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:296
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:108 [inlined]
   [21] gpu_call(::CuArrays.CuArrayBackend, ::Function, ::Tuple{CuArray{Float64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CuArray{Float64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64; name::String) at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/gpuarrays.jl:32
   [22] #gpu_call#1 at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/device/execution.jl:61 [inlined]
   [23] copyto! at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/broadcast.jl:63 [inlined]
   [24] copyto! at ./broadcast.jl:886 [inlined]
   [25] copy at ./broadcast.jl:862 [inlined]
   [26] materialize at ./broadcast.jl:837 [inlined]
   [27] broadcast_preserving_zero_d at ./broadcast.jl:826 [inlined]
   [28] -(::CuArray{Float64,2,Nothing}, ::Array{Float64,2}) at ./arraymath.jl:39
   [29] isapprox(::CuArray{Float64,2,Nothing}, ::Array{Float64,2}; atol::Int64, rtol::Float64, nans::Bool, norm::typeof(LinearAlgebra.norm)) at /workspace/srcdir/usr/share/julia/stdlib/v1.6/LinearAlgebra/src/generic.jl:1635
   [30] isapprox(::CuArray{Float64,2,Nothing}, ::Array{Float64,2}) at /workspace/srcdir/usr/share/julia/stdlib/v1.6/LinearAlgebra/src/generic.jl:1635
   [31] eval_test(::Expr, ::Expr, ::LineNumberNode, ::Bool) at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:246
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:129 [inlined]
   [33] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114 [inlined]
   [34] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:126 [inlined]
   [35] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114 [inlined]
   [36] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:80 [inlined]
   [37] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114 [inlined]
   [38] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:7
  
divpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:130
  Test threw exception
  Expression: divpool(cluster, T.(X)) ≈ T.(y)
  GPU compilation of kernel broadcast(CuArrays.CuKernelContext, CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global}, Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}) failed
  KernelError: passing and using non-bitstype argument
  
  Argument 4 to your kernel function is of type Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}, which is not isbits:
    .args is of type Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}} which is not isbits.
      .2 is of type Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}} which is not isbits.
        .x is of type Array{Float64,2} which is not isbits.
  
  
  Stacktrace:
   [1] check_invocation(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::LLVM.Function) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/validation.jl:68
   [2] macro expansion at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:184 [inlined]
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [4] codegen(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:183
   [5] compile(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:36
   [6] _cufunction(::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:308
   [7] _cufunction at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:302 [inlined]
   [8] #77 at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:21 [inlined]
   [9] get!(::GPUCompiler.var"#77#78"{Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CUDAnative._cufunction),GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}}, ::Dict{UInt64,Any}, ::UInt64) at ./dict.jl:450
   [10] macro expansion at ./lock.jl:183 [inlined]
   [11] check_cache(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:19
   [12] + at ./int.jl:86 [inlined]
   [13] hash_64_64 at ./hashing.jl:35 [inlined]
   [14] hash_uint64 at ./hashing.jl:62 [inlined]
   [15] hx at ./float.jl:568 [inlined]
   [16] hash at ./float.jl:571 [inlined]
   [17] cached_compilation(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:0
   [18] cached_compilation(::Function, ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:33
   [19] cufunction(::Function, ::Type{T} where T; name::String, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:296
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:108 [inlined]
   [21] gpu_call(::CuArrays.CuArrayBackend, ::Function, ::Tuple{CuArray{Float64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CuArray{Float64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64; name::String) at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/gpuarrays.jl:32
   [22] #gpu_call#1 at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/device/execution.jl:61 [inlined]
   [23] copyto! at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/broadcast.jl:63 [inlined]
   [24] copyto! at ./broadcast.jl:886 [inlined]
   [25] copy at ./broadcast.jl:862 [inlined]
   [26] materialize at ./broadcast.jl:837 [inlined]
   [27] broadcast_preserving_zero_d at ./broadcast.jl:826 [inlined]
   [28] -(::CuArray{Float64,2,Nothing}, ::Array{Float64,2}) at ./arraymath.jl:39
   [29] isapprox(::CuArray{Float64,2,Nothing}, ::Array{Float64,2}; atol::Int64, rtol::Float64, nans::Bool, norm::typeof(LinearAlgebra.norm)) at /workspace/srcdir/usr/share/julia/stdlib/v1.6/LinearAlgebra/src/generic.jl:1635
   [30] isapprox(::CuArray{Float64,2,Nothing}, ::Array{Float64,2}) at /workspace/srcdir/usr/share/julia/stdlib/v1.6/LinearAlgebra/src/generic.jl:1635
   [31] eval_test(::Expr, ::Expr, ::LineNumberNode, ::Bool) at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:246
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:130 [inlined]
   [33] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114 [inlined]
   [34] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:126 [inlined]
   [35] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114 [inlined]
   [36] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:80 [inlined]
   [37] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114 [inlined]
   [38] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:7
  
divpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:131
  Test threw exception
  Expression: pool(:div, cluster, T.(X)) ≈ T.(y)
  GPU compilation of kernel broadcast(CuArrays.CuKernelContext, CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global}, Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}) failed
  KernelError: passing and using non-bitstype argument
  
  Argument 4 to your kernel function is of type Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}, which is not isbits:
    .args is of type Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}} which is not isbits.
      .2 is of type Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}} which is not isbits.
        .x is of type Array{Float64,2} which is not isbits.
  
  
  Stacktrace:
   [1] check_invocation(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::LLVM.Function) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/validation.jl:68
   [2] macro expansion at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:184 [inlined]
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [4] codegen(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:183
   [5] compile(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:36
   [6] _cufunction(::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:308
   [7] _cufunction at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:302 [inlined]
   [8] #77 at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:21 [inlined]
   [9] get!(::GPUCompiler.var"#77#78"{Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CUDAnative._cufunction),GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}}, ::Dict{UInt64,Any}, ::UInt64) at ./dict.jl:450
   [10] macro expansion at ./lock.jl:183 [inlined]
   [11] check_cache(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:19
   [12] + at ./int.jl:86 [inlined]
   [13] hash_64_64 at ./hashing.jl:35 [inlined]
   [14] hash_uint64 at ./hashing.jl:62 [inlined]
   [15] hx at ./float.jl:568 [inlined]
   [16] hash at ./float.jl:571 [inlined]
   [17] cached_compilation(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:0
   [18] cached_compilation(::Function, ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:33
   [19] cufunction(::Function, ::Type{T} where T; name::String, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:296
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:108 [inlined]
   [21] gpu_call(::CuArrays.CuArrayBackend, ::Function, ::Tuple{CuArray{Float64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CuArray{Float64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64; name::String) at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/gpuarrays.jl:32
   [22] #gpu_call#1 at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/device/execution.jl:61 [inlined]
   [23] copyto! at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/broadcast.jl:63 [inlined]
   [24] copyto! at ./broadcast.jl:886 [inlined]
   [25] copy at ./broadcast.jl:862 [inlined]
   [26] materialize at ./broadcast.jl:837 [inlined]
   [27] broadcast_preserving_zero_d at ./broadcast.jl:826 [inlined]
   [28] -(::CuArray{Float64,2,Nothing}, ::Array{Float64,2}) at ./arraymath.jl:39
   [29] isapprox(::CuArray{Float64,2,Nothing}, ::Array{Float64,2}; atol::Int64, rtol::Float64, nans::Bool, norm::typeof(LinearAlgebra.norm)) at /workspace/srcdir/usr/share/julia/stdlib/v1.6/LinearAlgebra/src/generic.jl:1635
   [30] isapprox(::CuArray{Float64,2,Nothing}, ::Array{Float64,2}) at /workspace/srcdir/usr/share/julia/stdlib/v1.6/LinearAlgebra/src/generic.jl:1635
   [31] eval_test(::Expr, ::Expr, ::LineNumberNode, ::Bool) at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:246
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:131 [inlined]
   [33] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114 [inlined]
   [34] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:126 [inlined]
   [35] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114 [inlined]
   [36] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:80 [inlined]
   [37] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114 [inlined]
   [38] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:7
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:33
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_mul!(x, us, xs))
            end), ys) == (∇y_mul,)
  Thread local storage is not implemented
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] lower_ptls!(::LLVM.Module) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/optim.jl:154
   [3] (::LLVM.var"#callback#19"{typeof(GPUCompiler.lower_ptls!)})(::Ptr{Nothing}) at /home/pkgeval/.julia/packages/LLVM/5PewI/src/pass.jl:20
   [4] macro expansion at /home/pkgeval/.julia/packages/LLVM/5PewI/src/base.jl:18 [inlined]
   [5] LLVMRunPassManager at /home/pkgeval/.julia/packages/LLVM/5PewI/lib/9.0/libLLVM_h.jl:2813 [inlined]
   [6] run! at /home/pkgeval/.julia/packages/LLVM/5PewI/src/passmanager.jl:34 [inlined]
   [7] (::GPUCompiler.var"#37#42"{LLVM.Module,GPUCompiler.var"#initialize!#40"{LLVM.Module,LLVM.TargetMachine}})(::LLVM.ModulePassManager) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/optim.jl:41
   [8] LLVM.ModulePassManager(::GPUCompiler.var"#37#42"{LLVM.Module,GPUCompiler.var"#initialize!#40"{LLVM.Module,LLVM.TargetMachine}}) at /home/pkgeval/.julia/packages/LLVM/5PewI/src/passmanager.jl:28
   [9] optimize!(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::LLVM.Module, ::LLVM.Function) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/optim.jl:28
   [10] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [11] macro expansion at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:107 [inlined]
   [12] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [13] codegen(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:97
   [14] compile(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:36
   [15] _cufunction(::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#183#186"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:308
   [16] _cufunction at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:302 [inlined]
   [17] #77 at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:21 [inlined]
   [18] get!(::GPUCompiler.var"#77#78"{Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CUDAnative._cufunction),GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#183#186"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}}, ::Dict{UInt64,Any}, ::UInt64) at ./dict.jl:450
   [19] macro expansion at ./lock.jl:183 [inlined]
   [20] check_cache(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#183#186"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:19
   [21] + at ./int.jl:86 [inlined]
   [22] hash_64_64 at ./hashing.jl:35 [inlined]
   [23] hash_uint64 at ./hashing.jl:62 [inlined]
   [24] hx at ./float.jl:568 [inlined]
   [25] hash at ./float.jl:571 [inlined]
   [26] cached_compilation(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#183#186"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:0
   [27] cached_compilation(::Function, ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#183#186"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:33
   [28] cufunction(::Function, ::Type{T} where T; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:296
   [29] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:291
   [30] macro expansion at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/mapreduce.jl:197 [inlined]
   [31] mapreducedim!(::Function, ::Function, ::CuArray{Float32,1,Nothing}, ::Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#183#186"{Int64,CuArray{Float32,3,Nothing}},Tuple{CuArray{Tuple{Int64,Int64},1,Nothing}}}; init::Float32) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/nvtx/highlevel.jl:83
   [32] _mapreduce(::Function, ::Function, ::CuArray{Tuple{Int64,Int64},1,Nothing}; dims::Function, init::Float32) at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/mapreduce.jl:62
   [33] #mapreduce#31 at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/mapreduce.jl:28 [inlined]
   [34] (::GeometricFlux.var"#181#184"{Float32,CuArray{Float32,2,Nothing},CuArray{Float32,3,Nothing},CuArray{Int64,2,Nothing}})(::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/scatter.jl:125
   [35] #93#back at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49 [inlined]
   [36] #86 at ./none:0 [inlined]
   [37] (::Zygote.var"#36#37"{typeof(∂(#86))})(::Float32) at /home/pkgeval/.julia/packages/Zygote/YeCEW/src/compiler/interface.jl:46
   [38] gradient(::Function, ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/YeCEW/src/compiler/interface.jl:55
   [39] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:33
   [40] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114
   [41] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:17
   [42] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114
   [43] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:16
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:34
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_mul!(copy(ys), x, xs))
            end), us) == (2048 * gather(ys, xs),)
  Thread local storage is not implemented
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] lower_ptls!(::LLVM.Module) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/optim.jl:154
   [3] (::LLVM.var"#callback#19"{typeof(GPUCompiler.lower_ptls!)})(::Ptr{Nothing}) at /home/pkgeval/.julia/packages/LLVM/5PewI/src/pass.jl:20
   [4] macro expansion at /home/pkgeval/.julia/packages/LLVM/5PewI/src/base.jl:18 [inlined]
   [5] LLVMRunPassManager at /home/pkgeval/.julia/packages/LLVM/5PewI/lib/9.0/libLLVM_h.jl:2813 [inlined]
   [6] run! at /home/pkgeval/.julia/packages/LLVM/5PewI/src/passmanager.jl:34 [inlined]
   [7] (::GPUCompiler.var"#37#42"{LLVM.Module,GPUCompiler.var"#initialize!#40"{LLVM.Module,LLVM.TargetMachine}})(::LLVM.ModulePassManager) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/optim.jl:41
   [8] LLVM.ModulePassManager(::GPUCompiler.var"#37#42"{LLVM.Module,GPUCompiler.var"#initialize!#40"{LLVM.Module,LLVM.TargetMachine}}) at /home/pkgeval/.julia/packages/LLVM/5PewI/src/passmanager.jl:28
   [9] optimize!(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::LLVM.Module, ::LLVM.Function) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/optim.jl:28
   [10] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [11] macro expansion at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:107 [inlined]
   [12] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [13] codegen(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:97
   [14] compile(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:36
   [15] _cufunction(::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#183#186"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:308
   [16] _cufunction at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:302 [inlined]
   [17] #77 at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:21 [inlined]
   [18] get!(::GPUCompiler.var"#77#78"{Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CUDAnative._cufunction),GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#183#186"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}}, ::Dict{UInt64,Any}, ::UInt64) at ./dict.jl:450
   [19] macro expansion at ./lock.jl:183 [inlined]
   [20] check_cache(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#183#186"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:19
   [21] + at ./int.jl:86 [inlined]
   [22] hash_64_64 at ./hashing.jl:35 [inlined]
   [23] hash_uint64 at ./hashing.jl:62 [inlined]
   [24] hx at ./float.jl:568 [inlined]
   [25] hash at ./float.jl:571 [inlined]
   [26] cached_compilation(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#183#186"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:0
   [27] cached_compilation(::Function, ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#183#186"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:33
   [28] cufunction(::Function, ::Type{T} where T; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:296
   [29] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:291
   [30] macro expansion at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/mapreduce.jl:197 [inlined]
   [31] mapreducedim!(::Function, ::Function, ::CuArray{Float32,1,Nothing}, ::Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#183#186"{Int64,CuArray{Float32,3,Nothing}},Tuple{CuArray{Tuple{Int64,Int64},1,Nothing}}}; init::Float32) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/nvtx/highlevel.jl:83
   [32] _mapreduce(::Function, ::Function, ::CuArray{Tuple{Int64,Int64},1,Nothing}; dims::Function, init::Float32) at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/mapreduce.jl:62
   [33] #mapreduce#31 at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/mapreduce.jl:28 [inlined]
   [34] (::GeometricFlux.var"#181#184"{Float32,CuArray{Float32,2,Nothing},CuArray{Float32,3,Nothing},CuArray{Int64,2,Nothing}})(::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/scatter.jl:125
   [35] #93#back at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49 [inlined]
   [36] #87 at ./none:0 [inlined]
   [37] (::Zygote.var"#36#37"{typeof(∂(#87))})(::Float32) at /home/pkgeval/.julia/packages/Zygote/YeCEW/src/compiler/interface.jl:46
   [38] gradient(::Function, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/YeCEW/src/compiler/interface.jl:55
   [39] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:34
   [40] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114
   [41] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:17
   [42] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114
   [43] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:16
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:35
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_mul!(copy(ys), us, x))
            end), xs) == (nothing,)
  Thread local storage is not implemented
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] lower_ptls!(::LLVM.Module) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/optim.jl:154
   [3] (::LLVM.var"#callback#19"{typeof(GPUCompiler.lower_ptls!)})(::Ptr{Nothing}) at /home/pkgeval/.julia/packages/LLVM/5PewI/src/pass.jl:20
   [4] macro expansion at /home/pkgeval/.julia/packages/LLVM/5PewI/src/base.jl:18 [inlined]
   [5] LLVMRunPassManager at /home/pkgeval/.julia/packages/LLVM/5PewI/lib/9.0/libLLVM_h.jl:2813 [inlined]
   [6] run! at /home/pkgeval/.julia/packages/LLVM/5PewI/src/passmanager.jl:34 [inlined]
   [7] (::GPUCompiler.var"#37#42"{LLVM.Module,GPUCompiler.var"#initialize!#40"{LLVM.Module,LLVM.TargetMachine}})(::LLVM.ModulePassManager) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/optim.jl:41
   [8] LLVM.ModulePassManager(::GPUCompiler.var"#37#42"{LLVM.Module,GPUCompiler.var"#initialize!#40"{LLVM.Module,LLVM.TargetMachine}}) at /home/pkgeval/.julia/packages/LLVM/5PewI/src/passmanager.jl:28
   [9] optimize!(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::LLVM.Module, ::LLVM.Function) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/optim.jl:28
   [10] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [11] macro expansion at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:107 [inlined]
   [12] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [13] codegen(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:97
   [14] compile(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:36
   [15] _cufunction(::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#183#186"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:308
   [16] _cufunction at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:302 [inlined]
   [17] #77 at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:21 [inlined]
   [18] get!(::GPUCompiler.var"#77#78"{Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CUDAnative._cufunction),GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#183#186"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}}, ::Dict{UInt64,Any}, ::UInt64) at ./dict.jl:450
   [19] macro expansion at ./lock.jl:183 [inlined]
   [20] check_cache(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#183#186"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:19
   [21] + at ./int.jl:86 [inlined]
   [22] hash_64_64 at ./hashing.jl:35 [inlined]
   [23] hash_uint64 at ./hashing.jl:62 [inlined]
   [24] hx at ./float.jl:568 [inlined]
   [25] hash at ./float.jl:571 [inlined]
   [26] cached_compilation(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#183#186"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:0
   [27] cached_compilation(::Function, ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#183#186"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:33
   [28] cufunction(::Function, ::Type{T} where T; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:296
   [29] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:291
   [30] macro expansion at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/mapreduce.jl:197 [inlined]
   [31] mapreducedim!(::Function, ::Function, ::CuArray{Float32,1,Nothing}, ::Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#183#186"{Int64,CuArray{Float32,3,Nothing}},Tuple{CuArray{Tuple{Int64,Int64},1,Nothing}}}; init::Float32) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/nvtx/highlevel.jl:83
   [32] _mapreduce(::Function, ::Function, ::CuArray{Tuple{Int64,Int64},1,Nothing}; dims::Function, init::Float32) at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/mapreduce.jl:62
   [33] #mapreduce#31 at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/mapreduce.jl:28 [inlined]
   [34] (::GeometricFlux.var"#181#184"{Float32,CuArray{Float32,2,Nothing},CuArray{Float32,3,Nothing},CuArray{Int64,2,Nothing}})(::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/scatter.jl:125
   [35] #93#back at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49 [inlined]
   [36] #88 at ./none:0 [inlined]
   [37] (::typeof(∂(#88)))(::Float32) at /home/pkgeval/.julia/packages/Zygote/YeCEW/src/compiler/interface2.jl:0
   [38] (::Zygote.var"#36#37"{typeof(∂(#88))})(::Float32) at /home/pkgeval/.julia/packages/Zygote/YeCEW/src/compiler/interface.jl:46
   [39] gradient(::Function, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/YeCEW/src/compiler/interface.jl:55
   [40] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:35
   [41] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114
   [42] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:17
   [43] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114
   [44] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:16
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:37
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_div!(x, us, xs))
            end), ys) == (∇y_div,)
  Thread local storage is not implemented
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] lower_ptls!(::LLVM.Module) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/optim.jl:154
   [3] (::LLVM.var"#callback#19"{typeof(GPUCompiler.lower_ptls!)})(::Ptr{Nothing}) at /home/pkgeval/.julia/packages/LLVM/5PewI/src/pass.jl:20
   [4] macro expansion at /home/pkgeval/.julia/packages/LLVM/5PewI/src/base.jl:18 [inlined]
   [5] LLVMRunPassManager at /home/pkgeval/.julia/packages/LLVM/5PewI/lib/9.0/libLLVM_h.jl:2813 [inlined]
   [6] run! at /home/pkgeval/.julia/packages/LLVM/5PewI/src/passmanager.jl:34 [inlined]
   [7] (::GPUCompiler.var"#37#42"{LLVM.Module,GPUCompiler.var"#initialize!#40"{LLVM.Module,LLVM.TargetMachine}})(::LLVM.ModulePassManager) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/optim.jl:41
   [8] LLVM.ModulePassManager(::GPUCompiler.var"#37#42"{LLVM.Module,GPUCompiler.var"#initialize!#40"{LLVM.Module,LLVM.TargetMachine}}) at /home/pkgeval/.julia/packages/LLVM/5PewI/src/passmanager.jl:28
   [9] optimize!(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::LLVM.Module, ::LLVM.Function) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/optim.jl:28
   [10] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [11] macro expansion at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:107 [inlined]
   [12] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [13] codegen(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:97
   [14] compile(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:36
   [15] _cufunction(::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#191#194"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:308
   [16] _cufunction at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:302 [inlined]
   [17] #77 at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:21 [inlined]
   [18] get!(::GPUCompiler.var"#77#78"{Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CUDAnative._cufunction),GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#191#194"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}}, ::Dict{UInt64,Any}, ::UInt64) at ./dict.jl:450
   [19] macro expansion at ./lock.jl:183 [inlined]
   [20] check_cache(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#191#194"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:19
   [21] + at ./int.jl:86 [inlined]
   [22] hash_64_64 at ./hashing.jl:35 [inlined]
   [23] hash_uint64 at ./hashing.jl:62 [inlined]
   [24] hx at ./float.jl:568 [inlined]
   [25] hash at ./float.jl:571 [inlined]
   [26] cached_compilation(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#191#194"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:0
   [27] cached_compilation(::Function, ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#191#194"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:33
   [28] cufunction(::Function, ::Type{T} where T; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:296
   [29] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:291
   [30] macro expansion at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/mapreduce.jl:197 [inlined]
   [31] mapreducedim!(::Function, ::Function, ::CuArray{Float32,1,Nothing}, ::Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#191#194"{Int64,CuArray{Float32,3,Nothing}},Tuple{CuArray{Tuple{Int64,Int64},1,Nothing}}}; init::Float32) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/nvtx/highlevel.jl:83
   [32] _mapreduce(::Function, ::Function, ::CuArray{Tuple{Int64,Int64},1,Nothing}; dims::Function, init::Float32) at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/mapreduce.jl:62
   [33] #mapreduce#31 at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/mapreduce.jl:28 [inlined]
   [34] (::GeometricFlux.var"#189#192"{Float32,CuArray{Float32,2,Nothing},CuArray{Float32,3,Nothing},CuArray{Int64,2,Nothing}})(::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/scatter.jl:146
   [35] #111#back at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49 [inlined]
   [36] #89 at ./none:0 [inlined]
   [37] (::Zygote.var"#36#37"{typeof(∂(#89))})(::Float32) at /home/pkgeval/.julia/packages/Zygote/YeCEW/src/compiler/interface.jl:46
   [38] gradient(::Function, ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/YeCEW/src/compiler/interface.jl:55
   [39] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:37
   [40] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114
   [41] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:17
   [42] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114
   [43] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:16
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:38
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_div!(copy(ys), x, xs))
            end), us) == (-(gather(ys, xs)) / 8192,)
  Thread local storage is not implemented
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] lower_ptls!(::LLVM.Module) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/optim.jl:154
   [3] (::LLVM.var"#callback#19"{typeof(GPUCompiler.lower_ptls!)})(::Ptr{Nothing}) at /home/pkgeval/.julia/packages/LLVM/5PewI/src/pass.jl:20
   [4] macro expansion at /home/pkgeval/.julia/packages/LLVM/5PewI/src/base.jl:18 [inlined]
   [5] LLVMRunPassManager at /home/pkgeval/.julia/packages/LLVM/5PewI/lib/9.0/libLLVM_h.jl:2813 [inlined]
   [6] run! at /home/pkgeval/.julia/packages/LLVM/5PewI/src/passmanager.jl:34 [inlined]
   [7] (::GPUCompiler.var"#37#42"{LLVM.Module,GPUCompiler.var"#initialize!#40"{LLVM.Module,LLVM.TargetMachine}})(::LLVM.ModulePassManager) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/optim.jl:41
   [8] LLVM.ModulePassManager(::GPUCompiler.var"#37#42"{LLVM.Module,GPUCompiler.var"#initialize!#40"{LLVM.Module,LLVM.TargetMachine}}) at /home/pkgeval/.julia/packages/LLVM/5PewI/src/passmanager.jl:28
   [9] optimize!(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::LLVM.Module, ::LLVM.Function) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/optim.jl:28
   [10] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [11] macro expansion at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:107 [inlined]
   [12] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [13] codegen(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:97
   [14] compile(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:36
   [15] _cufunction(::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#191#194"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:308
   [16] _cufunction at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:302 [inlined]
   [17] #77 at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:21 [inlined]
   [18] get!(::GPUCompiler.var"#77#78"{Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CUDAnative._cufunction),GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#191#194"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}}, ::Dict{UInt64,Any}, ::UInt64) at ./dict.jl:450
   [19] macro expansion at ./lock.jl:183 [inlined]
   [20] check_cache(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#191#194"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:19
   [21] + at ./int.jl:86 [inlined]
   [22] hash_64_64 at ./hashing.jl:35 [inlined]
   [23] hash_uint64 at ./hashing.jl:62 [inlined]
   [24] hx at ./float.jl:568 [inlined]
   [25] hash at ./float.jl:571 [inlined]
   [26] cached_compilation(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#191#194"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:0
   [27] cached_compilation(::Function, ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#191#194"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:33
   [28] cufunction(::Function, ::Type{T} where T; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:296
   [29] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:291
   [30] macro expansion at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/mapreduce.jl:197 [inlined]
   [31] mapreducedim!(::Function, ::Function, ::CuArray{Float32,1,Nothing}, ::Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#191#194"{Int64,CuArray{Float32,3,Nothing}},Tuple{CuArray{Tuple{Int64,Int64},1,Nothing}}}; init::Float32) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/nvtx/highlevel.jl:83
   [32] _mapreduce(::Function, ::Function, ::CuArray{Tuple{Int64,Int64},1,Nothing}; dims::Function, init::Float32) at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/mapreduce.jl:62
   [33] #mapreduce#31 at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/mapreduce.jl:28 [inlined]
   [34] (::GeometricFlux.var"#189#192"{Float32,CuArray{Float32,2,Nothing},CuArray{Float32,3,Nothing},CuArray{Int64,2,Nothing}})(::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/scatter.jl:146
   [35] #111#back at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49 [inlined]
   [36] #90 at ./none:0 [inlined]
   [37] (::Zygote.var"#36#37"{typeof(∂(#90))})(::Float32) at /home/pkgeval/.julia/packages/Zygote/YeCEW/src/compiler/interface.jl:46
   [38] gradient(::Function, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/YeCEW/src/compiler/interface.jl:55
   [39] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:38
   [40] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114
   [41] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:17
   [42] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114
   [43] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:16
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:39
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_div!(copy(ys), us, x))
            end), xs) == (nothing,)
  Thread local storage is not implemented
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] lower_ptls!(::LLVM.Module) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/optim.jl:154
   [3] (::LLVM.var"#callback#19"{typeof(GPUCompiler.lower_ptls!)})(::Ptr{Nothing}) at /home/pkgeval/.julia/packages/LLVM/5PewI/src/pass.jl:20
   [4] macro expansion at /home/pkgeval/.julia/packages/LLVM/5PewI/src/base.jl:18 [inlined]
   [5] LLVMRunPassManager at /home/pkgeval/.julia/packages/LLVM/5PewI/lib/9.0/libLLVM_h.jl:2813 [inlined]
   [6] run! at /home/pkgeval/.julia/packages/LLVM/5PewI/src/passmanager.jl:34 [inlined]
   [7] (::GPUCompiler.var"#37#42"{LLVM.Module,GPUCompiler.var"#initialize!#40"{LLVM.Module,LLVM.TargetMachine}})(::LLVM.ModulePassManager) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/optim.jl:41
   [8] LLVM.ModulePassManager(::GPUCompiler.var"#37#42"{LLVM.Module,GPUCompiler.var"#initialize!#40"{LLVM.Module,LLVM.TargetMachine}}) at /home/pkgeval/.julia/packages/LLVM/5PewI/src/passmanager.jl:28
   [9] optimize!(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::LLVM.Module, ::LLVM.Function) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/optim.jl:28
   [10] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [11] macro expansion at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:107 [inlined]
   [12] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [13] codegen(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:97
   [14] compile(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:36
   [15] _cufunction(::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#191#194"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:308
   [16] _cufunction at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:302 [inlined]
   [17] #77 at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:21 [inlined]
   [18] get!(::GPUCompiler.var"#77#78"{Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CUDAnative._cufunction),GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#191#194"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}}, ::Dict{UInt64,Any}, ::UInt64) at ./dict.jl:450
   [19] macro expansion at ./lock.jl:183 [inlined]
   [20] check_cache(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#191#194"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:19
   [21] + at ./int.jl:86 [inlined]
   [22] hash_64_64 at ./hashing.jl:35 [inlined]
   [23] hash_uint64 at ./hashing.jl:62 [inlined]
   [24] hx at ./float.jl:568 [inlined]
   [25] hash at ./float.jl:571 [inlined]
   [26] cached_compilation(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#191#194"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:0
   [27] cached_compilation(::Function, ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#191#194"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:33
   [28] cufunction(::Function, ::Type{T} where T; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:296
   [29] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:291
   [30] macro expansion at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/mapreduce.jl:197 [inlined]
   [31] mapreducedim!(::Function, ::Function, ::CuArray{Float32,1,Nothing}, ::Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#191#194"{Int64,CuArray{Float32,3,Nothing}},Tuple{CuArray{Tuple{Int64,Int64},1,Nothing}}}; init::Float32) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/nvtx/highlevel.jl:83
   [32] _mapreduce(::Function, ::Function, ::CuArray{Tuple{Int64,Int64},1,Nothing}; dims::Function, init::Float32) at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/mapreduce.jl:62
   [33] #mapreduce#31 at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/mapreduce.jl:28 [inlined]
   [34] (::GeometricFlux.var"#189#192"{Float32,CuArray{Float32,2,Nothing},CuArray{Float32,3,Nothing},CuArray{Int64,2,Nothing}})(::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/scatter.jl:146
   [35] #111#back at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49 [inlined]
   [36] #91 at ./none:0 [inlined]
   [37] (::typeof(∂(#91)))(::Float32) at /home/pkgeval/.julia/packages/Zygote/YeCEW/src/compiler/interface2.jl:0
   [38] (::Zygote.var"#36#37"{typeof(∂(#91))})(::Float32) at /home/pkgeval/.julia/packages/Zygote/YeCEW/src/compiler/interface.jl:46
   [39] gradient(::Function, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/YeCEW/src/compiler/interface.jl:55
   [40] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:39
   [41] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114
   [42] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:17
   [43] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114
   [44] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:16
  
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:59
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(prodpool(x, us))
            end), xs) == (nothing,)
  Thread local storage is not implemented
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] lower_ptls!(::LLVM.Module) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/optim.jl:154
   [3] (::LLVM.var"#callback#19"{typeof(GPUCompiler.lower_ptls!)})(::Ptr{Nothing}) at /home/pkgeval/.julia/packages/LLVM/5PewI/src/pass.jl:20
   [4] macro expansion at /home/pkgeval/.julia/packages/LLVM/5PewI/src/base.jl:18 [inlined]
   [5] LLVMRunPassManager at /home/pkgeval/.julia/packages/LLVM/5PewI/lib/9.0/libLLVM_h.jl:2813 [inlined]
   [6] run! at /home/pkgeval/.julia/packages/LLVM/5PewI/src/passmanager.jl:34 [inlined]
   [7] (::GPUCompiler.var"#37#42"{LLVM.Module,GPUCompiler.var"#initialize!#40"{LLVM.Module,LLVM.TargetMachine}})(::LLVM.ModulePassManager) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/optim.jl:41
   [8] LLVM.ModulePassManager(::GPUCompiler.var"#37#42"{LLVM.Module,GPUCompiler.var"#initialize!#40"{LLVM.Module,LLVM.TargetMachine}}) at /home/pkgeval/.julia/packages/LLVM/5PewI/src/passmanager.jl:28
   [9] optimize!(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::LLVM.Module, ::LLVM.Function) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/optim.jl:28
   [10] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [11] macro expansion at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:107 [inlined]
   [12] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [13] codegen(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:97
   [14] compile(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:36
   [15] _cufunction(::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#208#211"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:308
   [16] _cufunction at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:302 [inlined]
   [17] #77 at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:21 [inlined]
   [18] get!(::GPUCompiler.var"#77#78"{Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CUDAnative._cufunction),GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#208#211"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}}, ::Dict{UInt64,Any}, ::UInt64) at ./dict.jl:450
   [19] macro expansion at ./lock.jl:183 [inlined]
   [20] check_cache(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#208#211"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:19
   [21] + at ./int.jl:86 [inlined]
   [22] hash_64_64 at ./hashing.jl:35 [inlined]
   [23] hash_uint64 at ./hashing.jl:62 [inlined]
   [24] hx at ./float.jl:568 [inlined]
   [25] hash at ./float.jl:571 [inlined]
   [26] cached_compilation(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#208#211"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:0
   [27] cached_compilation(::Function, ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#208#211"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:33
   [28] cufunction(::Function, ::Type{T} where T; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:296
   [29] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:291
   [30] macro expansion at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/mapreduce.jl:197 [inlined]
   [31] mapreducedim!(::Function, ::Function, ::CuArray{Float32,1,Nothing}, ::Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#208#211"{Int64,CuArray{Float32,3,Nothing}},Tuple{CuArray{Tuple{Int64,Int64},1,Nothing}}}; init::Float32) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/nvtx/highlevel.jl:83
   [32] _mapreduce(::Function, ::Function, ::CuArray{Tuple{Int64,Int64},1,Nothing}; dims::Function, init::Float32) at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/mapreduce.jl:62
   [33] #mapreduce#31 at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/mapreduce.jl:28 [inlined]
   [34] (::GeometricFlux.var"#206#209"{Float32,CuArray{Int64,2,Nothing},CuArray{Float32,3,Nothing}})(::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/pool.jl:88
   [35] #154#back at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49 [inlined]
   [36] #103 at ./none:0 [inlined]
   [37] (::typeof(∂(#103)))(::Float32) at /home/pkgeval/.julia/packages/Zygote/YeCEW/src/compiler/interface2.jl:0
   [38] (::Zygote.var"#36#37"{typeof(∂(#103))})(::Float32) at /home/pkgeval/.julia/packages/Zygote/YeCEW/src/compiler/interface.jl:46
   [39] gradient(::Function, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/YeCEW/src/compiler/interface.jl:55
   [40] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:59
   [41] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114
   [42] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:47
   [43] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114
   [44] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:16
  
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:60
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(prodpool(xs, x))
            end), us) == (2048 * ones(2, 3, 4),)
  Thread local storage is not implemented
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] lower_ptls!(::LLVM.Module) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/optim.jl:154
   [3] (::LLVM.var"#callback#19"{typeof(GPUCompiler.lower_ptls!)})(::Ptr{Nothing}) at /home/pkgeval/.julia/packages/LLVM/5PewI/src/pass.jl:20
   [4] macro expansion at /home/pkgeval/.julia/packages/LLVM/5PewI/src/base.jl:18 [inlined]
   [5] LLVMRunPassManager at /home/pkgeval/.julia/packages/LLVM/5PewI/lib/9.0/libLLVM_h.jl:2813 [inlined]
   [6] run! at /home/pkgeval/.julia/packages/LLVM/5PewI/src/passmanager.jl:34 [inlined]
   [7] (::GPUCompiler.var"#37#42"{LLVM.Module,GPUCompiler.var"#initialize!#40"{LLVM.Module,LLVM.TargetMachine}})(::LLVM.ModulePassManager) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/optim.jl:41
   [8] LLVM.ModulePassManager(::GPUCompiler.var"#37#42"{LLVM.Module,GPUCompiler.var"#initialize!#40"{LLVM.Module,LLVM.TargetMachine}}) at /home/pkgeval/.julia/packages/LLVM/5PewI/src/passmanager.jl:28
   [9] optimize!(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::LLVM.Module, ::LLVM.Function) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/optim.jl:28
   [10] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [11] macro expansion at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:107 [inlined]
   [12] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [13] codegen(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:97
   [14] compile(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:36
   [15] _cufunction(::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#208#211"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:308
   [16] _cufunction at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:302 [inlined]
   [17] #77 at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:21 [inlined]
   [18] get!(::GPUCompiler.var"#77#78"{Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CUDAnative._cufunction),GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#208#211"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}}, ::Dict{UInt64,Any}, ::UInt64) at ./dict.jl:450
   [19] macro expansion at ./lock.jl:183 [inlined]
   [20] check_cache(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#208#211"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:19
   [21] + at ./int.jl:86 [inlined]
   [22] hash_64_64 at ./hashing.jl:35 [inlined]
   [23] hash_uint64 at ./hashing.jl:62 [inlined]
   [24] hx at ./float.jl:568 [inlined]
   [25] hash at ./float.jl:571 [inlined]
   [26] cached_compilation(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#208#211"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:0
   [27] cached_compilation(::Function, ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#208#211"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:33
   [28] cufunction(::Function, ::Type{T} where T; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:296
   [29] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:291
   [30] macro expansion at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/mapreduce.jl:197 [inlined]
   [31] mapreducedim!(::Function, ::Function, ::CuArray{Float32,1,Nothing}, ::Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#208#211"{Int64,CuArray{Float32,3,Nothing}},Tuple{CuArray{Tuple{Int64,Int64},1,Nothing}}}; init::Float32) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/nvtx/highlevel.jl:83
   [32] _mapreduce(::Function, ::Function, ::CuArray{Tuple{Int64,Int64},1,Nothing}; dims::Function, init::Float32) at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/mapreduce.jl:62
   [33] #mapreduce#31 at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/mapreduce.jl:28 [inlined]
   [34] (::GeometricFlux.var"#206#209"{Float32,CuArray{Int64,2,Nothing},CuArray{Float32,3,Nothing}})(::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/pool.jl:88
   [35] #154#back at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49 [inlined]
   [36] #104 at ./none:0 [inlined]
   [37] (::Zygote.var"#36#37"{typeof(∂(#104))})(::Float32) at /home/pkgeval/.julia/packages/Zygote/YeCEW/src/compiler/interface.jl:46
   [38] gradient(::Function, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/YeCEW/src/compiler/interface.jl:55
   [39] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:60
   [40] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114
   [41] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:47
   [42] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114
   [43] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:16
  
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:62
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(divpool(x, us))
            end), xs) == (nothing,)
  Thread local storage is not implemented
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] lower_ptls!(::LLVM.Module) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/optim.jl:154
   [3] (::LLVM.var"#callback#19"{typeof(GPUCompiler.lower_ptls!)})(::Ptr{Nothing}) at /home/pkgeval/.julia/packages/LLVM/5PewI/src/pass.jl:20
   [4] macro expansion at /home/pkgeval/.julia/packages/LLVM/5PewI/src/base.jl:18 [inlined]
   [5] LLVMRunPassManager at /home/pkgeval/.julia/packages/LLVM/5PewI/lib/9.0/libLLVM_h.jl:2813 [inlined]
   [6] run! at /home/pkgeval/.julia/packages/LLVM/5PewI/src/passmanager.jl:34 [inlined]
   [7] (::GPUCompiler.var"#37#42"{LLVM.Module,GPUCompiler.var"#initialize!#40"{LLVM.Module,LLVM.TargetMachine}})(::LLVM.ModulePassManager) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/optim.jl:41
   [8] LLVM.ModulePassManager(::GPUCompiler.var"#37#42"{LLVM.Module,GPUCompiler.var"#initialize!#40"{LLVM.Module,LLVM.TargetMachine}}) at /home/pkgeval/.julia/packages/LLVM/5PewI/src/passmanager.jl:28
   [9] optimize!(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::LLVM.Module, ::LLVM.Function) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/optim.jl:28
   [10] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [11] macro expansion at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:107 [inlined]
   [12] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [13] codegen(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:97
   [14] compile(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:36
   [15] _cufunction(::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#216#219"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:308
   [16] _cufunction at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:302 [inlined]
   [17] #77 at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:21 [inlined]
   [18] get!(::GPUCompiler.var"#77#78"{Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CUDAnative._cufunction),GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#216#219"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}}, ::Dict{UInt64,Any}, ::UInt64) at ./dict.jl:450
   [19] macro expansion at ./lock.jl:183 [inlined]
   [20] check_cache(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#216#219"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:19
   [21] + at ./int.jl:86 [inlined]
   [22] hash_64_64 at ./hashing.jl:35 [inlined]
   [23] hash_uint64 at ./hashing.jl:62 [inlined]
   [24] hx at ./float.jl:568 [inlined]
   [25] hash at ./float.jl:571 [inlined]
   [26] cached_compilation(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#216#219"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:0
   [27] cached_compilation(::Function, ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#216#219"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:33
   [28] cufunction(::Function, ::Type{T} where T; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:296
   [29] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:291
   [30] macro expansion at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/mapreduce.jl:197 [inlined]
   [31] mapreducedim!(::Function, ::Function, ::CuArray{Float32,1,Nothing}, ::Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#216#219"{Int64,CuArray{Float32,3,Nothing}},Tuple{CuArray{Tuple{Int64,Int64},1,Nothing}}}; init::Float32) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/nvtx/highlevel.jl:83
   [32] _mapreduce(::Function, ::Function, ::CuArray{Tuple{Int64,Int64},1,Nothing}; dims::Function, init::Float32) at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/mapreduce.jl:62
   [33] #mapreduce#31 at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/mapreduce.jl:28 [inlined]
   [34] (::GeometricFlux.var"#214#217"{Float32,CuArray{Int64,2,Nothing},CuArray{Float32,3,Nothing}})(::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/pool.jl:104
   [35] #172#back at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49 [inlined]
   [36] #105 at ./none:0 [inlined]
   [37] (::typeof(∂(#105)))(::Float32) at /home/pkgeval/.julia/packages/Zygote/YeCEW/src/compiler/interface2.jl:0
   [38] (::Zygote.var"#36#37"{typeof(∂(#105))})(::Float32) at /home/pkgeval/.julia/packages/Zygote/YeCEW/src/compiler/interface.jl:46
   [39] gradient(::Function, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/YeCEW/src/compiler/interface.jl:55
   [40] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:62
   [41] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114
   [42] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:47
   [43] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114
   [44] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:16
  
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:63
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(divpool(xs, x))
            end), us) == (-(ones(2, 3, 4)) / 8192,)
  Thread local storage is not implemented
  Stacktrace:
   [1] error(::String) at ./error.jl:33
   [2] lower_ptls!(::LLVM.Module) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/optim.jl:154
   [3] (::LLVM.var"#callback#19"{typeof(GPUCompiler.lower_ptls!)})(::Ptr{Nothing}) at /home/pkgeval/.julia/packages/LLVM/5PewI/src/pass.jl:20
   [4] macro expansion at /home/pkgeval/.julia/packages/LLVM/5PewI/src/base.jl:18 [inlined]
   [5] LLVMRunPassManager at /home/pkgeval/.julia/packages/LLVM/5PewI/lib/9.0/libLLVM_h.jl:2813 [inlined]
   [6] run! at /home/pkgeval/.julia/packages/LLVM/5PewI/src/passmanager.jl:34 [inlined]
   [7] (::GPUCompiler.var"#37#42"{LLVM.Module,GPUCompiler.var"#initialize!#40"{LLVM.Module,LLVM.TargetMachine}})(::LLVM.ModulePassManager) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/optim.jl:41
   [8] LLVM.ModulePassManager(::GPUCompiler.var"#37#42"{LLVM.Module,GPUCompiler.var"#initialize!#40"{LLVM.Module,LLVM.TargetMachine}}) at /home/pkgeval/.julia/packages/LLVM/5PewI/src/passmanager.jl:28
   [9] optimize!(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::LLVM.Module, ::LLVM.Function) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/optim.jl:28
   [10] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [11] macro expansion at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:107 [inlined]
   [12] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [13] codegen(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:97
   [14] compile(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:36
   [15] _cufunction(::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#216#219"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:308
   [16] _cufunction at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:302 [inlined]
   [17] #77 at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:21 [inlined]
   [18] get!(::GPUCompiler.var"#77#78"{Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CUDAnative._cufunction),GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#216#219"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}}, ::Dict{UInt64,Any}, ::UInt64) at ./dict.jl:450
   [19] macro expansion at ./lock.jl:183 [inlined]
   [20] check_cache(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#216#219"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:19
   [21] + at ./int.jl:86 [inlined]
   [22] hash_64_64 at ./hashing.jl:35 [inlined]
   [23] hash_uint64 at ./hashing.jl:62 [inlined]
   [24] hx at ./float.jl:568 [inlined]
   [25] hash at ./float.jl:571 [inlined]
   [26] cached_compilation(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#216#219"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:0
   [27] cached_compilation(::Function, ::GPUCompiler.FunctionSpec{typeof(CuArrays.partial_mapreduce_grid),Tuple{typeof(identity),typeof(*),Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true},CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#216#219"{Int64,CuArray{Float32,3,Nothing}},Tuple{CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}}}}}, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:33
   [28] cufunction(::Function, ::Type{T} where T; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:296
   [29] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:291
   [30] macro expansion at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/mapreduce.jl:197 [inlined]
   [31] mapreducedim!(::Function, ::Function, ::CuArray{Float32,1,Nothing}, ::Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},GeometricFlux.var"#216#219"{Int64,CuArray{Float32,3,Nothing}},Tuple{CuArray{Tuple{Int64,Int64},1,Nothing}}}; init::Float32) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/nvtx/highlevel.jl:83
   [32] _mapreduce(::Function, ::Function, ::CuArray{Tuple{Int64,Int64},1,Nothing}; dims::Function, init::Float32) at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/mapreduce.jl:62
   [33] #mapreduce#31 at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/mapreduce.jl:28 [inlined]
   [34] (::GeometricFlux.var"#214#217"{Float32,CuArray{Int64,2,Nothing},CuArray{Float32,3,Nothing}})(::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/pool.jl:104
   [35] #172#back at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49 [inlined]
   [36] #106 at ./none:0 [inlined]
   [37] (::Zygote.var"#36#37"{typeof(∂(#106))})(::Float32) at /home/pkgeval/.julia/packages/Zygote/YeCEW/src/compiler/interface.jl:46
   [38] gradient(::Function, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/YeCEW/src/compiler/interface.jl:55
   [39] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:63
   [40] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114
   [41] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:47
   [42] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1114
   [43] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:16
  
Test Summary:                    | Pass  Error  Total
GeometricFlux                    | 1208     18   1226
  msgpass                        |    1             1
  layer                          |   42            42
  pool                           |   82            82
  grad                           |   35            35
  Test InnerProductDecoder layer |    2             2
  Test VariationalEncoder layer  |    1             1
  Test GAE model                 |    1             1
  Test VGAE model                |    1             1
  Test Linear Algebra            |   24            24
  scatter                        |  360           360
  scatter-staticarray            |  312           312
  simplegraphs                   |   32            32
  weightedgraphs                 |   32            32
  metagraphs                     |   38            38
  Test adjlist                   |    4             4
  utils                          |   22            22
  cuda/scatter                   |   60            60
  cuda/pool                      |  104      8    112
    UInt32                       |   12            12
    UInt64                       |   12            12
    Int32                        |   16            16
    Int64                        |   16            16
    Float32                      |   24      4     28
      sumpool                    |    4             4
      subpool                    |    4             4
      maxpool                    |    4             4
      minpool                    |    4             4
      prodpool                   |    4             4
      divpool                    |           4      4
      meanpool                   |    4             4
    Float64                      |   24      4     28
      sumpool                    |    4             4
      subpool                    |    4             4
      maxpool                    |    4             4
      minpool                    |    4             4
      prodpool                   |    4             4
      divpool                    |           4      4
      meanpool                   |    4             4
  cuda/grad                      |   25     10     35
    scatter                      |   15      6     21
    pool                         |   10      4     14
  cuda/conv                      |   29            29
  cuda/msgpass                   |    1             1
ERROR: LoadError: Some tests did not pass: 1208 passed, 0 failed, 18 errored, 0 broken.
in expression starting at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/runtests.jl:51
ERROR: Package GeometricFlux errored during testing
Stacktrace:
 [1] pkgerror(::String) at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Pkg/src/Types.jl:52
 [2] test(::Pkg.Types.Context, ::Array{Pkg.Types.PackageSpec,1}; coverage::Bool, julia_args::Cmd, test_args::Cmd, test_fn::Nothing) at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Pkg/src/Operations.jl:1557
 [3] test(::Pkg.Types.Context, ::Array{Pkg.Types.PackageSpec,1}; coverage::Bool, test_fn::Nothing, julia_args::Cmd, test_args::Cmd, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Pkg/src/API.jl:327
 [4] test(::Pkg.Types.Context, ::Array{Pkg.Types.PackageSpec,1}) at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Pkg/src/API.jl:314
 [5] #test#61 at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Pkg/src/API.jl:67 [inlined]
 [6] test at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Pkg/src/API.jl:67 [inlined]
 [7] #test#60 at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Pkg/src/API.jl:66 [inlined]
 [8] test at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Pkg/src/API.jl:66 [inlined]
 [9] test(::String; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Pkg/src/API.jl:65
 [10] test(::String) at /workspace/srcdir/usr/share/julia/stdlib/v1.6/Pkg/src/API.jl:65
 [11] top-level scope at none:16
