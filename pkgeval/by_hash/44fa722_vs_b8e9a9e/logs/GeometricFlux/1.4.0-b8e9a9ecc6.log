Julia Version 1.4.0
Commit b8e9a9ecc6 (2020-03-21 16:36 UTC)
Platform Info:
  OS: Linux (x86_64-linux-gnu)
  CPU: Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-8.0.1 (ORCJIT, skylake)
Environment:
  JULIA_DEPOT_PATH = ::/usr/local/share/julia
  JULIA_NUM_THREADS = 2

  Resolving package versions...
  Installed CompilerSupportLibraries_jll ─ v0.3.3+0
  Installed SortingAlgorithms ──────────── v0.3.1
  Installed CEnum ──────────────────────── v0.2.0
  Installed AbstractTrees ──────────────── v0.3.2
  Installed BinaryProvider ─────────────── v0.5.8
  Installed Colors ─────────────────────── v0.12.0
  Installed GeometricFlux ──────────────── v0.5.0
  Installed LLVM ───────────────────────── v1.3.4
  Installed Requires ───────────────────── v1.0.1
  Installed DiffResults ────────────────── v1.0.2
  Installed ExprTools ──────────────────── v0.1.0
  Installed FillArrays ─────────────────── v0.8.7
  Installed ZipFile ────────────────────── v0.9.1
  Installed NNlib ──────────────────────── v0.6.6
  Installed ZygoteRules ────────────────── v0.2.0
  Installed CodeTracking ───────────────── v0.5.8
  Installed CuArrays ───────────────────── v2.0.1
  Installed DataAPI ────────────────────── v1.1.0
  Installed CUDAapi ────────────────────── v4.0.0
  Installed DiffRules ──────────────────── v1.0.1
  Installed MacroTools ─────────────────── v0.5.5
  Installed Missings ───────────────────── v0.4.3
  Installed Zlib_jll ───────────────────── v1.2.11+9
  Installed SpecialFunctions ───────────── v0.10.0
  Installed OrderedCollections ─────────── v1.1.0
  Installed ArrayLayouts ───────────────── v0.2.4
  Installed ColorTypes ─────────────────── v0.10.0
  Installed Media ──────────────────────── v0.5.0
  Installed Reexport ───────────────────── v0.2.0
  Installed ForwardDiff ────────────────── v0.10.10
  Installed StaticArrays ───────────────── v0.12.1
  Installed DataStructures ─────────────── v0.17.11
  Installed StatsBase ──────────────────── v0.33.0
  Installed GPUArrays ──────────────────── v3.1.0
  Installed CUDAnative ─────────────────── v3.0.4
  Installed Flux ───────────────────────── v0.10.4
  Installed TimerOutputs ───────────────── v0.5.3
  Installed Juno ───────────────────────── v0.8.1
  Installed NaNMath ────────────────────── v0.3.3
  Installed AbstractFFTs ───────────────── v0.5.0
  Installed CommonSubexpressions ───────── v0.2.0
  Installed IRTools ────────────────────── v0.3.1
  Installed FixedPointNumbers ──────────── v0.8.0
  Installed Adapt ──────────────────────── v1.0.1
  Installed OpenSpecFun_jll ────────────── v0.5.3+3
  Installed CodecZlib ──────────────────── v0.7.0
  Installed Zygote ─────────────────────── v0.4.15
  Installed CUDAdrv ────────────────────── v6.2.2
  Installed Cthulhu ────────────────────── v1.0.2
  Installed TranscodingStreams ─────────── v0.9.5
   Updating `~/.julia/environments/v1.4/Project.toml`
  [7e08b658] + GeometricFlux v0.5.0
   Updating `~/.julia/environments/v1.4/Manifest.toml`
  [621f4979] + AbstractFFTs v0.5.0
  [1520ce14] + AbstractTrees v0.3.2
  [79e6a3ab] + Adapt v1.0.1
  [4c555306] + ArrayLayouts v0.2.4
  [b99e7846] + BinaryProvider v0.5.8
  [fa961155] + CEnum v0.2.0
  [3895d2a7] + CUDAapi v4.0.0
  [c5f51814] + CUDAdrv v6.2.2
  [be33ccc6] + CUDAnative v3.0.4
  [da1fd8a2] + CodeTracking v0.5.8
  [944b1d66] + CodecZlib v0.7.0
  [3da002f7] + ColorTypes v0.10.0
  [5ae59095] + Colors v0.12.0
  [bbf7d656] + CommonSubexpressions v0.2.0
  [e66e0078] + CompilerSupportLibraries_jll v0.3.3+0
  [f68482b8] + Cthulhu v1.0.2
  [3a865a2d] + CuArrays v2.0.1
  [9a962f9c] + DataAPI v1.1.0
  [864edb3b] + DataStructures v0.17.11
  [163ba53b] + DiffResults v1.0.2
  [b552c78f] + DiffRules v1.0.1
  [e2ba6199] + ExprTools v0.1.0
  [1a297f60] + FillArrays v0.8.7
  [53c48c17] + FixedPointNumbers v0.8.0
  [587475ba] + Flux v0.10.4
  [f6369f11] + ForwardDiff v0.10.10
  [0c68f7d7] + GPUArrays v3.1.0
  [7e08b658] + GeometricFlux v0.5.0
  [7869d1d1] + IRTools v0.3.1
  [e5e0dc1b] + Juno v0.8.1
  [929cbde3] + LLVM v1.3.4
  [1914dd2f] + MacroTools v0.5.5
  [e89f7d12] + Media v0.5.0
  [e1d29d7a] + Missings v0.4.3
  [872c559c] + NNlib v0.6.6
  [77ba4419] + NaNMath v0.3.3
  [efe28fd5] + OpenSpecFun_jll v0.5.3+3
  [bac558e1] + OrderedCollections v1.1.0
  [189a3867] + Reexport v0.2.0
  [ae029012] + Requires v1.0.1
  [a2af1166] + SortingAlgorithms v0.3.1
  [276daf66] + SpecialFunctions v0.10.0
  [90137ffa] + StaticArrays v0.12.1
  [2913bbd2] + StatsBase v0.33.0
  [a759f4b9] + TimerOutputs v0.5.3
  [3bb67fe8] + TranscodingStreams v0.9.5
  [a5390f91] + ZipFile v0.9.1
  [83775a58] + Zlib_jll v1.2.11+9
  [e88e6eb3] + Zygote v0.4.15
  [700de1a5] + ZygoteRules v0.2.0
  [2a0f44e3] + Base64 
  [ade2ca70] + Dates 
  [8bb1440f] + DelimitedFiles 
  [8ba89e20] + Distributed 
  [b77e0a4c] + InteractiveUtils 
  [76f85450] + LibGit2 
  [8f399da3] + Libdl 
  [37e2e46d] + LinearAlgebra 
  [56ddb016] + Logging 
  [d6f4376e] + Markdown 
  [a63ad114] + Mmap 
  [44cfe95a] + Pkg 
  [de0858da] + Printf 
  [9abbd945] + Profile 
  [3fa0cd96] + REPL 
  [9a3f8284] + Random 
  [ea8e919c] + SHA 
  [9e88b42a] + Serialization 
  [6462fe0b] + Sockets 
  [2f01184e] + SparseArrays 
  [10745b16] + Statistics 
  [8dfed614] + Test 
  [cf7118a7] + UUIDs 
  [4ec0a83e] + Unicode 
   Building NNlib → `~/.julia/packages/NNlib/FAI3o/deps/build.log`
    Testing GeometricFlux
Status `/tmp/jl_7guyXK/Manifest.toml`
  [621f4979] AbstractFFTs v0.5.0
  [1520ce14] AbstractTrees v0.3.2
  [79e6a3ab] Adapt v1.0.1
  [ec485272] ArnoldiMethod v0.0.4
  [4c555306] ArrayLayouts v0.2.4
  [b99e7846] BinaryProvider v0.5.8
  [fa961155] CEnum v0.2.0
  [3895d2a7] CUDAapi v4.0.0
  [c5f51814] CUDAdrv v6.2.2
  [be33ccc6] CUDAnative v3.0.4
  [da1fd8a2] CodeTracking v0.5.8
  [944b1d66] CodecZlib v0.7.0
  [3da002f7] ColorTypes v0.10.0
  [5ae59095] Colors v0.12.0
  [bbf7d656] CommonSubexpressions v0.2.0
  [e66e0078] CompilerSupportLibraries_jll v0.3.3+0
  [f68482b8] Cthulhu v1.0.2
  [3a865a2d] CuArrays v2.0.1
  [9a962f9c] DataAPI v1.1.0
  [864edb3b] DataStructures v0.17.11
  [163ba53b] DiffResults v1.0.2
  [b552c78f] DiffRules v1.0.1
  [e2ba6199] ExprTools v0.1.0
  [5789e2e9] FileIO v1.2.4
  [1a297f60] FillArrays v0.8.7
  [53c48c17] FixedPointNumbers v0.8.0
  [587475ba] Flux v0.10.4
  [f6369f11] ForwardDiff v0.10.10
  [0c68f7d7] GPUArrays v3.1.0
  [7e08b658] GeometricFlux v0.5.0
  [7869d1d1] IRTools v0.3.1
  [d25df0c9] Inflate v0.1.2
  [033835bb] JLD2 v0.1.3
  [e5e0dc1b] Juno v0.8.1
  [929cbde3] LLVM v1.3.4
  [093fc24a] LightGraphs v1.3.1
  [1914dd2f] MacroTools v0.5.5
  [e89f7d12] Media v0.5.0
  [626554b9] MetaGraphs v0.6.4
  [e1d29d7a] Missings v0.4.3
  [872c559c] NNlib v0.6.6
  [77ba4419] NaNMath v0.3.3
  [efe28fd5] OpenSpecFun_jll v0.5.3+3
  [bac558e1] OrderedCollections v1.1.0
  [189a3867] Reexport v0.2.0
  [ae029012] Requires v1.0.1
  [699a6c99] SimpleTraits v0.9.1
  [47aef6b3] SimpleWeightedGraphs v1.1.1
  [a2af1166] SortingAlgorithms v0.3.1
  [276daf66] SpecialFunctions v0.10.0
  [90137ffa] StaticArrays v0.12.1
  [2913bbd2] StatsBase v0.33.0
  [a759f4b9] TimerOutputs v0.5.3
  [3bb67fe8] TranscodingStreams v0.9.5
  [a5390f91] ZipFile v0.9.1
  [83775a58] Zlib_jll v1.2.11+9
  [e88e6eb3] Zygote v0.4.15
  [700de1a5] ZygoteRules v0.2.0
  [2a0f44e3] Base64 
  [ade2ca70] Dates 
  [8bb1440f] DelimitedFiles 
  [8ba89e20] Distributed 
  [b77e0a4c] InteractiveUtils 
  [76f85450] LibGit2 
  [8f399da3] Libdl 
  [37e2e46d] LinearAlgebra 
  [56ddb016] Logging 
  [d6f4376e] Markdown 
  [a63ad114] Mmap 
  [44cfe95a] Pkg 
  [de0858da] Printf 
  [9abbd945] Profile 
  [3fa0cd96] REPL 
  [9a3f8284] Random 
  [ea8e919c] SHA 
  [9e88b42a] Serialization 
  [1a1011a3] SharedArrays 
  [6462fe0b] Sockets 
  [2f01184e] SparseArrays 
  [10745b16] Statistics 
  [8dfed614] Test 
  [cf7118a7] UUIDs 
  [4ec0a83e] Unicode 
┌ Warning: Package GeometricFlux does not have LightGraphs in its dependencies:
│ - If you have GeometricFlux checked out for development and have
│   added LightGraphs as a dependency but haven't updated your primary
│   environment's manifest file, try `Pkg.resolve()`.
│ - Otherwise you may need to report an issue with GeometricFlux
└ Loading LightGraphs into GeometricFlux from project dependency, future warnings for GeometricFlux are suppressed.
┌ Warning: Performing scalar operations on GPU arrays: This is very slow, consider disallowing these operations with `allowscalar(false)`
└ @ GPUArrays ~/.julia/packages/GPUArrays/QDGmr/src/host/indexing.jl:43
divpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:128
  Test threw exception
  Expression: divpool(CuArray{Int64}(cluster), T.(X)) ≈ T.(y)
  GPU compilation of broadcast(CuArrays.CuKernelContext, CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global}, Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}) failed
  KernelError: passing and using non-bitstype argument
  
  Argument 4 to your kernel function is of type Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}.
  That type is not isbits, and such arguments are only allowed when they are unused by the kernel.  .args is of type Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}} which is not isbits.
      .2 is of type Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}} which is not isbits.
        .x is of type Array{Float32,2} which is not isbits.
  
  
  Stacktrace:
   [1] check_invocation(::CUDAnative.CompilerJob, ::LLVM.Function) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/validation.jl:72
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:185 [inlined]
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:184
   [5] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:45
   [6] #compile#171 at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:33 [inlined]
   [7] cufunction_slow(::Function, ::Type{T} where T, ::Int64; name::String, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:326
   [8] #219 at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:393 [inlined]
   [9] get!(::CUDAnative.var"#219#220"{String,Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},GPUArrays.var"#26#27",DataType,Int64}, ::Dict{UInt64,CUDAnative.HostKernel}, ::UInt64) at ./dict.jl:452
   [10] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:392 [inlined]
   [11] macro expansion at ./lock.jl:183 [inlined]
   [12] cufunction_fast(::Function, ::Type{T} where T, ::Int64; name::String, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:391
   [13] getproperty at ./Base.jl:33 [inlined]
   [14] merge at ./namedtuple.jl:235 [inlined]
   [15] cufunction(::GPUArrays.var"#26#27", ::Type{Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; kwargs::Base.Iterators.Pairs{Symbol,String,Tuple{Symbol},NamedTuple{(:name,),Tuple{String}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:0
   [16] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:157 [inlined]
   [17] gpu_call(::CuArrays.CuArrayBackend, ::Function, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64; name::String) at /home/pkgeval/.julia/packages/CuArrays/e8PLr/src/gpuarrays.jl:32
   [18] #gpu_call#1 at /home/pkgeval/.julia/packages/GPUArrays/QDGmr/src/device/execution.jl:60 [inlined]
   [19] copyto! at /home/pkgeval/.julia/packages/GPUArrays/QDGmr/src/host/broadcast.jl:63 [inlined]
   [20] copyto! at ./broadcast.jl:864 [inlined]
   [21] copy at ./broadcast.jl:840 [inlined]
   [22] materialize at ./broadcast.jl:820 [inlined]
   [23] broadcast_preserving_zero_d at ./broadcast.jl:809 [inlined]
   [24] -(::CuArray{Float32,2,Nothing}, ::Array{Float32,2}) at ./arraymath.jl:39
   [25] isapprox(::CuArray{Float32,2,Nothing}, ::Array{Float32,2}; atol::Int64, rtol::Float32, nans::Bool, norm::typeof(LinearAlgebra.norm)) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/generic.jl:1588
   [26] isapprox(::CuArray{Float32,2,Nothing}, ::Array{Float32,2}) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/generic.jl:1588
   [27] eval_test(::Expr, ::Expr, ::LineNumberNode, ::Bool) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:246
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:128 [inlined]
   [29] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:126 [inlined]
   [31] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:80 [inlined]
   [33] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:7
  
divpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:129
  Test threw exception
  Expression: pool(:div, CuArray{Int64}(cluster), T.(X)) ≈ T.(y)
  GPU compilation of broadcast(CuArrays.CuKernelContext, CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global}, Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}) failed
  KernelError: passing and using non-bitstype argument
  
  Argument 4 to your kernel function is of type Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}.
  That type is not isbits, and such arguments are only allowed when they are unused by the kernel.  .args is of type Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}} which is not isbits.
      .2 is of type Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}} which is not isbits.
        .x is of type Array{Float32,2} which is not isbits.
  
  
  Stacktrace:
   [1] check_invocation(::CUDAnative.CompilerJob, ::LLVM.Function) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/validation.jl:72
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:185 [inlined]
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:184
   [5] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:45
   [6] #compile#171 at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:33 [inlined]
   [7] cufunction_slow(::Function, ::Type{T} where T, ::Int64; name::String, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:326
   [8] #219 at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:393 [inlined]
   [9] get!(::CUDAnative.var"#219#220"{String,Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},GPUArrays.var"#26#27",DataType,Int64}, ::Dict{UInt64,CUDAnative.HostKernel}, ::UInt64) at ./dict.jl:452
   [10] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:392 [inlined]
   [11] macro expansion at ./lock.jl:183 [inlined]
   [12] cufunction_fast(::Function, ::Type{T} where T, ::Int64; name::String, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:391
   [13] getproperty at ./Base.jl:33 [inlined]
   [14] merge at ./namedtuple.jl:235 [inlined]
   [15] cufunction(::GPUArrays.var"#26#27", ::Type{Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; kwargs::Base.Iterators.Pairs{Symbol,String,Tuple{Symbol},NamedTuple{(:name,),Tuple{String}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:0
   [16] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:157 [inlined]
   [17] gpu_call(::CuArrays.CuArrayBackend, ::Function, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64; name::String) at /home/pkgeval/.julia/packages/CuArrays/e8PLr/src/gpuarrays.jl:32
   [18] #gpu_call#1 at /home/pkgeval/.julia/packages/GPUArrays/QDGmr/src/device/execution.jl:60 [inlined]
   [19] copyto! at /home/pkgeval/.julia/packages/GPUArrays/QDGmr/src/host/broadcast.jl:63 [inlined]
   [20] copyto! at ./broadcast.jl:864 [inlined]
   [21] copy at ./broadcast.jl:840 [inlined]
   [22] materialize at ./broadcast.jl:820 [inlined]
   [23] broadcast_preserving_zero_d at ./broadcast.jl:809 [inlined]
   [24] -(::CuArray{Float32,2,Nothing}, ::Array{Float32,2}) at ./arraymath.jl:39
   [25] isapprox(::CuArray{Float32,2,Nothing}, ::Array{Float32,2}; atol::Int64, rtol::Float32, nans::Bool, norm::typeof(LinearAlgebra.norm)) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/generic.jl:1588
   [26] isapprox(::CuArray{Float32,2,Nothing}, ::Array{Float32,2}) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/generic.jl:1588
   [27] eval_test(::Expr, ::Expr, ::LineNumberNode, ::Bool) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:246
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:129 [inlined]
   [29] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:126 [inlined]
   [31] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:80 [inlined]
   [33] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:7
  
divpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:130
  Test threw exception
  Expression: divpool(cluster, T.(X)) ≈ T.(y)
  GPU compilation of broadcast(CuArrays.CuKernelContext, CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global}, Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}) failed
  KernelError: passing and using non-bitstype argument
  
  Argument 4 to your kernel function is of type Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}.
  That type is not isbits, and such arguments are only allowed when they are unused by the kernel.  .args is of type Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}} which is not isbits.
      .2 is of type Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}} which is not isbits.
        .x is of type Array{Float32,2} which is not isbits.
  
  
  Stacktrace:
   [1] check_invocation(::CUDAnative.CompilerJob, ::LLVM.Function) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/validation.jl:72
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:185 [inlined]
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:184
   [5] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:45
   [6] #compile#171 at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:33 [inlined]
   [7] cufunction_slow(::Function, ::Type{T} where T, ::Int64; name::String, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:326
   [8] #219 at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:393 [inlined]
   [9] get!(::CUDAnative.var"#219#220"{String,Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},GPUArrays.var"#26#27",DataType,Int64}, ::Dict{UInt64,CUDAnative.HostKernel}, ::UInt64) at ./dict.jl:452
   [10] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:392 [inlined]
   [11] macro expansion at ./lock.jl:183 [inlined]
   [12] cufunction_fast(::Function, ::Type{T} where T, ::Int64; name::String, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:391
   [13] getproperty at ./Base.jl:33 [inlined]
   [14] merge at ./namedtuple.jl:235 [inlined]
   [15] cufunction(::GPUArrays.var"#26#27", ::Type{Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; kwargs::Base.Iterators.Pairs{Symbol,String,Tuple{Symbol},NamedTuple{(:name,),Tuple{String}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:0
   [16] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:157 [inlined]
   [17] gpu_call(::CuArrays.CuArrayBackend, ::Function, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64; name::String) at /home/pkgeval/.julia/packages/CuArrays/e8PLr/src/gpuarrays.jl:32
   [18] #gpu_call#1 at /home/pkgeval/.julia/packages/GPUArrays/QDGmr/src/device/execution.jl:60 [inlined]
   [19] copyto! at /home/pkgeval/.julia/packages/GPUArrays/QDGmr/src/host/broadcast.jl:63 [inlined]
   [20] copyto! at ./broadcast.jl:864 [inlined]
   [21] copy at ./broadcast.jl:840 [inlined]
   [22] materialize at ./broadcast.jl:820 [inlined]
   [23] broadcast_preserving_zero_d at ./broadcast.jl:809 [inlined]
   [24] -(::CuArray{Float32,2,Nothing}, ::Array{Float32,2}) at ./arraymath.jl:39
   [25] isapprox(::CuArray{Float32,2,Nothing}, ::Array{Float32,2}; atol::Int64, rtol::Float32, nans::Bool, norm::typeof(LinearAlgebra.norm)) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/generic.jl:1588
   [26] isapprox(::CuArray{Float32,2,Nothing}, ::Array{Float32,2}) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/generic.jl:1588
   [27] eval_test(::Expr, ::Expr, ::LineNumberNode, ::Bool) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:246
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:130 [inlined]
   [29] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:126 [inlined]
   [31] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:80 [inlined]
   [33] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:7
  
divpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:131
  Test threw exception
  Expression: pool(:div, cluster, T.(X)) ≈ T.(y)
  GPU compilation of broadcast(CuArrays.CuKernelContext, CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global}, Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}) failed
  KernelError: passing and using non-bitstype argument
  
  Argument 4 to your kernel function is of type Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}.
  That type is not isbits, and such arguments are only allowed when they are unused by the kernel.  .args is of type Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}} which is not isbits.
      .2 is of type Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}} which is not isbits.
        .x is of type Array{Float32,2} which is not isbits.
  
  
  Stacktrace:
   [1] check_invocation(::CUDAnative.CompilerJob, ::LLVM.Function) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/validation.jl:72
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:185 [inlined]
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:184
   [5] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:45
   [6] #compile#171 at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:33 [inlined]
   [7] cufunction_slow(::Function, ::Type{T} where T, ::Int64; name::String, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:326
   [8] #219 at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:393 [inlined]
   [9] get!(::CUDAnative.var"#219#220"{String,Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},GPUArrays.var"#26#27",DataType,Int64}, ::Dict{UInt64,CUDAnative.HostKernel}, ::UInt64) at ./dict.jl:452
   [10] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:392 [inlined]
   [11] macro expansion at ./lock.jl:183 [inlined]
   [12] cufunction_fast(::Function, ::Type{T} where T, ::Int64; name::String, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:391
   [13] getproperty at ./Base.jl:33 [inlined]
   [14] merge at ./namedtuple.jl:235 [inlined]
   [15] cufunction(::GPUArrays.var"#26#27", ::Type{Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float32,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; kwargs::Base.Iterators.Pairs{Symbol,String,Tuple{Symbol},NamedTuple{(:name,),Tuple{String}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:0
   [16] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:157 [inlined]
   [17] gpu_call(::CuArrays.CuArrayBackend, ::Function, ::Tuple{CuArray{Float32,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CuArray{Float32,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float32,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64; name::String) at /home/pkgeval/.julia/packages/CuArrays/e8PLr/src/gpuarrays.jl:32
   [18] #gpu_call#1 at /home/pkgeval/.julia/packages/GPUArrays/QDGmr/src/device/execution.jl:60 [inlined]
   [19] copyto! at /home/pkgeval/.julia/packages/GPUArrays/QDGmr/src/host/broadcast.jl:63 [inlined]
   [20] copyto! at ./broadcast.jl:864 [inlined]
   [21] copy at ./broadcast.jl:840 [inlined]
   [22] materialize at ./broadcast.jl:820 [inlined]
   [23] broadcast_preserving_zero_d at ./broadcast.jl:809 [inlined]
   [24] -(::CuArray{Float32,2,Nothing}, ::Array{Float32,2}) at ./arraymath.jl:39
   [25] isapprox(::CuArray{Float32,2,Nothing}, ::Array{Float32,2}; atol::Int64, rtol::Float32, nans::Bool, norm::typeof(LinearAlgebra.norm)) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/generic.jl:1588
   [26] isapprox(::CuArray{Float32,2,Nothing}, ::Array{Float32,2}) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/generic.jl:1588
   [27] eval_test(::Expr, ::Expr, ::LineNumberNode, ::Bool) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:246
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:131 [inlined]
   [29] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:126 [inlined]
   [31] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:80 [inlined]
   [33] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:7
  
divpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:128
  Test threw exception
  Expression: divpool(CuArray{Int64}(cluster), T.(X)) ≈ T.(y)
  GPU compilation of broadcast(CuArrays.CuKernelContext, CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global}, Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}) failed
  KernelError: passing and using non-bitstype argument
  
  Argument 4 to your kernel function is of type Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}.
  That type is not isbits, and such arguments are only allowed when they are unused by the kernel.  .args is of type Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}} which is not isbits.
      .2 is of type Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}} which is not isbits.
        .x is of type Array{Float64,2} which is not isbits.
  
  
  Stacktrace:
   [1] check_invocation(::CUDAnative.CompilerJob, ::LLVM.Function) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/validation.jl:72
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:185 [inlined]
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:184
   [5] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:45
   [6] #compile#171 at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:33 [inlined]
   [7] cufunction_slow(::Function, ::Type{T} where T, ::Int64; name::String, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:326
   [8] #219 at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:393 [inlined]
   [9] get!(::CUDAnative.var"#219#220"{String,Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},GPUArrays.var"#26#27",DataType,Int64}, ::Dict{UInt64,CUDAnative.HostKernel}, ::UInt64) at ./dict.jl:452
   [10] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:392 [inlined]
   [11] macro expansion at ./lock.jl:183 [inlined]
   [12] cufunction_fast(::Function, ::Type{T} where T, ::Int64; name::String, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:391
   [13] getproperty at ./Base.jl:33 [inlined]
   [14] merge at ./namedtuple.jl:235 [inlined]
   [15] cufunction(::GPUArrays.var"#26#27", ::Type{Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; kwargs::Base.Iterators.Pairs{Symbol,String,Tuple{Symbol},NamedTuple{(:name,),Tuple{String}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:0
   [16] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:157 [inlined]
   [17] gpu_call(::CuArrays.CuArrayBackend, ::Function, ::Tuple{CuArray{Float64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CuArray{Float64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64; name::String) at /home/pkgeval/.julia/packages/CuArrays/e8PLr/src/gpuarrays.jl:32
   [18] #gpu_call#1 at /home/pkgeval/.julia/packages/GPUArrays/QDGmr/src/device/execution.jl:60 [inlined]
   [19] copyto! at /home/pkgeval/.julia/packages/GPUArrays/QDGmr/src/host/broadcast.jl:63 [inlined]
   [20] copyto! at ./broadcast.jl:864 [inlined]
   [21] copy at ./broadcast.jl:840 [inlined]
   [22] materialize at ./broadcast.jl:820 [inlined]
   [23] broadcast_preserving_zero_d at ./broadcast.jl:809 [inlined]
   [24] -(::CuArray{Float64,2,Nothing}, ::Array{Float64,2}) at ./arraymath.jl:39
   [25] isapprox(::CuArray{Float64,2,Nothing}, ::Array{Float64,2}; atol::Int64, rtol::Float64, nans::Bool, norm::typeof(LinearAlgebra.norm)) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/generic.jl:1588
   [26] isapprox(::CuArray{Float64,2,Nothing}, ::Array{Float64,2}) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/generic.jl:1588
   [27] eval_test(::Expr, ::Expr, ::LineNumberNode, ::Bool) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:246
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:128 [inlined]
   [29] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:126 [inlined]
   [31] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:80 [inlined]
   [33] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:7
  
divpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:129
  Test threw exception
  Expression: pool(:div, CuArray{Int64}(cluster), T.(X)) ≈ T.(y)
  GPU compilation of broadcast(CuArrays.CuKernelContext, CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global}, Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}) failed
  KernelError: passing and using non-bitstype argument
  
  Argument 4 to your kernel function is of type Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}.
  That type is not isbits, and such arguments are only allowed when they are unused by the kernel.  .args is of type Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}} which is not isbits.
      .2 is of type Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}} which is not isbits.
        .x is of type Array{Float64,2} which is not isbits.
  
  
  Stacktrace:
   [1] check_invocation(::CUDAnative.CompilerJob, ::LLVM.Function) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/validation.jl:72
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:185 [inlined]
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:184
   [5] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:45
   [6] #compile#171 at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:33 [inlined]
   [7] cufunction_slow(::Function, ::Type{T} where T, ::Int64; name::String, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:326
   [8] #219 at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:393 [inlined]
   [9] get!(::CUDAnative.var"#219#220"{String,Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},GPUArrays.var"#26#27",DataType,Int64}, ::Dict{UInt64,CUDAnative.HostKernel}, ::UInt64) at ./dict.jl:452
   [10] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:392 [inlined]
   [11] macro expansion at ./lock.jl:183 [inlined]
   [12] cufunction_fast(::Function, ::Type{T} where T, ::Int64; name::String, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:391
   [13] getproperty at ./Base.jl:33 [inlined]
   [14] merge at ./namedtuple.jl:235 [inlined]
   [15] cufunction(::GPUArrays.var"#26#27", ::Type{Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; kwargs::Base.Iterators.Pairs{Symbol,String,Tuple{Symbol},NamedTuple{(:name,),Tuple{String}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:0
   [16] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:157 [inlined]
   [17] gpu_call(::CuArrays.CuArrayBackend, ::Function, ::Tuple{CuArray{Float64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CuArray{Float64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64; name::String) at /home/pkgeval/.julia/packages/CuArrays/e8PLr/src/gpuarrays.jl:32
   [18] #gpu_call#1 at /home/pkgeval/.julia/packages/GPUArrays/QDGmr/src/device/execution.jl:60 [inlined]
   [19] copyto! at /home/pkgeval/.julia/packages/GPUArrays/QDGmr/src/host/broadcast.jl:63 [inlined]
   [20] copyto! at ./broadcast.jl:864 [inlined]
   [21] copy at ./broadcast.jl:840 [inlined]
   [22] materialize at ./broadcast.jl:820 [inlined]
   [23] broadcast_preserving_zero_d at ./broadcast.jl:809 [inlined]
   [24] -(::CuArray{Float64,2,Nothing}, ::Array{Float64,2}) at ./arraymath.jl:39
   [25] isapprox(::CuArray{Float64,2,Nothing}, ::Array{Float64,2}; atol::Int64, rtol::Float64, nans::Bool, norm::typeof(LinearAlgebra.norm)) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/generic.jl:1588
   [26] isapprox(::CuArray{Float64,2,Nothing}, ::Array{Float64,2}) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/generic.jl:1588
   [27] eval_test(::Expr, ::Expr, ::LineNumberNode, ::Bool) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:246
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:129 [inlined]
   [29] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:126 [inlined]
   [31] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:80 [inlined]
   [33] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:7
  
divpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:130
  Test threw exception
  Expression: divpool(cluster, T.(X)) ≈ T.(y)
  GPU compilation of broadcast(CuArrays.CuKernelContext, CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global}, Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}) failed
  KernelError: passing and using non-bitstype argument
  
  Argument 4 to your kernel function is of type Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}.
  That type is not isbits, and such arguments are only allowed when they are unused by the kernel.  .args is of type Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}} which is not isbits.
      .2 is of type Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}} which is not isbits.
        .x is of type Array{Float64,2} which is not isbits.
  
  
  Stacktrace:
   [1] check_invocation(::CUDAnative.CompilerJob, ::LLVM.Function) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/validation.jl:72
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:185 [inlined]
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:184
   [5] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:45
   [6] #compile#171 at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:33 [inlined]
   [7] cufunction_slow(::Function, ::Type{T} where T, ::Int64; name::String, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:326
   [8] #219 at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:393 [inlined]
   [9] get!(::CUDAnative.var"#219#220"{String,Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},GPUArrays.var"#26#27",DataType,Int64}, ::Dict{UInt64,CUDAnative.HostKernel}, ::UInt64) at ./dict.jl:452
   [10] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:392 [inlined]
   [11] macro expansion at ./lock.jl:183 [inlined]
   [12] cufunction_fast(::Function, ::Type{T} where T, ::Int64; name::String, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:391
   [13] getproperty at ./Base.jl:33 [inlined]
   [14] merge at ./namedtuple.jl:235 [inlined]
   [15] cufunction(::GPUArrays.var"#26#27", ::Type{Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; kwargs::Base.Iterators.Pairs{Symbol,String,Tuple{Symbol},NamedTuple{(:name,),Tuple{String}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:0
   [16] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:157 [inlined]
   [17] gpu_call(::CuArrays.CuArrayBackend, ::Function, ::Tuple{CuArray{Float64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CuArray{Float64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64; name::String) at /home/pkgeval/.julia/packages/CuArrays/e8PLr/src/gpuarrays.jl:32
   [18] #gpu_call#1 at /home/pkgeval/.julia/packages/GPUArrays/QDGmr/src/device/execution.jl:60 [inlined]
   [19] copyto! at /home/pkgeval/.julia/packages/GPUArrays/QDGmr/src/host/broadcast.jl:63 [inlined]
   [20] copyto! at ./broadcast.jl:864 [inlined]
   [21] copy at ./broadcast.jl:840 [inlined]
   [22] materialize at ./broadcast.jl:820 [inlined]
   [23] broadcast_preserving_zero_d at ./broadcast.jl:809 [inlined]
   [24] -(::CuArray{Float64,2,Nothing}, ::Array{Float64,2}) at ./arraymath.jl:39
   [25] isapprox(::CuArray{Float64,2,Nothing}, ::Array{Float64,2}; atol::Int64, rtol::Float64, nans::Bool, norm::typeof(LinearAlgebra.norm)) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/generic.jl:1588
   [26] isapprox(::CuArray{Float64,2,Nothing}, ::Array{Float64,2}) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/generic.jl:1588
   [27] eval_test(::Expr, ::Expr, ::LineNumberNode, ::Bool) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:246
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:130 [inlined]
   [29] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:126 [inlined]
   [31] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:80 [inlined]
   [33] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:7
  
divpool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:131
  Test threw exception
  Expression: pool(:div, cluster, T.(X)) ≈ T.(y)
  GPU compilation of broadcast(CuArrays.CuKernelContext, CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global}, Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}) failed
  KernelError: passing and using non-bitstype argument
  
  Argument 4 to your kernel function is of type Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}.
  That type is not isbits, and such arguments are only allowed when they are unused by the kernel.  .args is of type Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}} which is not isbits.
      .2 is of type Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}} which is not isbits.
        .x is of type Array{Float64,2} which is not isbits.
  
  
  Stacktrace:
   [1] check_invocation(::CUDAnative.CompilerJob, ::LLVM.Function) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/validation.jl:72
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:185 [inlined]
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:184
   [5] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:45
   [6] #compile#171 at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:33 [inlined]
   [7] cufunction_slow(::Function, ::Type{T} where T, ::Int64; name::String, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:326
   [8] #219 at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:393 [inlined]
   [9] get!(::CUDAnative.var"#219#220"{String,Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},GPUArrays.var"#26#27",DataType,Int64}, ::Dict{UInt64,CUDAnative.HostKernel}, ::UInt64) at ./dict.jl:452
   [10] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:392 [inlined]
   [11] macro expansion at ./lock.jl:183 [inlined]
   [12] cufunction_fast(::Function, ::Type{T} where T, ::Int64; name::String, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:391
   [13] getproperty at ./Base.jl:33 [inlined]
   [14] merge at ./namedtuple.jl:235 [inlined]
   [15] cufunction(::GPUArrays.var"#26#27", ::Type{Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,2,CUDAnative.AS.Global},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}}; kwargs::Base.Iterators.Pairs{Symbol,String,Tuple{Symbol},NamedTuple{(:name,),Tuple{String}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:0
   [16] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:157 [inlined]
   [17] gpu_call(::CuArrays.CuArrayBackend, ::Function, ::Tuple{CuArray{Float64,2,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CuArray{Float64,2,Nothing},Tuple{Bool,Bool},Tuple{Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,2},Tuple{Bool,Bool},Tuple{Int64,Int64}}}}}, ::Int64; name::String) at /home/pkgeval/.julia/packages/CuArrays/e8PLr/src/gpuarrays.jl:32
   [18] #gpu_call#1 at /home/pkgeval/.julia/packages/GPUArrays/QDGmr/src/device/execution.jl:60 [inlined]
   [19] copyto! at /home/pkgeval/.julia/packages/GPUArrays/QDGmr/src/host/broadcast.jl:63 [inlined]
   [20] copyto! at ./broadcast.jl:864 [inlined]
   [21] copy at ./broadcast.jl:840 [inlined]
   [22] materialize at ./broadcast.jl:820 [inlined]
   [23] broadcast_preserving_zero_d at ./broadcast.jl:809 [inlined]
   [24] -(::CuArray{Float64,2,Nothing}, ::Array{Float64,2}) at ./arraymath.jl:39
   [25] isapprox(::CuArray{Float64,2,Nothing}, ::Array{Float64,2}; atol::Int64, rtol::Float64, nans::Bool, norm::typeof(LinearAlgebra.norm)) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/generic.jl:1588
   [26] isapprox(::CuArray{Float64,2,Nothing}, ::Array{Float64,2}) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/generic.jl:1588
   [27] eval_test(::Expr, ::Expr, ::LineNumberNode, ::Bool) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:246
   [28] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:131 [inlined]
   [29] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [30] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:126 [inlined]
   [31] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [32] macro expansion at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:80 [inlined]
   [33] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/pool.jl:7
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:33
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_mul!(x, us, xs))
            end), ys) == (∇y_mul,)
  GPU compilation of mapreduce_grid(GeometricFlux.var"#183#186"{Int64,CuArray{Float32,3,Nothing}}, typeof(*), CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}, CUDAnative.CuDeviceArray{Float32,1,CUDAnative.AS.Global}, Float32, CartesianIndices{1,Tuple{Base.OneTo{Int64}}}, CartesianIndices{1,Tuple{Base.OneTo{Int64}}}, Val{true}) failed
  KernelError: recursion is currently not supported
  
  Try inspecting the generated code with any of the @device_code_... macros.
  
  Stacktrace:
   [1] _nextind_str at strings/string.jl:141
   [2] nextind at strings/string.jl:137
   [3] _nextind_str at strings/string.jl:141
   [4] _split at strings/util.jl:325
   [5] env_override_minlevel at logging.jl:423
   [6] current_logger_for_env at logging.jl:386
   [7] assertscalar at /home/pkgeval/.julia/packages/GPUArrays/QDGmr/src/host/indexing.jl:40
   [8] #183 at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/scatter.jl:125
   [9] mapreduce_grid at /home/pkgeval/.julia/packages/CuArrays/e8PLr/src/mapreduce.jl:89
  Stacktrace:
   [1] (::CUDAnative.var"#hook_emit_function#122"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}})(::Core.MethodInstance, ::Core.CodeInfo, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/irgen.jl:205
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/irgen.jl:261
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/irgen.jl:319
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:97 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:96
   [9] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:45
   [10] #compile#171 at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:33 [inlined]
   [11] cufunction_slow(::Function, ::Type{T} where T, ::Int64; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:326
   [12] #219 at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:393 [inlined]
   [13] get!(::CUDAnative.var"#219#220"{Nothing,Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CuArrays.mapreduce_grid),DataType,Int64}, ::Dict{UInt64,CUDAnative.HostKernel}, ::UInt64) at ./dict.jl:452
   [14] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:392 [inlined]
   [15] macro expansion at ./lock.jl:183 [inlined]
   [16] cufunction_fast(::Function, ::Type{T} where T, ::Int64; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:391
   [17] cufunction(::typeof(CuArrays.mapreduce_grid), ::Type{Tuple{GeometricFlux.var"#183#186"{Int64,CuArray{Float32,3,Nothing}},typeof(*),CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float32,1,CUDAnative.AS.Global},Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:422
   [18] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:422
   [19] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:157 [inlined]
   [20] macro expansion at /home/pkgeval/.julia/packages/CuArrays/e8PLr/src/mapreduce.jl:161 [inlined]
   [21] mapreducedim!(::Function, ::Function, ::CuArray{Float32,1,Nothing}, ::CuArray{Tuple{Int64,Int64},1,Nothing}, ::Float32) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/nvtx/highlevel.jl:83
   [22] mapreduce(::Function, ::Function, ::CuArray{Tuple{Int64,Int64},1,Nothing}; dims::Function, init::Float32) at /home/pkgeval/.julia/packages/GPUArrays/QDGmr/src/host/mapreduce.jl:38
   [23] (::GeometricFlux.var"#181#184"{Float32,CuArray{Float32,2,Nothing},CuArray{Float32,3,Nothing},CuArray{Int64,2,Nothing}})(::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/scatter.jl:125
   [24] #98#back at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49 [inlined]
   [25] #88 at ./none:0 [inlined]
   [26] (::Zygote.var"#36#37"{typeof(∂(#88))})(::Float32) at /home/pkgeval/.julia/packages/Zygote/4tJp5/src/compiler/interface.jl:36
   [27] gradient(::Function, ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/4tJp5/src/compiler/interface.jl:45
   [28] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:33
   [29] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113
   [30] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:17
   [31] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113
   [32] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:16
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:34
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_mul!(copy(ys), x, xs))
            end), us) == (2048 * gather(ys, xs),)
  GPU compilation of mapreduce_grid(GeometricFlux.var"#183#186"{Int64,CuArray{Float32,3,Nothing}}, typeof(*), CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}, CUDAnative.CuDeviceArray{Float32,1,CUDAnative.AS.Global}, Float32, CartesianIndices{1,Tuple{Base.OneTo{Int64}}}, CartesianIndices{1,Tuple{Base.OneTo{Int64}}}, Val{true}) failed
  KernelError: recursion is currently not supported
  
  Try inspecting the generated code with any of the @device_code_... macros.
  
  Stacktrace:
   [1] _nextind_str at strings/string.jl:141
   [2] nextind at strings/string.jl:137
   [3] _nextind_str at strings/string.jl:141
   [4] _split at strings/util.jl:325
   [5] env_override_minlevel at logging.jl:423
   [6] current_logger_for_env at logging.jl:386
   [7] assertscalar at /home/pkgeval/.julia/packages/GPUArrays/QDGmr/src/host/indexing.jl:40
   [8] #183 at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/scatter.jl:125
   [9] mapreduce_grid at /home/pkgeval/.julia/packages/CuArrays/e8PLr/src/mapreduce.jl:89
  Stacktrace:
   [1] (::CUDAnative.var"#hook_emit_function#122"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}})(::Core.MethodInstance, ::Core.CodeInfo, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/irgen.jl:205
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/irgen.jl:261
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/irgen.jl:319
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:97 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:96
   [9] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:45
   [10] #compile#171 at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:33 [inlined]
   [11] cufunction_slow(::Function, ::Type{T} where T, ::Int64; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:326
   [12] #219 at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:393 [inlined]
   [13] get!(::CUDAnative.var"#219#220"{Nothing,Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CuArrays.mapreduce_grid),DataType,Int64}, ::Dict{UInt64,CUDAnative.HostKernel}, ::UInt64) at ./dict.jl:452
   [14] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:392 [inlined]
   [15] macro expansion at ./lock.jl:183 [inlined]
   [16] cufunction_fast(::Function, ::Type{T} where T, ::Int64; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:391
   [17] cufunction(::typeof(CuArrays.mapreduce_grid), ::Type{Tuple{GeometricFlux.var"#183#186"{Int64,CuArray{Float32,3,Nothing}},typeof(*),CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float32,1,CUDAnative.AS.Global},Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:422
   [18] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:422
   [19] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:157 [inlined]
   [20] macro expansion at /home/pkgeval/.julia/packages/CuArrays/e8PLr/src/mapreduce.jl:161 [inlined]
   [21] mapreducedim!(::Function, ::Function, ::CuArray{Float32,1,Nothing}, ::CuArray{Tuple{Int64,Int64},1,Nothing}, ::Float32) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/nvtx/highlevel.jl:83
   [22] mapreduce(::Function, ::Function, ::CuArray{Tuple{Int64,Int64},1,Nothing}; dims::Function, init::Float32) at /home/pkgeval/.julia/packages/GPUArrays/QDGmr/src/host/mapreduce.jl:38
   [23] (::GeometricFlux.var"#181#184"{Float32,CuArray{Float32,2,Nothing},CuArray{Float32,3,Nothing},CuArray{Int64,2,Nothing}})(::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/scatter.jl:125
   [24] #98#back at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49 [inlined]
   [25] #89 at ./none:0 [inlined]
   [26] (::Zygote.var"#36#37"{typeof(∂(#89))})(::Float32) at /home/pkgeval/.julia/packages/Zygote/4tJp5/src/compiler/interface.jl:36
   [27] gradient(::Function, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/4tJp5/src/compiler/interface.jl:45
   [28] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:34
   [29] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113
   [30] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:17
   [31] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113
   [32] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:16
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:35
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_mul!(copy(ys), us, x))
            end), xs) == (nothing,)
  GPU compilation of mapreduce_grid(GeometricFlux.var"#183#186"{Int64,CuArray{Float32,3,Nothing}}, typeof(*), CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}, CUDAnative.CuDeviceArray{Float32,1,CUDAnative.AS.Global}, Float32, CartesianIndices{1,Tuple{Base.OneTo{Int64}}}, CartesianIndices{1,Tuple{Base.OneTo{Int64}}}, Val{true}) failed
  KernelError: recursion is currently not supported
  
  Try inspecting the generated code with any of the @device_code_... macros.
  
  Stacktrace:
   [1] _nextind_str at strings/string.jl:141
   [2] nextind at strings/string.jl:137
   [3] _nextind_str at strings/string.jl:141
   [4] _split at strings/util.jl:325
   [5] env_override_minlevel at logging.jl:423
   [6] current_logger_for_env at logging.jl:386
   [7] assertscalar at /home/pkgeval/.julia/packages/GPUArrays/QDGmr/src/host/indexing.jl:40
   [8] #183 at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/scatter.jl:125
   [9] mapreduce_grid at /home/pkgeval/.julia/packages/CuArrays/e8PLr/src/mapreduce.jl:89
  Stacktrace:
   [1] (::CUDAnative.var"#hook_emit_function#122"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}})(::Core.MethodInstance, ::Core.CodeInfo, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/irgen.jl:205
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/irgen.jl:261
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/irgen.jl:319
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:97 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:96
   [9] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:45
   [10] #compile#171 at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:33 [inlined]
   [11] cufunction_slow(::Function, ::Type{T} where T, ::Int64; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:326
   [12] #219 at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:393 [inlined]
   [13] get!(::CUDAnative.var"#219#220"{Nothing,Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CuArrays.mapreduce_grid),DataType,Int64}, ::Dict{UInt64,CUDAnative.HostKernel}, ::UInt64) at ./dict.jl:452
   [14] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:392 [inlined]
   [15] macro expansion at ./lock.jl:183 [inlined]
   [16] cufunction_fast(::Function, ::Type{T} where T, ::Int64; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:391
   [17] cufunction(::typeof(CuArrays.mapreduce_grid), ::Type{Tuple{GeometricFlux.var"#183#186"{Int64,CuArray{Float32,3,Nothing}},typeof(*),CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float32,1,CUDAnative.AS.Global},Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:422
   [18] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:422
   [19] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:157 [inlined]
   [20] macro expansion at /home/pkgeval/.julia/packages/CuArrays/e8PLr/src/mapreduce.jl:161 [inlined]
   [21] mapreducedim!(::Function, ::Function, ::CuArray{Float32,1,Nothing}, ::CuArray{Tuple{Int64,Int64},1,Nothing}, ::Float32) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/nvtx/highlevel.jl:83
   [22] mapreduce(::Function, ::Function, ::CuArray{Tuple{Int64,Int64},1,Nothing}; dims::Function, init::Float32) at /home/pkgeval/.julia/packages/GPUArrays/QDGmr/src/host/mapreduce.jl:38
   [23] (::GeometricFlux.var"#181#184"{Float32,CuArray{Float32,2,Nothing},CuArray{Float32,3,Nothing},CuArray{Int64,2,Nothing}})(::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/scatter.jl:125
   [24] #98#back at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49 [inlined]
   [25] #90 at ./none:0 [inlined]
   [26] (::typeof(∂(#90)))(::Float32) at /home/pkgeval/.julia/packages/Zygote/4tJp5/src/compiler/interface2.jl:0
   [27] (::Zygote.var"#36#37"{typeof(∂(#90))})(::Float32) at /home/pkgeval/.julia/packages/Zygote/4tJp5/src/compiler/interface.jl:36
   [28] gradient(::Function, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/4tJp5/src/compiler/interface.jl:45
   [29] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:35
   [30] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113
   [31] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:17
   [32] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113
   [33] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:16
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:37
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_div!(x, us, xs))
            end), ys) == (∇y_div,)
  GPU compilation of mapreduce_grid(GeometricFlux.var"#191#194"{Int64,CuArray{Float32,3,Nothing}}, typeof(*), CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}, CUDAnative.CuDeviceArray{Float32,1,CUDAnative.AS.Global}, Float32, CartesianIndices{1,Tuple{Base.OneTo{Int64}}}, CartesianIndices{1,Tuple{Base.OneTo{Int64}}}, Val{true}) failed
  KernelError: recursion is currently not supported
  
  Try inspecting the generated code with any of the @device_code_... macros.
  
  Stacktrace:
   [1] _nextind_str at strings/string.jl:141
   [2] nextind at strings/string.jl:137
   [3] _nextind_str at strings/string.jl:141
   [4] _split at strings/util.jl:325
   [5] env_override_minlevel at logging.jl:423
   [6] current_logger_for_env at logging.jl:386
   [7] assertscalar at /home/pkgeval/.julia/packages/GPUArrays/QDGmr/src/host/indexing.jl:40
   [8] #191 at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/scatter.jl:146
   [9] mapreduce_grid at /home/pkgeval/.julia/packages/CuArrays/e8PLr/src/mapreduce.jl:89
  Stacktrace:
   [1] (::CUDAnative.var"#hook_emit_function#122"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}})(::Core.MethodInstance, ::Core.CodeInfo, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/irgen.jl:205
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/irgen.jl:261
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/irgen.jl:319
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:97 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:96
   [9] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:45
   [10] #compile#171 at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:33 [inlined]
   [11] cufunction_slow(::Function, ::Type{T} where T, ::Int64; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:326
   [12] #219 at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:393 [inlined]
   [13] get!(::CUDAnative.var"#219#220"{Nothing,Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CuArrays.mapreduce_grid),DataType,Int64}, ::Dict{UInt64,CUDAnative.HostKernel}, ::UInt64) at ./dict.jl:452
   [14] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:392 [inlined]
   [15] macro expansion at ./lock.jl:183 [inlined]
   [16] cufunction_fast(::Function, ::Type{T} where T, ::Int64; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:391
   [17] cufunction(::typeof(CuArrays.mapreduce_grid), ::Type{Tuple{GeometricFlux.var"#191#194"{Int64,CuArray{Float32,3,Nothing}},typeof(*),CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float32,1,CUDAnative.AS.Global},Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:422
   [18] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:422
   [19] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:157 [inlined]
   [20] macro expansion at /home/pkgeval/.julia/packages/CuArrays/e8PLr/src/mapreduce.jl:161 [inlined]
   [21] mapreducedim!(::Function, ::Function, ::CuArray{Float32,1,Nothing}, ::CuArray{Tuple{Int64,Int64},1,Nothing}, ::Float32) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/nvtx/highlevel.jl:83
   [22] mapreduce(::Function, ::Function, ::CuArray{Tuple{Int64,Int64},1,Nothing}; dims::Function, init::Float32) at /home/pkgeval/.julia/packages/GPUArrays/QDGmr/src/host/mapreduce.jl:38
   [23] (::GeometricFlux.var"#189#192"{Float32,CuArray{Float32,2,Nothing},CuArray{Float32,3,Nothing},CuArray{Int64,2,Nothing}})(::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/scatter.jl:146
   [24] #116#back at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49 [inlined]
   [25] #91 at ./none:0 [inlined]
   [26] (::Zygote.var"#36#37"{typeof(∂(#91))})(::Float32) at /home/pkgeval/.julia/packages/Zygote/4tJp5/src/compiler/interface.jl:36
   [27] gradient(::Function, ::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/4tJp5/src/compiler/interface.jl:45
   [28] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:37
   [29] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113
   [30] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:17
   [31] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113
   [32] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:16
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:38
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_div!(copy(ys), x, xs))
            end), us) == (-(gather(ys, xs)) / 8192,)
  GPU compilation of mapreduce_grid(GeometricFlux.var"#191#194"{Int64,CuArray{Float32,3,Nothing}}, typeof(*), CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}, CUDAnative.CuDeviceArray{Float32,1,CUDAnative.AS.Global}, Float32, CartesianIndices{1,Tuple{Base.OneTo{Int64}}}, CartesianIndices{1,Tuple{Base.OneTo{Int64}}}, Val{true}) failed
  KernelError: recursion is currently not supported
  
  Try inspecting the generated code with any of the @device_code_... macros.
  
  Stacktrace:
   [1] _nextind_str at strings/string.jl:141
   [2] nextind at strings/string.jl:137
   [3] _nextind_str at strings/string.jl:141
   [4] _split at strings/util.jl:325
   [5] env_override_minlevel at logging.jl:423
   [6] current_logger_for_env at logging.jl:386
   [7] assertscalar at /home/pkgeval/.julia/packages/GPUArrays/QDGmr/src/host/indexing.jl:40
   [8] #191 at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/scatter.jl:146
   [9] mapreduce_grid at /home/pkgeval/.julia/packages/CuArrays/e8PLr/src/mapreduce.jl:89
  Stacktrace:
   [1] (::CUDAnative.var"#hook_emit_function#122"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}})(::Core.MethodInstance, ::Core.CodeInfo, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/irgen.jl:205
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/irgen.jl:261
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/irgen.jl:319
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:97 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:96
   [9] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:45
   [10] #compile#171 at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:33 [inlined]
   [11] cufunction_slow(::Function, ::Type{T} where T, ::Int64; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:326
   [12] #219 at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:393 [inlined]
   [13] get!(::CUDAnative.var"#219#220"{Nothing,Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CuArrays.mapreduce_grid),DataType,Int64}, ::Dict{UInt64,CUDAnative.HostKernel}, ::UInt64) at ./dict.jl:452
   [14] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:392 [inlined]
   [15] macro expansion at ./lock.jl:183 [inlined]
   [16] cufunction_fast(::Function, ::Type{T} where T, ::Int64; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:391
   [17] cufunction(::typeof(CuArrays.mapreduce_grid), ::Type{Tuple{GeometricFlux.var"#191#194"{Int64,CuArray{Float32,3,Nothing}},typeof(*),CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float32,1,CUDAnative.AS.Global},Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:422
   [18] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:422
   [19] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:157 [inlined]
   [20] macro expansion at /home/pkgeval/.julia/packages/CuArrays/e8PLr/src/mapreduce.jl:161 [inlined]
   [21] mapreducedim!(::Function, ::Function, ::CuArray{Float32,1,Nothing}, ::CuArray{Tuple{Int64,Int64},1,Nothing}, ::Float32) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/nvtx/highlevel.jl:83
   [22] mapreduce(::Function, ::Function, ::CuArray{Tuple{Int64,Int64},1,Nothing}; dims::Function, init::Float32) at /home/pkgeval/.julia/packages/GPUArrays/QDGmr/src/host/mapreduce.jl:38
   [23] (::GeometricFlux.var"#189#192"{Float32,CuArray{Float32,2,Nothing},CuArray{Float32,3,Nothing},CuArray{Int64,2,Nothing}})(::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/scatter.jl:146
   [24] #116#back at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49 [inlined]
   [25] #92 at ./none:0 [inlined]
   [26] (::Zygote.var"#36#37"{typeof(∂(#92))})(::Float32) at /home/pkgeval/.julia/packages/Zygote/4tJp5/src/compiler/interface.jl:36
   [27] gradient(::Function, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/4tJp5/src/compiler/interface.jl:45
   [28] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:38
   [29] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113
   [30] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:17
   [31] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113
   [32] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:16
  
scatter: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:39
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(scatter_div!(copy(ys), us, x))
            end), xs) == (nothing,)
  GPU compilation of mapreduce_grid(GeometricFlux.var"#191#194"{Int64,CuArray{Float32,3,Nothing}}, typeof(*), CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}, CUDAnative.CuDeviceArray{Float32,1,CUDAnative.AS.Global}, Float32, CartesianIndices{1,Tuple{Base.OneTo{Int64}}}, CartesianIndices{1,Tuple{Base.OneTo{Int64}}}, Val{true}) failed
  KernelError: recursion is currently not supported
  
  Try inspecting the generated code with any of the @device_code_... macros.
  
  Stacktrace:
   [1] _nextind_str at strings/string.jl:141
   [2] nextind at strings/string.jl:137
   [3] _nextind_str at strings/string.jl:141
   [4] _split at strings/util.jl:325
   [5] env_override_minlevel at logging.jl:423
   [6] current_logger_for_env at logging.jl:386
   [7] assertscalar at /home/pkgeval/.julia/packages/GPUArrays/QDGmr/src/host/indexing.jl:40
   [8] #191 at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/scatter.jl:146
   [9] mapreduce_grid at /home/pkgeval/.julia/packages/CuArrays/e8PLr/src/mapreduce.jl:89
  Stacktrace:
   [1] (::CUDAnative.var"#hook_emit_function#122"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}})(::Core.MethodInstance, ::Core.CodeInfo, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/irgen.jl:205
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/irgen.jl:261
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/irgen.jl:319
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:97 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:96
   [9] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:45
   [10] #compile#171 at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:33 [inlined]
   [11] cufunction_slow(::Function, ::Type{T} where T, ::Int64; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:326
   [12] #219 at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:393 [inlined]
   [13] get!(::CUDAnative.var"#219#220"{Nothing,Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CuArrays.mapreduce_grid),DataType,Int64}, ::Dict{UInt64,CUDAnative.HostKernel}, ::UInt64) at ./dict.jl:452
   [14] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:392 [inlined]
   [15] macro expansion at ./lock.jl:183 [inlined]
   [16] cufunction_fast(::Function, ::Type{T} where T, ::Int64; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:391
   [17] cufunction(::typeof(CuArrays.mapreduce_grid), ::Type{Tuple{GeometricFlux.var"#191#194"{Int64,CuArray{Float32,3,Nothing}},typeof(*),CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float32,1,CUDAnative.AS.Global},Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:422
   [18] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:422
   [19] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:157 [inlined]
   [20] macro expansion at /home/pkgeval/.julia/packages/CuArrays/e8PLr/src/mapreduce.jl:161 [inlined]
   [21] mapreducedim!(::Function, ::Function, ::CuArray{Float32,1,Nothing}, ::CuArray{Tuple{Int64,Int64},1,Nothing}, ::Float32) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/nvtx/highlevel.jl:83
   [22] mapreduce(::Function, ::Function, ::CuArray{Tuple{Int64,Int64},1,Nothing}; dims::Function, init::Float32) at /home/pkgeval/.julia/packages/GPUArrays/QDGmr/src/host/mapreduce.jl:38
   [23] (::GeometricFlux.var"#189#192"{Float32,CuArray{Float32,2,Nothing},CuArray{Float32,3,Nothing},CuArray{Int64,2,Nothing}})(::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/scatter.jl:146
   [24] #116#back at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49 [inlined]
   [25] #93 at ./none:0 [inlined]
   [26] (::typeof(∂(#93)))(::Float32) at /home/pkgeval/.julia/packages/Zygote/4tJp5/src/compiler/interface2.jl:0
   [27] (::Zygote.var"#36#37"{typeof(∂(#93))})(::Float32) at /home/pkgeval/.julia/packages/Zygote/4tJp5/src/compiler/interface.jl:36
   [28] gradient(::Function, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/4tJp5/src/compiler/interface.jl:45
   [29] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:39
   [30] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113
   [31] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:17
   [32] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113
   [33] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:16
  
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:59
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(prodpool(x, us))
            end), xs) == (nothing,)
  GPU compilation of mapreduce_grid(GeometricFlux.var"#208#211"{Int64,CuArray{Float32,3,Nothing}}, typeof(*), CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}, CUDAnative.CuDeviceArray{Float32,1,CUDAnative.AS.Global}, Float32, CartesianIndices{1,Tuple{Base.OneTo{Int64}}}, CartesianIndices{1,Tuple{Base.OneTo{Int64}}}, Val{true}) failed
  KernelError: recursion is currently not supported
  
  Try inspecting the generated code with any of the @device_code_... macros.
  
  Stacktrace:
   [1] _nextind_str at strings/string.jl:141
   [2] nextind at strings/string.jl:137
   [3] _nextind_str at strings/string.jl:141
   [4] _split at strings/util.jl:325
   [5] env_override_minlevel at logging.jl:423
   [6] current_logger_for_env at logging.jl:386
   [7] assertscalar at /home/pkgeval/.julia/packages/GPUArrays/QDGmr/src/host/indexing.jl:40
   [8] #208 at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/pool.jl:88
   [9] mapreduce_grid at /home/pkgeval/.julia/packages/CuArrays/e8PLr/src/mapreduce.jl:89
  Stacktrace:
   [1] (::CUDAnative.var"#hook_emit_function#122"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}})(::Core.MethodInstance, ::Core.CodeInfo, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/irgen.jl:205
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/irgen.jl:261
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/irgen.jl:319
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:97 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:96
   [9] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:45
   [10] #compile#171 at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:33 [inlined]
   [11] cufunction_slow(::Function, ::Type{T} where T, ::Int64; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:326
   [12] #219 at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:393 [inlined]
   [13] get!(::CUDAnative.var"#219#220"{Nothing,Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CuArrays.mapreduce_grid),DataType,Int64}, ::Dict{UInt64,CUDAnative.HostKernel}, ::UInt64) at ./dict.jl:452
   [14] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:392 [inlined]
   [15] macro expansion at ./lock.jl:183 [inlined]
   [16] cufunction_fast(::Function, ::Type{T} where T, ::Int64; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:391
   [17] cufunction(::typeof(CuArrays.mapreduce_grid), ::Type{Tuple{GeometricFlux.var"#208#211"{Int64,CuArray{Float32,3,Nothing}},typeof(*),CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float32,1,CUDAnative.AS.Global},Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:422
   [18] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:422
   [19] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:157 [inlined]
   [20] macro expansion at /home/pkgeval/.julia/packages/CuArrays/e8PLr/src/mapreduce.jl:161 [inlined]
   [21] mapreducedim!(::Function, ::Function, ::CuArray{Float32,1,Nothing}, ::CuArray{Tuple{Int64,Int64},1,Nothing}, ::Float32) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/nvtx/highlevel.jl:83
   [22] mapreduce(::Function, ::Function, ::CuArray{Tuple{Int64,Int64},1,Nothing}; dims::Function, init::Float32) at /home/pkgeval/.julia/packages/GPUArrays/QDGmr/src/host/mapreduce.jl:38
   [23] (::GeometricFlux.var"#206#209"{Float32,CuArray{Int64,2,Nothing},CuArray{Float32,3,Nothing}})(::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/pool.jl:88
   [24] #159#back at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49 [inlined]
   [25] #105 at ./none:0 [inlined]
   [26] (::typeof(∂(#105)))(::Float32) at /home/pkgeval/.julia/packages/Zygote/4tJp5/src/compiler/interface2.jl:0
   [27] (::Zygote.var"#36#37"{typeof(∂(#105))})(::Float32) at /home/pkgeval/.julia/packages/Zygote/4tJp5/src/compiler/interface.jl:36
   [28] gradient(::Function, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/4tJp5/src/compiler/interface.jl:45
   [29] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:59
   [30] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113
   [31] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:47
   [32] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113
   [33] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:16
  
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:60
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(prodpool(xs, x))
            end), us) == (2048 * ones(2, 3, 4),)
  GPU compilation of mapreduce_grid(GeometricFlux.var"#208#211"{Int64,CuArray{Float32,3,Nothing}}, typeof(*), CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}, CUDAnative.CuDeviceArray{Float32,1,CUDAnative.AS.Global}, Float32, CartesianIndices{1,Tuple{Base.OneTo{Int64}}}, CartesianIndices{1,Tuple{Base.OneTo{Int64}}}, Val{true}) failed
  KernelError: recursion is currently not supported
  
  Try inspecting the generated code with any of the @device_code_... macros.
  
  Stacktrace:
   [1] _nextind_str at strings/string.jl:141
   [2] nextind at strings/string.jl:137
   [3] _nextind_str at strings/string.jl:141
   [4] _split at strings/util.jl:325
   [5] env_override_minlevel at logging.jl:423
   [6] current_logger_for_env at logging.jl:386
   [7] assertscalar at /home/pkgeval/.julia/packages/GPUArrays/QDGmr/src/host/indexing.jl:40
   [8] #208 at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/pool.jl:88
   [9] mapreduce_grid at /home/pkgeval/.julia/packages/CuArrays/e8PLr/src/mapreduce.jl:89
  Stacktrace:
   [1] (::CUDAnative.var"#hook_emit_function#122"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}})(::Core.MethodInstance, ::Core.CodeInfo, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/irgen.jl:205
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/irgen.jl:261
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/irgen.jl:319
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:97 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:96
   [9] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:45
   [10] #compile#171 at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:33 [inlined]
   [11] cufunction_slow(::Function, ::Type{T} where T, ::Int64; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:326
   [12] #219 at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:393 [inlined]
   [13] get!(::CUDAnative.var"#219#220"{Nothing,Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CuArrays.mapreduce_grid),DataType,Int64}, ::Dict{UInt64,CUDAnative.HostKernel}, ::UInt64) at ./dict.jl:452
   [14] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:392 [inlined]
   [15] macro expansion at ./lock.jl:183 [inlined]
   [16] cufunction_fast(::Function, ::Type{T} where T, ::Int64; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:391
   [17] cufunction(::typeof(CuArrays.mapreduce_grid), ::Type{Tuple{GeometricFlux.var"#208#211"{Int64,CuArray{Float32,3,Nothing}},typeof(*),CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float32,1,CUDAnative.AS.Global},Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:422
   [18] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:422
   [19] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:157 [inlined]
   [20] macro expansion at /home/pkgeval/.julia/packages/CuArrays/e8PLr/src/mapreduce.jl:161 [inlined]
   [21] mapreducedim!(::Function, ::Function, ::CuArray{Float32,1,Nothing}, ::CuArray{Tuple{Int64,Int64},1,Nothing}, ::Float32) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/nvtx/highlevel.jl:83
   [22] mapreduce(::Function, ::Function, ::CuArray{Tuple{Int64,Int64},1,Nothing}; dims::Function, init::Float32) at /home/pkgeval/.julia/packages/GPUArrays/QDGmr/src/host/mapreduce.jl:38
   [23] (::GeometricFlux.var"#206#209"{Float32,CuArray{Int64,2,Nothing},CuArray{Float32,3,Nothing}})(::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/pool.jl:88
   [24] #159#back at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49 [inlined]
   [25] #106 at ./none:0 [inlined]
   [26] (::Zygote.var"#36#37"{typeof(∂(#106))})(::Float32) at /home/pkgeval/.julia/packages/Zygote/4tJp5/src/compiler/interface.jl:36
   [27] gradient(::Function, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/4tJp5/src/compiler/interface.jl:45
   [28] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:60
   [29] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113
   [30] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:47
   [31] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113
   [32] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:16
  
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:62
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(divpool(x, us))
            end), xs) == (nothing,)
  GPU compilation of mapreduce_grid(GeometricFlux.var"#216#219"{Int64,CuArray{Float32,3,Nothing}}, typeof(*), CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}, CUDAnative.CuDeviceArray{Float32,1,CUDAnative.AS.Global}, Float32, CartesianIndices{1,Tuple{Base.OneTo{Int64}}}, CartesianIndices{1,Tuple{Base.OneTo{Int64}}}, Val{true}) failed
  KernelError: recursion is currently not supported
  
  Try inspecting the generated code with any of the @device_code_... macros.
  
  Stacktrace:
   [1] _nextind_str at strings/string.jl:141
   [2] nextind at strings/string.jl:137
   [3] _nextind_str at strings/string.jl:141
   [4] _split at strings/util.jl:325
   [5] env_override_minlevel at logging.jl:423
   [6] current_logger_for_env at logging.jl:386
   [7] assertscalar at /home/pkgeval/.julia/packages/GPUArrays/QDGmr/src/host/indexing.jl:40
   [8] #216 at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/pool.jl:104
   [9] mapreduce_grid at /home/pkgeval/.julia/packages/CuArrays/e8PLr/src/mapreduce.jl:89
  Stacktrace:
   [1] (::CUDAnative.var"#hook_emit_function#122"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}})(::Core.MethodInstance, ::Core.CodeInfo, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/irgen.jl:205
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/irgen.jl:261
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/irgen.jl:319
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:97 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:96
   [9] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:45
   [10] #compile#171 at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:33 [inlined]
   [11] cufunction_slow(::Function, ::Type{T} where T, ::Int64; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:326
   [12] #219 at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:393 [inlined]
   [13] get!(::CUDAnative.var"#219#220"{Nothing,Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CuArrays.mapreduce_grid),DataType,Int64}, ::Dict{UInt64,CUDAnative.HostKernel}, ::UInt64) at ./dict.jl:452
   [14] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:392 [inlined]
   [15] macro expansion at ./lock.jl:183 [inlined]
   [16] cufunction_fast(::Function, ::Type{T} where T, ::Int64; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:391
   [17] cufunction(::typeof(CuArrays.mapreduce_grid), ::Type{Tuple{GeometricFlux.var"#216#219"{Int64,CuArray{Float32,3,Nothing}},typeof(*),CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float32,1,CUDAnative.AS.Global},Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:422
   [18] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:422
   [19] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:157 [inlined]
   [20] macro expansion at /home/pkgeval/.julia/packages/CuArrays/e8PLr/src/mapreduce.jl:161 [inlined]
   [21] mapreducedim!(::Function, ::Function, ::CuArray{Float32,1,Nothing}, ::CuArray{Tuple{Int64,Int64},1,Nothing}, ::Float32) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/nvtx/highlevel.jl:83
   [22] mapreduce(::Function, ::Function, ::CuArray{Tuple{Int64,Int64},1,Nothing}; dims::Function, init::Float32) at /home/pkgeval/.julia/packages/GPUArrays/QDGmr/src/host/mapreduce.jl:38
   [23] (::GeometricFlux.var"#214#217"{Float32,CuArray{Int64,2,Nothing},CuArray{Float32,3,Nothing}})(::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/pool.jl:104
   [24] #177#back at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49 [inlined]
   [25] #107 at ./none:0 [inlined]
   [26] (::typeof(∂(#107)))(::Float32) at /home/pkgeval/.julia/packages/Zygote/4tJp5/src/compiler/interface2.jl:0
   [27] (::Zygote.var"#36#37"{typeof(∂(#107))})(::Float32) at /home/pkgeval/.julia/packages/Zygote/4tJp5/src/compiler/interface.jl:36
   [28] gradient(::Function, ::CuArray{Int64,2,Nothing}) at /home/pkgeval/.julia/packages/Zygote/4tJp5/src/compiler/interface.jl:45
   [29] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:62
   [30] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113
   [31] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:47
   [32] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113
   [33] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:16
  
pool: Error During Test at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:63
  Test threw exception
  Expression: Zygote.gradient((x->begin
                sum(divpool(xs, x))
            end), us) == (-(ones(2, 3, 4)) / 8192,)
  GPU compilation of mapreduce_grid(GeometricFlux.var"#216#219"{Int64,CuArray{Float32,3,Nothing}}, typeof(*), CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global}, CUDAnative.CuDeviceArray{Float32,1,CUDAnative.AS.Global}, Float32, CartesianIndices{1,Tuple{Base.OneTo{Int64}}}, CartesianIndices{1,Tuple{Base.OneTo{Int64}}}, Val{true}) failed
  KernelError: recursion is currently not supported
  
  Try inspecting the generated code with any of the @device_code_... macros.
  
  Stacktrace:
   [1] _nextind_str at strings/string.jl:141
   [2] nextind at strings/string.jl:137
   [3] _nextind_str at strings/string.jl:141
   [4] _split at strings/util.jl:325
   [5] env_override_minlevel at logging.jl:423
   [6] current_logger_for_env at logging.jl:386
   [7] assertscalar at /home/pkgeval/.julia/packages/GPUArrays/QDGmr/src/host/indexing.jl:40
   [8] #216 at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/pool.jl:104
   [9] mapreduce_grid at /home/pkgeval/.julia/packages/CuArrays/e8PLr/src/mapreduce.jl:89
  Stacktrace:
   [1] (::CUDAnative.var"#hook_emit_function#122"{CUDAnative.CompilerJob,Array{Core.MethodInstance,1}})(::Core.MethodInstance, ::Core.CodeInfo, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/irgen.jl:205
   [2] compile_method_instance(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/irgen.jl:261
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [4] irgen(::CUDAnative.CompilerJob, ::Core.MethodInstance, ::UInt64) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/irgen.jl:319
   [5] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [6] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:97 [inlined]
   [7] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/7Id5J/src/TimerOutput.jl:228 [inlined]
   [8] codegen(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:96
   [9] compile(::Symbol, ::CUDAnative.CompilerJob; libraries::Bool, dynamic_parallelism::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:45
   [10] #compile#171 at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/compiler/driver.jl:33 [inlined]
   [11] cufunction_slow(::Function, ::Type{T} where T, ::Int64; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:326
   [12] #219 at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:393 [inlined]
   [13] get!(::CUDAnative.var"#219#220"{Nothing,Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CuArrays.mapreduce_grid),DataType,Int64}, ::Dict{UInt64,CUDAnative.HostKernel}, ::UInt64) at ./dict.jl:452
   [14] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:392 [inlined]
   [15] macro expansion at ./lock.jl:183 [inlined]
   [16] cufunction_fast(::Function, ::Type{T} where T, ::Int64; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:391
   [17] cufunction(::typeof(CuArrays.mapreduce_grid), ::Type{Tuple{GeometricFlux.var"#216#219"{Int64,CuArray{Float32,3,Nothing}},typeof(*),CUDAnative.CuDeviceArray{Tuple{Int64,Int64},1,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float32,1,CUDAnative.AS.Global},Float32,CartesianIndices{1,Tuple{Base.OneTo{Int64}}},CartesianIndices{1,Tuple{Base.OneTo{Int64}}},Val{true}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:422
   [18] cufunction(::Function, ::Type{T} where T) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:422
   [19] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/execution.jl:157 [inlined]
   [20] macro expansion at /home/pkgeval/.julia/packages/CuArrays/e8PLr/src/mapreduce.jl:161 [inlined]
   [21] mapreducedim!(::Function, ::Function, ::CuArray{Float32,1,Nothing}, ::CuArray{Tuple{Int64,Int64},1,Nothing}, ::Float32) at /home/pkgeval/.julia/packages/CUDAnative/ierw8/src/nvtx/highlevel.jl:83
   [22] mapreduce(::Function, ::Function, ::CuArray{Tuple{Int64,Int64},1,Nothing}; dims::Function, init::Float32) at /home/pkgeval/.julia/packages/GPUArrays/QDGmr/src/host/mapreduce.jl:38
   [23] (::GeometricFlux.var"#214#217"{Float32,CuArray{Int64,2,Nothing},CuArray{Float32,3,Nothing}})(::CuArray{Float32,2,Nothing}) at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/src/cuda/pool.jl:104
   [24] #177#back at /home/pkgeval/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49 [inlined]
   [25] #108 at ./none:0 [inlined]
   [26] (::Zygote.var"#36#37"{typeof(∂(#108))})(::Float32) at /home/pkgeval/.julia/packages/Zygote/4tJp5/src/compiler/interface.jl:36
   [27] gradient(::Function, ::CuArray{Float32,3,Nothing}) at /home/pkgeval/.julia/packages/Zygote/4tJp5/src/compiler/interface.jl:45
   [28] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:63
   [29] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113
   [30] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:47
   [31] top-level scope at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113
   [32] top-level scope at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/cuda/grad.jl:16
  
Test Summary:                    | Pass  Error  Total
GeometricFlux                    | 1208     18   1226
  msgpass                        |    1             1
  layer                          |   42            42
  pool                           |   82            82
  grad                           |   35            35
  Test InnerProductDecoder layer |    2             2
  Test VariationalEncoder layer  |    1             1
  Test GAE model                 |    1             1
  Test VGAE model                |    1             1
  Test Linear Algebra            |   24            24
  scatter                        |  360           360
  scatter-staticarray            |  312           312
  simplegraphs                   |   32            32
  weightedgraphs                 |   32            32
  metagraphs                     |   38            38
  Test adjlist                   |    4             4
  utils                          |   22            22
  cuda/scatter                   |   60            60
  cuda/pool                      |  104      8    112
    UInt32                       |   12            12
    UInt64                       |   12            12
    Int32                        |   16            16
    Int64                        |   16            16
    Float32                      |   24      4     28
      sumpool                    |    4             4
      subpool                    |    4             4
      maxpool                    |    4             4
      minpool                    |    4             4
      prodpool                   |    4             4
      divpool                    |           4      4
      meanpool                   |    4             4
    Float64                      |   24      4     28
      sumpool                    |    4             4
      subpool                    |    4             4
      maxpool                    |    4             4
      minpool                    |    4             4
      prodpool                   |    4             4
      divpool                    |           4      4
      meanpool                   |    4             4
  cuda/grad                      |   25     10     35
    scatter                      |   15      6     21
    pool                         |   10      4     14
  cuda/conv                      |   29            29
  cuda/msgpass                   |    1             1
ERROR: LoadError: Some tests did not pass: 1208 passed, 0 failed, 18 errored, 0 broken.
in expression starting at /home/pkgeval/.julia/packages/GeometricFlux/u3Wfx/test/runtests.jl:51
ERROR: Package GeometricFlux errored during testing
Stacktrace:
 [1] pkgerror(::String, ::Vararg{String,N} where N) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Pkg/src/Types.jl:53
 [2] test(::Pkg.Types.Context, ::Array{Pkg.Types.PackageSpec,1}; coverage::Bool, julia_args::Cmd, test_args::Cmd, test_fn::Nothing) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Pkg/src/Operations.jl:1503
 [3] test(::Pkg.Types.Context, ::Array{Pkg.Types.PackageSpec,1}; coverage::Bool, test_fn::Nothing, julia_args::Cmd, test_args::Cmd, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Pkg/src/API.jl:316
 [4] test(::Pkg.Types.Context, ::Array{Pkg.Types.PackageSpec,1}) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Pkg/src/API.jl:303
 [5] #test#68 at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Pkg/src/API.jl:297 [inlined]
 [6] test at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Pkg/src/API.jl:297 [inlined]
 [7] #test#67 at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Pkg/src/API.jl:296 [inlined]
 [8] test at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Pkg/src/API.jl:296 [inlined]
 [9] test(::String; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Pkg/src/API.jl:295
 [10] test(::String) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Pkg/src/API.jl:295
 [11] top-level scope at none:16
