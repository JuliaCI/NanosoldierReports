Julia Version 1.5.0-DEV.80
Commit ce211b9af9 (2020-01-16 21:59 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-8.0.1 (ORCJIT, skylake)
Environment:
  JULIA_DEPOT_PATH = ::/usr/local/share/julia
  JULIA_NUM_THREADS = 2

 Resolving package versions...
 Installed SortingAlgorithms ── v0.3.1
 Installed GaussianMixtures ─── v0.3.0
 Installed CMake ────────────── v1.1.2
 Installed FillArrays ───────── v0.8.4
 Installed Compat ───────────── v2.2.0
 Installed SpecialFunctions ─── v0.9.0
 Installed DataStructures ───── v0.17.9
 Installed StatsBase ────────── v0.32.0
 Installed NearestNeighbors ─── v0.4.4
 Installed Arpack ───────────── v0.4.0
 Installed JLD ──────────────── v0.9.1
 Installed PDMats ───────────── v0.9.10
 Installed DataAPI ──────────── v1.1.0
 Installed BinDeps ──────────── v1.0.0
 Installed URIParser ────────── v0.4.0
 Installed BinaryProvider ───── v0.5.8
 Installed OpenSpecFun_jll ──── v0.5.3+1
 Installed HDF5 ─────────────── v0.12.5
 Installed ScikitLearnBase ──── v0.5.0
 Installed Blosc ────────────── v0.5.1
 Installed StaticArrays ─────── v0.12.1
 Installed StatsFuns ────────── v0.9.3
 Installed Rmath ────────────── v0.6.0
 Installed FileIO ───────────── v1.2.1
 Installed LegacyStrings ────── v0.4.1
 Installed Arpack_jll ───────── v3.5.0+2
 Installed CMakeWrapper ─────── v0.2.3
 Installed Clustering ───────── v0.13.3
 Installed OpenBLAS_jll ─────── v0.3.7+4
 Installed OrderedCollections ─ v1.1.0
 Installed Missings ─────────── v0.4.3
 Installed Parameters ───────── v0.12.0
 Installed Distances ────────── v0.8.2
 Installed QuadGK ───────────── v2.3.1
 Installed Distributions ────── v0.22.3
  Updating `~/.julia/environments/v1.5/Project.toml`
  [cc18c42c] + GaussianMixtures v0.3.0
  Updating `~/.julia/environments/v1.5/Manifest.toml`
  [7d9fca2a] + Arpack v0.4.0
  [68821587] + Arpack_jll v3.5.0+2
  [9e28174c] + BinDeps v1.0.0
  [b99e7846] + BinaryProvider v0.5.8
  [a74b3585] + Blosc v0.5.1
  [631607c0] + CMake v1.1.2
  [d5fb7624] + CMakeWrapper v0.2.3
  [aaaa29a8] + Clustering v0.13.3
  [34da2185] + Compat v2.2.0
  [9a962f9c] + DataAPI v1.1.0
  [864edb3b] + DataStructures v0.17.9
  [b4f34e82] + Distances v0.8.2
  [31c24e10] + Distributions v0.22.3
  [5789e2e9] + FileIO v1.2.1
  [1a297f60] + FillArrays v0.8.4
  [cc18c42c] + GaussianMixtures v0.3.0
  [f67ccb44] + HDF5 v0.12.5
  [4138dd39] + JLD v0.9.1
  [1b4a561d] + LegacyStrings v0.4.1
  [e1d29d7a] + Missings v0.4.3
  [b8a86587] + NearestNeighbors v0.4.4
  [4536629a] + OpenBLAS_jll v0.3.7+4
  [efe28fd5] + OpenSpecFun_jll v0.5.3+1
  [bac558e1] + OrderedCollections v1.1.0
  [90014a1f] + PDMats v0.9.10
  [d96e819e] + Parameters v0.12.0
  [1fd47b50] + QuadGK v2.3.1
  [79098fc4] + Rmath v0.6.0
  [6e75b9c4] + ScikitLearnBase v0.5.0
  [a2af1166] + SortingAlgorithms v0.3.1
  [276daf66] + SpecialFunctions v0.9.0
  [90137ffa] + StaticArrays v0.12.1
  [2913bbd2] + StatsBase v0.32.0
  [4c63d2b9] + StatsFuns v0.9.3
  [30578b45] + URIParser v0.4.0
  [2a0f44e3] + Base64 
  [ade2ca70] + Dates 
  [8bb1440f] + DelimitedFiles 
  [8ba89e20] + Distributed 
  [b77e0a4c] + InteractiveUtils 
  [76f85450] + LibGit2 
  [8f399da3] + Libdl 
  [37e2e46d] + LinearAlgebra 
  [56ddb016] + Logging 
  [d6f4376e] + Markdown 
  [a63ad114] + Mmap 
  [44cfe95a] + Pkg 
  [de0858da] + Printf 
  [9abbd945] + Profile 
  [3fa0cd96] + REPL 
  [9a3f8284] + Random 
  [ea8e919c] + SHA 
  [9e88b42a] + Serialization 
  [1a1011a3] + SharedArrays 
  [6462fe0b] + Sockets 
  [2f01184e] + SparseArrays 
  [10745b16] + Statistics 
  [4607b0f0] + SuiteSparse 
  [8dfed614] + Test 
  [cf7118a7] + UUIDs 
  [4ec0a83e] + Unicode 
  Building CMake → `~/.julia/packages/CMake/nSK2r/deps/build.log`
  Updating `/tmp/jl_k3gJK0/Project.toml`
 [no changes]
  Updating `/tmp/jl_k3gJK0/Manifest.toml`
 [no changes]
  Building Blosc → `~/.julia/packages/Blosc/lzFr0/deps/build.log`
  Updating `/tmp/jl_VLUMdq/Project.toml`
 [no changes]
  Updating `/tmp/jl_VLUMdq/Manifest.toml`
 [no changes]
  Building HDF5 ─→ `~/.julia/packages/HDF5/Zh9on/deps/build.log`
  Updating `/tmp/jl_6iOMMR/Project.toml`
 [no changes]
  Updating `/tmp/jl_6iOMMR/Manifest.toml`
 [no changes]
  Building Rmath → `~/.julia/packages/Rmath/BoBag/deps/build.log`
  Updating `/tmp/jl_1328RF/Project.toml`
 [no changes]
  Updating `/tmp/jl_1328RF/Manifest.toml`
 [no changes]
   Testing GaussianMixtures
  Updating `/tmp/jl_W9swrQ/Project.toml`
 [no changes]
  Updating `/tmp/jl_W9swrQ/Manifest.toml`
 [no changes]
Running sandbox
Status `/tmp/jl_W9swrQ/Project.toml`
  [aaaa29a8] Clustering v0.13.3
  [34da2185] Compat v2.2.0
  [31c24e10] Distributions v0.22.3
  [5789e2e9] FileIO v1.2.1
  [cc18c42c] GaussianMixtures v0.3.0
  [4138dd39] JLD v0.9.1
  [90014a1f] PDMats v0.9.10
  [6e75b9c4] ScikitLearnBase v0.5.0
  [276daf66] SpecialFunctions v0.9.0
  [2913bbd2] StatsBase v0.32.0
  [8bb1440f] DelimitedFiles 
  [8ba89e20] Distributed 
  [37e2e46d] LinearAlgebra 
  [56ddb016] Logging 
  [de0858da] Printf 
  [9a3f8284] Random 
  [10745b16] Statistics 
[ Info: Testing Data
(100000, -785348.6972141388, [99338.96591174963, 661.034088250395], [1036.7728802643517 -887.3240524313566 -1216.0965478265284; -694.1098486684456 537.5135647275062 1195.9430657301464], [[99951.3082216973 729.6516311811368 1310.7026839053613; 729.6516311811367 99180.8084692007 203.81223966431986; 1310.7026839053613 203.81223966431986 97369.11572974383], [807.1318905629179 -518.3119713679732 -1336.9871779441328; -518.3119713679731 1161.1336730447904 823.0615340446705; -1336.9871779441326 823.0615340446706 2325.577743707684]])
┌ Warning: rmprocs: process 1 not removed
└ @ Distributed /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Distributed/src/cluster.jl:1015
[ Info: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.308828e+03
      1       9.885291e+02      -3.202985e+02 |        8
      2       9.106962e+02      -7.783288e+01 |        4
      3       8.702780e+02      -4.041826e+01 |        2
      4       8.683753e+02      -1.902626e+00 |        0
      5       8.683753e+02       0.000000e+00 |        0
K-means converged with 5 iterations (objv = 868.3753289164315)
┌ Info: K-means with 272 data points using 5 iterations
└ 11.3 data points per parameter
[ Info: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
┌ Info: EM with 272 data points 0 iterations avll -2.087396
└ 5.8 data points per parameter
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:221
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:221
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:221
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:221
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:631 [inlined]
└ @ Core ./broadcast.jl:631
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:230
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:230
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:631 [inlined]
└ @ Core ./broadcast.jl:631
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:631 [inlined]
└ @ Core ./broadcast.jl:631
[ Info: iteration 1, lowerbound -3.925836
[ Info: iteration 2, lowerbound -3.817344
[ Info: iteration 3, lowerbound -3.681978
[ Info: iteration 4, lowerbound -3.497485
[ Info: iteration 5, lowerbound -3.285949
[ Info: iteration 6, lowerbound -3.088880
[ Info: iteration 7, lowerbound -2.944085
[ Info: dropping number of Gaussions to 7
[ Info: iteration 8, lowerbound -2.860264
[ Info: dropping number of Gaussions to 5
[ Info: iteration 9, lowerbound -2.815107
[ Info: dropping number of Gaussions to 4
[ Info: iteration 10, lowerbound -2.778789
[ Info: iteration 11, lowerbound -2.764098
[ Info: dropping number of Gaussions to 3
[ Info: iteration 12, lowerbound -2.748815
[ Info: iteration 13, lowerbound -2.726393
[ Info: iteration 14, lowerbound -2.697006
[ Info: iteration 15, lowerbound -2.653804
[ Info: iteration 16, lowerbound -2.596050
[ Info: iteration 17, lowerbound -2.528667
[ Info: iteration 18, lowerbound -2.462247
[ Info: iteration 19, lowerbound -2.406511
[ Info: iteration 20, lowerbound -2.364118
[ Info: iteration 21, lowerbound -2.333161
[ Info: iteration 22, lowerbound -2.313478
[ Info: iteration 23, lowerbound -2.307426
[ Info: dropping number of Gaussions to 2
[ Info: iteration 24, lowerbound -2.302931
[ Info: iteration 25, lowerbound -2.299261
[ Info: iteration 26, lowerbound -2.299257
[ Info: iteration 27, lowerbound -2.299255
[ Info: iteration 28, lowerbound -2.299254
[ Info: iteration 29, lowerbound -2.299253
[ Info: iteration 30, lowerbound -2.299253
[ Info: iteration 31, lowerbound -2.299253
[ Info: iteration 32, lowerbound -2.299253
[ Info: iteration 33, lowerbound -2.299253
[ Info: iteration 34, lowerbound -2.299253
[ Info: iteration 35, lowerbound -2.299253
[ Info: iteration 36, lowerbound -2.299253
[ Info: iteration 37, lowerbound -2.299253
[ Info: iteration 38, lowerbound -2.299253
[ Info: iteration 39, lowerbound -2.299253
[ Info: iteration 40, lowerbound -2.299253
[ Info: iteration 41, lowerbound -2.299253
[ Info: iteration 42, lowerbound -2.299253
[ Info: iteration 43, lowerbound -2.299253
[ Info: iteration 44, lowerbound -2.299253
[ Info: iteration 45, lowerbound -2.299253
[ Info: iteration 46, lowerbound -2.299253
[ Info: iteration 47, lowerbound -2.299253
[ Info: iteration 48, lowerbound -2.299253
[ Info: iteration 49, lowerbound -2.299253
[ Info: iteration 50, lowerbound -2.299253
[ Info: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
History[Fri Jan 17 14:01:55 2020: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
, Fri Jan 17 14:02:04 2020: K-means with 272 data points using 5 iterations
11.3 data points per parameter
, Fri Jan 17 14:02:06 2020: EM with 272 data points 0 iterations avll -2.087396
5.8 data points per parameter
, Fri Jan 17 14:02:08 2020: GMM converted to Variational GMM
, Fri Jan 17 14:02:16 2020: iteration 1, lowerbound -3.925836
, Fri Jan 17 14:02:16 2020: iteration 2, lowerbound -3.817344
, Fri Jan 17 14:02:16 2020: iteration 3, lowerbound -3.681978
, Fri Jan 17 14:02:16 2020: iteration 4, lowerbound -3.497485
, Fri Jan 17 14:02:16 2020: iteration 5, lowerbound -3.285949
, Fri Jan 17 14:02:17 2020: iteration 6, lowerbound -3.088880
, Fri Jan 17 14:02:17 2020: iteration 7, lowerbound -2.944085
, Fri Jan 17 14:02:17 2020: dropping number of Gaussions to 7
, Fri Jan 17 14:02:17 2020: iteration 8, lowerbound -2.860264
, Fri Jan 17 14:02:17 2020: dropping number of Gaussions to 5
, Fri Jan 17 14:02:17 2020: iteration 9, lowerbound -2.815107
, Fri Jan 17 14:02:17 2020: dropping number of Gaussions to 4
, Fri Jan 17 14:02:17 2020: iteration 10, lowerbound -2.778789
, Fri Jan 17 14:02:17 2020: iteration 11, lowerbound -2.764098
, Fri Jan 17 14:02:17 2020: dropping number of Gaussions to 3
, Fri Jan 17 14:02:17 2020: iteration 12, lowerbound -2.748815
, Fri Jan 17 14:02:17 2020: iteration 13, lowerbound -2.726393
, Fri Jan 17 14:02:17 2020: iteration 14, lowerbound -2.697006
, Fri Jan 17 14:02:17 2020: iteration 15, lowerbound -2.653804
, Fri Jan 17 14:02:17 2020: iteration 16, lowerbound -2.596050
, Fri Jan 17 14:02:17 2020: iteration 17, lowerbound -2.528667
, Fri Jan 17 14:02:17 2020: iteration 18, lowerbound -2.462247
, Fri Jan 17 14:02:17 2020: iteration 19, lowerbound -2.406511
, Fri Jan 17 14:02:17 2020: iteration 20, lowerbound -2.364118
, Fri Jan 17 14:02:17 2020: iteration 21, lowerbound -2.333161
, Fri Jan 17 14:02:17 2020: iteration 22, lowerbound -2.313478
, Fri Jan 17 14:02:17 2020: iteration 23, lowerbound -2.307426
, Fri Jan 17 14:02:17 2020: dropping number of Gaussions to 2
, Fri Jan 17 14:02:17 2020: iteration 24, lowerbound -2.302931
, Fri Jan 17 14:02:17 2020: iteration 25, lowerbound -2.299261
, Fri Jan 17 14:02:17 2020: iteration 26, lowerbound -2.299257
, Fri Jan 17 14:02:17 2020: iteration 27, lowerbound -2.299255
, Fri Jan 17 14:02:17 2020: iteration 28, lowerbound -2.299254
, Fri Jan 17 14:02:17 2020: iteration 29, lowerbound -2.299253
, Fri Jan 17 14:02:17 2020: iteration 30, lowerbound -2.299253
, Fri Jan 17 14:02:17 2020: iteration 31, lowerbound -2.299253
, Fri Jan 17 14:02:17 2020: iteration 32, lowerbound -2.299253
, Fri Jan 17 14:02:17 2020: iteration 33, lowerbound -2.299253
, Fri Jan 17 14:02:17 2020: iteration 34, lowerbound -2.299253
, Fri Jan 17 14:02:17 2020: iteration 35, lowerbound -2.299253
, Fri Jan 17 14:02:17 2020: iteration 36, lowerbound -2.299253
, Fri Jan 17 14:02:17 2020: iteration 37, lowerbound -2.299253
, Fri Jan 17 14:02:17 2020: iteration 38, lowerbound -2.299253
, Fri Jan 17 14:02:17 2020: iteration 39, lowerbound -2.299253
, Fri Jan 17 14:02:17 2020: iteration 40, lowerbound -2.299253
, Fri Jan 17 14:02:17 2020: iteration 41, lowerbound -2.299253
, Fri Jan 17 14:02:17 2020: iteration 42, lowerbound -2.299253
, Fri Jan 17 14:02:17 2020: iteration 43, lowerbound -2.299253
, Fri Jan 17 14:02:17 2020: iteration 44, lowerbound -2.299253
, Fri Jan 17 14:02:17 2020: iteration 45, lowerbound -2.299253
, Fri Jan 17 14:02:17 2020: iteration 46, lowerbound -2.299253
, Fri Jan 17 14:02:17 2020: iteration 47, lowerbound -2.299253
, Fri Jan 17 14:02:17 2020: iteration 48, lowerbound -2.299253
, Fri Jan 17 14:02:17 2020: iteration 49, lowerbound -2.299253
, Fri Jan 17 14:02:17 2020: iteration 50, lowerbound -2.299253
, Fri Jan 17 14:02:17 2020: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [95.9549077739705, 178.0450922260295]
β = [95.9549077739705, 178.0450922260295]
m = [2.000229257775239 53.8519871724606; 4.250300733269785 79.28686694435997]
ν = [97.9549077739705, 180.0450922260295]
W = LinearAlgebra.UpperTriangular{Float64,Array{Float64,2}}[[0.3758763611950607 -0.008953123827348548; 0.0 0.012748664777410004], [0.18404155547483073 -0.007644049042328912; 0.0 0.008581705166331213]]
Kind: diag, size256
nx: 100000 sum(zeroth order stats): 100000.00000000001
avll from stats: -1.0059853385946766
avll from llpg:  -1.0059853385946764
avll direct:     -1.0059853385946764
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 100000.0
avll from stats: -0.9741419388878335
avll from llpg:  -0.9741419388878335
avll direct:     -0.9741419388878334
sum posterior: 100000.0
32×26 Array{Float64,2}:
  0.0939379   -0.159068     0.0327144    0.132269     -0.0598883   -0.136275     0.031561     0.0820696     0.0322101   -0.0147857    0.0124035    0.0527315    -0.1742       -0.179073    -0.0928786   -0.105207     0.0792642   -0.187264    -0.0371413    0.0237102   -0.034257     0.144787     0.264144   -0.144435     0.0204464     0.0589268
  0.0387078    0.083546    -0.0674828    0.0362257     0.00138682   0.0859235    0.0521373    0.119143      0.0292826    0.0583611    0.0555209   -0.031383      0.169101      0.0793404   -0.0530413    0.193181    -0.0332876   -0.120581     0.106337     0.0945515    0.0150011    0.0372407    0.025151    0.20197      0.0582617    -0.137831
  0.0397714    0.167065     0.103402    -0.0541596    -0.170883     0.0326041   -0.133554     0.0228772     0.0516848    0.0609321    0.0172       0.00536619   -0.110247     -0.0209296    0.0150347    0.0158137    0.192148     0.0205745   -0.125626     0.0528134   -0.0836532   -0.0394665   -0.0121511   0.0452771   -0.197156      0.0371916
 -0.0711791   -0.0790712    0.12971     -0.093039     -0.0671141    0.0821347    0.0488734    0.0292515     0.0170551   -0.0625367    0.00143387  -0.165651      0.053642     -0.0251442    0.0279788    0.0447299    0.116725     0.00654788  -0.105628     0.00661456  -0.113749    -0.0448772   -0.10967     0.107964     0.082708     -0.0380305
 -0.0112415   -0.068839    -0.0520974    0.0410969    -0.104107     0.0847666    0.0282668    0.0207706     0.0407737    0.0204744    0.0446109   -0.0516107     0.164574      0.0871006   -0.0601514   -0.117902    -0.165575    -0.0332467    0.0516839    0.116902     0.011015    -0.0498105    0.0125114  -0.0343848   -0.0168238    -0.000756024
 -0.0926316   -0.0309559   -0.0721957   -0.000824882   0.0781443   -0.0045386    0.0431456    0.0779018    -0.094389     0.0437907    0.0915914    0.176048     -0.0233323    -0.0163876   -0.191724     0.267007     0.0414192   -0.130273    -0.178493     0.0828834    0.141277    -0.203718     0.0400796  -0.105048    -0.0359233     0.0754608
 -0.108522    -0.235338    -0.0379327    0.178271     -0.0365344    0.150471     0.0154925   -0.0577789    -0.0281395   -0.0842439   -0.0406656   -0.023973     -0.18301       0.186319    -0.0134629   -0.0321473    0.145694     0.156976     0.139904     0.250196     0.0312259   -0.17641     -0.115686    0.187655    -0.124644     -0.146917
 -0.0507057   -0.0568528    0.0943958   -0.109512     -0.0485595   -0.239271    -0.0420866    0.102333     -0.13005     -0.0626803   -0.0221608   -0.180726      0.00244626   -0.0237849   -0.211665    -0.0204889    0.161288     0.196203     0.00890459   0.29828     -0.0681483    0.0352034   -0.251854    0.112653    -0.0106954    -0.116043
 -0.220761    -0.182805    -0.0685475   -0.131627      0.0569195   -0.0494249    0.0800499    0.151545      0.0799947   -0.0343005   -0.0192211    0.0188563     0.12753       0.183747    -0.0391921    0.0788172    0.0637023    0.127785     0.00195021  -0.208214     0.00136868  -0.0772576    0.218947   -0.109752    -0.039106     -0.0478836
  0.068985    -0.120654     0.135395     0.115284     -0.202114     0.0899737    0.128371     0.0577529    -0.0633961   -0.0136733    0.107605    -0.150685      0.100778     -0.0498734   -0.0538032   -0.0450802    0.0570168   -0.0269785    0.0545246   -0.0299622   -0.0316017    0.139105    -0.0639609  -0.00097929  -0.124443     -0.081396
 -0.0397228   -0.0737067   -0.0625134    0.175583     -0.105123    -0.0156655   -0.0286513   -0.067123      0.0213669    0.0247154   -0.0230331    0.0922556     0.0871197    -0.0318606    0.0365491    0.0452531   -0.027608    -0.0764927   -0.0832688    0.0454204   -0.0716747    0.0552235   -0.0616574   0.0812224    0.0334081    -0.0763373
 -0.0213406    0.0605639   -0.0917749   -0.0445202     0.0948332   -0.0285035    0.0475506    0.075367     -0.16779     -0.0492259    0.087994    -0.126468     -0.025974     -0.0349232    0.00626622   0.00415898   0.0303369    0.0442134   -0.0514325    0.229112    -0.039603    -0.0100502    0.0211747  -0.0502345    0.130642      0.10781
 -0.146679    -0.104567    -0.194247    -0.132843      0.0869117    0.0713456   -0.00396415   0.0632877    -0.016817     0.177299    -0.0829985    0.0864638     0.0879375     0.00673895  -0.0807611    0.208048    -0.0651588   -0.204314    -0.0865605    0.0397219    0.260396     0.0105093    0.0640889   0.0591574   -0.0255882     0.132452
 -0.0328592    0.211696     0.108624    -0.0362677     0.031672     0.0668148   -0.0143525    0.101333      0.0062218    0.00748042   0.0949733    0.061694      0.000941283   0.20068      0.141898    -0.0171337   -0.00346906   0.116249    -0.0557931    0.00362615   0.00794467   0.201932    -0.203458    0.0910081   -0.049314      0.175578
  0.0384397   -0.104438     0.0271172   -0.00485368    0.0127584    0.244445    -0.0183581    0.000600454  -0.111788    -0.219086    -0.044953     0.0195964    -0.00147709    0.0721769    0.0103906   -0.00141252   0.00283074  -0.0114227    0.223701     0.090387     0.173606    -0.102683    -0.0795462   0.0593615   -0.00555938   -0.0588315
 -0.130271     0.0865441    0.00405266   0.0618978    -0.133824     0.17411      0.308507     0.0488366    -0.0889857    0.0971133   -0.0552723    0.0154208     0.141392     -0.0505387   -0.037994    -0.0616418   -0.00930909   0.00713883   0.0414251   -0.0791777    0.0042037   -0.0142877    0.12194    -0.136133    -0.00170235   -0.0475685
 -0.0248367   -0.159114    -0.0302372    0.0430643    -0.0490202   -0.0399365    0.0296517    0.106206     -0.0372215    0.0223958    0.28686     -0.103763      0.0335629    -0.0510661    0.0207224    0.0813026    0.00685839  -0.0898965    0.055052    -0.0962647   -0.0454171   -0.07315      0.158655    0.0140742    0.000199761   0.085915
 -0.0725826    0.158106     0.0905711    0.0260992    -0.116591    -0.082572     0.0709752   -0.114044      0.079449    -0.0507684   -0.0941814   -0.295915     -0.0158663    -0.021835     0.0200027   -0.0126344    0.00482595  -0.082521    -0.0596983    0.351594     0.175844    -0.00150845  -0.118876   -0.0526851   -0.0708831    -0.143812
  0.186028    -0.161776    -0.0383797   -0.119038     -0.070093     0.105146     0.0934982    0.133365     -0.196609    -0.00835101   0.0682729   -0.000905423  -0.26239       0.0650116    0.0868795   -0.147669     0.0293176    0.0735977    0.120166    -0.0543539    0.134488     0.103689    -0.185148   -0.180927     0.0372266     0.0136929
 -0.0554026   -0.202738    -0.0520988   -0.141389     -0.0463629   -0.226261    -0.0550313    0.104059     -0.0950217   -0.167394    -0.00366913   0.0625338     0.012799      0.0266714    0.0285753    0.0361901   -0.0235337   -0.0603104    0.0934287    0.218136    -0.0878532    0.0134966    0.019043    0.0159438    0.0265719    -0.0464447
 -0.0200531   -0.205963    -0.0844429    0.104804      0.0891094    0.195752    -0.0618744   -0.0616319     0.00305027  -0.149276     0.0578025   -0.103391      0.25943      -0.0417656   -0.00620002  -0.0686919    0.0496149    0.0849983   -0.0434313   -0.0055353    0.0095224    0.106785     0.0496251   0.0711035    0.0612986    -0.0712794
  0.124866    -0.0570269    0.179977     0.131649     -0.0473075    0.00976581  -0.0206869   -0.222511     -0.124614     0.0371926    0.0261573    0.0641424    -0.0786272    -0.0193212    0.0550433    0.0759232   -0.119739     0.0276093   -0.00698933  -0.120864    -0.0996409   -0.0452181   -0.18476    -0.114084     0.034694     -0.000772464
 -0.00735445  -0.165835     0.142214    -0.074142      0.0887459   -0.029276     0.0378042    0.0340288    -0.028811    -0.00903017  -0.1013       0.0163014     0.109323      0.0883319   -0.0663847   -0.0321623   -0.0806996    0.054704    -0.11942     -0.130072    -0.179708    -0.114881    -0.125429    0.0249535   -0.018585      0.208256
 -0.075762     0.105025    -0.0559525   -0.0967564    -0.0186126    0.0467901   -0.039182     0.134473     -0.112495     0.0337175   -0.0116771   -0.0750619     0.108836      0.0833701   -0.0880663    0.0286508   -0.0331691   -0.0767462    0.197039    -0.033104    -0.151273     0.0350395   -0.0120291   0.0845031    0.0546516    -0.0289985
 -0.0532943   -0.107349    -0.00245805  -0.0175822     0.0145724    0.0321237   -0.0487622    0.0878947    -0.0771268    0.00633437  -0.253427    -0.124543     -0.135246      0.0165949    0.0133427   -0.0441166    0.049124    -0.0523422   -0.0382849    0.0587545    0.018466    -0.0335418   -0.0142359   0.135541     0.010967     -0.150307
  0.0500181    0.0289011   -0.00473478   0.00437909    0.123019    -0.0746852    0.0267396    0.0160332    -0.101405     0.0315392   -0.0586346    0.0275455    -0.161686      0.0268035   -0.0592673   -0.112162     0.00745523  -0.171752     0.172772    -0.00347916  -0.12771      0.165807    -0.0215055   0.0106161   -0.0749501     0.140696
  0.262296    -0.00795668  -0.0601701    0.0303822    -0.0401202    0.0357297    0.206532    -0.000788019   0.245356    -0.0677266   -0.0323933   -0.0255455    -0.112336      0.214408     0.0312863    0.0224304   -0.0610293    0.057752     0.0312054    0.0396376    0.0117174    0.134424     0.0679023   0.0973484   -0.0143422     0.0241926
  0.0147354   -0.0979677    0.0113761   -0.0306901    -0.0268354    0.108035     0.110894     0.00459257    0.0825354   -0.149026     0.0570563   -0.0100225    -0.108405      0.0773461    0.017202    -0.0634823   -0.105659    -0.123317    -0.0785803   -0.0553182   -0.204198    -0.165841    -0.0417762   0.0809933   -0.0355888     0.181401
  0.291266    -0.0854929    0.10083     -0.186651      0.0541072   -0.0647418    0.200362     0.0366842    -0.115606     0.0440309    0.0817586   -0.113021      0.00721052    0.0550464    0.111158     0.0564452   -0.150814    -0.0823209    0.133745     0.170811     0.0954273   -0.20465      0.0349223  -0.0166561   -0.000332657  -0.090239
 -0.00502189  -0.0649309    0.152957     0.132435     -0.0445186    0.174931     0.0624493    0.135657     -0.141066    -0.108072     0.0678088    0.0453923     0.108557     -0.0187193   -0.0676749    0.00772262  -0.0151768   -0.0109733   -0.0595502   -0.154009     0.00619266  -0.0555502    0.0362205   0.16401     -0.0995787    -0.0611528
  0.149645    -0.00647801  -0.0678344    0.00861591   -0.00137257   0.0150146    0.0219755   -0.00583461    0.0285632   -0.0594404    0.0711772   -0.00755418    0.00825918   -0.0322539   -0.0398133   -0.0315032   -0.0433305    0.0728887   -0.127144    -0.00334757   0.132552    -0.00386769   0.0209784  -0.022317     0.0504962     0.085146
 -0.216303     0.0520517   -0.0394702   -0.0336525     0.0616432    0.0990995   -0.109733    -0.0683886     0.0631823    0.0409276    0.0190073    0.210766      0.0253039     0.102149    -0.0229324   -0.152538     0.0627699    0.0870472    0.114549    -0.0261301    0.117071    -0.0424876    0.0563346   0.157528    -0.159851      0.113892kind diag, method split
┌ Info: 0: avll = 
└   tll[1] = -1.4375230810343427
[ Info: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.437673
[ Info: iteration 2, average log likelihood -1.437553
[ Info: iteration 3, average log likelihood -1.437120
[ Info: iteration 4, average log likelihood -1.432427
[ Info: iteration 5, average log likelihood -1.418407
[ Info: iteration 6, average log likelihood -1.411998
[ Info: iteration 7, average log likelihood -1.411126
[ Info: iteration 8, average log likelihood -1.410771
[ Info: iteration 9, average log likelihood -1.410534
[ Info: iteration 10, average log likelihood -1.410365
[ Info: iteration 11, average log likelihood -1.410247
[ Info: iteration 12, average log likelihood -1.410164
[ Info: iteration 13, average log likelihood -1.410105
[ Info: iteration 14, average log likelihood -1.410064
[ Info: iteration 15, average log likelihood -1.410034
[ Info: iteration 16, average log likelihood -1.410011
[ Info: iteration 17, average log likelihood -1.409992
[ Info: iteration 18, average log likelihood -1.409975
[ Info: iteration 19, average log likelihood -1.409959
[ Info: iteration 20, average log likelihood -1.409942
[ Info: iteration 21, average log likelihood -1.409926
[ Info: iteration 22, average log likelihood -1.409907
[ Info: iteration 23, average log likelihood -1.409886
[ Info: iteration 24, average log likelihood -1.409857
[ Info: iteration 25, average log likelihood -1.409811
[ Info: iteration 26, average log likelihood -1.409738
[ Info: iteration 27, average log likelihood -1.409628
[ Info: iteration 28, average log likelihood -1.409499
[ Info: iteration 29, average log likelihood -1.409391
[ Info: iteration 30, average log likelihood -1.409318
[ Info: iteration 31, average log likelihood -1.409270
[ Info: iteration 32, average log likelihood -1.409237
[ Info: iteration 33, average log likelihood -1.409212
[ Info: iteration 34, average log likelihood -1.409194
[ Info: iteration 35, average log likelihood -1.409178
[ Info: iteration 36, average log likelihood -1.409164
[ Info: iteration 37, average log likelihood -1.409153
[ Info: iteration 38, average log likelihood -1.409142
[ Info: iteration 39, average log likelihood -1.409133
[ Info: iteration 40, average log likelihood -1.409124
[ Info: iteration 41, average log likelihood -1.409117
[ Info: iteration 42, average log likelihood -1.409110
[ Info: iteration 43, average log likelihood -1.409104
[ Info: iteration 44, average log likelihood -1.409098
[ Info: iteration 45, average log likelihood -1.409093
[ Info: iteration 46, average log likelihood -1.409088
[ Info: iteration 47, average log likelihood -1.409084
[ Info: iteration 48, average log likelihood -1.409079
[ Info: iteration 49, average log likelihood -1.409075
[ Info: iteration 50, average log likelihood -1.409071
┌ Info: EM with 100000 data points 50 iterations avll -1.409071
└ 952.4 data points per parameter
┌ Info: 1
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4376728757624566
│     -1.4375532180304442
│      ⋮
└     -1.4090707967296499
[ Info: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.409285
[ Info: iteration 2, average log likelihood -1.409106
[ Info: iteration 3, average log likelihood -1.408775
[ Info: iteration 4, average log likelihood -1.405793
[ Info: iteration 5, average log likelihood -1.393860
[ Info: iteration 6, average log likelihood -1.382025
[ Info: iteration 7, average log likelihood -1.376325
[ Info: iteration 8, average log likelihood -1.372368
[ Info: iteration 9, average log likelihood -1.369741
[ Info: iteration 10, average log likelihood -1.368035
[ Info: iteration 11, average log likelihood -1.366892
[ Info: iteration 12, average log likelihood -1.365983
[ Info: iteration 13, average log likelihood -1.365101
[ Info: iteration 14, average log likelihood -1.364315
[ Info: iteration 15, average log likelihood -1.363592
[ Info: iteration 16, average log likelihood -1.363045
[ Info: iteration 17, average log likelihood -1.362665
[ Info: iteration 18, average log likelihood -1.362382
[ Info: iteration 19, average log likelihood -1.362149
[ Info: iteration 20, average log likelihood -1.361938
[ Info: iteration 21, average log likelihood -1.361729
[ Info: iteration 22, average log likelihood -1.361518
[ Info: iteration 23, average log likelihood -1.361290
[ Info: iteration 24, average log likelihood -1.361037
[ Info: iteration 25, average log likelihood -1.360758
[ Info: iteration 26, average log likelihood -1.360453
[ Info: iteration 27, average log likelihood -1.360121
[ Info: iteration 28, average log likelihood -1.359770
[ Info: iteration 29, average log likelihood -1.359424
[ Info: iteration 30, average log likelihood -1.359103
[ Info: iteration 31, average log likelihood -1.358825
[ Info: iteration 32, average log likelihood -1.358588
[ Info: iteration 33, average log likelihood -1.358389
[ Info: iteration 34, average log likelihood -1.358233
[ Info: iteration 35, average log likelihood -1.358114
[ Info: iteration 36, average log likelihood -1.358024
[ Info: iteration 37, average log likelihood -1.357954
[ Info: iteration 38, average log likelihood -1.357896
[ Info: iteration 39, average log likelihood -1.357847
[ Info: iteration 40, average log likelihood -1.357805
[ Info: iteration 41, average log likelihood -1.357768
[ Info: iteration 42, average log likelihood -1.357734
[ Info: iteration 43, average log likelihood -1.357704
[ Info: iteration 44, average log likelihood -1.357678
[ Info: iteration 45, average log likelihood -1.357656
[ Info: iteration 46, average log likelihood -1.357637
[ Info: iteration 47, average log likelihood -1.357621
[ Info: iteration 48, average log likelihood -1.357607
[ Info: iteration 49, average log likelihood -1.357594
[ Info: iteration 50, average log likelihood -1.357583
┌ Info: EM with 100000 data points 50 iterations avll -1.357583
└ 473.9 data points per parameter
┌ Info: 2
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4092850800380894
│     -1.4091064243699172
│      ⋮
└     -1.3575829069063006
[ Info: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.357830
[ Info: iteration 2, average log likelihood -1.357572
[ Info: iteration 3, average log likelihood -1.356537
[ Info: iteration 4, average log likelihood -1.348390
[ Info: iteration 5, average log likelihood -1.332382
[ Info: iteration 6, average log likelihood -1.320397
[ Info: iteration 7, average log likelihood -1.315445
[ Info: iteration 8, average log likelihood -1.312849
[ Info: iteration 9, average log likelihood -1.310733
[ Info: iteration 10, average log likelihood -1.308867
[ Info: iteration 11, average log likelihood -1.307261
[ Info: iteration 12, average log likelihood -1.305971
[ Info: iteration 13, average log likelihood -1.305285
[ Info: iteration 14, average log likelihood -1.305000
[ Info: iteration 15, average log likelihood -1.304804
[ Info: iteration 16, average log likelihood -1.304641
[ Info: iteration 17, average log likelihood -1.304490
[ Info: iteration 18, average log likelihood -1.304343
[ Info: iteration 19, average log likelihood -1.304200
[ Info: iteration 20, average log likelihood -1.304062
[ Info: iteration 21, average log likelihood -1.303930
[ Info: iteration 22, average log likelihood -1.303809
[ Info: iteration 23, average log likelihood -1.303705
[ Info: iteration 24, average log likelihood -1.303621
[ Info: iteration 25, average log likelihood -1.303553
[ Info: iteration 26, average log likelihood -1.303500
[ Info: iteration 27, average log likelihood -1.303459
[ Info: iteration 28, average log likelihood -1.303427
[ Info: iteration 29, average log likelihood -1.303402
[ Info: iteration 30, average log likelihood -1.303383
[ Info: iteration 31, average log likelihood -1.303369
[ Info: iteration 32, average log likelihood -1.303358
[ Info: iteration 33, average log likelihood -1.303350
[ Info: iteration 34, average log likelihood -1.303344
[ Info: iteration 35, average log likelihood -1.303339
[ Info: iteration 36, average log likelihood -1.303336
[ Info: iteration 37, average log likelihood -1.303333
[ Info: iteration 38, average log likelihood -1.303331
[ Info: iteration 39, average log likelihood -1.303329
[ Info: iteration 40, average log likelihood -1.303328
[ Info: iteration 41, average log likelihood -1.303327
[ Info: iteration 42, average log likelihood -1.303326
[ Info: iteration 43, average log likelihood -1.303326
[ Info: iteration 44, average log likelihood -1.303325
[ Info: iteration 45, average log likelihood -1.303325
[ Info: iteration 46, average log likelihood -1.303324
[ Info: iteration 47, average log likelihood -1.303324
[ Info: iteration 48, average log likelihood -1.303324
[ Info: iteration 49, average log likelihood -1.303324
[ Info: iteration 50, average log likelihood -1.303324
┌ Info: EM with 100000 data points 50 iterations avll -1.303324
└ 236.4 data points per parameter
┌ Info: 3
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.357830061738812
│     -1.35757175376417
│      ⋮
└     -1.3033237279568628
[ Info: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.303591
[ Info: iteration 2, average log likelihood -1.303310
[ Info: iteration 3, average log likelihood -1.302356
[ Info: iteration 4, average log likelihood -1.291417
[ Info: iteration 5, average log likelihood -1.258854
[ Info: iteration 6, average log likelihood -1.235136
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.223595
[ Info: iteration 8, average log likelihood -1.229906
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     13
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.221440
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.225239
[ Info: iteration 11, average log likelihood -1.227553
[ Info: iteration 12, average log likelihood -1.218905
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      5
│     11
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 13, average log likelihood -1.214612
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     13
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 14, average log likelihood -1.234227
[ Info: iteration 15, average log likelihood -1.231506
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 16, average log likelihood -1.221776
[ Info: iteration 17, average log likelihood -1.228467
[ Info: iteration 18, average log likelihood -1.221323
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      5
│     13
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 19, average log likelihood -1.217652
[ Info: iteration 20, average log likelihood -1.233638
[ Info: iteration 21, average log likelihood -1.222648
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 22, average log likelihood -1.218792
[ Info: iteration 23, average log likelihood -1.226748
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     13
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 24, average log likelihood -1.219565
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 25, average log likelihood -1.224394
[ Info: iteration 26, average log likelihood -1.228466
[ Info: iteration 27, average log likelihood -1.221038
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 28, average log likelihood -1.217579
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     13
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 29, average log likelihood -1.225527
[ Info: iteration 30, average log likelihood -1.226779
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 31, average log likelihood -1.219406
[ Info: iteration 32, average log likelihood -1.227165
[ Info: iteration 33, average log likelihood -1.220180
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      5
│     13
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 34, average log likelihood -1.216612
[ Info: iteration 35, average log likelihood -1.232983
[ Info: iteration 36, average log likelihood -1.222065
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 37, average log likelihood -1.218256
[ Info: iteration 38, average log likelihood -1.226425
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     13
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 39, average log likelihood -1.219277
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 40, average log likelihood -1.224075
[ Info: iteration 41, average log likelihood -1.228302
[ Info: iteration 42, average log likelihood -1.220917
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 43, average log likelihood -1.217510
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     13
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 44, average log likelihood -1.225528
[ Info: iteration 45, average log likelihood -1.226736
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     5
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 46, average log likelihood -1.219388
[ Info: iteration 47, average log likelihood -1.227166
[ Info: iteration 48, average log likelihood -1.220189
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      5
│     13
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 49, average log likelihood -1.216622
[ Info: iteration 50, average log likelihood -1.232979
┌ Info: EM with 100000 data points 50 iterations avll -1.232979
└ 118.1 data points per parameter
┌ Info: 4
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.3035907208507609
│     -1.303310008262435
│      ⋮
└     -1.2329785232710275
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.222398
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      9
│     10
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 2, average log likelihood -1.218160
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 3, average log likelihood -1.218505
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      9
│     10
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.201601
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     18
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.168804
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      9
│     10
│     22
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.129131
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     21
│     26
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.125319
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      9
│     10
│     18
│     22
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.119499
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     21
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.129236
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      9
│     10
│     22
│     26
│     29
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.113230
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     18
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 11, average log likelihood -1.126200
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      9
│     10
│     21
│     22
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 12, average log likelihood -1.108279
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 13, average log likelihood -1.128338
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      9
│     10
│     18
│     26
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 14, average log likelihood -1.114425
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     21
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 15, average log likelihood -1.118135
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      9
│     10
│     29
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 16, average log likelihood -1.117828
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     18
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 17, average log likelihood -1.123442
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      9
│     10
│     21
│     22
│     25
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 18, average log likelihood -1.107878
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 19, average log likelihood -1.128151
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      9
│     10
│     18
│     26
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 20, average log likelihood -1.114144
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     21
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 21, average log likelihood -1.117833
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      9
│     10
│     29
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 22, average log likelihood -1.117221
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     18
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 23, average log likelihood -1.122642
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      9
│     10
│     21
│     22
│     26
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 24, average log likelihood -1.106798
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 25, average log likelihood -1.127773
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      9
│     10
│     18
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 26, average log likelihood -1.113822
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     21
│     22
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 27, average log likelihood -1.114327
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      9
│     10
│     29
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 28, average log likelihood -1.117088
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     18
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 29, average log likelihood -1.121080
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      9
│     10
│     21
│     22
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 30, average log likelihood -1.102077
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     18
│     26
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 31, average log likelihood -1.116614
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      9
│     10
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 32, average log likelihood -1.111351
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     21
│     22
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 33, average log likelihood -1.101370
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      9
│     10
│     18
│     29
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 34, average log likelihood -1.105835
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 35, average log likelihood -1.118582
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      9
│     10
│     18
│     21
│     22
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 36, average log likelihood -1.093409
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     26
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 37, average log likelihood -1.119549
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      9
│     10
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 38, average log likelihood -1.106901
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     18
│     21
│     22
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 39, average log likelihood -1.099300
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      9
│     10
│     29
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 40, average log likelihood -1.111970
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 41, average log likelihood -1.114573
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      9
│     10
│     18
│     21
│     22
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 42, average log likelihood -1.091576
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     26
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 43, average log likelihood -1.119557
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      9
│     10
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 44, average log likelihood -1.106848
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     18
│     21
│     22
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 45, average log likelihood -1.099256
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      9
│     10
│     29
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 46, average log likelihood -1.111968
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 47, average log likelihood -1.114571
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      9
│     10
│     18
│     21
│     22
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 48, average log likelihood -1.091574
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     26
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 49, average log likelihood -1.119555
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      9
│     10
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 50, average log likelihood -1.106847
┌ Info: EM with 100000 data points 50 iterations avll -1.106847
└ 59.0 data points per parameter
┌ Info: 5
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.2223979354699142
│     -1.2181600128213763
│      ⋮
└     -1.106847351150735
┌ Info: Total log likelihood: 
│   tll =
│    251-element Array{Float64,1}:
│     -1.4375230810343427
│     -1.4376728757624566
│     -1.4375532180304442
│     -1.4371199270255748
│      ⋮
│     -1.0915744113532744
│     -1.1195554413422317
└     -1.106847351150735
32×26 Array{Float64,2}:
  0.039266   -0.0882614    0.00256327  -0.0302938    -0.0301266   0.101521     0.157701    -0.0208804    0.0714746   -0.160258     0.0563334    0.028737    -0.1053       0.104806    -0.0690458    -0.0630946   -0.113853     -0.0996608   -0.0385112   -0.12889     -0.226458    -0.186567    -0.0340737     0.0695917    -0.0345395    0.196384
  0.133826   -0.009767    -0.0787455   -0.00132334    0.0145353   0.0182352    0.0831691    0.0262487    0.0283619   -0.0528697    0.0534086   -0.00564087  -0.026944    -0.0242654   -0.0149683    -0.0755858   -0.0526449     0.0761972   -0.0681764   -0.00373656   0.127099    -0.00461235  -0.0202704    -0.0500299     0.0387535    0.0951828
 -0.0916132  -0.08176      0.13088     -0.101183     -0.0277151   0.0716357    0.0257132    0.0201406    0.00985117  -0.0660348   -0.0255061   -0.15986      0.0962924   -0.0254935    0.03106      -0.00375189   0.096314      0.00701942  -0.0851368    0.0150525   -0.110566    -0.0464785   -0.129504      0.0953255     0.073104    -0.0383762
 -0.0405099  -0.0595148    0.100015    -0.10167      -0.0366388  -0.211987    -0.01999      0.0918971   -0.132677    -0.070482    -0.00689641  -0.190648     0.0753125   -0.0147066   -0.213007     -0.0259702    0.10746       0.19487      0.01114      0.285009    -0.0377137    0.0810324   -0.220838      0.122949     -0.00912983  -0.10196
 -0.120152   -0.174527    -0.0575845    0.175017     -0.0355459   0.136527     0.0150054    0.0201011   -0.065303    -0.104743    -0.0453068   -0.0394979   -0.171095     0.189112    -0.0179078    -0.0288198    0.145784      0.151369     0.135255     0.240523     0.0304482   -0.199718    -0.0993962     0.198945     -0.121463    -0.136983
 -0.0641928   0.154875     0.0533555   -0.00417643   -0.124794   -0.0783306    0.0791286   -0.131884     0.0782423   -0.047002    -0.0753433   -0.284237    -0.0119716   -0.0169496    0.0213704    -0.0121997    0.000901363  -0.0842946   -0.0581447    0.356023     0.172077     0.00736119  -0.0933514    -0.0282756    -0.0693876   -0.143072
  0.0215592  -0.00886865   0.0640168   -0.000299816  -0.0108094   0.0917221    0.05929      0.103085    -0.0485259   -0.00524794   0.0382617    0.0152532    0.0703697    0.0967033   -0.011403     -0.00815204  -0.0434381     0.0140719    0.00123177  -0.0276454   -0.00746729   0.0143621   -0.0738024     0.0400523    -0.0215024    0.0276529
  0.0468634  -0.127779     0.00225978  -0.00387887   -0.0153026   0.257067    -0.00775638   0.0156975   -0.0861754   -0.203205    -0.0416898    0.00844605   0.00169058   0.0740337    0.0226199    -0.00637641   0.0177725    -0.0169335    0.215554     0.0740526    0.147104    -0.106653    -0.086275      0.0790486    -0.0309219   -0.0381263
 -0.0247349  -0.157373    -0.245532    -1.36601       0.0525172  -0.042156     0.0733803    0.175353    -0.0503698    0.0185223    0.29844     -0.036021     0.0538686   -0.129129     0.0243042     0.0594888   -0.0109657    -0.0914718    0.0546157   -0.240564     0.212031    -0.135068     0.107674      0.0141714    -0.00623509   0.0112329
 -0.0249357  -0.152161     0.0834083    0.656475     -0.100427   -0.0482464   -0.0165294    0.109418    -0.0315056    0.0196987    0.291377    -0.128245     0.0259074   -0.0210365    0.0293857     0.0989548    0.00856802   -0.0885408    0.0541746   -0.0429925   -0.135012    -0.0194029    0.179061      0.0118382     0.00502963   0.0378633
  0.0994192  -0.152876     0.132577     0.0544976    -0.0758611  -0.271497    -0.560855     0.0791936   -0.0664667    0.00138415  -0.0133193    0.0481574   -0.205736    -0.174612    -0.0848007    -0.152117     0.0516366    -0.11464     -0.0292222    0.0230355   -0.032768     0.16497      0.259188     -0.156116      0.127447    -0.116789
  0.0902386  -0.169541    -0.0291879    0.208871     -0.0184441  -0.0691537    0.424009     0.0812504    0.133167    -0.0380272   -0.0916761    0.0600581   -0.140138    -0.181742    -0.100019     -0.0572519    0.0967314    -0.213622    -0.0446327    0.0282298    0.0101833    0.155428     0.274094     -0.115054      0.0454586    0.253102
  0.0780868  -0.199108    -0.197979     0.120146     -0.251234    0.205424    -0.0739707   -0.136195     0.0621603   -0.160529     0.0492712   -0.18149      0.266185    -0.0748155   -0.0567918    -0.0358283    0.271595      0.0881297   -0.028686    -0.00514916  -1.50302     -0.00188585   0.0503069     0.0366726     0.0851612   -0.0115763
 -0.0847841  -0.201574    -0.278891     0.166613     -0.285815    0.194797    -0.0705633    0.0573474   -0.0621065   -0.176507     0.078727     0.0904489    0.275262    -0.108731    -0.0213401    -0.103705     0.275239      0.101767    -0.0214062   -0.00204842   1.4239       0.384143     0.0698715    -0.00923049   -0.434741    -0.0366087
 -0.126724   -0.191567     0.151309     0.0612127     0.758458    0.183207    -0.0446283   -0.047089     0.133419    -0.152232     0.0583687   -0.210053     0.2554      -0.0677229    0.000822606  -0.114233    -0.18362       0.0800743   -0.0721019    0.00251012  -0.622184     0.233838     0.0323671     0.0708489     0.110893    -0.2017
  0.0766861  -0.218059    -0.19524      0.112837      0.418902    0.195546    -0.0521458   -0.164949    -0.0883719   -0.107165     0.04138     -0.28866      0.246187     0.165161     0.0514678     0.0298162   -0.149333      0.0749209   -0.0399438   -0.0126011    1.17746     -0.235489     0.0453484     0.12914       0.66158     -0.0555834
  0.0427173   0.151467     0.101848    -0.0565925    -0.198184    0.0463073   -0.1257       0.0327149    0.0310618    0.0923266    0.0183919    0.0102412   -0.138712    -0.038329     0.0179854    -0.0574234    0.174056      0.0229027   -0.123644     0.0268707   -0.0835178   -0.0356321   -0.0411711     0.0406952    -0.205422     0.0414409
  0.0566607   0.0254646   -0.00442816   0.00451129    0.111517   -0.0349772   -0.0367197    0.0214538   -0.0992933    0.0294581   -0.0240506    0.00843264  -0.204104     0.0278383   -0.0491655    -0.102972    -0.0570672    -0.163452     0.18225     -8.58277e-5  -0.113462     0.164931    -0.0259693    -0.000312551  -0.073566     0.125515
 -0.0408637  -0.195151    -0.0290307   -0.160528     -0.058214   -0.222492    -0.0570346    0.0988208   -0.128836    -0.199639     0.0065077    0.0354393    0.00206817  -0.00189946   0.0363943     0.0500502   -0.0392678    -0.0626578    0.0836857    0.219679    -0.0741323    0.0101858    0.0147595     0.00162625    0.0227378   -0.0479806
 -0.19533    -0.180306    -0.0527917   -0.131013      0.0570394  -0.101533     0.0344182    0.147141     0.0553871   -0.0220755   -0.0158006    0.0388447    0.132384     0.171154    -0.0371647     0.0824529    0.0487226     0.126887     0.00381694  -0.229735    -0.00198439  -0.0861453    0.177623     -0.120957     -0.0263067   -0.0402395
 -0.0229017   0.0614006   -0.0752595   -0.0484761     0.100046   -0.0229459    0.0468019    0.0809613   -0.166381    -0.0480244    0.08428     -0.113627    -0.00456172  -0.0462293    0.0138253     0.0071103    0.0139197     0.0505565   -0.0318523    0.229881    -0.0371319    0.0168084   -0.000802387  -0.0418012     0.136376     0.118776
  0.287951   -0.099642     0.12158     -0.181827      0.04885    -0.047426     0.199448     0.0347735   -0.0984291    0.112802     0.0942817   -0.0661845    0.00255686   0.0322957    0.111031      0.0205264   -0.171507     -0.0846128    0.134436     0.186816     0.0807855   -0.201195     0.0304229    -0.0166078    -0.00759575  -0.0941736
 -0.0887754   0.0742923   -0.0551179   -0.0588104    -0.0161177   0.0457252   -0.0239326    0.153024    -0.107813     0.0737141   -0.0140434   -0.0652354    0.116927     0.103309    -0.0852497     0.0357335   -0.0360947    -0.079786     0.179302     0.01587     -0.150125     0.0166986   -0.0113873     0.105012      0.0499163   -0.0353602
 -0.0122564   0.00974147   0.0469409    0.0895212    -0.1703      0.130837     0.213327     0.0536635   -0.0754593    0.114212     0.0398988   -0.0770544    0.117912    -0.0452155   -0.0425535    -0.0582446    0.0275528    -0.0129699    0.0610867   -0.0504672   -0.00832757   0.0731105    0.0188576    -0.066579     -0.0653153   -0.0735874
 -0.213433    0.044521    -0.0367661   -0.0363255     0.0703887   0.102249    -0.105961    -0.0710553    0.0455546    0.0429509    0.0242821    0.205679     0.0309847    0.0986094   -0.047812     -0.139357     0.0572239     0.0878103    0.121343    -0.0255539    0.0929223   -0.0352206    0.0641374     0.158372     -0.154521     0.116932
 -0.0181517  -0.0738946   -0.0647547    0.162781     -0.103536   -0.0422859   -0.0164826   -0.0319657    0.00841812   0.0199595   -0.0222211    0.0921604    0.0846873    0.0325901    0.00138032    0.0464715   -0.0134783    -0.0749994   -0.122513     0.0837019   -0.0737133    0.0467556   -0.0600105     0.0784884     0.0146629   -0.080412
  0.0963343  -0.0910235    0.136633     0.110209     -0.318751   -0.00235451  -0.10342     -0.961948    -0.0730054    0.024538     0.0285243   -0.0677148   -0.140435    -0.00907851   0.0875371     0.0779074   -0.112793      0.0432957   -0.00667742  -0.15656     -0.0946867   -0.0485138   -0.203638     -0.110195      0.0203411   -0.00355092
  0.131116   -0.0408529    0.248078     0.181507      0.143165    0.0113085    0.0802892    0.610567    -0.285136     0.0833099    0.0230286    0.216791    -0.005819    -0.0148282    0.148023      0.0736823   -0.135229      0.0227781   -0.00669296  -0.121948    -0.108292    -0.0396612   -0.140559     -0.123605      0.0344038    0.00564924
 -0.0427656  -0.115376     0.00929771  -0.013019      0.0123108   0.0362527   -0.0343125    0.103688    -0.0636978   -0.00465116  -0.236201    -0.123279    -0.129847     0.00540771   0.02154      -0.0407071    0.0629208    -0.0407198   -0.0267613    0.054733     0.023891    -0.033344    -0.0213101     0.0967549    -0.00471524  -0.104649
  0.271482   -0.00610711  -0.0565549    0.0278597    -0.0244872   0.0408679    0.203        0.00396156   0.248271    -0.0758771   -0.0157245   -0.0275399   -0.13506      0.236267     0.0249008     0.0363134   -0.0612194     0.0612915    0.0466913    0.0377222    0.00932569   0.129384     0.0568939     0.0924434     0.00648543   0.0299369
 -0.215072   -0.0936789   -0.202457    -0.142629      0.0765363   0.0744772   -0.0101491    0.0245657   -0.0140907    0.18078     -0.0862797    0.0790654    0.0479213   -0.0129273   -0.0699736     0.204808    -0.0681491    -0.227114    -0.0942733    0.035751     0.25877      0.0108345    0.062459      0.0604007    -0.0270398    0.109732
 -0.0834217   0.0115381   -0.0638841   -0.0104905     0.0851847  -0.00907723   0.0350026    0.0610555   -0.10135      0.112963     0.0825824    0.184876    -0.0187111   -0.0168191   -0.19688       0.271587     0.0335049    -0.110838    -0.167887     0.10389      0.140558    -0.203659     0.0280784    -0.0963937    -0.0281713    0.0474919[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     18
│     21
│     22
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 1, average log likelihood -1.099254
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      9
│     10
│     18
│     21
│     22
│     26
│     29
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 2, average log likelihood -1.088525
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     18
│     21
│     22
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 3, average log likelihood -1.099243
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      9
│     10
│     18
│     21
│     22
│     26
│     29
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.088478
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     18
│     21
│     22
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.099243
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      9
│     10
│     18
│     21
│     22
│     26
│     29
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.088476
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     18
│     21
│     22
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.099243
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      9
│     10
│     18
│     21
│     22
│     26
│     29
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.088475
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     18
│     21
│     22
│     26
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.099242
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      9
│     10
│     18
│     21
│     22
│     26
│     29
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.088475
┌ Info: EM with 100000 data points 10 iterations avll -1.088475
└ 59.0 data points per parameter
kind diag, method kmeans
[ Info: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.120991e+05
      1       7.204003e+05      -1.916988e+05 |       32
      2       6.915491e+05      -2.885121e+04 |       32
      3       6.767520e+05      -1.479710e+04 |       32
      4       6.665291e+05      -1.022290e+04 |       32
      5       6.596584e+05      -6.870727e+03 |       32
      6       6.552835e+05      -4.374884e+03 |       32
      7       6.527042e+05      -2.579294e+03 |       32
      8       6.510123e+05      -1.691943e+03 |       32
      9       6.494995e+05      -1.512768e+03 |       32
     10       6.479206e+05      -1.578929e+03 |       32
     11       6.465124e+05      -1.408172e+03 |       32
     12       6.456801e+05      -8.322959e+02 |       32
     13       6.453062e+05      -3.738957e+02 |       32
     14       6.451673e+05      -1.388788e+02 |       31
     15       6.450959e+05      -7.139871e+01 |       31
     16       6.450415e+05      -5.446890e+01 |       31
     17       6.449905e+05      -5.092558e+01 |       31
     18       6.449465e+05      -4.404200e+01 |       31
     19       6.449148e+05      -3.173195e+01 |       31
     20       6.448892e+05      -2.556752e+01 |       29
     21       6.448680e+05      -2.122509e+01 |       31
     22       6.448511e+05      -1.688529e+01 |       30
     23       6.448386e+05      -1.248904e+01 |       30
     24       6.448277e+05      -1.087770e+01 |       30
     25       6.448193e+05      -8.434932e+00 |       29
     26       6.448111e+05      -8.164924e+00 |       28
     27       6.448039e+05      -7.241888e+00 |       26
     28       6.447992e+05      -4.691139e+00 |       30
     29       6.447955e+05      -3.629361e+00 |       25
     30       6.447927e+05      -2.852387e+00 |       22
     31       6.447900e+05      -2.653407e+00 |       26
     32       6.447875e+05      -2.585294e+00 |       19
     33       6.447855e+05      -1.997168e+00 |       21
     34       6.447838e+05      -1.611938e+00 |       20
     35       6.447828e+05      -1.038188e+00 |       14
     36       6.447816e+05      -1.202988e+00 |       18
     37       6.447802e+05      -1.399790e+00 |       22
     38       6.447783e+05      -1.924607e+00 |       21
     39       6.447761e+05      -2.201636e+00 |       21
     40       6.447733e+05      -2.814174e+00 |       25
     41       6.447691e+05      -4.170573e+00 |       24
     42       6.447653e+05      -3.768583e+00 |       22
     43       6.447621e+05      -3.186920e+00 |       16
     44       6.447599e+05      -2.267530e+00 |       17
     45       6.447587e+05      -1.173778e+00 |       19
     46       6.447576e+05      -1.087871e+00 |       14
     47       6.447569e+05      -6.639273e-01 |       11
     48       6.447567e+05      -2.539997e-01 |        7
     49       6.447565e+05      -2.384848e-01 |        9
     50       6.447560e+05      -4.208433e-01 |        5
K-means terminated without convergence after 50 iterations (objv = 644756.030774516)
┌ Info: K-means with 32000 data points using 50 iterations
└ 37.0 data points per parameter
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.360434
[ Info: iteration 2, average log likelihood -1.335881
[ Info: iteration 3, average log likelihood -1.308988
[ Info: iteration 4, average log likelihood -1.281388
[ Info: iteration 5, average log likelihood -1.248262
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.204673
[ Info: iteration 7, average log likelihood -1.182816
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     10
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.146041
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.130212
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      8
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.121205
[ Info: iteration 11, average log likelihood -1.138736
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     10
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 12, average log likelihood -1.117106
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      3
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 13, average log likelihood -1.108915
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      8
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 14, average log likelihood -1.102637
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 15, average log likelihood -1.105823
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     10
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 16, average log likelihood -1.092400
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      3
│      8
│     19
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 17, average log likelihood -1.080097
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 18, average log likelihood -1.118091
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     10
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 19, average log likelihood -1.106603
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 20, average log likelihood -1.096502
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      3
│     19
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 21, average log likelihood -1.087411
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     10
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 22, average log likelihood -1.104877
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 23, average log likelihood -1.104446
[ Info: iteration 24, average log likelihood -1.095997
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      3
│     10
│     19
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 25, average log likelihood -1.066940
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 26, average log likelihood -1.122585
[ Info: iteration 27, average log likelihood -1.114824
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     10
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 28, average log likelihood -1.088559
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      3
│      8
│     19
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 29, average log likelihood -1.078525
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 30, average log likelihood -1.118645
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     10
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 31, average log likelihood -1.100404
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      8
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 32, average log likelihood -1.087485
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      3
│     19
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 33, average log likelihood -1.094910
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     10
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 34, average log likelihood -1.109559
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      8
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 35, average log likelihood -1.099269
[ Info: iteration 36, average log likelihood -1.111103
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      3
│     10
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 37, average log likelihood -1.080215
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      8
│     19
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 38, average log likelihood -1.085856
[ Info: iteration 39, average log likelihood -1.125481
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     10
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 40, average log likelihood -1.093834
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      3
│      8
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 41, average log likelihood -1.089695
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     19
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 42, average log likelihood -1.111725
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 43, average log likelihood -1.094900
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      8
│     10
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 44, average log likelihood -1.079165
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 45, average log likelihood -1.110123
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     19
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 46, average log likelihood -1.092972
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 47, average log likelihood -1.094800
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     10
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 48, average log likelihood -1.091475
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 49, average log likelihood -1.107430
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 50, average log likelihood -1.091001
┌ Info: EM with 100000 data points 50 iterations avll -1.091001
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.21197     -0.182063    -0.0630902   -0.134476     0.0591496   -0.123088     0.0401587   0.146849      0.0736046   -0.0233816   -0.0150857     0.0392052    0.144945     0.174703    -0.0413062    0.0845155    0.0587392    0.130039     0.001004    -0.251562     -0.00592225  -0.0897003    0.200523    -0.11562     -0.0282473   -0.0440255
 -0.0116599   -0.0692713   -0.0362146    0.0362752   -0.0916933    0.112411     0.0393555   0.0198558     0.0499345    0.0408304    0.0455618    -0.0221924    0.150543     0.135004    -0.0503576   -0.0900148   -0.169777    -0.0415705    0.0512496    0.096723      0.00941973  -0.0454353    0.0064332   -0.0349531   -0.0324782    0.000922799
 -0.0192738    0.0764846   -0.0632967   -0.0505401    0.0965055   -0.0318788    0.0408883   0.0759739    -0.164795    -0.0419221    0.0789817    -0.115974     0.00783304  -0.053513     0.0128907    0.00169935   0.0149343    0.042133    -0.0404896    0.23602      -0.0315144    0.0404278   -0.00421034  -0.0463861    0.127697     0.118623
  0.189716    -0.164436    -0.0329452   -0.12539     -0.0610552    0.0986556    0.150354    0.133622     -0.231742    -0.0147071    0.0575765    -0.0208766   -0.239513     0.0719046    0.0819124   -0.175968     0.0221668    0.0797276    0.126665    -0.0370049     0.129413     0.0952273   -0.197049    -0.166867     0.0399223    0.0173081
 -0.0744571    0.153474     0.0399886   -0.00708349  -0.122092    -0.120931     0.0606864  -0.170559      0.0724806   -0.0493956   -0.0446031    -0.258829    -0.0132434   -0.00458379   0.015878     0.0321909   -0.00198671  -0.0478396   -0.0503392    0.359767      0.18633      0.0168929   -0.0350031    0.0570481   -0.323004    -0.142241
 -0.088767    -0.169125     0.146005    -0.0785213    0.091238    -0.0313373    0.0425416   0.0513969    -0.0110528   -0.00497059  -0.117296      0.00568499   0.149511     0.102624    -0.0659836   -0.0334155   -0.0727749    0.0577602   -0.11897     -0.139367     -0.19616     -0.137848    -0.123733     0.00395952  -0.0144057    0.221285
 -0.115855    -0.168765    -0.0565732    0.175754    -0.0354788    0.137548     0.0157671   0.0256694    -0.067111    -0.103988    -0.0439928    -0.0420859   -0.173786     0.187159    -0.0178533   -0.0279672    0.14783      0.150851     0.138062     0.243543      0.0292952   -0.200356    -0.103256     0.193315    -0.121562    -0.134967
 -0.0251327   -0.154302    -0.0179721    0.0347895   -0.05375     -0.0461204    0.0125803   0.130694     -0.0358681    0.0205573    0.29487      -0.102417     0.0333884   -0.0584924    0.0279405    0.0894693    0.0037066   -0.0898655    0.0539956   -0.107831     -0.028843    -0.0567779    0.15998      0.012566     0.00325846   0.0335063
 -0.0556414    0.196975     0.118083     0.00629506  -0.146643     0.117104     0.171946    0.0614835     0.104626    -0.0371109   -0.240114     -0.42913      0.0316373   -0.0812656    0.043063    -0.225261     0.0200183   -0.264406    -0.10439      0.371264      0.12685     -0.039554    -0.388778    -0.434855     1.15794     -0.151117
  0.263414    -0.0817966    0.103196    -0.172874     0.0571424   -0.055825     0.18478     0.0338132    -0.100723     0.109141     0.0867497    -0.0681201    0.00634428   0.035798     0.102794     0.0154749   -0.169291    -0.0772768    0.138288     0.191315      0.0798168   -0.227342     0.0487155   -0.0143933    0.00560112  -0.0761881
 -0.0322996   -0.194774    -0.0250595   -0.154708    -0.0430745   -0.217653    -0.0485224   0.0974988    -0.134008    -0.197243     0.0107394     0.0348036   -0.0103635   -0.0122584    0.0367018    0.0643555   -0.0449443   -0.0631159    0.0828073    0.217159     -0.0615632    0.0133801    0.0110093   -0.00340949   0.0251828   -0.0463239
  0.269545    -0.00630615  -0.0569626    0.0278067   -0.0210591    0.0400228    0.200372    0.00408541    0.24881     -0.0753683   -0.0160728    -0.0287901   -0.135086     0.235416     0.0242987    0.0350263   -0.059241     0.060947     0.0402239    0.0378609     0.00954663   0.127149     0.0570295    0.0962522    0.00685696   0.02906
 -0.168656    -0.0316123    0.164312    -0.0966252   -0.040935     0.0917658    0.038688   -0.0414945     0.00343368   0.129716     0.000457523  -0.15458      0.252908    -0.0229048   -0.0104231   -0.0278929    0.12865     -0.0170916   -0.134002     0.000489251  -0.112608    -0.0399976   -0.220471     0.639461     0.0453189   -0.0511351
 -0.00202594  -0.0591416    0.195649     0.123621    -0.0330749    0.153098     0.0586562   0.13367      -0.131453    -0.121861     0.0752345     0.0406246    0.100915    -0.0172021   -0.079177    -0.00815326  -0.0122257    0.00462843  -0.0590231   -0.146089      0.0206747   -0.0608131    0.0360742    0.075668    -0.0925114   -0.0645476
  0.0513057   -0.132397     0.00810063  -0.00539275  -0.00924643   0.266554    -0.0149672   0.00883224   -0.0895686   -0.223343    -0.0431623     0.00590423  -0.00549331   0.0721197    0.0195803   -0.00730165   0.0226877   -0.0138828    0.220557     0.0764569     0.15157     -0.104277    -0.0891048    0.0824879   -0.0319441   -0.0316955
  0.112837    -0.0680343    0.186641     0.142386    -0.110022     0.00311616  -0.0183848  -0.25147      -0.167407     0.0513838    0.0258981     0.0602504   -0.0787438   -0.0116373    0.117024     0.0759964   -0.122688     0.0346705   -0.0067422   -0.138697     -0.10041     -0.0444675   -0.174578    -0.115987     0.0257917    0.000540466
 -0.0827915    0.08214     -0.0588194   -0.0658311   -0.0170985    0.0454171   -0.0199515   0.155031     -0.107426     0.074784    -0.014353     -0.070097     0.112438     0.0986305   -0.0837142    0.0371464   -0.0343685   -0.0819207    0.186543     0.0275181    -0.149621     0.0249516   -0.0111369    0.0981751    0.0505214   -0.0349384
 -0.00909786  -0.202343    -0.131001     0.11401      0.146794     0.195314    -0.0606454  -0.0787543     0.015617    -0.149859     0.0561619    -0.153808     0.261078    -0.0239438   -0.00956957  -0.0556426    0.0644794    0.0857499   -0.0411447   -0.00438781   -0.0120279    0.0846871    0.0488718    0.0561668    0.111369    -0.0757236
 -0.0173642   -0.0687104   -0.0662827    0.16655     -0.103803    -0.042121    -0.0206977  -0.037875      0.0117661    0.018773    -0.0217627     0.0954076    0.0841621    0.0254019    0.00300014   0.0468698   -0.0144297   -0.0791095   -0.127268     0.0856216    -0.0725489    0.0490974   -0.061408     0.0809319    0.0164703   -0.0800874
  0.0810748   -0.0871471    0.103041     0.117534    -0.185415     0.0623099    0.125249    0.0563755    -0.0684399    0.13141      0.121704     -0.157371     0.0972088   -0.0496572   -0.0592976   -0.0569792    0.0569422   -0.0273772    0.0759359   -0.0289984    -0.0241123    0.141126    -0.074454    -0.00438993  -0.118104    -0.0895641
 -0.0180568   -0.104138     0.101399    -0.102497    -0.0162685    0.0564497    0.0209517   0.0768524     0.0185849   -0.225346    -0.0509604    -0.166663    -0.0426423   -0.0293174    0.0629044    0.027591     0.0710002    0.0255181   -0.0367365    0.0257073    -0.103285    -0.04848     -0.0547297   -0.35506      0.0888822   -0.0291876
  0.094302    -0.161605     0.0446665    0.139004    -0.0446814   -0.162547    -0.0253969   0.0801894     0.042853    -0.0194262   -0.0568501     0.0550596   -0.169765    -0.178465    -0.0936435   -0.0997181    0.0760039   -0.169625    -0.0378652    0.0261354    -0.0114024    0.16006      0.267611    -0.133785     0.0825454    0.0843436
  0.0470435   -0.0364504    7.87425e-5  -0.012824     0.0471012    0.0422201    0.066416   -0.000510621  -0.00505183  -0.0710609    0.0171614     0.0151431   -0.169585     0.0711411   -0.0617798   -0.0815302   -0.0936285   -0.129156     0.0734886   -0.0776803    -0.195406    -0.0242753   -0.0347886    0.0366042   -0.0519059    0.179709
 -0.045452    -0.0583472    0.107871    -0.101979    -0.0346097   -0.218925    -0.0238889   0.0889496    -0.131615    -0.0683823   -0.00830047   -0.191537     0.0838485   -0.0173353   -0.21372     -0.023627     0.10882      0.195859     0.00888073   0.292189     -0.0385682    0.0867132   -0.226891     0.124302    -0.00847088  -0.10465
 -0.216127     0.051007    -0.0392268   -0.0361036    0.06733      0.103269    -0.106135   -0.0786425     0.0472889    0.0426587    0.0250945     0.211489     0.0298182    0.100214    -0.0491599   -0.142261     0.059307     0.0882683    0.126704    -0.023877      0.0992366   -0.0350353    0.0645086    0.160331    -0.160159     0.115873
 -0.0833202    0.00988389  -0.0620923   -0.00848721   0.0857882   -0.00868577   0.0379104   0.0592664    -0.0995847    0.11437      0.0817431     0.185156    -0.0175466   -0.0169115   -0.197385     0.271219     0.0328638   -0.111096    -0.169441     0.104863      0.140869    -0.202872     0.028937    -0.0956503   -0.0273433    0.0451586
 -0.113956     0.111136    -0.0244626    0.0432666   -0.143628     0.19639      0.322356    0.047402     -0.0842223    0.0971596   -0.0512441     0.0205189    0.147194    -0.0450723   -0.0225868   -0.060848    -0.0185337    0.00394457   0.0422826   -0.0779622     0.0173882   -0.0141514    0.122723    -0.136869    -0.00280324  -0.053151
  0.0728177    0.0921627   -0.0932929    0.034006    -0.0117254    0.111629     0.0457813   0.131292     -0.00107848   0.0507781    0.0534232    -0.00811634   0.166999     0.0664783   -0.046087     0.202788    -0.0268209   -0.110731     0.105442     0.104845      0.00894737   0.0533495    0.0205391    0.208411     0.0503016   -0.2034
  0.0615444    0.115549     0.0293569   -0.00901152   0.0285874    0.0406793    0.019852    0.0727395     0.0248771   -0.0264536    0.0870779     0.0313255   -0.011306     0.0893262    0.0436065   -0.0375485   -0.0208709    0.09748     -0.0823973    0.00109135    0.063027     0.0938653   -0.104995     0.024911    -0.00991729   0.154542
  0.0458817    0.164945     0.105467    -0.0597754   -0.200297     0.0431688   -0.123794    0.0284043     0.0429712    0.10383      0.017429      0.00280487  -0.137539    -0.038764     0.0201791   -0.0587476    0.186089     0.0183958   -0.12526      0.0331765    -0.087465    -0.0322927   -0.0393797    0.0418322   -0.204867     0.0406749
 -0.0547091   -0.11813      0.00347436  -0.00968164   0.0223882    0.0324223   -0.0334121   0.104171     -0.0593189   -0.00393085  -0.241178     -0.119594    -0.121432     0.00204125   0.0123831   -0.02686      0.0672517   -0.0514201   -0.0336339    0.0590789     0.0225979   -0.0319736   -0.0126699    0.113238    -0.00856073  -0.107386
 -0.207616    -0.0835537   -0.198371    -0.13782      0.0777386    0.0727188   -0.0130178   0.0286677    -0.034173     0.167889    -0.0778234     0.0787439    0.0424185   -0.0124183   -0.0653604    0.197108    -0.0636389   -0.224857    -0.0964016    0.0397235     0.246578     0.00918152   0.0706528    0.0620327   -0.0211453    0.111581[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     19
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 1, average log likelihood -1.084789
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     10
│     19
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 2, average log likelihood -1.064486
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      3
│      8
│     10
│     19
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 3, average log likelihood -1.062443
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     19
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.081067
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     10
│     19
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.067774
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      3
│      8
│     10
│     19
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.058713
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     19
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.084663
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     10
│     19
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.064230
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      3
│      8
│     10
│     19
│     31
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.062319
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     19
│     31
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.081064
┌ Info: EM with 100000 data points 10 iterations avll -1.081064
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.066525     -0.0723784    0.166768    0.25935      -0.0274935    0.0333209    0.090879     0.131242      0.203534     0.0543659  -0.0990029   -0.120138     0.0200587   -0.00911038   -0.135632    0.00346053  -0.106183     0.0329327     0.141954     0.0279357    0.0146812    0.0922359   -0.152507    -0.0717876  -0.14655     -0.0203966
  0.0334681     0.00687897   0.0537583  -0.100149     -0.189874     0.0395371    0.00923547  -0.131458      0.0295502   -0.221565   -0.0143441    0.112626    -0.0873134   -0.0299175    -0.110532    0.10138     -0.166962     0.144732     -0.057006     0.140299     0.0355395    0.0103278    0.0689322   -0.102862    0.0381491   -0.0354007
  0.0752885     0.0616878    0.0260963   0.142047     -0.14068     -0.163552     0.0825527   -0.113667      0.0525217    0.10067    -0.268094     0.0364353    0.130892     0.146058     -0.153883    0.0448685   -0.12322      0.000795373  -0.0746707    0.0977671    0.0354025    0.0618455    0.130797    -0.0938218  -0.0681391    0.172229
 -0.0929127    -0.180267     0.0416826   0.00350205    0.200191     0.0925649   -0.176353     0.0235047     0.15616      0.0918507  -0.00511075  -0.0355853   -0.0600881   -0.0813766    -0.0538807  -0.239168    -0.0604918    0.0577151     0.16184      0.0402195    0.105499     0.0613495    0.152739     0.092913   -0.0775182   -0.00123368
  0.0493287     0.0907364    0.0669837  -0.0812573     0.0153086    0.163362    -0.0805736   -0.123308      0.093096     0.166967    0.10487     -0.182438     0.00901692  -0.0782458    -0.137407   -0.042607    -0.0106048   -0.251433     -0.0282769   -0.114222     0.00921732  -0.176123    -0.0769857    0.097529   -0.0272357    0.0438389
  0.0209665    -0.0252966    0.0620142   0.112718      0.00855958   0.0158713    0.0550517   -0.127633      0.151293     0.172335    0.0237116   -0.0375427   -0.200715     0.144481      0.038538   -0.0352943    0.00357434  -0.0743302    -0.138779    -0.0240199   -0.00984127  -0.137761    -0.0725137   -0.0130275  -0.0449552    0.0499284
  0.076609      0.069371    -0.080672    0.197875     -0.0026597   -0.18738     -0.0793448    0.00843463   -0.116164    -0.0110801  -0.122868    -0.0933571   -0.101864    -0.0360397     0.0615345  -0.0594558   -0.113616     0.0351585     0.0232454    0.0636283   -0.064672    -0.0338813    0.0929799    0.0408833   0.0382253   -0.0791646
 -0.0158185    -0.0976804   -0.20018     0.0700478     0.103033     0.11588      0.0917154   -0.0384139     0.0916452   -0.0353303   0.140702     0.0106458   -0.0651621   -0.12156      -0.0682332  -0.0569556   -0.00297512   0.0190066     0.0445611   -0.0656357   -0.1513       0.109778     0.0177232   -0.130572    0.038904     0.0433536
 -0.0107165     0.028621     0.102244    0.0307082     0.187098    -0.0344258   -0.102611     0.0103701     0.0116956   -0.0568107   0.147069    -0.307243     0.00992716   0.226249      0.0551178  -0.0518757   -0.147129    -0.0240676     0.222172    -0.0168505   -0.123054    -0.0149412   -0.0320809   -0.126705    0.116382     0.116206
 -0.104443      0.235285    -0.0644567   0.0286505     0.00273138  -0.0497818   -0.0808406   -0.08396      -0.0674869   -0.08768     0.0182095    0.0390099   -0.0720278    0.0417727    -0.0658711   0.0557293    0.0291614    0.0388992    -0.00995286  -0.0727178    0.0709002    0.206089     0.0600487    0.114264    0.0757127   -0.0312677
 -0.0945036    -0.0870055    0.108802    0.0153509    -0.0101043    0.0340138   -0.0467696    0.0918535     0.0397047   -0.0213071  -0.0648044   -0.113685    -0.0177737   -0.0472358    -0.196993   -0.114647     0.119904     0.100046      0.255335     0.219984    -0.0219575   -0.0195222   -0.0224649   -0.125501    0.200297    -0.0940017
 -0.202604     -0.00841934   0.0671971   0.0619018     0.0792222   -0.00389363   0.109965     0.0584434    -0.115308     0.0135947  -0.108952    -0.119551     0.00793526   0.0275551     0.0302695  -0.093923     0.160676    -0.0233581    -0.10097     -0.128891     0.0636772   -0.111305     0.00674107   0.105412    0.044716    -0.00901856
  0.0132857     0.0751706    0.118247   -0.0799536    -0.131147     0.161037     0.0710227    0.177871     -0.0580183   -0.0995524   0.00980765  -0.00460715  -0.00557325   0.000655071   0.0831515  -0.0214136    0.0200921   -0.0336556    -0.126818    -0.112175    -0.117217     0.0115163   -0.159493     0.141226    0.133541     0.116769
 -0.0578578    -0.144952     0.0263939   0.0378587    -0.0778699   -0.012028     0.0425589   -0.0778862    -0.0274539   -0.0991233  -0.227856     0.0774794   -0.0483725   -0.176441     -0.0260541   0.0732285   -0.159319     0.0353713     0.0200084    0.166029     0.0926499    0.00994076   0.116744     0.0227694   0.234446     0.0286697
  0.0557412     0.107173     0.0405528  -0.114086     -0.0676282   -0.0402059   -0.119816    -0.0679156     0.0203237   -0.0100142  -0.299418     0.104665     0.0825284    0.13665      -0.085044   -0.27897      0.0230216    0.153535     -0.0436217    0.023746     0.241891     0.0478439    0.11387     -0.0925212   0.113996     0.0146106
 -0.227837      0.135871     0.072635    0.119632     -0.112808    -0.0972357   -0.0598011   -0.0200509     0.0594514    0.0433936   0.068556    -0.0363482    0.0702298   -0.0619202     0.0380339   0.0122715    0.00207793  -0.100259      0.0667485    0.0194587   -0.0370841    0.0574562    0.0723529   -0.0709186  -0.0117694   -0.0944808
  0.124779     -0.00576733  -0.0212188   0.0130761    -0.00793284   0.0651158    0.0797997    0.000614344  -0.0877444   -0.100028    0.158825     0.0285076    0.0453106    0.0745919    -0.0242781  -0.169663     0.0858266    0.00726853   -0.230072     0.00891591  -0.178105     0.0591928   -0.136846    -0.182382    0.203931     0.190726
  0.0623928     0.0590551   -0.0611361   0.0462989    -0.119517    -0.0375941   -0.114454    -0.0312024     0.0217233   -0.234253   -0.0132939   -0.0573714   -0.0430877   -0.0723129    -0.200759   -0.02528      0.0226381    0.0381568     0.0507612    0.173738     0.058217     0.0394138   -0.0651906    0.0101606   0.128454    -0.0408383
 -0.178437      0.0700344   -0.13171    -0.28879      -0.0852717    0.0263458    0.0291575    0.0204446     0.0376519    0.0962546  -0.0335367    0.0617637   -0.0301469    0.0563972    -0.0673296  -0.089504    -0.0388275   -0.162083      0.0408955   -0.0196203   -0.0850515    0.166958     0.0174459    0.0917398  -0.171705    -0.122
 -0.174467      0.122045    -0.173497    0.230037      0.11703     -0.216172    -0.207353    -0.0664231    -0.149623     0.153603    0.00560534  -0.0339927    0.161492     0.0614597    -0.175989   -0.0140394   -0.00578679   0.020665     -0.0674164    0.131237    -0.0223913    0.119621    -0.0508795    0.0427407   0.116057    -0.0339968
  0.0945617    -0.0139898   -0.0407704   0.0920809     0.117665    -0.170249    -0.175518     0.217929      0.0326841   -0.023666   -0.0200692    0.118728     0.00596583   0.0330032     0.191373   -0.0370967   -0.123021    -0.0177266    -0.101195    -0.0954504    0.0588025   -0.0135931    0.169162     0.162636   -0.12881     -0.0129712
  0.0608751    -0.00695594  -0.115176   -0.0444778     0.115023     0.0812044    0.00803741   0.194922     -0.0428724   -0.0256051  -0.121725    -0.0850418    0.142728     0.0478835     0.0804021  -0.158682    -0.0379522   -0.235631      0.212014     0.0918178    0.0328936    0.002838     0.0906423   -0.0121681   0.113088    -0.0609815
 -0.102019      0.0208572    0.135804    0.0525923     0.00350618   0.0410181    0.0196321    0.0501983    -0.0279499    0.0129039  -0.28879     -0.0528496   -0.0337697   -0.0194635     0.0594728  -0.00128985   0.0374505    0.16208       0.0902385   -0.100241     0.152448    -0.0200892    0.178013     0.0509418  -0.00963632   0.136652
 -0.106149     -0.144306    -0.0436976  -0.0368632     0.0253596   -0.139735    -0.162745    -0.0408243     0.0026685   -0.0286756  -0.0777497    0.0480019    0.0192121   -0.00585241   -0.0940511   0.199211    -0.160127    -0.153485     -0.036484    -0.00923476   0.003611     0.0216205   -0.0836862    0.105575    0.217693     0.00790566
  0.0359525     0.0966745   -0.0431794   0.0354527     0.0628214    3.52888e-5   0.0826255   -0.00706231    0.0346442   -0.039633    0.0648761   -0.124672    -0.108571     0.00434482   -0.0445409   0.100597    -0.0358215   -0.0383934    -0.0620626   -0.127957    -0.078669     0.00811969   0.068442     0.161959   -0.0834307    0.0354439
  0.000324472   0.0897525    0.0704323   0.125547      0.0435665   -0.00508547  -0.0903287    0.0123141     0.0475919    0.0440293   0.0952976   -0.0624497    0.0381701   -0.0226285    -0.104865    0.0298212   -0.164577     0.0167927     0.011231    -0.11258      0.0900022    0.119681    -0.0782444   -0.187602   -0.0211129   -0.0269857
 -0.0972679     0.0789175    0.162419    0.0567356     0.0555446    0.0320295    0.0222337   -0.125788      0.108435     0.100289    0.0218537    0.0149018   -0.157374     0.0219575    -0.179787    0.114785     0.042022    -0.0288927     0.03142      0.169388    -0.0947517    0.0239153   -0.167651    -0.0607681  -0.0604735    0.117227
  0.117003     -0.01021      0.0150243  -0.0110561     0.00611833  -0.0422726   -0.00607525   0.0966807     0.219313     0.0538334   0.0662876    0.0202614   -0.157199     0.0720581    -0.143571   -0.140968     0.198608    -0.029848      0.165024    -0.122663     0.0513659   -0.205346     0.0330922    0.0103148   0.0739303   -0.266229
 -0.0821288     0.0753316   -0.121473   -0.097622     -0.0750206   -0.122635     0.105957     0.197191     -0.0947804   -0.0632846  -0.0451055    0.104198    -0.0498293    0.0768138     0.187461   -0.0857407    0.0515282    0.153819     -0.131107    -0.123111    -0.0559145    0.108822     0.0136535   -0.0405348   0.0371931   -0.0496463
 -0.00849406    0.0995509    0.171009   -0.00442402    0.0708197   -0.0603212    0.0527392    0.0374075    -0.0347598   -0.0650678   0.0778806   -0.176102     0.116491    -0.0991737    -0.0648518   0.0298887    0.202707     0.12274       0.170553    -0.100438     0.0382792   -0.0673016    0.0457964    0.0465888  -0.0208544   -0.0630006
 -0.0023698    -0.142315    -0.0636677   0.000288311   0.072674    -0.169116     0.0820011    0.190999      0.147709    -0.084825   -0.0705848    0.0718478   -0.0404356   -0.0704301    -0.145027    0.143307     0.00685586  -0.130953      0.174523     0.132313     0.158048     0.052771     0.0148013    0.0432395   0.0535899   -0.0376862
 -0.0795104     0.0952222    0.0733174   0.0215719    -0.236467     0.103808    -0.247505    -0.0408243     0.00848479  -0.135885    0.0932955   -0.0572375   -0.0963101   -0.0275798    -0.0167578   0.0709954    0.0760818   -0.0662005    -0.0443338   -0.0344646   -0.0863807   -0.223121     0.00653482  -0.126795   -0.0401366    0.0308311kind full, method split
┌ Info: 0: avll = 
└   tll[1] = -1.4233910911634289
[ Info: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.423410
[ Info: iteration 2, average log likelihood -1.423350
[ Info: iteration 3, average log likelihood -1.423310
[ Info: iteration 4, average log likelihood -1.423265
[ Info: iteration 5, average log likelihood -1.423211
[ Info: iteration 6, average log likelihood -1.423140
[ Info: iteration 7, average log likelihood -1.423036
[ Info: iteration 8, average log likelihood -1.422849
[ Info: iteration 9, average log likelihood -1.422463
[ Info: iteration 10, average log likelihood -1.421700
[ Info: iteration 11, average log likelihood -1.420506
[ Info: iteration 12, average log likelihood -1.419255
[ Info: iteration 13, average log likelihood -1.418432
[ Info: iteration 14, average log likelihood -1.418060
[ Info: iteration 15, average log likelihood -1.417921
[ Info: iteration 16, average log likelihood -1.417870
[ Info: iteration 17, average log likelihood -1.417851
[ Info: iteration 18, average log likelihood -1.417844
[ Info: iteration 19, average log likelihood -1.417840
[ Info: iteration 20, average log likelihood -1.417839
[ Info: iteration 21, average log likelihood -1.417838
[ Info: iteration 22, average log likelihood -1.417837
[ Info: iteration 23, average log likelihood -1.417837
[ Info: iteration 24, average log likelihood -1.417836
[ Info: iteration 25, average log likelihood -1.417836
[ Info: iteration 26, average log likelihood -1.417836
[ Info: iteration 27, average log likelihood -1.417835
[ Info: iteration 28, average log likelihood -1.417835
[ Info: iteration 29, average log likelihood -1.417835
[ Info: iteration 30, average log likelihood -1.417834
[ Info: iteration 31, average log likelihood -1.417834
[ Info: iteration 32, average log likelihood -1.417834
[ Info: iteration 33, average log likelihood -1.417834
[ Info: iteration 34, average log likelihood -1.417834
[ Info: iteration 35, average log likelihood -1.417834
[ Info: iteration 36, average log likelihood -1.417834
[ Info: iteration 37, average log likelihood -1.417833
[ Info: iteration 38, average log likelihood -1.417833
[ Info: iteration 39, average log likelihood -1.417833
[ Info: iteration 40, average log likelihood -1.417833
[ Info: iteration 41, average log likelihood -1.417833
[ Info: iteration 42, average log likelihood -1.417833
[ Info: iteration 43, average log likelihood -1.417833
[ Info: iteration 44, average log likelihood -1.417833
[ Info: iteration 45, average log likelihood -1.417833
[ Info: iteration 46, average log likelihood -1.417833
[ Info: iteration 47, average log likelihood -1.417833
[ Info: iteration 48, average log likelihood -1.417833
[ Info: iteration 49, average log likelihood -1.417833
[ Info: iteration 50, average log likelihood -1.417833
┌ Info: EM with 100000 data points 50 iterations avll -1.417833
└ 952.4 data points per parameter
┌ Info: 1
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4234101223961166
│     -1.4233504712409077
│      ⋮
└     -1.4178328314216009
[ Info: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.417852
[ Info: iteration 2, average log likelihood -1.417790
[ Info: iteration 3, average log likelihood -1.417747
[ Info: iteration 4, average log likelihood -1.417700
[ Info: iteration 5, average log likelihood -1.417645
[ Info: iteration 6, average log likelihood -1.417583
[ Info: iteration 7, average log likelihood -1.417518
[ Info: iteration 8, average log likelihood -1.417453
[ Info: iteration 9, average log likelihood -1.417395
[ Info: iteration 10, average log likelihood -1.417346
[ Info: iteration 11, average log likelihood -1.417306
[ Info: iteration 12, average log likelihood -1.417275
[ Info: iteration 13, average log likelihood -1.417249
[ Info: iteration 14, average log likelihood -1.417229
[ Info: iteration 15, average log likelihood -1.417211
[ Info: iteration 16, average log likelihood -1.417196
[ Info: iteration 17, average log likelihood -1.417182
[ Info: iteration 18, average log likelihood -1.417169
[ Info: iteration 19, average log likelihood -1.417157
[ Info: iteration 20, average log likelihood -1.417146
[ Info: iteration 21, average log likelihood -1.417135
[ Info: iteration 22, average log likelihood -1.417124
[ Info: iteration 23, average log likelihood -1.417114
[ Info: iteration 24, average log likelihood -1.417104
[ Info: iteration 25, average log likelihood -1.417093
[ Info: iteration 26, average log likelihood -1.417083
[ Info: iteration 27, average log likelihood -1.417074
[ Info: iteration 28, average log likelihood -1.417064
[ Info: iteration 29, average log likelihood -1.417055
[ Info: iteration 30, average log likelihood -1.417045
[ Info: iteration 31, average log likelihood -1.417036
[ Info: iteration 32, average log likelihood -1.417028
[ Info: iteration 33, average log likelihood -1.417020
[ Info: iteration 34, average log likelihood -1.417012
[ Info: iteration 35, average log likelihood -1.417004
[ Info: iteration 36, average log likelihood -1.416997
[ Info: iteration 37, average log likelihood -1.416990
[ Info: iteration 38, average log likelihood -1.416984
[ Info: iteration 39, average log likelihood -1.416978
[ Info: iteration 40, average log likelihood -1.416972
[ Info: iteration 41, average log likelihood -1.416966
[ Info: iteration 42, average log likelihood -1.416961
[ Info: iteration 43, average log likelihood -1.416956
[ Info: iteration 44, average log likelihood -1.416951
[ Info: iteration 45, average log likelihood -1.416946
[ Info: iteration 46, average log likelihood -1.416941
[ Info: iteration 47, average log likelihood -1.416937
[ Info: iteration 48, average log likelihood -1.416932
[ Info: iteration 49, average log likelihood -1.416927
[ Info: iteration 50, average log likelihood -1.416923
┌ Info: EM with 100000 data points 50 iterations avll -1.416923
└ 473.9 data points per parameter
┌ Info: 2
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4178516407919515
│     -1.4177895590379719
│      ⋮
└     -1.4169226236437993
[ Info: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.416928
[ Info: iteration 2, average log likelihood -1.416880
[ Info: iteration 3, average log likelihood -1.416837
[ Info: iteration 4, average log likelihood -1.416789
[ Info: iteration 5, average log likelihood -1.416732
[ Info: iteration 6, average log likelihood -1.416664
[ Info: iteration 7, average log likelihood -1.416585
[ Info: iteration 8, average log likelihood -1.416497
[ Info: iteration 9, average log likelihood -1.416404
[ Info: iteration 10, average log likelihood -1.416308
[ Info: iteration 11, average log likelihood -1.416213
[ Info: iteration 12, average log likelihood -1.416123
[ Info: iteration 13, average log likelihood -1.416038
[ Info: iteration 14, average log likelihood -1.415961
[ Info: iteration 15, average log likelihood -1.415893
[ Info: iteration 16, average log likelihood -1.415834
[ Info: iteration 17, average log likelihood -1.415783
[ Info: iteration 18, average log likelihood -1.415740
[ Info: iteration 19, average log likelihood -1.415703
[ Info: iteration 20, average log likelihood -1.415671
[ Info: iteration 21, average log likelihood -1.415643
[ Info: iteration 22, average log likelihood -1.415619
[ Info: iteration 23, average log likelihood -1.415598
[ Info: iteration 24, average log likelihood -1.415579
[ Info: iteration 25, average log likelihood -1.415562
[ Info: iteration 26, average log likelihood -1.415546
[ Info: iteration 27, average log likelihood -1.415532
[ Info: iteration 28, average log likelihood -1.415520
[ Info: iteration 29, average log likelihood -1.415508
[ Info: iteration 30, average log likelihood -1.415497
[ Info: iteration 31, average log likelihood -1.415486
[ Info: iteration 32, average log likelihood -1.415477
[ Info: iteration 33, average log likelihood -1.415468
[ Info: iteration 34, average log likelihood -1.415460
[ Info: iteration 35, average log likelihood -1.415452
[ Info: iteration 36, average log likelihood -1.415444
[ Info: iteration 37, average log likelihood -1.415437
[ Info: iteration 38, average log likelihood -1.415430
[ Info: iteration 39, average log likelihood -1.415424
[ Info: iteration 40, average log likelihood -1.415418
[ Info: iteration 41, average log likelihood -1.415412
[ Info: iteration 42, average log likelihood -1.415406
[ Info: iteration 43, average log likelihood -1.415401
[ Info: iteration 44, average log likelihood -1.415396
[ Info: iteration 45, average log likelihood -1.415391
[ Info: iteration 46, average log likelihood -1.415386
[ Info: iteration 47, average log likelihood -1.415382
[ Info: iteration 48, average log likelihood -1.415377
[ Info: iteration 49, average log likelihood -1.415373
[ Info: iteration 50, average log likelihood -1.415369
┌ Info: EM with 100000 data points 50 iterations avll -1.415369
└ 236.4 data points per parameter
┌ Info: 3
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4169280514713027
│     -1.4168796351469906
│      ⋮
└     -1.4153689063834027
[ Info: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.415373
[ Info: iteration 2, average log likelihood -1.415328
[ Info: iteration 3, average log likelihood -1.415287
[ Info: iteration 4, average log likelihood -1.415238
[ Info: iteration 5, average log likelihood -1.415178
[ Info: iteration 6, average log likelihood -1.415103
[ Info: iteration 7, average log likelihood -1.415013
[ Info: iteration 8, average log likelihood -1.414911
[ Info: iteration 9, average log likelihood -1.414804
[ Info: iteration 10, average log likelihood -1.414697
[ Info: iteration 11, average log likelihood -1.414594
[ Info: iteration 12, average log likelihood -1.414498
[ Info: iteration 13, average log likelihood -1.414410
[ Info: iteration 14, average log likelihood -1.414329
[ Info: iteration 15, average log likelihood -1.414256
[ Info: iteration 16, average log likelihood -1.414191
[ Info: iteration 17, average log likelihood -1.414133
[ Info: iteration 18, average log likelihood -1.414081
[ Info: iteration 19, average log likelihood -1.414036
[ Info: iteration 20, average log likelihood -1.413995
[ Info: iteration 21, average log likelihood -1.413959
[ Info: iteration 22, average log likelihood -1.413927
[ Info: iteration 23, average log likelihood -1.413897
[ Info: iteration 24, average log likelihood -1.413870
[ Info: iteration 25, average log likelihood -1.413845
[ Info: iteration 26, average log likelihood -1.413822
[ Info: iteration 27, average log likelihood -1.413800
[ Info: iteration 28, average log likelihood -1.413780
[ Info: iteration 29, average log likelihood -1.413760
[ Info: iteration 30, average log likelihood -1.413742
[ Info: iteration 31, average log likelihood -1.413725
[ Info: iteration 32, average log likelihood -1.413709
[ Info: iteration 33, average log likelihood -1.413693
[ Info: iteration 34, average log likelihood -1.413678
[ Info: iteration 35, average log likelihood -1.413665
[ Info: iteration 36, average log likelihood -1.413651
[ Info: iteration 37, average log likelihood -1.413639
[ Info: iteration 38, average log likelihood -1.413627
[ Info: iteration 39, average log likelihood -1.413615
[ Info: iteration 40, average log likelihood -1.413604
[ Info: iteration 41, average log likelihood -1.413594
[ Info: iteration 42, average log likelihood -1.413584
[ Info: iteration 43, average log likelihood -1.413574
[ Info: iteration 44, average log likelihood -1.413565
[ Info: iteration 45, average log likelihood -1.413556
[ Info: iteration 46, average log likelihood -1.413548
[ Info: iteration 47, average log likelihood -1.413540
[ Info: iteration 48, average log likelihood -1.413532
[ Info: iteration 49, average log likelihood -1.413524
[ Info: iteration 50, average log likelihood -1.413517
┌ Info: EM with 100000 data points 50 iterations avll -1.413517
└ 118.1 data points per parameter
┌ Info: 4
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4153729804337807
│     -1.4153280453389518
│      ⋮
└     -1.4135165642471836
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.413518
[ Info: iteration 2, average log likelihood -1.413464
[ Info: iteration 3, average log likelihood -1.413416
[ Info: iteration 4, average log likelihood -1.413362
[ Info: iteration 5, average log likelihood -1.413297
[ Info: iteration 6, average log likelihood -1.413220
[ Info: iteration 7, average log likelihood -1.413126
[ Info: iteration 8, average log likelihood -1.413016
[ Info: iteration 9, average log likelihood -1.412893
[ Info: iteration 10, average log likelihood -1.412761
[ Info: iteration 11, average log likelihood -1.412626
[ Info: iteration 12, average log likelihood -1.412492
[ Info: iteration 13, average log likelihood -1.412366
[ Info: iteration 14, average log likelihood -1.412249
[ Info: iteration 15, average log likelihood -1.412144
[ Info: iteration 16, average log likelihood -1.412050
[ Info: iteration 17, average log likelihood -1.411968
[ Info: iteration 18, average log likelihood -1.411896
[ Info: iteration 19, average log likelihood -1.411832
[ Info: iteration 20, average log likelihood -1.411776
[ Info: iteration 21, average log likelihood -1.411727
[ Info: iteration 22, average log likelihood -1.411683
[ Info: iteration 23, average log likelihood -1.411643
[ Info: iteration 24, average log likelihood -1.411607
[ Info: iteration 25, average log likelihood -1.411575
[ Info: iteration 26, average log likelihood -1.411545
[ Info: iteration 27, average log likelihood -1.411517
[ Info: iteration 28, average log likelihood -1.411491
[ Info: iteration 29, average log likelihood -1.411467
[ Info: iteration 30, average log likelihood -1.411444
[ Info: iteration 31, average log likelihood -1.411423
[ Info: iteration 32, average log likelihood -1.411403
[ Info: iteration 33, average log likelihood -1.411383
[ Info: iteration 34, average log likelihood -1.411364
[ Info: iteration 35, average log likelihood -1.411346
[ Info: iteration 36, average log likelihood -1.411329
[ Info: iteration 37, average log likelihood -1.411312
[ Info: iteration 38, average log likelihood -1.411296
[ Info: iteration 39, average log likelihood -1.411280
[ Info: iteration 40, average log likelihood -1.411264
[ Info: iteration 41, average log likelihood -1.411249
[ Info: iteration 42, average log likelihood -1.411234
[ Info: iteration 43, average log likelihood -1.411219
[ Info: iteration 44, average log likelihood -1.411204
[ Info: iteration 45, average log likelihood -1.411189
[ Info: iteration 46, average log likelihood -1.411174
[ Info: iteration 47, average log likelihood -1.411160
[ Info: iteration 48, average log likelihood -1.411145
[ Info: iteration 49, average log likelihood -1.411131
[ Info: iteration 50, average log likelihood -1.411116
┌ Info: EM with 100000 data points 50 iterations avll -1.411116
└ 59.0 data points per parameter
┌ Info: 5
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4135183789193713
│     -1.4134641281742255
│      ⋮
└     -1.4111164905859903
┌ Info: Total log likelihood: 
│   tll =
│    251-element Array{Float64,1}:
│     -1.4233910911634289
│     -1.4234101223961166
│     -1.4233504712409077
│     -1.4233099355209082
│      ⋮
│     -1.4111454058696113
│     -1.411130934664799
└     -1.4111164905859903
32×26 Array{Float64,2}:
 -0.664659    -0.135009    -0.178921    -0.106811    0.0043425   0.0499618   -0.0146653   0.172479   -0.0159042   -0.370157   -0.213162   -0.394938    -0.100776    0.243843    -0.51129      0.514906    -0.346324    -0.94838      0.00592794  -0.34391     -0.0804197   0.187451     0.406963    0.063964     -0.113106     0.19148
 -0.388349    -0.321229     0.24906     -0.0991987  -0.27401    -0.116153    -0.241752   -0.47103     0.325122     0.785073   -0.371229   -0.359573     0.291514    0.637234    -0.223031     0.162581     0.126828    -0.390536    -0.151104    -0.523395    -0.151895    0.268558     0.503558    0.394497      0.556858     0.353816
  0.532731     0.0379374    0.233842     0.0293883  -0.268774   -0.336093     0.134438   -0.564406   -0.160306    -0.561569    0.324614   -0.399865     0.313295   -0.169379    -0.61956      0.101399    -0.416728    -0.32433     -0.447351     0.0184497   -0.328504   -0.435108     0.0852729  -0.0300391     0.158757     0.104324
  0.192373    -0.355977     0.345719    -0.0174715   0.0677742  -0.387842     0.217662   -0.183436    0.00928436  -0.321063   -0.305335   -0.184897    -0.37357     1.00141     -0.793383     0.00318592  -0.1543       0.0589041   -0.185331    -0.0683797   -0.0516505   0.0770955   -0.157916   -0.12761      -0.0507799    0.444056
 -0.0112379   -0.252598     0.0167258   -0.0818144  -0.0261165  -0.0311148    0.113391   -0.193164   -0.108045    -0.155506   -0.110387    0.00213291   0.0938344   0.0565242   -0.00609308  -0.0216038    0.0394596    0.0306301   -0.0393529   -0.0813576   -0.0955113   0.0151182    0.0719176   0.140475      0.00770423   0.0614867
 -0.13847      0.340106     0.126753     0.123934   -0.0702488   0.0198225   -0.217297    0.128931    0.185568     0.204346    0.102169   -0.0365708   -0.0969421  -0.00930308  -0.184992     0.0682176    0.0420218   -0.0773912   -0.00486827   0.0628115    0.176068   -0.038096     0.0101775  -0.191739      0.0332416   -0.0172745
 -0.260139     0.329516    -0.0417503   -0.124045    0.412631   -0.00710513   0.11288     0.0131213   0.232267     0.110109    0.0687569   0.258542    -0.53513     0.355024    -0.153999    -0.28372      0.351323     0.440433    -0.1257      -0.0133298    0.285269    0.583838     0.204677    0.359433     -0.482257     0.438767
  0.0468017   -0.180325    -0.0328371    0.122911    0.169223   -0.118384    -0.303969   -0.196785    0.110706     0.214139    0.435403   -0.227655     0.481819    0.0525586    0.380534    -0.324973     0.301094     0.128321    -0.0255326   -0.308844    -0.0499508   0.365654     0.491094   -0.243466      0.0632788    0.28382
 -0.266639     0.15687      0.0295532   -0.373816    0.242352   -0.273026     0.186865    0.113269   -0.152691     0.0655392  -0.404642    0.0310353    0.73554    -0.0073477    0.333058    -0.0854889   -0.00253037  -0.491866     0.0602169    0.0780251   -0.801412   -0.159975    -0.229472    0.40916       0.364266    -0.0244019
  0.0887484   -0.075243     0.106712    -0.222449    0.0546267  -0.258436     0.304099    0.325396   -0.170339    -0.0536273   0.226216   -0.0607958   -0.0778615   0.250451     0.566983     0.0183318    0.0412662    0.325106     0.0333414   -0.678264    -0.452934   -0.589614    -0.460115    0.647532      0.660528    -0.220167
  0.347947     0.272982    -0.274959    -0.203116   -0.409826    0.536567    -0.0881748   0.189925   -0.496851     0.122163    0.187148    0.148817     0.138483   -0.628736     0.40005     -0.291272    -0.637907    -0.361279     0.281547     0.184988    -0.0920305  -0.322352     0.0527657  -0.208837      0.503661    -0.0729006
  0.225646     0.317144     0.25849      0.741741    0.0854349   0.454863     0.405333    0.14833     0.558868    -0.0960275   0.207218   -0.173911    -0.0526271   0.0774009   -0.017187    -0.0345608   -0.26057      0.183148     0.773803    -0.100703    -0.146332   -0.265401    -0.0811629  -0.155394      0.350167     0.153722
 -0.259047    -0.285438    -0.182355     0.181459   -0.375342   -0.0310668   -0.116486    0.439952   -0.441787    -0.246359   -0.323318   -0.714488    -0.177124    0.00749872   0.0590131   -0.0584148   -0.0840284    0.220635    -0.149817     0.31831      0.288139    0.293962    -0.240862    0.405814     -0.0744471   -0.0739567
 -0.196528    -0.251742     0.284398    -0.186258   -0.407196   -0.243752    -0.658953   -0.0891329  -0.378829     0.263291   -0.368593   -0.0193873   -0.131831   -0.157409    -0.00879699  -0.452838     0.34005     -0.130922     0.140251     0.426744     0.184668    0.768293    -0.193297   -0.354062     -0.239261    -0.520715
  0.236759    -0.0976336   -0.391446    -0.364749    0.294368   -0.431339     0.0786804   0.292632    0.0290813   -0.329391   -0.543672    0.467315    -0.358684   -0.19505     -0.0393442   -0.0417532   -0.134726     0.144213    -0.0300028    0.129459     0.119872    0.0455493   -0.205867   -0.115903      0.0753725   -0.724838
  0.287358     0.999363    -0.276496    -0.308673    0.175426    0.0396362    0.30901     0.339284    0.137964    -0.0595024  -0.0735012   0.416258    -0.15712    -0.0345139    0.0199562    0.178444    -0.162815     0.353703     0.13279      0.474408     0.56948    -0.755063     0.101219   -0.0220154    -0.198462    -0.0271222
  0.0244753    0.0767536   -0.255575    -0.658728    0.0698328  -0.639247     0.0695103   0.222942    0.0285177   -0.0799554  -0.419668   -0.149265    -0.439384    0.361074    -0.391092    -0.397432     0.0237351    0.442619    -0.111473     0.277535     0.446598    0.293162    -0.0921919   0.306326     -0.450998    -0.158258
 -0.337622    -0.101299     0.00954518   0.287015    0.356691   -0.15929     -0.0738666  -0.194264    0.16711     -0.0974176   0.111207   -0.239212    -0.0821836   0.305209    -0.609247     0.177853     0.145106     0.00797717  -0.0429646   -0.201566     0.127885    0.59405      0.125869    0.0092435    -0.249416     0.331351
  0.205467     0.708512    -0.051462    -0.32729     0.0762623  -0.388532     0.255955    0.559272   -0.510867     0.0428217   0.663423   -0.295856    -0.207045    0.156249    -0.305048     0.236184     0.180532     0.0428545   -0.614802    -0.57073      0.354304    0.0567829    0.164739   -0.214183      0.530836    -0.089008
  0.0970034    0.239425    -0.216521     0.184687   -0.0934367  -0.217953    -0.462928    0.331503    0.277616    -0.0163007   0.199485   -0.0194116   -0.0301589  -0.0574004   -0.0127664    0.160718     0.596739     0.417929    -0.286933    -0.00785306   0.721243   -0.185354    -0.492581   -0.450991      0.059489    -0.730433
  0.134932    -0.674846    -0.168968     0.51619     0.154684    0.429033    -0.0793139  -0.540083    0.280649    -0.150782   -0.804456    0.329497    -0.0787727  -0.452518    -0.00385261  -0.0861613   -0.703373    -0.0258016    0.439891     0.60844     -0.152042   -0.104208     0.122123    0.0633473    -0.121392     0.191471
 -0.00192822   0.235875     0.764328     0.328842   -0.688296    0.293928     0.036686   -0.141501    0.187606     0.0607454  -0.328375    0.212792    -0.564635    0.0502222   -0.290136     0.0956532    0.47129     -0.0971977    0.175343     0.84094      0.0859114  -0.0815092    0.132637   -0.0966451     0.0771263    0.0510415
  0.364226     0.138446     0.447578    -0.31593    -0.41239     0.113219    -0.476208   -0.222919    0.473476     0.553181   -0.136635   -0.182183     0.168393   -0.356497    -0.711755    -0.327427    -0.0287467   -0.220556    -0.424841     0.871837     0.473101    0.190634     0.36934    -0.414155     -0.222321     0.665497
 -0.110318     0.0462038    0.426044    -0.0632727   0.175267    0.181935    -0.26318    -0.0360738   0.145559    -0.443769   -0.108092   -0.511158     0.647527    0.251285    -0.299635     0.106322     0.00505508   0.19532      0.61941      0.707904     0.126572    1.02914      0.461022   -0.186139     -0.369006     0.19848
  0.203464    -0.17436      0.175534    -0.355178   -0.380604    0.0529113    0.0809984   0.232506   -0.884785    -0.443631   -0.289264   -0.118568     0.329206   -0.580827     0.310656     0.0300294   -0.276285    -0.126395     0.0996723    0.397574    -0.48289    -0.146208    -0.0725152   0.000701018   0.389523    -0.425383
 -0.084212    -0.0156596   -0.160766     0.273459   -0.0236318   0.382028     0.0899837  -0.179385    0.143089     0.0717503   0.321753    0.259989     0.251287   -0.176409     0.815125    -0.217201    -0.146102    -0.0227559    0.46662     -0.112099    -0.413259   -0.378028     0.0522016   0.354786      0.291624    -0.0842709
 -0.216593    -0.283273    -0.552204     0.302381    0.570327    0.0445471    0.0979285  -0.156004   -0.674368    -0.592451    0.163505    0.205805     0.084106   -0.175772     0.371666    -0.373143    -0.292435     0.466779    -0.00451591  -0.453542    -0.153714    0.054398    -0.170078   -0.0547768    -0.493705    -0.0442198
  0.392101     0.0943233   -0.30798      0.138256    0.222667    0.220345    -0.481861   -0.0746464  -0.296661     0.751224    0.275854    0.387376     0.0110216  -0.263035     0.49306     -0.877812    -0.270203     0.487865     0.0751187    0.0670298   -0.135391    0.493334     0.0224003   0.0872766     0.0249096   -0.0337335
  0.182269     0.219315    -0.116984     0.342564   -0.785501   -0.0665303    0.356963    0.0638654   0.346107     0.0853193   0.0111764  -0.138382    -0.400967   -0.0806942   -0.224236     0.0956875   -0.154908    -0.683062    -0.2527      -0.226412     0.263683   -1.01186     -0.0680895  -0.356261      0.296259    -0.0962317
  0.344517     0.104563    -0.0643857    0.063107    0.274339   -0.216362    -0.452884   -0.0201018   0.473871     0.395938    0.378591    0.357832     0.31876     0.0560189   -0.237805     0.3041      -0.488007    -0.388601    -0.165065    -0.716642    -0.222421   -0.783127     0.112014   -0.705595     -0.435108     0.13756
 -0.942414     0.00192061  -0.434242     0.255098    0.348957    0.733124     0.213352    0.722821   -0.0359228    0.352304   -0.226708    0.148426    -0.254949   -0.471242     0.499215     0.340678    -0.0126095    0.0277806    0.284108    -0.424193     0.0999513   0.00092128  -0.572908    0.0681781     0.119935    -0.0261082
 -0.539311    -0.0540688    0.296201     0.127414    0.235469    0.204584     0.182455   -0.0954973   0.746749    -0.245131    0.316219    0.273412     0.0500372  -0.175704     0.0363854    0.507964     0.708692    -0.071966     0.334497    -0.20294     -0.302188   -0.109643    -0.151642    0.148348      0.0414425   -0.370899[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.411102
[ Info: iteration 2, average log likelihood -1.411088
[ Info: iteration 3, average log likelihood -1.411073
[ Info: iteration 4, average log likelihood -1.411059
[ Info: iteration 5, average log likelihood -1.411045
[ Info: iteration 6, average log likelihood -1.411031
[ Info: iteration 7, average log likelihood -1.411017
[ Info: iteration 8, average log likelihood -1.411004
[ Info: iteration 9, average log likelihood -1.410990
[ Info: iteration 10, average log likelihood -1.410977
┌ Info: EM with 100000 data points 10 iterations avll -1.410977
└ 59.0 data points per parameter
kind full, method kmeans
[ Info: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.549230e+05
      1       7.107259e+05      -2.441971e+05 |       32
      2       6.951718e+05      -1.555407e+04 |       32
      3       6.893330e+05      -5.838870e+03 |       32
      4       6.864276e+05      -2.905329e+03 |       32
      5       6.846372e+05      -1.790390e+03 |       32
      6       6.834087e+05      -1.228590e+03 |       32
      7       6.824718e+05      -9.368126e+02 |       32
      8       6.817090e+05      -7.628624e+02 |       32
      9       6.810784e+05      -6.305677e+02 |       32
     10       6.805360e+05      -5.423784e+02 |       32
     11       6.800830e+05      -4.530251e+02 |       32
     12       6.797063e+05      -3.767393e+02 |       32
     13       6.794008e+05      -3.055080e+02 |       32
     14       6.791270e+05      -2.737777e+02 |       32
     15       6.788659e+05      -2.610359e+02 |       32
     16       6.786006e+05      -2.653053e+02 |       32
     17       6.783633e+05      -2.373763e+02 |       32
     18       6.781544e+05      -2.088710e+02 |       32
     19       6.779596e+05      -1.947657e+02 |       32
     20       6.778062e+05      -1.534504e+02 |       32
     21       6.776723e+05      -1.338659e+02 |       32
     22       6.775526e+05      -1.196629e+02 |       32
     23       6.774440e+05      -1.086836e+02 |       32
     24       6.773438e+05      -1.001299e+02 |       32
     25       6.772528e+05      -9.107361e+01 |       32
     26       6.771632e+05      -8.955542e+01 |       32
     27       6.770911e+05      -7.208488e+01 |       32
     28       6.770269e+05      -6.420256e+01 |       32
     29       6.769633e+05      -6.364844e+01 |       32
     30       6.769053e+05      -5.801920e+01 |       32
     31       6.768425e+05      -6.279939e+01 |       32
     32       6.767794e+05      -6.304885e+01 |       32
     33       6.767156e+05      -6.379442e+01 |       32
     34       6.766578e+05      -5.784917e+01 |       32
     35       6.766048e+05      -5.294237e+01 |       32
     36       6.765504e+05      -5.439071e+01 |       32
     37       6.764999e+05      -5.055933e+01 |       32
     38       6.764518e+05      -4.806586e+01 |       32
     39       6.764117e+05      -4.011113e+01 |       32
     40       6.763771e+05      -3.463522e+01 |       32
     41       6.763395e+05      -3.754022e+01 |       32
     42       6.763054e+05      -3.411474e+01 |       32
     43       6.762776e+05      -2.781976e+01 |       32
     44       6.762491e+05      -2.846455e+01 |       32
     45       6.762224e+05      -2.674606e+01 |       32
     46       6.761954e+05      -2.701968e+01 |       32
     47       6.761688e+05      -2.652762e+01 |       32
     48       6.761381e+05      -3.072267e+01 |       32
     49       6.761027e+05      -3.541469e+01 |       32
     50       6.760605e+05      -4.213604e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 676060.5490771268)
┌ Info: K-means with 32000 data points using 50 iterations
└ 37.0 data points per parameter
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.423088
[ Info: iteration 2, average log likelihood -1.417933
[ Info: iteration 3, average log likelihood -1.416458
[ Info: iteration 4, average log likelihood -1.415303
[ Info: iteration 5, average log likelihood -1.414133
[ Info: iteration 6, average log likelihood -1.413150
[ Info: iteration 7, average log likelihood -1.412526
[ Info: iteration 8, average log likelihood -1.412184
[ Info: iteration 9, average log likelihood -1.411990
[ Info: iteration 10, average log likelihood -1.411864
[ Info: iteration 11, average log likelihood -1.411773
[ Info: iteration 12, average log likelihood -1.411701
[ Info: iteration 13, average log likelihood -1.411642
[ Info: iteration 14, average log likelihood -1.411591
[ Info: iteration 15, average log likelihood -1.411546
[ Info: iteration 16, average log likelihood -1.411506
[ Info: iteration 17, average log likelihood -1.411470
[ Info: iteration 18, average log likelihood -1.411437
[ Info: iteration 19, average log likelihood -1.411407
[ Info: iteration 20, average log likelihood -1.411378
[ Info: iteration 21, average log likelihood -1.411352
[ Info: iteration 22, average log likelihood -1.411326
[ Info: iteration 23, average log likelihood -1.411303
[ Info: iteration 24, average log likelihood -1.411280
[ Info: iteration 25, average log likelihood -1.411258
[ Info: iteration 26, average log likelihood -1.411238
[ Info: iteration 27, average log likelihood -1.411218
[ Info: iteration 28, average log likelihood -1.411198
[ Info: iteration 29, average log likelihood -1.411179
[ Info: iteration 30, average log likelihood -1.411161
[ Info: iteration 31, average log likelihood -1.411143
[ Info: iteration 32, average log likelihood -1.411125
[ Info: iteration 33, average log likelihood -1.411108
[ Info: iteration 34, average log likelihood -1.411091
[ Info: iteration 35, average log likelihood -1.411075
[ Info: iteration 36, average log likelihood -1.411058
[ Info: iteration 37, average log likelihood -1.411043
[ Info: iteration 38, average log likelihood -1.411027
[ Info: iteration 39, average log likelihood -1.411012
[ Info: iteration 40, average log likelihood -1.410997
[ Info: iteration 41, average log likelihood -1.410983
[ Info: iteration 42, average log likelihood -1.410969
[ Info: iteration 43, average log likelihood -1.410956
[ Info: iteration 44, average log likelihood -1.410943
[ Info: iteration 45, average log likelihood -1.410930
[ Info: iteration 46, average log likelihood -1.410917
[ Info: iteration 47, average log likelihood -1.410905
[ Info: iteration 48, average log likelihood -1.410894
[ Info: iteration 49, average log likelihood -1.410882
[ Info: iteration 50, average log likelihood -1.410871
┌ Info: EM with 100000 data points 50 iterations avll -1.410871
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.730988    -0.242965    -0.420607     0.590398    0.342761     0.617264   -0.162101     0.22776     -0.414172   -0.0376472   0.0633474  -0.124293    -0.0396894  -0.416127     0.615888     -0.123032   -0.194104     0.45123      0.110432    -0.475239    -0.0690888   0.261974   -0.367319    0.109219    -0.30888      0.310158
 -0.13167     -0.0189049    0.189009    -0.0342374  -0.343831     0.726479    0.452361    -0.38352      0.154895   -0.244364   -0.0268222  -0.579416     0.329384   -0.0103452   -0.197231      0.608095   -0.190236    -0.584088     0.0612625   -0.0475195   -0.155326   -0.62474     0.292551    0.200604     0.39895      0.166637
 -0.315684    -0.292565     0.38376     -0.0425104  -0.456349    -0.284707   -0.362492    -0.631662     0.300904    0.606154   -0.306181   -0.339421     0.141362    0.614501    -0.4916       -0.0116461   0.193409    -0.495904    -0.178128    -0.105309    -0.0370592   0.335926    0.373896    0.209208     0.394559     0.339708
 -0.816976     0.15594     -0.00480973  -0.175456    0.197466    -0.411171   -0.147042     0.733891     0.618336   -0.20814    -0.2184     -0.622823    -0.0331527   0.296216    -0.762277      0.549658    0.0814405   -0.116755     0.336044    -0.02224      0.461314    0.388769   -0.148549   -0.0448193   -0.732083     0.144752
  0.718464     0.461179     0.363579    -0.158832   -0.214005    -0.427595   -0.163982    -0.210549     0.689962    0.197761   -0.109219    0.514367    -0.033107    0.293679    -0.210431      0.109237   -0.188101    -0.048394    -0.00362652   0.026193     0.106215   -1.06898     0.245487   -0.414432    -0.318559     0.00763238
  0.167959     0.638737    -0.420923    -0.0966185   0.0767406   -0.0261931   0.0945053    0.337521    -0.114936   -0.163259    0.0793324   0.241366    -0.181093   -0.17303     -0.123627      0.0516354  -0.345635     0.127875    -0.118219     0.152574     0.36523    -0.422376   -0.13948    -0.209231    -0.147309    -0.125245
 -0.103081    -0.00426801   0.213957     0.0439538  -0.196424    -0.0261301  -0.0558565    0.00324601   0.0393189  -0.0427547  -0.0888415  -0.143677    -0.15428     0.0978418   -0.209102      0.0595983   0.0580238   -0.0400847   -0.077869     0.051108     0.115744    0.0158899  -0.0176859   0.0148782    0.0474341    0.012224
  0.0747953   -0.15694      0.436427     0.398029   -0.402278     0.2237      0.419339    -0.338219     0.312588   -0.166134   -0.383083    0.185753    -0.447331    0.132558    -0.264636      0.0853725   0.138639     0.0698715    0.484558     0.580034    -0.285698   -0.101651    0.0902185   0.0681512   -0.038883     0.0052173
  0.642506    -0.197898    -0.319903     0.727998   -0.360912    -0.414032   -0.579702    -0.204675    -0.0351236  -0.101484   -0.0771827  -0.469273     0.526742   -0.455867    -0.061516      0.509707   -0.857005    -0.508951    -0.569361    -0.0328144   -0.27208    -0.605354   -0.313589   -0.394741    -0.018715    -0.278818
  0.108749    -0.3854       0.560731    -0.204959    0.477028    -0.0237126  -0.171639    -0.625276     0.0774254  -0.424699   -0.579066   -0.253076     0.772774   -0.0443356   -0.139872      0.17953    -0.172262     0.524015     0.580678     0.851016     0.022645    0.906358   -0.0554634   0.0585764   -0.194554     0.213285
  0.424284    -0.0368069    0.418418    -0.205789    0.157279    -0.505729    0.235275    -0.418829     0.0329497  -0.570848    0.187716   -0.283734    -0.162906    0.229587    -1.08682       0.0341825  -0.50954     -0.380384    -0.353238     0.0581467   -0.335363   -0.136274   -0.0758133  -0.145792    -0.0547699    0.349947
  0.412668     0.4382       0.245091    -0.314709   -0.341757    -0.0555716   0.194583     0.669982    -0.516538    0.154173    0.0169842  -0.430786     0.0540327   0.085089     0.246539     -0.0950422  -0.640309    -0.173732     0.0153372   -0.166958    -0.310902   -0.347315   -0.284823    0.0115468    0.896785     0.19586
 -0.453745     0.650239     0.0738091   -0.282716    0.398571     0.0067243   0.437507     0.360254     0.227608    0.251911    0.201474    0.183119    -0.293515    0.450719     0.000885774  -0.100054    0.5854       0.426546     0.0105348   -0.0940812    0.2063      0.409711    0.261839    0.768685    -0.00472922   0.359782
 -0.531647     0.0592431   -0.289903     0.0684504   1.03379      0.342915    0.370787    -0.128841     0.267831    0.409054    0.0562085   0.682794     0.265169   -0.435047    -0.0306522     0.385868   -0.138539    -0.337838     0.0833617   -0.497525    -0.108263   -0.476395   -0.12073    -0.324888     0.299026     0.135967
 -0.00842178   0.37352     -0.579387     0.569881   -0.467704    -0.227371    0.533964     0.401519     0.183808    0.147809    0.757191   -0.040712    -0.552899   -0.0465548   -0.398957      0.25892    -0.101597    -0.640785    -0.419419    -0.859714     0.140864   -0.51273     0.605097   -0.335464     0.490819     0.0654093
 -0.393677     0.346946     0.0821277    0.472722   -0.175156     0.577403    0.0793922    0.456785     0.59059     0.0281952  -0.0332174   0.176147    -0.297178   -0.125825     0.0529084     0.262913    0.268044     0.0361327    0.523436    -0.128757     0.131536   -0.443591   -0.554354   -0.0994781    0.382901    -0.412894
 -0.0838928    0.00377622  -0.198405    -0.252254    0.417518    -0.216672   -0.0970534    0.130408     0.173463    0.148283   -0.241504    0.326582    -0.124503   -0.0550749    0.319785     -0.0830412   0.147523     0.330703     0.106208    -0.0108758   -0.0754795   0.124792   -0.207837    0.167702     0.0076147   -0.418031
  0.0213248   -0.351423     0.0181485   -0.0127239   0.156176    -0.143831    0.133259    -0.267163    -0.0422675  -0.241532    0.0783026  -0.109573     0.467981    0.240832     0.0796533    -0.0517262   0.0297072   -0.147481    -0.0548623   -0.515673    -0.370275    0.0882581   0.228943    0.0787484    0.22178      0.182757
 -0.19225      0.0245931    0.245617     0.0683693  -0.510413     0.0733363  -0.620183     0.100056    -0.128077    0.320042   -0.113425   -0.0983896   -0.198079   -0.33163     -0.017536     -0.220357    0.441349     0.00373588   0.144167     0.459183     0.411152    0.632609   -0.159574   -0.359785     0.0325039   -0.744877
 -0.635564    -0.332189    -0.33515     -0.100339    0.0871825   -0.0797498  -0.0938384    0.336315    -0.280837   -0.381675   -0.368876   -0.22077     -0.318021    0.362588    -0.430067      0.205725   -0.276827    -0.524078    -0.0237946   -0.1258       0.101444    0.439584    0.0544218   0.195919    -0.273986     0.0368105
  0.299187     0.210835     0.336583     0.115986   -0.184408     0.715134   -0.345532     0.196218     0.298751    0.557645   -0.434808   -0.0666171   -0.312324   -0.210443    -0.286623     -0.359765   -0.0839603   -0.0522778   -0.0200472    0.907593     0.605415    0.31659     0.306748   -0.382829    -0.160968     0.84317
  0.0914688    0.283581     0.0478703    0.308551   -7.80075e-6   0.0837154  -0.386915    -0.0502121    0.185548    0.308006    0.497241   -0.0729184    0.276588   -0.00212914   0.109639     -0.24628     0.0798756    0.0722188    0.188194     0.00957517   0.14972     0.141577    0.360369   -0.399892    -0.0960639    0.314069
  0.293277    -0.14431     -0.102451    -0.150271    0.30109      0.059727    0.201456     0.814827    -0.304415   -0.605149    0.0841095  -0.0122698    0.278424   -0.622176     0.706895      0.523615    0.362636     0.47339      0.509906     0.117629    -0.119247   -0.178089   -0.281768   -0.24355     -0.0566687   -0.848752
 -0.23359     -0.134782     0.0841046    0.358754    0.228771     0.234556   -0.138822    -0.973712     0.145974   -0.365844    0.418874    0.0261923   -0.131922   -0.0304548   -0.53623       0.135147    0.699591     0.423429    -0.378204    -0.0439401    0.266766    0.439607    0.561845    0.00663525  -0.765718     0.250528
  0.242342    -0.458743    -0.513804     0.0253714   0.0689923    0.386443    0.0819315   -0.241965    -0.292213   -0.129287   -0.345418    0.522092    -0.0478004  -0.682428     0.198629     -0.574752   -0.837206    -0.308229     0.417604     0.487443    -0.328756   -0.0568684   0.0585104  -0.067743     0.054696    -0.201129
  0.434482     0.0475449   -0.567855    -0.179752    0.355341    -0.123545    0.286579    -0.17687     -0.196355   -0.331455   -0.0332409  -0.221783    -0.513918    0.332478    -0.154719     -0.495675   -0.273455     1.00527     -0.329406    -0.38518      0.610945    0.143469   -0.084974    0.252033    -0.167437    -0.0444202
  0.293929    -0.335903    -0.164695    -0.504447   -0.51249     -0.864026   -0.112121    -0.0710588   -0.611109    0.0965064  -0.216282   -0.177851    -0.15224     0.205083    -0.0605615    -0.659806    0.112119     0.255596    -0.469795     0.511671     0.232516    0.393249    0.164157    0.062922    -0.589539    -0.0510829
  0.452171     0.358256     0.0437004   -0.293985    0.0708084   -0.491954   -0.0262621    0.522194    -0.154924   -0.246365    0.166371   -0.0479326   -0.429915    0.131855    -0.456425      0.293345    0.747006     0.282444    -0.828744    -0.111764     0.946115   -0.146648   -0.252724   -0.548014     0.376865    -0.441696
 -0.234706    -0.14691     -0.224024     0.0924738   0.322828    -0.385488    0.262013     0.0192855    0.021643   -0.229681    0.168622    0.0856314    0.0465777   0.394008     0.644188     -0.0826003   0.00701125   0.24521      0.225241    -0.712218    -0.506594   -0.473868   -0.470277    0.715734     0.274709    -0.460495
  0.0726705   -0.0194502   -0.111592     0.0459356  -0.208591     0.320279    0.00997106  -0.109763    -0.134165    0.0457165   0.086431    0.200244     0.111695   -0.356358     0.402525     -0.339992   -0.175924     0.00505746   0.250979     0.242028    -0.202949   -0.112669    0.093035    0.132407     0.117555    -0.111969
 -0.411499     0.161415     0.199955    -0.411574   -0.198407    -0.187516   -0.110748     0.101629    -0.360098    0.0729549  -0.205854   -0.0580101    0.660775   -0.376624     0.289663     -0.0611156   0.547907    -0.432346    -0.36394      0.237681    -0.574367   -0.148106   -0.0259716   0.453247     0.338241    -0.018825
  0.134726    -0.0725217    0.0285639    0.152673    0.606144    -0.55472    -0.639658     0.215616     0.113264    0.37239     0.0759008  -0.00275271   0.0886435   0.289851     0.0438298    -0.453623   -0.202194     0.175213     0.0234718   -0.47471     -0.315896    1.01047     0.053894   -0.313177    -0.0133563    0.134177[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.410860
[ Info: iteration 2, average log likelihood -1.410850
[ Info: iteration 3, average log likelihood -1.410840
[ Info: iteration 4, average log likelihood -1.410830
[ Info: iteration 5, average log likelihood -1.410820
[ Info: iteration 6, average log likelihood -1.410810
[ Info: iteration 7, average log likelihood -1.410801
[ Info: iteration 8, average log likelihood -1.410792
[ Info: iteration 9, average log likelihood -1.410783
[ Info: iteration 10, average log likelihood -1.410774
┌ Info: EM with 100000 data points 10 iterations avll -1.410774
└ 59.0 data points per parameter
[ Info: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.678561e+05
      1       2.230230e+04      -1.455538e+05 |        2
      2       7.823675e+03      -1.447862e+04 |        0
      3       7.823675e+03       0.000000e+00 |        0
K-means converged with 3 iterations (objv = 7823.67549422947)
┌ Info: K-means with 900 data points using 3 iterations
└ 150.0 data points per parameter
[ Info: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
[ Info: iteration 1, average log likelihood -2.043155
[ Info: iteration 2, average log likelihood -2.043154
[ Info: iteration 3, average log likelihood -2.043154
[ Info: iteration 4, average log likelihood -2.043154
[ Info: iteration 5, average log likelihood -2.043154
[ Info: iteration 6, average log likelihood -2.043154
[ Info: iteration 7, average log likelihood -2.043154
[ Info: iteration 8, average log likelihood -2.043154
[ Info: iteration 9, average log likelihood -2.043154
[ Info: iteration 10, average log likelihood -2.043154
┌ Info: EM with 900 data points 10 iterations avll -2.043154
└ 81.8 data points per parameter
   Testing GaussianMixtures tests passed 
