Julia Version 1.5.0-DEV.202
Commit 0029072192 (2020-01-29 16:15 UTC)
Platform Info:
  OS: Linux (x86_64-linux-gnu)
  CPU: Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-9.0.1 (ORCJIT, skylake)
Environment:
  JULIA_DEPOT_PATH = ::/usr/local/share/julia
  JULIA_NUM_THREADS = 2

 Resolving package versions...
 Installed OpenBLAS_jll ─────── v0.3.7+5
 Installed GaussianMixtures ─── v0.3.0
 Installed Missings ─────────── v0.4.3
 Installed OpenSpecFun_jll ──── v0.5.3+1
 Installed Blosc ────────────── v0.5.1
 Installed CMake ────────────── v1.1.2
 Installed OrderedCollections ─ v1.1.0
 Installed DataStructures ───── v0.17.9
 Installed QuadGK ───────────── v2.3.1
 Installed URIParser ────────── v0.4.0
 Installed Arpack ───────────── v0.4.0
 Installed Parameters ───────── v0.12.0
 Installed BinDeps ──────────── v1.0.0
 Installed JLD ──────────────── v0.9.2
 Installed Distributions ────── v0.22.3
 Installed SortingAlgorithms ── v0.3.1
 Installed DataAPI ──────────── v1.1.0
 Installed ScikitLearnBase ──── v0.5.0
 Installed Rmath ────────────── v0.6.0
 Installed Clustering ───────── v0.13.3
 Installed NearestNeighbors ─── v0.4.4
 Installed StatsBase ────────── v0.32.0
 Installed BinaryProvider ───── v0.5.8
 Installed Distances ────────── v0.8.2
 Installed Compat ───────────── v2.2.0
 Installed FileIO ───────────── v1.2.1
 Installed StatsFuns ────────── v0.9.3
 Installed LegacyStrings ────── v0.4.1
 Installed SpecialFunctions ─── v0.9.0
 Installed Arpack_jll ───────── v3.5.0+2
 Installed PDMats ───────────── v0.9.11
 Installed CMakeWrapper ─────── v0.2.3
 Installed FillArrays ───────── v0.8.4
 Installed HDF5 ─────────────── v0.12.5
 Installed StaticArrays ─────── v0.12.1
  Updating `~/.julia/environments/v1.5/Project.toml`
  [cc18c42c] + GaussianMixtures v0.3.0
  Updating `~/.julia/environments/v1.5/Manifest.toml`
  [7d9fca2a] + Arpack v0.4.0
  [68821587] + Arpack_jll v3.5.0+2
  [9e28174c] + BinDeps v1.0.0
  [b99e7846] + BinaryProvider v0.5.8
  [a74b3585] + Blosc v0.5.1
  [631607c0] + CMake v1.1.2
  [d5fb7624] + CMakeWrapper v0.2.3
  [aaaa29a8] + Clustering v0.13.3
  [34da2185] + Compat v2.2.0
  [9a962f9c] + DataAPI v1.1.0
  [864edb3b] + DataStructures v0.17.9
  [b4f34e82] + Distances v0.8.2
  [31c24e10] + Distributions v0.22.3
  [5789e2e9] + FileIO v1.2.1
  [1a297f60] + FillArrays v0.8.4
  [cc18c42c] + GaussianMixtures v0.3.0
  [f67ccb44] + HDF5 v0.12.5
  [4138dd39] + JLD v0.9.2
  [1b4a561d] + LegacyStrings v0.4.1
  [e1d29d7a] + Missings v0.4.3
  [b8a86587] + NearestNeighbors v0.4.4
  [4536629a] + OpenBLAS_jll v0.3.7+5
  [efe28fd5] + OpenSpecFun_jll v0.5.3+1
  [bac558e1] + OrderedCollections v1.1.0
  [90014a1f] + PDMats v0.9.11
  [d96e819e] + Parameters v0.12.0
  [1fd47b50] + QuadGK v2.3.1
  [79098fc4] + Rmath v0.6.0
  [6e75b9c4] + ScikitLearnBase v0.5.0
  [a2af1166] + SortingAlgorithms v0.3.1
  [276daf66] + SpecialFunctions v0.9.0
  [90137ffa] + StaticArrays v0.12.1
  [2913bbd2] + StatsBase v0.32.0
  [4c63d2b9] + StatsFuns v0.9.3
  [30578b45] + URIParser v0.4.0
  [2a0f44e3] + Base64 
  [ade2ca70] + Dates 
  [8bb1440f] + DelimitedFiles 
  [8ba89e20] + Distributed 
  [b77e0a4c] + InteractiveUtils 
  [76f85450] + LibGit2 
  [8f399da3] + Libdl 
  [37e2e46d] + LinearAlgebra 
  [56ddb016] + Logging 
  [d6f4376e] + Markdown 
  [a63ad114] + Mmap 
  [44cfe95a] + Pkg 
  [de0858da] + Printf 
  [3fa0cd96] + REPL 
  [9a3f8284] + Random 
  [ea8e919c] + SHA 
  [9e88b42a] + Serialization 
  [1a1011a3] + SharedArrays 
  [6462fe0b] + Sockets 
  [2f01184e] + SparseArrays 
  [10745b16] + Statistics 
  [4607b0f0] + SuiteSparse 
  [8dfed614] + Test 
  [cf7118a7] + UUIDs 
  [4ec0a83e] + Unicode 
  Building CMake → `~/.julia/packages/CMake/nSK2r/deps/build.log`
  Updating `/tmp/jl_i1Mkxf/Project.toml`
 [no changes]
  Updating `/tmp/jl_i1Mkxf/Manifest.toml`
 [no changes]
  Building Blosc → `~/.julia/packages/Blosc/lzFr0/deps/build.log`
  Updating `/tmp/jl_Zc9hjP/Project.toml`
 [no changes]
  Updating `/tmp/jl_Zc9hjP/Manifest.toml`
 [no changes]
  Building Rmath → `~/.julia/packages/Rmath/BoBag/deps/build.log`
  Updating `/tmp/jl_eVS0Jj/Project.toml`
 [no changes]
  Updating `/tmp/jl_eVS0Jj/Manifest.toml`
 [no changes]
  Building HDF5 ─→ `~/.julia/packages/HDF5/Zh9on/deps/build.log`
  Updating `/tmp/jl_FulFW1/Project.toml`
 [no changes]
  Updating `/tmp/jl_FulFW1/Manifest.toml`
 [no changes]
   Testing GaussianMixtures
  Updating `/tmp/jl_CM9vx9/Project.toml`
 [no changes]
  Updating `/tmp/jl_CM9vx9/Manifest.toml`
 [no changes]
Running sandbox
Status `/tmp/jl_CM9vx9/Project.toml`
  [aaaa29a8] Clustering v0.13.3
  [34da2185] Compat v2.2.0
  [31c24e10] Distributions v0.22.3
  [5789e2e9] FileIO v1.2.1
  [cc18c42c] GaussianMixtures v0.3.0
  [4138dd39] JLD v0.9.2
  [90014a1f] PDMats v0.9.11
  [6e75b9c4] ScikitLearnBase v0.5.0
  [276daf66] SpecialFunctions v0.9.0
  [2913bbd2] StatsBase v0.32.0
  [8bb1440f] DelimitedFiles 
  [8ba89e20] Distributed 
  [37e2e46d] LinearAlgebra 
  [56ddb016] Logging 
  [de0858da] Printf 
  [9a3f8284] Random 
  [10745b16] Statistics 
[ Info: Testing Data
(100000, -6.237136910739931e6, [823.9046715806285, 99176.09532841937], [1589.653332927826 987.0789853205561 1147.6941591420657; -1255.0080566345707 -1084.7082163064601 -1337.797640221305], [[3534.308107906804 1649.4922341569213 1916.7530977186777; 1649.4922341569213 2080.0551652953395 992.0999049595256; 1916.7530977186777 992.0999049595256 2330.4789925427513], [96174.8565628045 -1643.594968371475 -2058.0975632384334; -1643.594968371475 97629.19854375342 -718.8477290399258; -2058.0975632384334 -718.8477290399258 97832.1919040838]])
┌ Warning: rmprocs: process 1 not removed
└ @ Distributed /workspace/srcdir/usr/share/julia/stdlib/v1.5/Distributed/src/cluster.jl:1030
[ Info: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.163907e+03
      1       1.067732e+03      -9.617532e+01 |        4
      2       1.030520e+03      -3.721198e+01 |        2
      3       9.842146e+02      -4.630499e+01 |        2
      4       9.717086e+02      -1.250597e+01 |        4
      5       9.614845e+02      -1.022415e+01 |        2
      6       9.492212e+02      -1.226329e+01 |        2
      7       9.430420e+02      -6.179232e+00 |        0
      8       9.430420e+02       0.000000e+00 |        0
K-means converged with 8 iterations (objv = 943.0419761887497)
┌ Info: K-means with 272 data points using 8 iterations
└ 11.3 data points per parameter
[ Info: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
┌ Info: EM with 272 data points 0 iterations avll -2.074897
└ 5.8 data points per parameter
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:221
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:221
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:221
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:221
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:631 [inlined]
└ @ Core ./broadcast.jl:631
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:230
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/bayes.jl:230
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:631 [inlined]
└ @ Core ./broadcast.jl:631
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:631 [inlined]
└ @ Core ./broadcast.jl:631
[ Info: iteration 1, lowerbound -3.768267
[ Info: iteration 2, lowerbound -3.645023
[ Info: iteration 3, lowerbound -3.523561
[ Info: iteration 4, lowerbound -3.387999
[ Info: iteration 5, lowerbound -3.246802
[ Info: iteration 6, lowerbound -3.111665
[ Info: dropping number of Gaussions to 7
[ Info: iteration 7, lowerbound -2.988044
[ Info: dropping number of Gaussions to 6
[ Info: iteration 8, lowerbound -2.882546
[ Info: iteration 9, lowerbound -2.821289
[ Info: dropping number of Gaussions to 4
[ Info: iteration 10, lowerbound -2.783986
[ Info: iteration 11, lowerbound -2.762634
[ Info: dropping number of Gaussions to 3
[ Info: iteration 12, lowerbound -2.745705
[ Info: iteration 13, lowerbound -2.724637
[ Info: iteration 14, lowerbound -2.701610
[ Info: iteration 15, lowerbound -2.672432
[ Info: iteration 16, lowerbound -2.637235
[ Info: iteration 17, lowerbound -2.597034
[ Info: iteration 18, lowerbound -2.553758
[ Info: iteration 19, lowerbound -2.509961
[ Info: iteration 20, lowerbound -2.468104
[ Info: iteration 21, lowerbound -2.429657
[ Info: iteration 22, lowerbound -2.394727
[ Info: iteration 23, lowerbound -2.362822
[ Info: iteration 24, lowerbound -2.334820
[ Info: iteration 25, lowerbound -2.314701
[ Info: iteration 26, lowerbound -2.307410
[ Info: dropping number of Gaussions to 2
[ Info: iteration 27, lowerbound -2.302943
[ Info: iteration 28, lowerbound -2.299261
[ Info: iteration 29, lowerbound -2.299256
[ Info: iteration 30, lowerbound -2.299255
[ Info: iteration 31, lowerbound -2.299254
[ Info: iteration 32, lowerbound -2.299253
[ Info: iteration 33, lowerbound -2.299253
[ Info: iteration 34, lowerbound -2.299253
[ Info: iteration 35, lowerbound -2.299253
[ Info: iteration 36, lowerbound -2.299253
[ Info: iteration 37, lowerbound -2.299253
[ Info: iteration 38, lowerbound -2.299253
[ Info: iteration 39, lowerbound -2.299253
[ Info: iteration 40, lowerbound -2.299253
[ Info: iteration 41, lowerbound -2.299253
[ Info: iteration 42, lowerbound -2.299253
[ Info: iteration 43, lowerbound -2.299253
[ Info: iteration 44, lowerbound -2.299253
[ Info: iteration 45, lowerbound -2.299253
[ Info: iteration 46, lowerbound -2.299253
[ Info: iteration 47, lowerbound -2.299253
[ Info: iteration 48, lowerbound -2.299253
[ Info: iteration 49, lowerbound -2.299253
[ Info: iteration 50, lowerbound -2.299253
[ Info: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
History[Wed Jan 29 23:09:07 2020: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
, Wed Jan 29 23:09:15 2020: K-means with 272 data points using 8 iterations
11.3 data points per parameter
, Wed Jan 29 23:09:17 2020: EM with 272 data points 0 iterations avll -2.074897
5.8 data points per parameter
, Wed Jan 29 23:09:18 2020: GMM converted to Variational GMM
, Wed Jan 29 23:09:26 2020: iteration 1, lowerbound -3.768267
, Wed Jan 29 23:09:26 2020: iteration 2, lowerbound -3.645023
, Wed Jan 29 23:09:26 2020: iteration 3, lowerbound -3.523561
, Wed Jan 29 23:09:26 2020: iteration 4, lowerbound -3.387999
, Wed Jan 29 23:09:26 2020: iteration 5, lowerbound -3.246802
, Wed Jan 29 23:09:26 2020: iteration 6, lowerbound -3.111665
, Wed Jan 29 23:09:27 2020: dropping number of Gaussions to 7
, Wed Jan 29 23:09:27 2020: iteration 7, lowerbound -2.988044
, Wed Jan 29 23:09:27 2020: dropping number of Gaussions to 6
, Wed Jan 29 23:09:27 2020: iteration 8, lowerbound -2.882546
, Wed Jan 29 23:09:27 2020: iteration 9, lowerbound -2.821289
, Wed Jan 29 23:09:27 2020: dropping number of Gaussions to 4
, Wed Jan 29 23:09:27 2020: iteration 10, lowerbound -2.783986
, Wed Jan 29 23:09:27 2020: iteration 11, lowerbound -2.762634
, Wed Jan 29 23:09:27 2020: dropping number of Gaussions to 3
, Wed Jan 29 23:09:27 2020: iteration 12, lowerbound -2.745705
, Wed Jan 29 23:09:27 2020: iteration 13, lowerbound -2.724637
, Wed Jan 29 23:09:27 2020: iteration 14, lowerbound -2.701610
, Wed Jan 29 23:09:27 2020: iteration 15, lowerbound -2.672432
, Wed Jan 29 23:09:27 2020: iteration 16, lowerbound -2.637235
, Wed Jan 29 23:09:27 2020: iteration 17, lowerbound -2.597034
, Wed Jan 29 23:09:27 2020: iteration 18, lowerbound -2.553758
, Wed Jan 29 23:09:27 2020: iteration 19, lowerbound -2.509961
, Wed Jan 29 23:09:27 2020: iteration 20, lowerbound -2.468104
, Wed Jan 29 23:09:27 2020: iteration 21, lowerbound -2.429657
, Wed Jan 29 23:09:27 2020: iteration 22, lowerbound -2.394727
, Wed Jan 29 23:09:27 2020: iteration 23, lowerbound -2.362822
, Wed Jan 29 23:09:27 2020: iteration 24, lowerbound -2.334820
, Wed Jan 29 23:09:27 2020: iteration 25, lowerbound -2.314701
, Wed Jan 29 23:09:27 2020: iteration 26, lowerbound -2.307410
, Wed Jan 29 23:09:27 2020: dropping number of Gaussions to 2
, Wed Jan 29 23:09:27 2020: iteration 27, lowerbound -2.302943
, Wed Jan 29 23:09:27 2020: iteration 28, lowerbound -2.299261
, Wed Jan 29 23:09:27 2020: iteration 29, lowerbound -2.299256
, Wed Jan 29 23:09:27 2020: iteration 30, lowerbound -2.299255
, Wed Jan 29 23:09:27 2020: iteration 31, lowerbound -2.299254
, Wed Jan 29 23:09:27 2020: iteration 32, lowerbound -2.299253
, Wed Jan 29 23:09:27 2020: iteration 33, lowerbound -2.299253
, Wed Jan 29 23:09:27 2020: iteration 34, lowerbound -2.299253
, Wed Jan 29 23:09:27 2020: iteration 35, lowerbound -2.299253
, Wed Jan 29 23:09:27 2020: iteration 36, lowerbound -2.299253
, Wed Jan 29 23:09:27 2020: iteration 37, lowerbound -2.299253
, Wed Jan 29 23:09:27 2020: iteration 38, lowerbound -2.299253
, Wed Jan 29 23:09:27 2020: iteration 39, lowerbound -2.299253
, Wed Jan 29 23:09:27 2020: iteration 40, lowerbound -2.299253
, Wed Jan 29 23:09:27 2020: iteration 41, lowerbound -2.299253
, Wed Jan 29 23:09:27 2020: iteration 42, lowerbound -2.299253
, Wed Jan 29 23:09:27 2020: iteration 43, lowerbound -2.299253
, Wed Jan 29 23:09:27 2020: iteration 44, lowerbound -2.299253
, Wed Jan 29 23:09:27 2020: iteration 45, lowerbound -2.299253
, Wed Jan 29 23:09:27 2020: iteration 46, lowerbound -2.299253
, Wed Jan 29 23:09:27 2020: iteration 47, lowerbound -2.299253
, Wed Jan 29 23:09:27 2020: iteration 48, lowerbound -2.299253
, Wed Jan 29 23:09:27 2020: iteration 49, lowerbound -2.299253
, Wed Jan 29 23:09:27 2020: iteration 50, lowerbound -2.299253
, Wed Jan 29 23:09:27 2020: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [178.04509222618555, 95.95490777381448]
β = [178.04509222618555, 95.95490777381448]
m = [4.250300733268516 79.28686694434134; 2.000229257773929 53.851987172453796]
ν = [180.04509222618555, 97.95490777381448]
W = LinearAlgebra.UpperTriangular{Float64,Array{Float64,2}}[[0.18404155547465464 -0.007644049042345499; 0.0 0.008581705166307449], [0.3758763611972388 -0.008953123827374601; 0.0 0.012748664777416652]]
┌ Warning: Assignment to `x` in soft scope is ambiguous because a global variable by the same name exists: `x` will be treated as a new local. Disambiguate by using `local x` to suppress this warning or `global x` to assign to the existing global variable.
└ @ ~/.julia/packages/GaussianMixtures/RGtTJ/test/train.jl:7
┌ Warning: Assignment to `p` in soft scope is ambiguous because a global variable by the same name exists: `p` will be treated as a new local. Disambiguate by using `local p` to suppress this warning or `global p` to assign to the existing global variable.
└ @ ~/.julia/packages/GaussianMixtures/RGtTJ/test/train.jl:17
Kind: diag, size256
nx: 100000 sum(zeroth order stats): 99999.99999999997
avll from stats: -0.9864763295202011
avll from llpg:  -0.9864763295202013
avll direct:     -0.9864763295202013
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 100000.00000000003
avll from stats: -1.0194547089976245
avll from llpg:  -1.0194547089976245
avll direct:     -1.0194547089976245
sum posterior: 100000.0
┌ Warning: Assignment to `x` in soft scope is ambiguous because a global variable by the same name exists: `x` will be treated as a new local. Disambiguate by using `local x` to suppress this warning or `global x` to assign to the existing global variable.
└ @ ~/.julia/packages/GaussianMixtures/RGtTJ/test/train.jl:26
32×26 Array{Float64,2}:
 -0.124648      0.0783289   -0.0246547   -0.0323136   -0.0414607    0.0203466    -0.0541618    -0.123865    -0.064191    -0.0384996     0.00436679  -0.188275      0.14142      0.0505467    0.181719     0.145993     0.0618478   -0.206985     0.0402951  -0.147035    -0.0389383    0.178408      0.0794626   0.000736511   0.112178     -0.0515388
  0.178379     -0.202458     0.122006    -0.0256014   -0.191676    -0.0392904     0.0548456     0.131631    -0.144441    -0.125507      0.0664123   -0.0599116    -0.0378904   -0.0764381   -0.154478     0.0149056    0.121313    -0.110132     0.132391   -0.0822373   -0.0689584   -0.174321      0.0700346  -0.168998     -0.153253      0.0211382
  0.101359      0.152671     0.0753292   -0.103138     0.0357308    0.0146514    -0.0902232    -0.104546     0.0741159   -0.0895646    -0.111766     0.192564      0.127818     0.028253    -0.0166893   -0.178288    -0.103122     0.0354919   -0.0562639   0.0280072    0.0217882   -0.097218     -0.0518328   0.27673      -0.103246      0.109518
  0.177029      0.140565     0.0378647    0.0198956   -0.00518369  -0.0589514    -0.0954413    -0.106152    -0.131242    -0.0513295    -0.117921    -0.0361599    -0.132324    -0.106714    -0.0289214    0.0326491    0.0528304    0.188923    -0.0396026  -0.0485186    0.00172918   0.128503      0.0292834   0.106682     -0.0806408     0.0477209
 -0.00420931    0.0977017   -0.0234147   -0.0396194   -0.0136772    0.0359126     0.0749598    -0.0666873   -0.136463     0.0397221     0.0479846   -0.0230113     0.0503418    0.237852    -0.0155138   -0.271983    -0.101621     0.207052     0.0539468  -0.0374924   -0.0982526   -0.0313603     0.152007   -0.0962826    -0.0671211     0.00325047
 -0.013789     -0.0748099    0.0259691   -0.10697     -0.0653202   -0.000966196  -0.0555854     0.143918    -0.0671834   -0.101706     -0.183014    -0.0485891    -0.00147158   0.10377      0.0762151    0.0754018    0.143748    -0.139126     0.0934796  -0.0251682   -0.0569163   -0.163732     -0.0591443   0.0318731    -0.0986104    -0.225874
 -0.28611      -0.0387552   -0.116383     0.0467778   -0.208765     0.0333522     0.0422149     0.0884312   -0.101753     0.175038      0.00198773  -0.133011      0.180863    -0.0488519   -0.0585712   -0.0605862   -0.0327952    0.160764     0.0573657  -0.0135375    0.241674    -0.126243      0.13125    -0.111175      0.0158139    -0.144719
  0.0350726    -0.035787    -0.0163845    0.0171825   -0.0119539    0.0937781     0.0161121     0.0341837   -0.0850642    0.0157058    -0.00321748   0.132333      0.0148451    0.177902    -0.0335697   -0.0481414   -0.0324673    0.153895    -0.0352302  -0.104012     0.0323035   -0.0481351     0.0569107  -0.0131287     0.0456785    -0.00253938
 -0.0518715     0.0739264    0.0866162    0.145262    -0.0161772    0.143099     -0.119664     -0.0612654    0.113984    -0.00775121   -0.0906526    0.13573       0.0425918    0.156178     0.128312     0.143519     0.025719    -0.0111443    0.0342046   0.0281453   -0.0995557   -0.0156918    -0.0596791  -0.175626     -0.053118     -0.0582108
  0.210212     -0.0513636   -0.124976    -0.0533163   -0.0890727   -0.02423       0.000884595   0.101        0.0380472   -0.00615231    0.0523172   -0.0368945     0.0220283    0.180067    -0.156918     0.0948353    0.111338    -0.0473976   -0.0181199  -0.0347453    0.0766116    0.196455      0.191153   -0.0384523    -0.0456507    -0.163159
 -0.0155004     0.0975317    0.119775    -0.138875    -0.0502735    0.0201219    -0.218094     -0.0629123    0.0537424   -0.115917     -0.157691    -0.00775198    0.0167638   -0.12549     -0.239667     0.059882     0.0643078    0.0297605    0.0295605   0.0885924   -0.243616     0.0216321    -0.0470606   0.0710268    -0.019457     -0.0184027
  0.0772601    -0.201726    -0.00373905   0.143331    -0.0194846   -0.0272433    -0.228518      0.114046    -0.179107     0.133082      0.00783653  -0.100405     -0.167292    -0.139721     0.150839    -0.0214065    0.0739921   -0.238546     0.0106938   0.0558147    0.0284292    0.0620346    -0.0353267   0.0933136     0.182938      0.0314406
 -0.116003      0.169193    -0.116166     0.138171     0.114264     0.142025     -0.0261415    -0.153271    -0.138747    -0.0413087    -0.0459072    0.0649748    -0.0156479    0.0816558   -0.0480672   -0.0457909    0.22397      0.0741078   -0.0685666  -0.0154672    0.0552369    0.0706197    -0.137848   -0.13854      -0.0620933     0.0698343
 -0.0127223    -0.181241     0.152882    -0.0185832   -0.0187212    0.110539      0.0792186     0.00280764   0.0423832   -0.173821     -0.0602877   -0.017531      0.0253218    0.0879412    0.114668     0.0765579   -0.0307774   -0.0125317   -0.0693055  -0.135947     0.18844      0.087543     -0.103792   -0.0646268    -0.0381127     0.0196115
  0.0453571    -0.181437     0.197418     0.105839     0.0868194    0.152752      0.0406275    -0.00600299  -0.00673711   0.113647      0.0162354    0.0109855     0.00239638  -0.01061      0.147238     0.0152537   -0.056432     0.0638229    0.0771799   0.129337     0.149456    -0.153209     -0.0571948  -0.0230742    -0.0415579    -0.0277708
 -0.155517      0.0461416   -0.145644     0.108621    -0.110906    -0.0244125    -0.0219414    -0.0546074   -0.091138    -0.0122935     0.117096    -0.144118     -0.0814973   -0.0480295   -0.00878893  -0.110093     0.0592383   -0.0413788    0.169464    0.143016    -0.0359625    0.0711119     0.159853   -0.0264322    -0.0859948     0.0215595
  0.0830197     0.0126099    0.0989731    0.0719274   -0.105017    -0.00533622   -0.156743     -0.0133616   -0.024213     0.026136     -0.00157001   0.267031      0.208753    -0.0993332    0.115699    -0.0603234   -0.143998    -0.0310898    0.138814    0.0639778   -0.0792302   -0.033859     -0.0647547  -0.0195721    -0.00318884    0.0878654
  0.0256336    -0.00187723   0.0539967    0.00592363   0.0311202   -0.0641948    -0.03793       0.114021    -0.070962    -0.0632166    -0.0194107    0.0213738    -0.173843     0.0653397   -0.110375     0.119616     0.0296014   -0.0477322    0.106273   -0.060972     0.134474    -0.0065698     0.183558   -0.00952137    0.0357623     0.155924
  0.133201      0.0586219    0.12724     -0.0332362    0.037906    -0.00551331   -0.142061      0.00611855   0.0193864    0.111786     -0.0887862   -0.000187427   0.0633486    0.144449    -0.00649244   0.00723989  -0.0639917   -0.0907419    0.0964323   0.00842725  -0.0631777   -0.102142      0.0669382  -0.0623257     0.132889     -0.0231583
 -0.154524     -0.016327    -0.0747051    0.00450969   0.0876126    0.0289849    -0.0266352     0.141377     0.0592386   -0.0584604     0.265058     0.016018     -0.0089928    0.0675002    0.257009     0.0493475   -0.0549019   -0.0531019    0.0341369   0.0466175    0.0891531   -0.0844988     0.211657   -0.0359752     0.105538      0.0201743
 -0.257335     -0.299765     0.165118     0.0639998    0.24352     -0.0244102    -0.00318335    0.105812    -0.186282     0.000467425   0.22642      0.0199066     0.133267    -0.148492    -0.0666074   -0.106109     0.0277013   -0.0574433   -0.106314    0.0164876   -0.0611282    0.00824677    0.0558106  -0.0959635    -0.14393      -0.0766023
  0.0415126     0.00526186   0.0400435    0.0905412   -0.0453442    0.0338082     0.0925737    -0.175727    -0.0510111   -0.11799      -0.138072     0.0322192     0.00767502  -0.0254098   -0.0220564   -0.0220165    0.0305711   -0.0202625   -0.0559949  -0.0442265    0.132361    -0.147906     -0.0193597  -0.0333172     0.00742917   -0.167729
  0.0662317     0.0386819    0.00197556  -0.0121579   -0.12757      0.245984     -0.129377      0.318814     0.0586699   -0.15573       0.0270601   -0.0599194    -0.0126277   -0.00158048   0.0658902    0.0498221   -0.00400902  -0.0373451   -0.0482897   0.166379    -0.033673     0.0305587     0.12377     0.0924679    -0.0463394     0.0347659
  0.000449815  -0.122135     0.00470393  -0.133435    -0.137962    -0.056454      0.0607552     0.0611303   -0.00979193   0.159113      0.193902    -0.0390845    -0.00458163   0.0392748    0.133338     0.129157    -0.0625701   -0.00937696   0.0534477  -0.0167828   -0.027417     0.10578       0.0497363   0.0712312     0.0795357    -0.115027
 -0.147739      0.00186567  -0.0110805   -0.0186966   -0.0601757   -0.0216319     0.0242987    -0.0213891   -0.00345097  -0.282831      0.0309269    0.115679     -0.0382293    0.0288562    0.0967019   -0.0585575    0.0297691   -0.0464442    0.108597   -0.00200368   0.262488    -0.0534946    -0.321415    0.128357     -0.0901622    -0.163763
 -0.0480721     0.0358149   -0.0761006   -0.121118    -0.0391517   -0.00391746    0.125558     -0.0174352    0.00839289  -0.0984098     0.192326     0.0502756     0.204661     0.0556894   -0.0486006   -0.0258554    0.111571    -0.0399656   -0.0380923  -0.0654692   -0.0836012    0.0359988     0.0511029   0.120253     -0.000553316  -0.133712
  0.175902      0.148043     0.10924     -0.179983     0.00757461   0.036659     -0.122218      0.0357371   -0.125685    -0.0487178    -0.0898076    0.262883     -0.12105      0.134971    -0.116712    -0.177224     0.116272     0.209529    -0.190659   -0.016564     0.102334    -0.139922      0.092317    0.0383574    -0.0685093    -0.0199263
  0.107782      0.0508471    0.0448262    0.030382    -0.0710531    0.147939      0.0319097     0.144475    -0.0565544   -0.0514826    -0.00755068  -0.0231679    -0.211738    -0.00674437  -0.0171225   -0.0472972    0.104714     0.021359    -0.0737377  -0.0164583   -0.0386529    0.0778137    -0.120487    0.149943     -0.177669     -0.0478628
  0.120884     -0.0667762   -0.100087    -0.0370184    0.00971498   0.159276      0.017752     -0.116881     0.00445686  -0.0196621    -0.0217274    0.0439001    -0.0202127   -0.101772     0.13503     -0.0475442    0.103951     0.0154824   -0.203725    0.0666355   -0.0174274   -0.0481976     0.0197045  -0.219405     -0.0815318    -0.107884
 -0.163703     -0.0118605    0.0721534   -0.115447     0.0238679    0.0364689    -0.102304     -0.0648777    0.0163204    0.11461      -0.128504    -0.0176713    -0.0672282    0.148059    -0.020714    -0.0178487    0.243312     0.0907185    0.0408201   0.0229868   -0.099879    -0.000527258   0.109102    0.0555074    -0.0662856     0.0460657
  0.0663579     0.034364     0.0985643   -0.00584862   0.0694209   -0.00151738    0.000691289  -0.149283     0.0535242   -0.0430751     0.0543392   -0.0574242     0.126256    -0.0450699   -0.0366767   -0.157025    -0.00059902  -0.0397171    0.0276606  -0.0808537   -0.0821398    0.0691734    -0.10761     0.11038       0.075679      0.23239
  0.0890636     0.0254394   -0.214443    -0.0790238    0.0258899   -0.129743     -0.0425461     0.125537     0.114204    -0.0622359     0.118041    -0.127305     -0.0443677    0.0659983    0.021506     0.053685    -0.0636086   -0.20708      0.0961612  -0.0258795    0.0949634    0.161789     -0.0338164  -0.0984601    -0.0523446     0.0400828kind diag, method split
┌ Info: 0: avll = 
└   tll[1] = -1.4053661087331255
[ Info: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.405440
[ Info: iteration 2, average log likelihood -1.405360
[ Info: iteration 3, average log likelihood -1.404817
[ Info: iteration 4, average log likelihood -1.399672
[ Info: iteration 5, average log likelihood -1.385120
[ Info: iteration 6, average log likelihood -1.377515
[ Info: iteration 7, average log likelihood -1.375426
[ Info: iteration 8, average log likelihood -1.374241
[ Info: iteration 9, average log likelihood -1.373285
[ Info: iteration 10, average log likelihood -1.372203
[ Info: iteration 11, average log likelihood -1.371179
[ Info: iteration 12, average log likelihood -1.370545
[ Info: iteration 13, average log likelihood -1.370139
[ Info: iteration 14, average log likelihood -1.369805
[ Info: iteration 15, average log likelihood -1.369445
[ Info: iteration 16, average log likelihood -1.369077
[ Info: iteration 17, average log likelihood -1.368772
[ Info: iteration 18, average log likelihood -1.368530
[ Info: iteration 19, average log likelihood -1.368351
[ Info: iteration 20, average log likelihood -1.368224
[ Info: iteration 21, average log likelihood -1.368135
[ Info: iteration 22, average log likelihood -1.368071
[ Info: iteration 23, average log likelihood -1.368025
[ Info: iteration 24, average log likelihood -1.367993
[ Info: iteration 25, average log likelihood -1.367969
[ Info: iteration 26, average log likelihood -1.367952
[ Info: iteration 27, average log likelihood -1.367940
[ Info: iteration 28, average log likelihood -1.367930
[ Info: iteration 29, average log likelihood -1.367923
[ Info: iteration 30, average log likelihood -1.367918
[ Info: iteration 31, average log likelihood -1.367914
[ Info: iteration 32, average log likelihood -1.367911
[ Info: iteration 33, average log likelihood -1.367909
[ Info: iteration 34, average log likelihood -1.367907
[ Info: iteration 35, average log likelihood -1.367906
[ Info: iteration 36, average log likelihood -1.367905
[ Info: iteration 37, average log likelihood -1.367904
[ Info: iteration 38, average log likelihood -1.367903
[ Info: iteration 39, average log likelihood -1.367903
[ Info: iteration 40, average log likelihood -1.367902
[ Info: iteration 41, average log likelihood -1.367902
[ Info: iteration 42, average log likelihood -1.367902
[ Info: iteration 43, average log likelihood -1.367902
[ Info: iteration 44, average log likelihood -1.367902
[ Info: iteration 45, average log likelihood -1.367901
[ Info: iteration 46, average log likelihood -1.367901
[ Info: iteration 47, average log likelihood -1.367901
[ Info: iteration 48, average log likelihood -1.367901
[ Info: iteration 49, average log likelihood -1.367901
[ Info: iteration 50, average log likelihood -1.367901
┌ Info: EM with 100000 data points 50 iterations avll -1.367901
└ 952.4 data points per parameter
┌ Info: 1
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4054404042744895
│     -1.4053601004171643
│      ⋮
└     -1.3679011045163079
[ Info: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.368038
[ Info: iteration 2, average log likelihood -1.367907
[ Info: iteration 3, average log likelihood -1.367313
[ Info: iteration 4, average log likelihood -1.361840
[ Info: iteration 5, average log likelihood -1.346914
[ Info: iteration 6, average log likelihood -1.336195
[ Info: iteration 7, average log likelihood -1.331294
[ Info: iteration 8, average log likelihood -1.327968
[ Info: iteration 9, average log likelihood -1.325513
[ Info: iteration 10, average log likelihood -1.323795
[ Info: iteration 11, average log likelihood -1.322640
[ Info: iteration 12, average log likelihood -1.321871
[ Info: iteration 13, average log likelihood -1.321365
[ Info: iteration 14, average log likelihood -1.321035
[ Info: iteration 15, average log likelihood -1.320832
[ Info: iteration 16, average log likelihood -1.320702
[ Info: iteration 17, average log likelihood -1.320610
[ Info: iteration 18, average log likelihood -1.320542
[ Info: iteration 19, average log likelihood -1.320491
[ Info: iteration 20, average log likelihood -1.320450
[ Info: iteration 21, average log likelihood -1.320415
[ Info: iteration 22, average log likelihood -1.320382
[ Info: iteration 23, average log likelihood -1.320349
[ Info: iteration 24, average log likelihood -1.320316
[ Info: iteration 25, average log likelihood -1.320283
[ Info: iteration 26, average log likelihood -1.320249
[ Info: iteration 27, average log likelihood -1.320213
[ Info: iteration 28, average log likelihood -1.320181
[ Info: iteration 29, average log likelihood -1.320152
[ Info: iteration 30, average log likelihood -1.320125
[ Info: iteration 31, average log likelihood -1.320102
[ Info: iteration 32, average log likelihood -1.320081
[ Info: iteration 33, average log likelihood -1.320062
[ Info: iteration 34, average log likelihood -1.320045
[ Info: iteration 35, average log likelihood -1.320031
[ Info: iteration 36, average log likelihood -1.320019
[ Info: iteration 37, average log likelihood -1.320009
[ Info: iteration 38, average log likelihood -1.320002
[ Info: iteration 39, average log likelihood -1.319996
[ Info: iteration 40, average log likelihood -1.319991
[ Info: iteration 41, average log likelihood -1.319987
[ Info: iteration 42, average log likelihood -1.319983
[ Info: iteration 43, average log likelihood -1.319980
[ Info: iteration 44, average log likelihood -1.319977
[ Info: iteration 45, average log likelihood -1.319974
[ Info: iteration 46, average log likelihood -1.319971
[ Info: iteration 47, average log likelihood -1.319969
[ Info: iteration 48, average log likelihood -1.319967
[ Info: iteration 49, average log likelihood -1.319965
[ Info: iteration 50, average log likelihood -1.319964
┌ Info: EM with 100000 data points 50 iterations avll -1.319964
└ 473.9 data points per parameter
┌ Info: 2
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.3680381721302772
│     -1.3679066818728793
│      ⋮
└     -1.3199635729720747
[ Info: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.320126
[ Info: iteration 2, average log likelihood -1.319946
[ Info: iteration 3, average log likelihood -1.319201
[ Info: iteration 4, average log likelihood -1.312682
[ Info: iteration 5, average log likelihood -1.293338
[ Info: iteration 6, average log likelihood -1.278390
[ Info: iteration 7, average log likelihood -1.270660
[ Info: iteration 8, average log likelihood -1.264725
[ Info: iteration 9, average log likelihood -1.260498
[ Info: iteration 10, average log likelihood -1.257337
[ Info: iteration 11, average log likelihood -1.254942
[ Info: iteration 12, average log likelihood -1.253445
[ Info: iteration 13, average log likelihood -1.252501
[ Info: iteration 14, average log likelihood -1.251742
[ Info: iteration 15, average log likelihood -1.250986
[ Info: iteration 16, average log likelihood -1.250193
[ Info: iteration 17, average log likelihood -1.249504
[ Info: iteration 18, average log likelihood -1.249067
[ Info: iteration 19, average log likelihood -1.248905
[ Info: iteration 20, average log likelihood -1.248856
[ Info: iteration 21, average log likelihood -1.248836
[ Info: iteration 22, average log likelihood -1.248825
[ Info: iteration 23, average log likelihood -1.248817
[ Info: iteration 24, average log likelihood -1.248812
[ Info: iteration 25, average log likelihood -1.248809
[ Info: iteration 26, average log likelihood -1.248806
[ Info: iteration 27, average log likelihood -1.248805
[ Info: iteration 28, average log likelihood -1.248804
[ Info: iteration 29, average log likelihood -1.248803
[ Info: iteration 30, average log likelihood -1.248802
[ Info: iteration 31, average log likelihood -1.248802
[ Info: iteration 32, average log likelihood -1.248801
[ Info: iteration 33, average log likelihood -1.248801
[ Info: iteration 34, average log likelihood -1.248801
[ Info: iteration 35, average log likelihood -1.248800
[ Info: iteration 36, average log likelihood -1.248800
[ Info: iteration 37, average log likelihood -1.248800
[ Info: iteration 38, average log likelihood -1.248800
[ Info: iteration 39, average log likelihood -1.248800
[ Info: iteration 40, average log likelihood -1.248800
[ Info: iteration 41, average log likelihood -1.248800
[ Info: iteration 42, average log likelihood -1.248800
[ Info: iteration 43, average log likelihood -1.248800
[ Info: iteration 44, average log likelihood -1.248800
[ Info: iteration 45, average log likelihood -1.248800
[ Info: iteration 46, average log likelihood -1.248800
[ Info: iteration 47, average log likelihood -1.248800
[ Info: iteration 48, average log likelihood -1.248800
[ Info: iteration 49, average log likelihood -1.248800
[ Info: iteration 50, average log likelihood -1.248800
┌ Info: EM with 100000 data points 50 iterations avll -1.248800
└ 236.4 data points per parameter
┌ Info: 3
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.320126234394161
│     -1.3199458496126268
│      ⋮
└     -1.2487995915376087
[ Info: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.249017
[ Info: iteration 2, average log likelihood -1.248701
[ Info: iteration 3, average log likelihood -1.246130
[ Info: iteration 4, average log likelihood -1.225021
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.187128
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      7
│     10
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.172901
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.179126
[ Info: iteration 8, average log likelihood -1.169045
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      7
│      8
│     10
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.157538
[ Info: iteration 10, average log likelihood -1.175233
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 11, average log likelihood -1.163719
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      7
│     10
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 12, average log likelihood -1.159327
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 13, average log likelihood -1.169939
[ Info: iteration 14, average log likelihood -1.162831
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      7
│      8
│     10
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 15, average log likelihood -1.153491
[ Info: iteration 16, average log likelihood -1.173137
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 17, average log likelihood -1.162769
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      7
│     10
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 18, average log likelihood -1.158678
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 19, average log likelihood -1.169257
[ Info: iteration 20, average log likelihood -1.162031
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      7
│      8
│     10
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 21, average log likelihood -1.152766
[ Info: iteration 22, average log likelihood -1.172348
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 23, average log likelihood -1.162083
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      7
│     10
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 24, average log likelihood -1.158205
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 25, average log likelihood -1.168782
[ Info: iteration 26, average log likelihood -1.161713
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      7
│      8
│     10
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 27, average log likelihood -1.152612
[ Info: iteration 28, average log likelihood -1.171783
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 29, average log likelihood -1.161218
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 30, average log likelihood -1.156577
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      1
│      8
│     10
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 31, average log likelihood -1.156807
[ Info: iteration 32, average log likelihood -1.176989
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     7
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 33, average log likelihood -1.158706
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     10
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 34, average log likelihood -1.165640
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 35, average log likelihood -1.167825
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 36, average log likelihood -1.160273
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      8
│     10
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 37, average log likelihood -1.161297
[ Info: iteration 38, average log likelihood -1.167332
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     7
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 39, average log likelihood -1.154630
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     10
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 40, average log likelihood -1.164718
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 41, average log likelihood -1.167530
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 42, average log likelihood -1.160154
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      8
│     10
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 43, average log likelihood -1.161272
[ Info: iteration 44, average log likelihood -1.167263
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     7
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 45, average log likelihood -1.154599
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     10
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 46, average log likelihood -1.164692
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 47, average log likelihood -1.167485
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     7
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 48, average log likelihood -1.160114
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      8
│     10
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 49, average log likelihood -1.161267
[ Info: iteration 50, average log likelihood -1.167214
┌ Info: EM with 100000 data points 50 iterations avll -1.167214
└ 118.1 data points per parameter
┌ Info: 4
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.2490172214052482
│     -1.248701031382121
│      ⋮
└     -1.1672135584327827
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     13
│     14
│     15
│     16
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 1, average log likelihood -1.154857
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│     13
│     14
│     15
│     16
│     19
│     20
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 2, average log likelihood -1.152460
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│     13
│     14
│     15
│     16
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 3, average log likelihood -1.152194
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│     13
│     14
│     15
│     16
│     19
│     20
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.128787
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      1
│      4
│     13
│     14
│     15
│     16
│     23
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.081885
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      6
│     12
│     13
│     14
│      ⋮
│     19
│     20
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.082186
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      1
│      4
│     13
│     14
│      ⋮
│     23
│     24
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.072944
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      8
│     12
│     13
│     14
│     15
│     16
│     19
│     20
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.073995
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      1
│      4
│      6
│     13
│      ⋮
│     23
│     24
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.076490
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│     12
│     13
│     14
│     15
│     16
│     19
│     20
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.082833
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      1
│      4
│      9
│     13
│      ⋮
│     16
│     23
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 11, average log likelihood -1.064103
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      6
│     13
│     14
│     15
│     16
│     19
│     20
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 12, average log likelihood -1.075941
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      1
│      4
│     12
│     13
│      ⋮
│     16
│     23
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 13, average log likelihood -1.054381
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      8
│     13
│     14
│     15
│     16
│     19
│     20
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 14, average log likelihood -1.070985
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      1
│      4
│      6
│     12
│      ⋮
│     16
│     23
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 15, average log likelihood -1.060606
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│     13
│     14
│     15
│     16
│     19
│     20
│     28
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 16, average log likelihood -1.078274
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      4
│     12
│     13
│     14
│     15
│     16
│     23
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 17, average log likelihood -1.065057
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      1
│      6
│     13
│     14
│      ⋮
│     19
│     20
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 18, average log likelihood -1.073297
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      4
│     12
│     13
│     14
│     15
│     16
│     23
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 19, average log likelihood -1.064098
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│     13
│     14
│     15
│     16
│     19
│     20
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 20, average log likelihood -1.071608
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      1
│      4
│     12
│     13
│      ⋮
│     23
│     24
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 21, average log likelihood -1.042587
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      8
│     13
│     14
│     15
│     16
│     19
│     20
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 22, average log likelihood -1.079223
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      4
│      6
│      7
│     13
│      ⋮
│     16
│     23
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 23, average log likelihood -1.066044
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      1
│     12
│     13
│     14
│      ⋮
│     19
│     20
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 24, average log likelihood -1.074044
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      4
│     13
│     14
│     15
│     16
│     23
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 25, average log likelihood -1.065065
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      1
│      6
│     12
│     13
│      ⋮
│     20
│     28
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 26, average log likelihood -1.060749
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      4
│     13
│     14
│     15
│     16
│     23
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 27, average log likelihood -1.077919
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│     13
│     14
│     15
│     16
│     19
│     20
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 28, average log likelihood -1.071899
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      1
│      4
│     12
│     13
│      ⋮
│     16
│     23
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 29, average log likelihood -1.045555
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      6
│      8
│     13
│     14
│     15
│     16
│     19
│     20
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 30, average log likelihood -1.067524
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      1
│      4
│     12
│     13
│      ⋮
│     24
│     28
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 31, average log likelihood -1.060844
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      7
│     13
│     14
│     15
│     16
│     19
│     20
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 32, average log likelihood -1.087301
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      1
│      4
│      6
│     12
│      ⋮
│     23
│     24
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 33, average log likelihood -1.064836
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│     13
│     14
│     15
│     16
│     19
│     20
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 34, average log likelihood -1.083464
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      1
│      4
│     12
│     13
│      ⋮
│     16
│     23
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 35, average log likelihood -1.052377
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      6
│     13
│     14
│     15
│      ⋮
│     20
│     28
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 36, average log likelihood -1.066551
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      4
│      8
│     12
│     13
│      ⋮
│     16
│     23
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 37, average log likelihood -1.062584
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      1
│     13
│     14
│     15
│     16
│     19
│     20
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 38, average log likelihood -1.083100
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      4
│      6
│     12
│     13
│      ⋮
│     23
│     24
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 39, average log likelihood -1.060504
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      1
│     13
│     14
│     15
│     16
│     19
│     20
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 40, average log likelihood -1.075922
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      4
│     12
│     13
│     14
│      ⋮
│     23
│     24
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 41, average log likelihood -1.052340
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      6
│      7
│     13
│     14
│      ⋮
│     19
│     20
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 42, average log likelihood -1.081313
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      1
│      4
│     13
│     14
│     15
│     16
│     23
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 43, average log likelihood -1.066161
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│     12
│     13
│     14
│     15
│     16
│     19
│     20
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 44, average log likelihood -1.069529
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      1
│      4
│      6
│     13
│      ⋮
│     23
│     24
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 45, average log likelihood -1.054927
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      8
│     12
│     13
│     14
│      ⋮
│     19
│     20
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 46, average log likelihood -1.063244
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      4
│     13
│     14
│     15
│     16
│     23
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 47, average log likelihood -1.079159
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      1
│      6
│     13
│     14
│      ⋮
│     19
│     20
│     28
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 48, average log likelihood -1.069775
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      4
│     12
│     13
│     14
│     15
│     16
│     23
│     24
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 49, average log likelihood -1.060050
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      1
│     13
│     14
│     15
│     16
│     19
│     20
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 50, average log likelihood -1.068517
┌ Info: EM with 100000 data points 50 iterations avll -1.068517
└ 59.0 data points per parameter
┌ Info: 5
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.154857171102143
│     -1.1524598338109036
│      ⋮
└     -1.0685169859436612
┌ Info: Total log likelihood: 
│   tll =
│    251-element Array{Float64,1}:
│     -1.4053661087331255
│     -1.4054404042744895
│     -1.4053601004171643
│     -1.4048170536197477
│      ⋮
│     -1.0697745937311112
│     -1.0600502605848852
└     -1.0685169859436612
32×26 Array{Float64,2}:
 -0.171342    -0.025206     0.00108253  -0.0280246   -0.0614326   -0.0177757     0.0100155   -0.0239867   -0.00988483  -0.265662     0.0465038    0.094607     -0.0551356     0.0220374    0.0970457    -0.0581943    0.0533693   -0.0570331    0.0938591   -0.0087539    0.259676    -0.0572905   -0.302856     0.12734     -0.0892089   -0.163237
  0.0057579    0.028384     0.0407057    0.0582306   -0.0554648    0.0350167     0.0337401   -0.176761    -0.0359102   -0.128538    -0.129087     0.050763      0.0353874    -0.0274952   -0.0103014    -0.0221993    0.013347    -0.04524     -0.0429387   -0.0361769    0.117065    -0.127783    -0.024675    -0.0290268    0.00142875  -0.160333
  0.00345664  -0.0319788   -0.00412102   0.0291537   -0.036504     0.0901097    -0.00441724   0.0324922   -0.0893941    0.01788     -0.00612927   0.13452       0.000280321   0.156962    -0.0493038    -0.0537607   -0.0378236    0.177346    -0.0465022   -0.101094     0.00393659  -0.043982     0.0641109   -0.00764155   0.0524911   -0.010511
 -0.14713     -0.0324804   -0.0344294    0.0340738    0.0982788    0.0314635    -0.035634     0.13683      0.0633672   -0.0560119    0.269604     0.0211835    -0.0267196     0.0608951    0.246422      0.0583242   -0.0715228   -0.0491015    0.0223953    0.0454063    0.109172    -0.0888892    0.171103    -0.0357098    0.103829     0.0199013
  0.122105    -0.0619305   -0.124034    -0.0402607    0.00908237   0.160472     -0.00213584  -0.113446     0.00960833  -0.0186312   -0.0283235    0.0384416    -0.0201635    -0.0994578    0.131079     -0.0234957    0.104912     0.0239893   -0.285377     0.0516259   -0.0139602   -0.0408605   -0.0129234   -0.215507    -0.0685646   -0.0483413
 -0.128114    -0.0622626    0.0346531   -0.105314     0.0208307    0.0356802    -0.13829     -0.0655989    0.0316096    0.122892    -0.149573    -0.000168109  -0.0642308     0.106124    -0.0156792    -0.0242405    0.221511     0.0837195    0.0320327    0.0145874   -0.086127     0.0207792    0.0996469    0.0560307   -0.0448833    0.0243402
  0.033027    -0.00393133   0.0807647    0.0259302   -0.0104018   -0.0291611    -0.0738329    0.0874147   -0.0794002   -0.0320629   -0.0140081    0.138333     -0.00225061   -0.00603549  -0.0109615     0.0447464   -0.0329294   -0.0371762    0.124037    -0.0138577    0.0370748   -0.0208525    0.0763995   -0.0117114    0.0172887    0.136403
 -0.0382237    0.072143     0.0819129    0.159087    -0.0160144    0.138356     -0.121388    -0.0615753    0.147791    -0.00521056  -0.0913547    0.1551        0.0569936     0.14306      0.124389      0.130744     0.00821976  -0.0127103    2.18122e-5   0.0119903   -0.0475611   -0.00404264  -0.0597363   -0.179993    -0.0782898   -0.0545773
  0.114127    -0.234454     0.0682086    0.0470268   -0.124999    -0.0145799    -0.0884793    0.106233    -0.161204     0.0134969    0.0288705   -0.082198     -0.0865525    -0.12016      0.00551028    0.00909397   0.090138    -0.178792     0.0631331   -0.00927882  -0.0206776   -0.0455542    0.0125211   -0.029494     0.0211502    0.0230963
  0.10351      0.0288635   -0.223054    -0.111036     0.0242853   -0.148078     -0.0376792    0.14877      0.112057    -0.0559784    0.117846    -0.128107     -0.0381754     0.0545901    0.0262334     0.0473196   -0.0605255   -0.215223     0.103437    -0.0162322    0.0947412    0.159535    -0.0383887   -0.0885174   -0.0382667    0.0335839
  0.178341     0.113029     0.065948     0.0297776   -0.0063056   -0.0265296    -0.0973835   -0.107456    -0.112347    -0.0750476   -0.14326      0.00238122   -0.128489     -0.0491991   -0.0337541     0.0239202    0.0512948    0.185068    -0.0572763   -0.0536632    0.00222619   0.150967    -0.00477135   0.0959116   -0.0446997    0.0472418
 -0.149456     0.0755443   -0.140076     0.116204    -0.0941487   -0.00168036   -0.0102403   -0.0577838   -0.0923956   -0.0542252    0.133184    -0.147418     -0.0792091    -0.0579937   -0.00245549   -0.0853709    0.0612518   -0.0411899    0.136158     0.129442    -0.0534255    0.0725718    0.133417    -0.0225363   -0.0803884    0.0195769
  0.107474     0.0627574    0.00311327  -0.0641752    0.00680759   0.101319     -0.138834     0.178336    -0.0442449   -0.01958     -0.328024    -0.0906467    -0.212411     -0.00669588  -0.0323335    -0.0626601    0.242568     0.0155776   -0.026793     0.00860153  -0.0447911    0.0720688   -0.0961887    0.2785      -0.172649    -0.0423862
  0.107923     0.0411508    0.0828483    0.127418    -0.16557      0.211492      0.228849     0.107421    -0.0707193   -0.0409172    0.317561     0.0387892    -0.208468     -0.00676822   0.0158712    -0.0379498   -0.0787215    0.0489563   -0.0872608   -0.0433608   -0.0290263    0.0849558   -0.165871     0.0696898   -0.192988    -0.0424807
 -0.145397     0.10765     -0.113332     0.222632     0.0684851    0.172138      0.011551    -0.153174    -0.136993    -0.0386982   -0.129077     0.0692677     0.00335702   -0.0536925    0.107696     -0.0801209   -0.374667     0.0986424   -0.106071     0.026304     0.0551385    0.0726486   -0.19791     -0.147666    -0.0557032    0.0660524
 -0.0947303    0.232293    -0.11876      0.0486531    0.130321     0.107919     -0.0805428   -0.143193    -0.147705    -0.0466473    0.00424128   0.0653892    -0.0048513     0.157083    -0.15886      -0.0268382    0.936365     0.0626343    0.00707392  -0.042434     0.0551463    0.0739664   -0.0511893   -0.144551    -0.0772822    0.0685406
  0.19817      0.0527517    0.168937    -0.0808341    0.0327361    0.101534     -0.112041     0.0018871    0.0043111    0.113184    -0.0135686   -0.0293146     0.0715194     0.190037     0.0519212    -1.11014     -0.0475843   -0.160932     0.109593    -0.0887829   -0.15895     -0.101809     0.0763881   -0.101811     0.129788    -0.0279453
  0.0701106    0.0627321    0.041388    -0.0328581    0.0406425   -0.107002     -0.154893    -0.0360706    0.0285205    0.0581803   -0.125242    -0.0459229     0.0576555     0.127032    -0.0358705     1.03356     -0.0702791    0.00367135   0.0808783    0.0281885    0.0295097   -0.102629     0.0608844    0.00220355   0.132435    -0.0301739
  0.20169     -0.01596     -0.0974059    0.115866    -0.155505    -0.0293232    -0.164811     0.0994164    0.115909    -0.00897516   0.198428     0.0279378     0.0441077    -0.037852    -0.394386      0.277471     0.0405963   -0.0713039   -0.0176356   -0.0494061    0.178535    -0.178177     0.192391    -0.596482    -0.0966568   -0.271645
  0.207675    -0.0866973   -0.125507    -0.232175    -0.0410086   -0.0155632     0.187829     0.0816056   -0.00635621  -0.00237363  -0.0552433   -0.0149173     0.0415862     0.304016    -0.0134716    -0.00234097   0.167097     0.0406524   -0.0177251   -0.0484903    0.0707871    0.515299     0.189175     0.402174    -0.0123998   -0.162679
  0.0963292    0.153124     0.0899927   -0.0807823   -0.0697761   -0.025616     -0.127439    -0.495968     0.0669403   -0.090249    -0.10711      0.188502      0.123113      0.0219457    0.000894277  -0.224729    -0.159085     0.0364739   -0.0680033    0.179001     0.0176591   -0.0925637   -0.0359994    0.273934    -0.10113      0.103603
  0.103829     0.151591     0.0370734   -0.132934     0.107047     0.0561509    -0.0750051    0.213663     0.0946969   -0.0940788   -0.16145      0.235509      0.151231      0.0282646   -0.0544649    -0.18655     -0.0478137    0.0305575   -0.059167    -0.127749     0.0344765   -0.100888    -0.0493694    0.266239    -0.106348     0.108444
  0.0761079    0.0344964    0.0338335    0.00739991  -0.136758     0.220042     -0.142363     0.348781     0.0786578   -0.150056     0.0120342   -0.13042      -0.0480202    -0.00832135   0.121091      0.0471525   -0.0117756   -0.0309308   -0.0645598    0.19607     -0.0417667    0.0388413    0.122071     0.0877896   -0.0464967    0.0616617
 -0.0725219    0.0560441   -0.0764245   -0.147831    -0.00734964   0.0216774     0.116058    -0.025353     0.0130101   -0.0904803    0.126472     0.10798       0.229208      0.0606445   -0.0549102    -0.0307976    0.111294    -0.0152354   -0.0177349   -0.0692612   -0.0829483    0.0434572    0.0477453    0.121467     0.00454132  -0.143902
 -0.737302    -0.0860766   -0.130298     0.026811    -0.153086     0.0372959     0.0258444    0.116761    -0.123511     0.235137     0.0601795   -0.159262      0.220667     -0.0154086   -0.03109      -0.223991    -0.0459206    0.161937     0.208844     0.0557627    0.221742    -0.117215     0.131156    -0.10997      0.0923379   -0.164804
  0.227092    -0.0422      -0.104832     0.0624396   -0.303153     0.0310154     0.0536572    0.0634661   -0.0854079    0.116756    -0.0972343   -0.119117      0.161004     -0.0840428   -0.0371237     0.0763429   -0.0120458    0.149879    -0.0839474   -0.0996063    0.23281     -0.130148     0.13279     -0.098869    -0.051331    -0.114071
  0.046411    -0.152983     0.0810974   -0.207474    -0.187758    -0.113329      0.0833712    0.0585847   -0.0448895    0.160381     0.105895    -0.0682433    -0.507034     -0.00403933   0.176929      0.350462    -0.122533     0.00674518   0.0492198   -0.041196    -0.0173453    0.103965     0.0488352    0.0776771    0.103292    -0.191301
 -0.0182768   -0.121489    -0.0492583   -0.0733495   -0.0178633    0.0418289     0.0517646    0.0569701    0.00752466   0.155524     0.294154    -0.00639234    0.517107      0.08393      0.0985171    -0.118454    -0.040622    -0.0120042    0.0432741    0.0399679   -0.0495474    0.107746     0.0493278    0.0733588    0.0603858   -0.0357174
 -0.0223469   -0.0847936    0.0297405   -0.0624838   -0.0680999    0.000360776  -0.0350456    0.106573    -0.0636243   -0.0830752   -0.17973     -0.0355002     0.000439743   0.0943959    0.0751395     0.0544709    0.128299    -0.110951     0.111713    -0.0334759   -0.0522371   -0.151242    -0.0545082    0.0286636   -0.0889157   -0.206749
  0.0697522   -0.0294148    0.141035    -0.0311396    0.0344322    0.0923279    -0.0282275    0.00216762  -0.0736434    0.0383039    0.0427946    0.132292     -0.0220693     0.0474071    0.00716172   -0.0696801    0.0335716    0.13381     -0.0540919    0.0445861    0.103101    -0.148163     0.0301436   -0.0083704   -0.0502791    0.00523037
 -0.0416924    0.0788812   -0.0395464   -0.0363613   -0.0230872    0.0307836     0.0720637   -0.0666537   -0.140097     0.0248662    0.0652593   -0.0248123     0.0586996     0.238516    -0.0129342    -0.274714    -0.10832      0.20645      0.0786256   -0.0650495   -0.0888025   -0.0405792    0.148361    -0.0876041   -0.0668309    0.00833994
 -0.0803587   -0.0677647    0.0992372   -0.0250116    0.0372405    0.0385844    -0.0315484   -0.0394655   -0.0280387   -0.0856256    0.00785279  -0.0565238     0.0875391    -0.0545936   -6.01403e-5    0.00282861   0.0200158   -0.0689969   -0.0164668   -0.0571391   -0.0430269    0.0726971    0.00222509   0.00412542   0.0145875    0.0186037[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
┌ Warning: Variances had to be floored 
│   ind =
│    12-element Array{Int64,1}:
│      4
│      6
│      8
│     12
│      ⋮
│     24
│     28
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 1, average log likelihood -1.044180
┌ Warning: Variances had to be floored 
│   ind =
│    15-element Array{Int64,1}:
│      1
│      4
│      6
│      8
│      ⋮
│     24
│     28
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 2, average log likelihood -1.037447
┌ Warning: Variances had to be floored 
│   ind =
│    12-element Array{Int64,1}:
│      4
│      6
│      8
│     12
│      ⋮
│     24
│     28
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 3, average log likelihood -1.043956
┌ Warning: Variances had to be floored 
│   ind =
│    15-element Array{Int64,1}:
│      1
│      4
│      6
│      8
│      ⋮
│     24
│     28
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.037314
┌ Warning: Variances had to be floored 
│   ind =
│    12-element Array{Int64,1}:
│      4
│      6
│      8
│     12
│      ⋮
│     24
│     28
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.043945
┌ Warning: Variances had to be floored 
│   ind =
│    15-element Array{Int64,1}:
│      1
│      4
│      6
│      8
│      ⋮
│     24
│     28
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.037298
┌ Warning: Variances had to be floored 
│   ind =
│    12-element Array{Int64,1}:
│      4
│      6
│      8
│     12
│      ⋮
│     24
│     28
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.043943
┌ Warning: Variances had to be floored 
│   ind =
│    15-element Array{Int64,1}:
│      1
│      4
│      6
│      8
│      ⋮
│     24
│     28
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.037296
┌ Warning: Variances had to be floored 
│   ind =
│    12-element Array{Int64,1}:
│      4
│      6
│      8
│     12
│      ⋮
│     24
│     28
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.043942
┌ Warning: Variances had to be floored 
│   ind =
│    15-element Array{Int64,1}:
│      1
│      4
│      6
│      8
│      ⋮
│     24
│     28
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.037296
┌ Info: EM with 100000 data points 10 iterations avll -1.037296
└ 59.0 data points per parameter
kind diag, method kmeans
[ Info: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.483435e+05
      1       6.761996e+05      -1.721439e+05 |       32
      2       6.435492e+05      -3.265041e+04 |       32
      3       6.259198e+05      -1.762943e+04 |       32
      4       6.159473e+05      -9.972440e+03 |       32
      5       6.108627e+05      -5.084659e+03 |       32
      6       6.088400e+05      -2.022652e+03 |       32
      7       6.077461e+05      -1.093912e+03 |       32
      8       6.067969e+05      -9.491534e+02 |       32
      9       6.057951e+05      -1.001887e+03 |       32
     10       6.047055e+05      -1.089598e+03 |       32
     11       6.034580e+05      -1.247461e+03 |       32
     12       6.022331e+05      -1.224846e+03 |       32
     13       6.014004e+05      -8.327248e+02 |       32
     14       6.007880e+05      -6.123849e+02 |       32
     15       6.001780e+05      -6.099927e+02 |       32
     16       5.996092e+05      -5.688736e+02 |       32
     17       5.991269e+05      -4.822964e+02 |       32
     18       5.987922e+05      -3.346478e+02 |       32
     19       5.986066e+05      -1.856192e+02 |       32
     20       5.985079e+05      -9.875221e+01 |       31
     21       5.984505e+05      -5.731788e+01 |       32
     22       5.984108e+05      -3.969206e+01 |       32
     23       5.983865e+05      -2.435704e+01 |       31
     24       5.983711e+05      -1.542215e+01 |       31
     25       5.983615e+05      -9.531956e+00 |       29
     26       5.983546e+05      -6.928306e+00 |       26
     27       5.983468e+05      -7.811012e+00 |       28
     28       5.983390e+05      -7.825919e+00 |       29
     29       5.983316e+05      -7.355710e+00 |       27
     30       5.983265e+05      -5.154267e+00 |       27
     31       5.983228e+05      -3.694912e+00 |       25
     32       5.983188e+05      -3.938413e+00 |       25
     33       5.983148e+05      -4.052042e+00 |       22
     34       5.983123e+05      -2.492676e+00 |       22
     35       5.983105e+05      -1.786742e+00 |       19
     36       5.983091e+05      -1.359205e+00 |       14
     37       5.983080e+05      -1.179955e+00 |       19
     38       5.983067e+05      -1.249463e+00 |       18
     39       5.983058e+05      -9.156940e-01 |       13
     40       5.983050e+05      -7.754715e-01 |       13
     41       5.983044e+05      -5.784879e-01 |        8
     42       5.983040e+05      -4.200134e-01 |       10
     43       5.983033e+05      -6.722020e-01 |       16
     44       5.983023e+05      -1.083788e+00 |       16
     45       5.983017e+05      -5.707147e-01 |       12
     46       5.983012e+05      -4.805098e-01 |       15
     47       5.983001e+05      -1.088414e+00 |       19
     48       5.982987e+05      -1.430487e+00 |       22
     49       5.982970e+05      -1.639382e+00 |       22
     50       5.982952e+05      -1.870369e+00 |       21
K-means terminated without convergence after 50 iterations (objv = 598295.1792678884)
┌ Info: K-means with 32000 data points using 50 iterations
└ 37.0 data points per parameter
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.315728
[ Info: iteration 2, average log likelihood -1.280906
[ Info: iteration 3, average log likelihood -1.239062
[ Info: iteration 4, average log likelihood -1.188191
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     11
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -1.134158
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     24
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.116773
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.084201
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      7
│      9
│     11
│     17
│     23
│     27
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.030796
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     22
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -1.095808
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     10
│     24
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.058215
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      6
│     17
│     27
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 11, average log likelihood -1.048940
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      9
│     11
│     22
│     23
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 12, average log likelihood -1.059040
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      7
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 13, average log likelihood -1.084469
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     24
│     27
│     30
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 14, average log likelihood -1.044648
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      3
│     10
│     11
│     17
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 15, average log likelihood -1.042883
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      6
│      9
│     23
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 16, average log likelihood -1.079279
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 17, average log likelihood -1.068113
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      7
│     11
│     24
│     27
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 18, average log likelihood -1.039076
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     17
│     22
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 19, average log likelihood -1.068857
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     10
│     23
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 20, average log likelihood -1.046531
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      6
│      9
│     24
│     29
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 21, average log likelihood -1.048031
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      3
│      7
│     11
│     27
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 22, average log likelihood -1.046506
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     17
│     22
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 23, average log likelihood -1.082380
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     23
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 24, average log likelihood -1.066302
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      9
│     10
│     24
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 25, average log likelihood -1.025274
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      6
│      7
│     11
│     17
│     22
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 26, average log likelihood -1.044973
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      3
│     25
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 27, average log likelihood -1.097662
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 28, average log likelihood -1.070879
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│     11
│     22
│     23
│     24
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 29, average log likelihood -1.008965
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      6
│      7
│      9
│     10
│     17
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 30, average log likelihood -1.055452
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 31, average log likelihood -1.095487
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     11
│     24
│     29
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 32, average log likelihood -1.039988
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     22
│     23
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 33, average log likelihood -1.043678
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      3
│     17
│     25
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 34, average log likelihood -1.048868
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      6
│      7
│     10
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 35, average log likelihood -1.058797
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      9
│     11
│     22
│     24
│     27
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 36, average log likelihood -1.035815
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     23
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 37, average log likelihood -1.079481
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     17
│     25
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 38, average log likelihood -1.050359
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      7
│     11
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 39, average log likelihood -1.053247
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      6
│      9
│     22
│     24
│     27
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 40, average log likelihood -1.024369
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      3
│     17
│     23
│     25
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 41, average log likelihood -1.058350
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     29
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 42, average log likelihood -1.073672
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      7
│     11
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 43, average log likelihood -1.043851
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      9
│     22
│     23
│     24
│     25
│     27
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 44, average log likelihood -1.017740
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      6
│     17
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 45, average log likelihood -1.081927
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     10
│     11
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 46, average log likelihood -1.056473
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      3
│      7
│     22
│     24
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 47, average log likelihood -1.043708
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      9
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 48, average log likelihood -1.057376
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│     17
│     23
│     25
│     30
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 49, average log likelihood -1.030736
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      6
│     10
│     11
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 50, average log likelihood -1.062603
┌ Info: EM with 100000 data points 50 iterations avll -1.062603
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
  0.031852    -0.180071     0.189576     0.110701      0.0764056    0.140024      0.0419919    -0.000555037  -0.0112919    0.117035     0.0604933    0.0236382    0.0256471    -0.0130541    0.145298    -0.00337052   -0.0608668    0.0869736    0.0742407    0.109649      0.141852    -0.15585     -0.0519887   -0.0205817   -0.0396269   -0.00802347
  0.101276     0.0283118   -0.223611    -0.111834      0.0244445   -0.150308     -0.0377565     0.147279      0.113825    -0.056007     0.117939    -0.128298    -0.0376809     0.0556408    0.0264401    0.0480373    -0.0604051   -0.215445     0.103698    -0.01519       0.0949131    0.157711    -0.0376116   -0.0880288   -0.037812     0.032846
 -0.0680642   -0.0984556    0.0631536   -0.066471      0.109182    -0.00143903    0.0162536     0.0465186    -0.143446    -0.0428856   -0.22851     -0.0466366    0.054828     -0.0106965    0.0530612    0.0330724     0.0830202   -0.285218     0.151123    -0.0322858    -0.0645358   -0.207466    -0.046501     0.0073889   -0.118778    -0.20556
 -0.127797    -0.225569     0.148965     0.00690077    0.087385     0.0382806     0.042741      0.0805092    -0.0695367   -0.124599     0.0473153   -0.015852     0.0574631    -0.0348523    0.0370876    0.000544808   0.0144434   -0.066948    -0.0716047   -0.073929      0.0640235    0.0293601    0.0285192   -0.074599    -0.0573441   -0.0341519
  0.13922     -0.201015     0.120296    -0.0244936    -0.245993    -0.0240179     0.0426351     0.102017     -0.141848    -0.138427     0.0775936   -0.0600675   -0.0360871    -0.0885675   -0.154549     0.01444       0.117469    -0.0927674    0.124114    -0.0828314    -0.0752783   -0.175043     0.0700778   -0.159733    -0.168277     0.0184116
  0.0687326   -0.26035      0.0188878    0.125144     -0.0266142   -0.00812464   -0.226118      0.113022     -0.184363     0.144575    -0.00449865  -0.101162    -0.149798     -0.139718     0.146357    -0.000530502   0.0812573   -0.238758     0.00223609   0.0605074     0.0255388    0.0706814   -0.0358914    0.0949813    0.186411     0.0253853
 -0.118749     0.162775    -0.11928      0.129965      0.0980953    0.13467      -0.0466379    -0.145752     -0.138251    -0.0418689   -0.0714244    0.0647462   -0.00446485    0.0455614   -0.0236163   -0.0512622     0.247555     0.0787617   -0.0423485   -0.00543005    0.0528898    0.0665153   -0.128571    -0.140475    -0.0713314    0.0632233
 -0.00664809  -0.032832    -0.00665864   0.0273079    -0.0300808    0.0892803    -0.00188763    0.0328838    -0.0891449    0.0154436   -0.00583682   0.128719    -0.000414782   0.160691    -0.0450392   -0.0522223    -0.0401801    0.177719    -0.0478988   -0.0995162     0.0137354   -0.0429638    0.0732963   -0.00797638   0.056123    -0.0116275
 -0.071855     0.0427701   -0.066345    -0.131955     -0.0161352   -0.000903622   0.111677      0.0318727     0.00952978  -0.0957611    0.107813     0.0722784    0.193891      0.0552213   -0.03856     -0.02814       0.110792    -0.0141133   -0.015399    -0.0574272    -0.0725771    0.0334671    0.0464684    0.109144    -0.00896471  -0.141784
  0.107472     0.0523932    0.0416004    0.0291248    -0.0758091    0.155268      0.0408389     0.143781     -0.0571222   -0.0315474   -0.0147119   -0.0274095   -0.210366     -0.00656662  -0.00831677  -0.0505806     0.0874958    0.030745    -0.0554631   -0.0170214    -0.0372471    0.0777741   -0.129979     0.177206    -0.182244    -0.0419681
  0.204243    -0.0549659   -0.115043    -0.084616     -0.0872844   -0.0206664     0.0356351     0.0881802     0.040376    -0.00555386   0.0537018    0.00307585   0.0398221     0.156143    -0.177597     0.118264      0.113724    -0.00340687  -0.0176542   -0.0491888     0.116002     0.213777     0.190944    -0.0258978   -0.0492485   -0.212771
  0.0129303   -0.137324     0.0143382   -0.139893     -0.101315    -0.0333205     0.0675221     0.0579999    -0.0181162    0.157968     0.202032    -0.0368448    0.0158263     0.0408377    0.136807     0.11078      -0.0799242   -0.00323759   0.0460253   -0.000149804  -0.0341015    0.105697     0.0490794    0.0756804    0.0811744   -0.112709
  0.00506839   0.0289245    0.0440118    0.0533244    -0.0365316    0.0385223     0.0625647    -0.182405     -0.0303217   -0.126933    -0.137329     0.0234317    0.0164274    -0.0211527   -0.0129608   -0.0186821     0.0265799   -0.0489351   -0.0531353   -0.0445525     0.129495    -0.136721    -0.0275445   -0.0270255    0.00218258  -0.176417
  0.0733799   -0.0051291    0.100482     0.0752898    -0.100644    -0.00592197   -0.153383     -0.0142633    -0.0490735    0.0251017   -0.00276039   0.270439     0.218923     -0.0998918    0.112234    -0.0619906    -0.143501    -0.0286621    0.138547     0.061434     -0.0717982   -0.0461537   -0.0644994   -0.0250482   -0.0202592    0.111607
 -0.12488      0.0383543   -0.0542028   -0.0334541    -0.0297952    0.0453382    -0.0521271    -0.164308     -0.0649402   -0.0472068   -0.00723426  -0.194161     0.132296      0.025809     0.176161     0.13575       0.05452     -0.206496     0.0401102   -0.143188     -0.0337762    0.194289     0.0803188    0.017503     0.0896536   -0.0454398
  0.120054     0.135195     0.0906709   -0.178463      0.0168159    0.0432896    -0.115178      0.0102462    -0.145111    -0.0333245   -0.021086     0.263047    -0.115428      0.130778    -0.116373    -0.149349      0.10348      0.219324    -0.186656    -0.0355312     0.11672     -0.163276     0.123928     0.0162636   -0.041921    -0.0194989
 -0.155208     0.0595765   -0.159607     0.118718     -0.137478    -0.0225594     0.00147343   -0.0640699    -0.090567    -0.0301242    0.147601    -0.152245    -0.0774227    -0.0539581   -0.00260241  -0.103392      0.0763719   -0.0297648    0.165821     0.132381     -0.0610551    0.0590487    0.155436    -0.0405853   -0.0843879    0.0146892
 -0.0415148    0.0823341   -0.040542    -0.0361288    -0.0246271    0.0334053     0.072278     -0.0679118    -0.140318     0.0266871    0.0657664   -0.0250106    0.0592002     0.23866     -0.0131415   -0.273242     -0.107347     0.206567     0.0787539   -0.0672029    -0.0878174   -0.0408077    0.147986    -0.0870955   -0.0667802    0.00655135
  0.0409362    0.0349447    0.10602     -0.000542454   0.0615417    0.0531734     0.00277107   -0.155027      0.0488052   -0.0317831    0.0679111   -0.0216988    0.12797      -0.0863382   -0.0339692   -0.15888       0.00306577  -0.0350771    0.0262015   -0.0728491    -0.0827443    0.0531806   -0.113688     0.0975818    0.101128     0.245963
  0.100367     0.150996     0.0603144   -0.105597      0.0288368    0.0179082    -0.101949     -0.114072      0.081046    -0.0924354   -0.132069     0.209949     0.138472      0.0264647   -0.0262879   -0.201719     -0.0993177    0.0354019   -0.0609142    0.0136663     0.0286541   -0.0969933   -0.0477061    0.269983    -0.103737     0.101864
  0.132645     0.057561     0.106352    -0.0576478     0.0367107   -0.00489156   -0.133478     -0.0185536     0.0160367    0.0867118   -0.0683446   -0.0381688    0.0647553     0.157785     0.00669075  -0.0345616    -0.0577445   -0.0799145    0.0949786   -0.0304881    -0.0629297   -0.102198     0.0683114   -0.0499895    0.131111    -0.0285972
  0.0663603    0.0366201    0.0231974   -0.00485779   -0.131208     0.23908      -0.129905      0.322194      0.0732752   -0.144952     0.0207496   -0.107397    -0.0340112    -0.00332475   0.109332     0.0404803    -0.00353467  -0.0197868   -0.064998     0.172791     -0.0507956    0.0377184    0.115776     0.0896609   -0.0415481    0.0426464
 -0.0413022   -0.0976478    0.0390229   -0.0634726    -0.147597    -0.0173223    -0.0576071     0.154934     -0.0866926   -0.0934208   -0.20849     -0.0311219   -0.00136781    0.137495     0.0701921    0.0340539     0.146746    -0.199839     0.117705    -0.0245322    -0.0403957   -0.14672     -0.081555     0.0484041   -0.0969271   -0.283041
  0.0034731   -0.00211343   0.0601218   -0.00827309    0.029989    -0.0375732    -0.0296423     0.144222     -0.0879835   -0.0774      -0.0213079    0.0542959   -0.151667      0.0573484   -0.0862437    0.116447      0.0311819   -0.0405167    0.11533     -0.0610007     0.105835    -0.0052421    0.165459    -0.0126999    0.039589     0.135346
 -0.171347    -0.0452851   -0.0465497   -0.0415326    -0.06098     -0.00890248    0.0438317    -0.0201518    -0.0100267   -0.185895     0.0666604    0.0883742   -0.0592191     0.0560255    0.088176    -0.0293227     0.0260897   -0.0574532    0.0805269    0.0105686     0.278652    -0.0593801   -0.351079     0.0940415   -0.0431734   -0.117364
 -0.268783    -0.0668617   -0.118366     0.0455072    -0.226476     0.0344089     0.0391139     0.0922158    -0.104926     0.17853     -0.0165441   -0.140025     0.190695     -0.0489625   -0.0333977   -0.0783313    -0.029869     0.156255     0.0685878   -0.0185917     0.228786    -0.123393     0.132148    -0.105014     0.0233764   -0.14068
 -0.0483574    0.071904     0.0817203    0.167312     -0.0125975    0.138066     -0.11776      -0.0648789     0.158765    -0.0037903   -0.0950556    0.150325     0.0522298     0.146011     0.122338     0.131505      0.0143885   -0.0121358   -0.0118324    0.0115145    -0.0461804   -0.00372828  -0.0560847   -0.177725    -0.0822266   -0.0546583
  0.122968    -0.0618285   -0.127695    -0.040152      0.00911469   0.159434     -0.000918275  -0.1132        0.0103347   -0.0161258   -0.0294795    0.0375707   -0.0207156    -0.0975778    0.130045    -0.0238553     0.109032     0.0279642   -0.287155     0.0503337    -0.0165111   -0.0430063   -0.0133131   -0.215678    -0.0663334   -0.048326
 -0.0211409    0.105679     0.134       -0.144598     -0.0511313    0.048253     -0.198455     -0.0665661     0.0383003   -0.10609     -0.155041    -0.0190339    0.00699445   -0.110299    -0.202802     0.0533366     0.0921318    0.0560009    0.0228672    0.132251     -0.246015     0.0256547   -0.0581432    0.0828494   -0.0368084   -0.0488764
 -0.153691    -0.0342606   -0.0869854    0.0686173     0.157063     0.0343799    -0.0334279     0.182747      0.054298    -0.0723516    0.310746     0.00551209  -0.0291297     0.068616     0.22899      0.0490903    -0.153193    -0.0659248   -0.00649039   0.0387051     0.10886     -0.0934989    0.365862    -0.0266478    0.052624    -0.00274479
  0.174436     0.113941     0.0567468    0.0310473    -0.00134623  -0.0290469    -0.0951936    -0.108389     -0.114588    -0.0715688   -0.140715    -0.00595024  -0.131929     -0.0536679   -0.0382257    0.0246672     0.0539314    0.187411    -0.0561408   -0.0513159     0.00118073   0.152065     0.00344608   0.0980276   -0.0535716    0.0472448
 -0.145729    -0.0914681    0.123317    -0.116842      0.0418133    0.0289027    -0.195383     -0.0703473     0.0523722    0.107437    -0.145709    -0.0120402   -0.0674363     0.121263     0.0410693   -0.0101823     0.383881     0.141048     0.0323926    0.0237716    -0.123234     0.0182128    0.141902     0.0440786   -0.0293608    0.056094[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      7
│     22
│     24
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 1, average log likelihood -1.050689
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      7
│      9
│     22
│     24
│     27
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 2, average log likelihood -1.008334
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      7
│      9
│     11
│     17
│      ⋮
│     27
│     30
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 3, average log likelihood -0.984101
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      3
│      7
│     22
│     24
│     27
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 4, average log likelihood -1.021639
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      6
│      7
│      9
│     10
│     17
│     22
│     24
│     27
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 5, average log likelihood -0.991489
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      7
│     11
│     22
│     23
│      ⋮
│     27
│     29
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 6, average log likelihood -1.000094
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      3
│      7
│      9
│     22
│     24
│     27
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 7, average log likelihood -1.021619
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      7
│     17
│     22
│     24
│     25
│     27
│     29
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 8, average log likelihood -1.007874
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      3
│      7
│      9
│     10
│      ⋮
│     24
│     27
│     30
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 9, average log likelihood -0.988975
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      6
│      7
│      9
│     17
│      ⋮
│     27
│     29
│     32
└ @ GaussianMixtures ~/.julia/packages/GaussianMixtures/RGtTJ/src/train.jl:255
[ Info: iteration 10, average log likelihood -1.011893
┌ Info: EM with 100000 data points 10 iterations avll -1.011893
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.0993206   -0.0223218    0.0287697    0.0693471    -0.0720584   -0.0820773   -0.0396588    0.181228    0.0249053   -0.110726     0.126907    -0.297973     0.0103977   -0.129106      0.214433     0.0278398   -0.0655567   -0.148265    0.130089     0.0485661    0.109832     0.10936      0.00436227  -0.107243    -0.0806704   -0.0212177
  0.017573     0.0197248   -0.0419934    0.115955      0.0574747   -0.094338     0.0902919   -0.107671    0.105284     0.105416    -0.0173888    0.119572    -0.16279      0.103147     -0.121685     0.0191316   -0.0735371   -0.017136   -0.242469    -0.0160704   -0.0782692    0.0605623    0.112179     0.0199219   -0.0549721   -0.0623969
 -0.182865    -0.00233338   0.0700635   -0.0632834    -0.01736      0.0398914    0.00574513   0.135241   -0.00285335   0.189058    -0.0742416    0.106117     0.0568848    0.0951031    -0.0619817    0.0362525   -0.0527854   -0.0274233  -0.0767302    0.0272827    0.0717326    0.0363986   -0.149528    -0.087366    -0.0468351    0.0628529
  0.133694     0.0399897    0.0588871   -0.0551384     0.203538     0.0821469   -0.18099      0.0178793   0.168938    -0.00962196  -0.177811    -0.031118    -0.0349023   -0.0698766     0.0552332   -0.089909     0.0151878    0.214484   -0.0163664   -0.0120437    0.0853043    0.132775     0.0105289    0.061069     0.0774991   -0.0784485
 -0.0479524    0.0303837   -0.0935901   -0.269195      0.00566017  -0.0809566    0.11924     -0.0675979   0.056004    -0.0281223    0.0350806   -0.0985271    0.0949801    0.0621391     0.0287774    0.0236418   -0.023188    -0.0249877  -0.0810018   -0.0249394   -0.0126548   -0.00836707   0.137986    -0.136699    -0.1213       0.0700746
  0.178493     0.084455    -0.130435    -0.0825105    -0.0516805   -0.0920605    0.0762971    0.05665     0.0542476    0.179562    -0.01813     -0.024924     0.0316205   -0.0960757     0.186411     0.0420627    0.025749     0.0186077   0.113753    -0.0365431    0.149009     0.031057     0.0581296   -0.191321     0.0264299    0.0687099
  0.0483207   -0.0734947    0.0218458   -0.0535493     0.116557     0.113584    -0.0231528   -0.0846751   0.00363949  -0.0370481    0.0376382   -0.0258706    0.0623281   -0.094148     -0.0453282    0.0209053   -0.0343386   -0.116504    0.0423076    0.00922427  -0.0832809   -0.0636246   -0.0451059   -0.223445     0.0388207   -0.0875616
 -0.0884149   -0.138962     0.0459412   -0.074472      0.0755546    0.00743064   0.149973     0.0442774  -0.0304744   -0.102527     0.0372489    0.0682447    0.0403179    0.176475      0.0398146    0.18766     -0.00252124   0.0530725   0.0731803    0.0785393    0.0761473   -0.109935    -0.049412     0.0394934    0.0226182   -0.0311176
  0.0342585   -0.193663    -0.020041     0.0958502    -0.12784     -0.00749253   0.00871999  -0.118328   -0.204817     0.039621     0.0147026   -0.0496816   -0.0484616   -0.108068      0.181546     0.0160063    0.105038    -0.0958611   0.077385     0.176617    -0.0577789   -0.109838    -0.148531     0.0153472    0.18916      0.0274659
 -0.122915    -0.0957102   -0.107358     0.039937     -0.013284    -0.00211508   0.0342663    0.0412219   0.110584    -0.292312     0.159797     0.121727    -0.00796308  -0.000856165  -0.0161155   -0.0670595   -0.0261498   -0.0138828  -0.0142242    0.166661    -0.0624529   -0.148815     0.0655511   -0.0247069    0.0830967   -0.0937081
  0.0849458   -0.00441321  -0.105639     0.042721      0.0687651   -0.00768764  -0.176883     0.0209308  -0.149014     0.0919865   -0.0596425   -0.104334    -0.013226    -0.0719017    -0.0256164   -0.0489128   -0.0807147   -0.19112     0.0628591    0.0487447    0.120042    -0.0946714   -0.0926513    0.0820633    0.111247    -0.0833644
 -0.0800136   -0.0155938   -0.0977276    0.0082487    -0.132313     0.0238851    0.0146076   -0.143209    0.078416    -0.166572     0.0128639   -0.0073133   -0.0898584    0.00144736   -0.00802096   0.132712     0.00184965  -0.082701   -0.00365591   0.00563351  -0.121449    -0.0533119    0.142976     0.1147       0.00221722   0.0194358
 -0.161125    -0.19024      0.0456266    0.0292953     0.0857772   -0.106828    -0.028254    -0.0484173   0.060438     0.081186    -0.17085      0.0594464    0.105555    -0.186583      0.231434     0.14937      0.0762815    0.0201084  -0.109521    -0.0831673    0.045455    -0.128918     0.0273137   -0.0803451   -0.0428531    0.148918
 -0.0186327    0.0237681    0.0498614   -0.00590111    0.117527     0.0582783   -0.0458023   -0.0811305  -0.0272124   -0.146064    -0.103468    -0.00478604   0.0168136    0.0982206    -0.0404089   -0.0656242    0.219192    -0.0265246   0.0424991    0.0501638   -0.0252401   -0.0751049    0.00932199   0.0629869   -0.105793     6.00221e-5
  0.0114444   -0.0511995   -0.0221608   -0.129202     -0.0129529   -0.0421386    0.131474    -0.0594505   0.0746557    0.0377121    0.0450713    0.0660965    0.157866     0.106055     -0.0437717   -0.0985072    0.0373991    0.0649219   0.00771054   0.0165196   -0.163514     0.149322    -0.0260203   -0.120603     0.19109     -0.0747513
  0.166184    -0.185923     0.0490253    0.0642145     0.060877    -0.06368     -0.0197942   -0.0780516   0.125803    -0.0169065   -0.00611725   0.212807    -0.0481119    0.00478675   -0.070842     0.151238    -0.139804    -0.0550911   0.144865    -0.133979     0.101809     0.0298421    0.169109     0.00768722   0.0513861    0.096281
 -0.0873431   -0.112917    -0.0565362    0.0810287     0.16671     -0.145946     0.0541692    0.0973744  -0.0586971    0.00851932  -0.226788     0.100775     0.201318     0.0766475    -0.0678406    0.0172942   -0.106507     0.035944    0.0709512   -0.141514     0.0836526    0.142231     0.106748    -0.115788     0.110809     0.190161
 -0.00994842  -0.03575     -0.0616354   -0.0895551     0.111204    -0.0955236    0.0794924    0.0234278   0.053036    -0.0594561   -0.135592     0.0526054   -0.0235376   -2.87979e-5   -0.0337006    0.00090498   0.0174899   -0.0439457   0.142116     0.00947225   0.146481     0.126117     0.215352    -0.12505      0.124655     0.0886367
 -0.0864555   -0.115907     0.0582937   -0.043581      0.104329     0.191265    -0.115208    -0.164894    0.0289961    0.125625    -0.257677    -0.20436      0.0868521   -0.0853806     0.0431286   -0.00840453   0.0201556    0.130052   -0.0775763    0.0861495   -0.143756     0.0264693    0.0561943   -0.185073     0.0685728   -0.0340087
 -0.00813062   0.213319     0.0167746   -0.183285     -0.0421698    0.193611    -0.0382086    0.145408   -0.117554     0.0603027   -0.0703652    0.189115     0.0538114    0.144196      0.154148     0.0227561   -0.0885655   -0.035491   -0.050315     0.102885     0.113124     0.00591745  -0.0434628    0.129206    -0.00662601  -0.0418337
  0.143951    -0.083854    -0.0876479   -0.230535     -0.00635708   0.0954582   -0.0858694   -0.203051    0.180832    -0.130337     0.168267     0.00386504  -0.0279396    0.0578735     0.0463053    0.0968634   -0.111353     0.100807    0.0929842   -0.0399208    0.061618     0.135985     0.00934023   0.100671     0.0608893   -0.0670914
  0.0581116    0.0704515    0.12889      0.0929278    -0.0408157    0.0163585   -0.0530057    0.0898829  -0.0192907    0.0264756   -0.0854005    0.0418018    0.0838761   -0.0708417     0.167095     0.0226956   -0.0502452   -0.135707   -0.0798073    0.0745155    0.00930048  -0.112324    -0.0744241   -0.0645409   -0.0849743   -0.0484989
  0.136554     0.00404176  -0.00592291  -0.000965174   0.120698     0.0983069   -0.0112597   -0.0214436   0.127985    -0.280918    -0.017533    -0.0989057    0.0993782   -0.156805      0.0424398   -0.027664    -0.0298811   -0.1598      0.078463    -0.166901     0.190466     0.0287372   -0.128121     0.0634748   -0.133551    -0.199187
  0.00719225  -0.0436629   -0.0175297   -0.101023      0.0153626    0.092303    -0.0460676    0.0574942   0.209709    -0.0211569   -0.0797884    0.299044     0.105931     0.10866       0.108547    -0.0364781    0.217194    -0.0158314   0.0839331    0.0230324   -0.0375022    0.114206    -0.0199873   -0.165496    -0.00161661   0.0833983
  0.0262816   -0.108376     0.0849898   -0.171203     -0.0033753    0.0777857   -0.0301505   -0.0872202   0.0197483   -0.0303389    0.0552128    0.0583822    0.00883327   0.0313083     0.0136159    0.0509385    0.0603581    0.044511   -0.1585       0.00400425  -0.038865    -0.0490312   -0.106949    -0.0385838   -0.0527164    0.120725
 -0.096594    -0.0306466    0.0619941   -0.0758573     0.0209458    0.0917223    0.0403697    0.141218   -0.0400639    0.201528    -0.0266339   -0.0013349    0.093233    -0.062868     -0.0410319    0.0119581   -0.0325222    0.0830342   0.134147    -0.0503026   -0.0637553    0.0276357   -0.193602    -0.0520458   -0.0210118   -0.0221713
  0.00391948   0.00521489  -0.0336189    0.0415135     0.0151209    0.0189134    0.183613     0.0953867   0.0971401    0.140849    -0.0584921    0.0653967    0.183964    -0.0971994     0.0948299    0.115606     0.0845942    0.014748   -0.129816     0.0456285    0.0408984   -0.0830413   -0.118094    -0.127233     0.0806187   -0.0840653
  0.088124     0.00118972   0.0612249    0.0205125     0.0669368   -0.0503611   -0.148454    -0.0800429   0.00271859   0.0984099   -0.113709    -0.0439366    0.163973    -0.033194      0.107892     0.00229366   0.0592707   -0.109693   -0.00239488  -0.0189671    0.00474307   0.0912217    0.1283       0.204217    -0.143557    -0.0906807
 -0.0628116    0.0743748    0.12523      0.0617279     0.0435745   -0.021789    -0.0322482   -0.0881882   0.0387498   -0.0969207   -0.0765432   -0.135866     0.0803394   -0.0546045     0.212103    -0.0784663    0.0872302    0.120269    0.0461513   -0.0986568   -0.0608109   -0.103826    -0.0634379    0.0941146   -0.130257    -0.0681657
  0.0556346    0.0120616    0.0611484   -0.205767     -0.0834782   -0.0700821    0.0240069    0.018153    0.0950817    0.070763    -0.171901    -0.220614     0.0584424   -0.115525     -0.0173428    0.0756192   -0.0227925    0.0424417  -0.0730893   -0.0269696   -0.102757     0.181582    -0.0211676    0.0273638   -0.0042332   -0.105766
 -0.0120855   -0.0308543    0.0278274    0.0178162     0.0146176   -0.0806036   -0.213146    -0.0496838   0.0514132   -0.0274857    0.00743173   0.101345    -0.0729608   -0.0262072     0.142596     0.0278851   -0.0783228    0.087444   -0.0450088    0.10778     -0.0631138   -0.0800837   -0.0243037   -0.0105573   -0.0493379   -0.113472
  0.0762773   -0.00239537   0.0215816    0.0422784     0.0643983    0.0234112   -0.0822871    0.0927633  -0.0165315   -0.0741819   -0.0786853   -0.203156    -0.0339624    0.0821925     0.197707     0.0135929   -0.0946612   -0.0157183  -0.0565804   -0.0809815    0.173164    -0.0589266   -0.0794087   -0.23234     -0.00571618  -0.156341kind full, method split
┌ Info: 0: avll = 
└   tll[1] = -1.4226803785423
[ Info: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.422698
[ Info: iteration 2, average log likelihood -1.422636
[ Info: iteration 3, average log likelihood -1.422591
[ Info: iteration 4, average log likelihood -1.422541
[ Info: iteration 5, average log likelihood -1.422483
[ Info: iteration 6, average log likelihood -1.422413
[ Info: iteration 7, average log likelihood -1.422326
[ Info: iteration 8, average log likelihood -1.422199
[ Info: iteration 9, average log likelihood -1.421976
[ Info: iteration 10, average log likelihood -1.421540
[ Info: iteration 11, average log likelihood -1.420753
[ Info: iteration 12, average log likelihood -1.419646
[ Info: iteration 13, average log likelihood -1.418574
[ Info: iteration 14, average log likelihood -1.417879
[ Info: iteration 15, average log likelihood -1.417546
[ Info: iteration 16, average log likelihood -1.417410
[ Info: iteration 17, average log likelihood -1.417356
[ Info: iteration 18, average log likelihood -1.417334
[ Info: iteration 19, average log likelihood -1.417325
[ Info: iteration 20, average log likelihood -1.417321
[ Info: iteration 21, average log likelihood -1.417319
[ Info: iteration 22, average log likelihood -1.417317
[ Info: iteration 23, average log likelihood -1.417317
[ Info: iteration 24, average log likelihood -1.417316
[ Info: iteration 25, average log likelihood -1.417316
[ Info: iteration 26, average log likelihood -1.417316
[ Info: iteration 27, average log likelihood -1.417315
[ Info: iteration 28, average log likelihood -1.417315
[ Info: iteration 29, average log likelihood -1.417315
[ Info: iteration 30, average log likelihood -1.417314
[ Info: iteration 31, average log likelihood -1.417314
[ Info: iteration 32, average log likelihood -1.417314
[ Info: iteration 33, average log likelihood -1.417314
[ Info: iteration 34, average log likelihood -1.417314
[ Info: iteration 35, average log likelihood -1.417314
[ Info: iteration 36, average log likelihood -1.417314
[ Info: iteration 37, average log likelihood -1.417313
[ Info: iteration 38, average log likelihood -1.417313
[ Info: iteration 39, average log likelihood -1.417313
[ Info: iteration 40, average log likelihood -1.417313
[ Info: iteration 41, average log likelihood -1.417313
[ Info: iteration 42, average log likelihood -1.417313
[ Info: iteration 43, average log likelihood -1.417313
[ Info: iteration 44, average log likelihood -1.417313
[ Info: iteration 45, average log likelihood -1.417313
[ Info: iteration 46, average log likelihood -1.417313
[ Info: iteration 47, average log likelihood -1.417313
[ Info: iteration 48, average log likelihood -1.417313
[ Info: iteration 49, average log likelihood -1.417313
[ Info: iteration 50, average log likelihood -1.417313
┌ Info: EM with 100000 data points 50 iterations avll -1.417313
└ 952.4 data points per parameter
┌ Info: 1
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4226984974923307
│     -1.422636334357167
│      ⋮
└     -1.4173127596380293
[ Info: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.417327
[ Info: iteration 2, average log likelihood -1.417270
[ Info: iteration 3, average log likelihood -1.417222
[ Info: iteration 4, average log likelihood -1.417168
[ Info: iteration 5, average log likelihood -1.417103
[ Info: iteration 6, average log likelihood -1.417028
[ Info: iteration 7, average log likelihood -1.416945
[ Info: iteration 8, average log likelihood -1.416860
[ Info: iteration 9, average log likelihood -1.416777
[ Info: iteration 10, average log likelihood -1.416700
[ Info: iteration 11, average log likelihood -1.416630
[ Info: iteration 12, average log likelihood -1.416568
[ Info: iteration 13, average log likelihood -1.416513
[ Info: iteration 14, average log likelihood -1.416465
[ Info: iteration 15, average log likelihood -1.416422
[ Info: iteration 16, average log likelihood -1.416384
[ Info: iteration 17, average log likelihood -1.416350
[ Info: iteration 18, average log likelihood -1.416318
[ Info: iteration 19, average log likelihood -1.416289
[ Info: iteration 20, average log likelihood -1.416262
[ Info: iteration 21, average log likelihood -1.416236
[ Info: iteration 22, average log likelihood -1.416213
[ Info: iteration 23, average log likelihood -1.416191
[ Info: iteration 24, average log likelihood -1.416171
[ Info: iteration 25, average log likelihood -1.416153
[ Info: iteration 26, average log likelihood -1.416137
[ Info: iteration 27, average log likelihood -1.416122
[ Info: iteration 28, average log likelihood -1.416109
[ Info: iteration 29, average log likelihood -1.416097
[ Info: iteration 30, average log likelihood -1.416086
[ Info: iteration 31, average log likelihood -1.416077
[ Info: iteration 32, average log likelihood -1.416069
[ Info: iteration 33, average log likelihood -1.416061
[ Info: iteration 34, average log likelihood -1.416054
[ Info: iteration 35, average log likelihood -1.416048
[ Info: iteration 36, average log likelihood -1.416042
[ Info: iteration 37, average log likelihood -1.416037
[ Info: iteration 38, average log likelihood -1.416032
[ Info: iteration 39, average log likelihood -1.416028
[ Info: iteration 40, average log likelihood -1.416024
[ Info: iteration 41, average log likelihood -1.416020
[ Info: iteration 42, average log likelihood -1.416017
[ Info: iteration 43, average log likelihood -1.416013
[ Info: iteration 44, average log likelihood -1.416010
[ Info: iteration 45, average log likelihood -1.416007
[ Info: iteration 46, average log likelihood -1.416005
[ Info: iteration 47, average log likelihood -1.416002
[ Info: iteration 48, average log likelihood -1.416000
[ Info: iteration 49, average log likelihood -1.415997
[ Info: iteration 50, average log likelihood -1.415995
┌ Info: EM with 100000 data points 50 iterations avll -1.415995
└ 473.9 data points per parameter
┌ Info: 2
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4173274616726308
│     -1.4172695387235985
│      ⋮
└     -1.4159950867726177
[ Info: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.416003
[ Info: iteration 2, average log likelihood -1.415955
[ Info: iteration 3, average log likelihood -1.415910
[ Info: iteration 4, average log likelihood -1.415857
[ Info: iteration 5, average log likelihood -1.415790
[ Info: iteration 6, average log likelihood -1.415709
[ Info: iteration 7, average log likelihood -1.415616
[ Info: iteration 8, average log likelihood -1.415515
[ Info: iteration 9, average log likelihood -1.415414
[ Info: iteration 10, average log likelihood -1.415321
[ Info: iteration 11, average log likelihood -1.415239
[ Info: iteration 12, average log likelihood -1.415169
[ Info: iteration 13, average log likelihood -1.415110
[ Info: iteration 14, average log likelihood -1.415062
[ Info: iteration 15, average log likelihood -1.415022
[ Info: iteration 16, average log likelihood -1.414989
[ Info: iteration 17, average log likelihood -1.414960
[ Info: iteration 18, average log likelihood -1.414936
[ Info: iteration 19, average log likelihood -1.414915
[ Info: iteration 20, average log likelihood -1.414895
[ Info: iteration 21, average log likelihood -1.414877
[ Info: iteration 22, average log likelihood -1.414860
[ Info: iteration 23, average log likelihood -1.414844
[ Info: iteration 24, average log likelihood -1.414829
[ Info: iteration 25, average log likelihood -1.414814
[ Info: iteration 26, average log likelihood -1.414800
[ Info: iteration 27, average log likelihood -1.414786
[ Info: iteration 28, average log likelihood -1.414772
[ Info: iteration 29, average log likelihood -1.414758
[ Info: iteration 30, average log likelihood -1.414744
[ Info: iteration 31, average log likelihood -1.414730
[ Info: iteration 32, average log likelihood -1.414717
[ Info: iteration 33, average log likelihood -1.414703
[ Info: iteration 34, average log likelihood -1.414689
[ Info: iteration 35, average log likelihood -1.414676
[ Info: iteration 36, average log likelihood -1.414662
[ Info: iteration 37, average log likelihood -1.414648
[ Info: iteration 38, average log likelihood -1.414635
[ Info: iteration 39, average log likelihood -1.414622
[ Info: iteration 40, average log likelihood -1.414608
[ Info: iteration 41, average log likelihood -1.414595
[ Info: iteration 42, average log likelihood -1.414583
[ Info: iteration 43, average log likelihood -1.414570
[ Info: iteration 44, average log likelihood -1.414558
[ Info: iteration 45, average log likelihood -1.414547
[ Info: iteration 46, average log likelihood -1.414535
[ Info: iteration 47, average log likelihood -1.414525
[ Info: iteration 48, average log likelihood -1.414514
[ Info: iteration 49, average log likelihood -1.414504
[ Info: iteration 50, average log likelihood -1.414494
┌ Info: EM with 100000 data points 50 iterations avll -1.414494
└ 236.4 data points per parameter
┌ Info: 3
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4160028035306373
│     -1.4159548467293501
│      ⋮
└     -1.4144941682690138
[ Info: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.414494
[ Info: iteration 2, average log likelihood -1.414435
[ Info: iteration 3, average log likelihood -1.414383
[ Info: iteration 4, average log likelihood -1.414324
[ Info: iteration 5, average log likelihood -1.414255
[ Info: iteration 6, average log likelihood -1.414171
[ Info: iteration 7, average log likelihood -1.414072
[ Info: iteration 8, average log likelihood -1.413960
[ Info: iteration 9, average log likelihood -1.413840
[ Info: iteration 10, average log likelihood -1.413718
[ Info: iteration 11, average log likelihood -1.413601
[ Info: iteration 12, average log likelihood -1.413493
[ Info: iteration 13, average log likelihood -1.413397
[ Info: iteration 14, average log likelihood -1.413314
[ Info: iteration 15, average log likelihood -1.413243
[ Info: iteration 16, average log likelihood -1.413184
[ Info: iteration 17, average log likelihood -1.413133
[ Info: iteration 18, average log likelihood -1.413091
[ Info: iteration 19, average log likelihood -1.413054
[ Info: iteration 20, average log likelihood -1.413021
[ Info: iteration 21, average log likelihood -1.412993
[ Info: iteration 22, average log likelihood -1.412967
[ Info: iteration 23, average log likelihood -1.412943
[ Info: iteration 24, average log likelihood -1.412921
[ Info: iteration 25, average log likelihood -1.412900
[ Info: iteration 26, average log likelihood -1.412881
[ Info: iteration 27, average log likelihood -1.412862
[ Info: iteration 28, average log likelihood -1.412845
[ Info: iteration 29, average log likelihood -1.412828
[ Info: iteration 30, average log likelihood -1.412812
[ Info: iteration 31, average log likelihood -1.412796
[ Info: iteration 32, average log likelihood -1.412782
[ Info: iteration 33, average log likelihood -1.412768
[ Info: iteration 34, average log likelihood -1.412755
[ Info: iteration 35, average log likelihood -1.412743
[ Info: iteration 36, average log likelihood -1.412731
[ Info: iteration 37, average log likelihood -1.412720
[ Info: iteration 38, average log likelihood -1.412709
[ Info: iteration 39, average log likelihood -1.412699
[ Info: iteration 40, average log likelihood -1.412690
[ Info: iteration 41, average log likelihood -1.412681
[ Info: iteration 42, average log likelihood -1.412673
[ Info: iteration 43, average log likelihood -1.412665
[ Info: iteration 44, average log likelihood -1.412657
[ Info: iteration 45, average log likelihood -1.412650
[ Info: iteration 46, average log likelihood -1.412642
[ Info: iteration 47, average log likelihood -1.412636
[ Info: iteration 48, average log likelihood -1.412629
[ Info: iteration 49, average log likelihood -1.412623
[ Info: iteration 50, average log likelihood -1.412617
┌ Info: EM with 100000 data points 50 iterations avll -1.412617
└ 118.1 data points per parameter
┌ Info: 4
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4144940378638104
│     -1.414435379566872
│      ⋮
└     -1.4126168639756285
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.412619
[ Info: iteration 2, average log likelihood -1.412555
[ Info: iteration 3, average log likelihood -1.412492
[ Info: iteration 4, average log likelihood -1.412417
[ Info: iteration 5, average log likelihood -1.412323
[ Info: iteration 6, average log likelihood -1.412206
[ Info: iteration 7, average log likelihood -1.412068
[ Info: iteration 8, average log likelihood -1.411911
[ Info: iteration 9, average log likelihood -1.411742
[ Info: iteration 10, average log likelihood -1.411570
[ Info: iteration 11, average log likelihood -1.411402
[ Info: iteration 12, average log likelihood -1.411246
[ Info: iteration 13, average log likelihood -1.411105
[ Info: iteration 14, average log likelihood -1.410982
[ Info: iteration 15, average log likelihood -1.410875
[ Info: iteration 16, average log likelihood -1.410782
[ Info: iteration 17, average log likelihood -1.410702
[ Info: iteration 18, average log likelihood -1.410633
[ Info: iteration 19, average log likelihood -1.410572
[ Info: iteration 20, average log likelihood -1.410518
[ Info: iteration 21, average log likelihood -1.410469
[ Info: iteration 22, average log likelihood -1.410425
[ Info: iteration 23, average log likelihood -1.410385
[ Info: iteration 24, average log likelihood -1.410348
[ Info: iteration 25, average log likelihood -1.410314
[ Info: iteration 26, average log likelihood -1.410282
[ Info: iteration 27, average log likelihood -1.410252
[ Info: iteration 28, average log likelihood -1.410223
[ Info: iteration 29, average log likelihood -1.410196
[ Info: iteration 30, average log likelihood -1.410170
[ Info: iteration 31, average log likelihood -1.410146
[ Info: iteration 32, average log likelihood -1.410122
[ Info: iteration 33, average log likelihood -1.410099
[ Info: iteration 34, average log likelihood -1.410077
[ Info: iteration 35, average log likelihood -1.410056
[ Info: iteration 36, average log likelihood -1.410036
[ Info: iteration 37, average log likelihood -1.410016
[ Info: iteration 38, average log likelihood -1.409998
[ Info: iteration 39, average log likelihood -1.409979
[ Info: iteration 40, average log likelihood -1.409962
[ Info: iteration 41, average log likelihood -1.409945
[ Info: iteration 42, average log likelihood -1.409929
[ Info: iteration 43, average log likelihood -1.409913
[ Info: iteration 44, average log likelihood -1.409898
[ Info: iteration 45, average log likelihood -1.409884
[ Info: iteration 46, average log likelihood -1.409870
[ Info: iteration 47, average log likelihood -1.409857
[ Info: iteration 48, average log likelihood -1.409844
[ Info: iteration 49, average log likelihood -1.409832
[ Info: iteration 50, average log likelihood -1.409820
┌ Info: EM with 100000 data points 50 iterations avll -1.409820
└ 59.0 data points per parameter
┌ Info: 5
│   : avll =  = ": avll = "
│   avll =
│    50-element Array{Float64,1}:
│     -1.4126191381839064
│     -1.412555202300385
│      ⋮
└     -1.4098201867328313
┌ Info: Total log likelihood: 
│   tll =
│    251-element Array{Float64,1}:
│     -1.4226803785423
│     -1.4226984974923307
│     -1.422636334357167
│     -1.4225911562505853
│      ⋮
│     -1.4098440668947199
│     -1.4098318744577174
└     -1.4098201867328313
32×26 Array{Float64,2}:
  0.0277509   -0.391509      0.253558    -0.283757   -0.357182   -0.152797    -0.0314573   -0.191823    -0.103571    -0.127687      0.560373    -0.222933     -0.155175   -0.368407   -0.414456    -0.532451    0.161847     0.493797   -0.131987     0.120652    0.650069     0.216631    -0.33107     -0.0671223  -0.333877     0.116037
 -0.339102     0.307083      0.225607    -0.210314    0.0256981  -0.0187926   -0.118916     0.00587391  -0.127583    -0.0458101     0.293473    -0.0200437    -0.227479    0.10956     0.0864672   -0.530063   -0.488878     0.088987    0.440402    -0.0605607   0.694961    -0.0521331    1.16713      0.294293    0.436185    -0.372753
 -0.107716    -0.521233      0.427719    -0.55797    -0.0624848  -0.214505    -0.133748    -0.0590205    0.299356     0.14487      -0.256727    -0.711633      0.515761   -0.413121    0.10395      0.573727    0.0904688    0.595553    0.709521    -0.0670705  -0.414995    -0.324218    -5.55151e-5  -0.0185567   0.0932821   -0.509887
 -0.264329     0.280468      0.303578     0.208352   -0.0971123   0.00115917  -0.0253072    0.169461     0.241932     0.218417      0.34144      0.152684      0.555692   -0.448001    0.150433     0.683025   -0.48198      0.520487    0.84973     -0.0900512  -0.0165864   -0.345534    -0.180398    -0.161858    0.0979628    0.381264
  0.520441    -0.174736      0.656999     0.0238828  -0.371419    0.496341     0.189262    -0.105298    -0.235262     0.113716      0.0462054    0.108217     -0.550376    0.393163   -0.0640441    0.0382426   0.311492     0.136356    0.247161     0.324627   -0.288666     0.0603632    0.0683161   -0.290249   -0.326048    -0.315628
 -0.551029    -0.0959644    -0.00936697   0.125476   -0.447406   -0.254987     0.138435    -0.00979642  -0.142207     0.356276      0.376094     0.437677      0.434269    0.503192    0.386451     0.348365    0.185738     0.19266     0.317219     0.0263596  -0.583434     0.497639     0.0887674    0.0794806  -0.428852    -0.272293
 -0.480553     0.412821      0.114931     0.750945   -0.410194    0.0940426    0.112623    -0.253547    -0.455366    -0.402107      0.274271     0.00626046   -0.430165   -0.25944     0.480311    -0.131398   -0.381973     0.0351142  -0.452416    -0.262712   -0.0972357    0.276241    -0.150014     0.296835   -0.381327     0.229862
 -0.667162     0.326791     -0.0974393    0.935815   -0.593515   -0.0493716    0.112664     0.0764137   -0.13026      0.15283       0.0794153    0.0696715    -0.734123    1.03972    -0.0465527    0.777722   -0.606219     0.153293   -0.0413553   -0.0420487  -0.332391    -0.395329    -0.169836    -0.297282    0.0223718   -0.288437
  0.11905     -0.11951      -0.124968    -0.241907    0.0562577   0.0352746   -0.0370141    0.0348987    0.0850264   -0.0418537    -0.0875151    0.000212823   0.0958053  -0.108491   -0.111103    -0.369793    0.097859    -0.197437   -0.141361    -0.0933305   0.302535     0.139661     0.0439666   -0.0595273   0.20139     -0.0882335
 -0.0931817   -0.0759215     0.0711933    0.184967   -0.022956    0.0478482    0.0299388   -0.0464302    0.0128254    0.0191206     0.245469    -0.142335      0.0665085  -0.0304321   0.221126     0.361624   -0.104744     0.184218    0.191401     0.133273   -0.0212127   -0.0628241   -0.0708195   -0.0735159  -0.100787    -0.08174
  0.341863     0.22518      -0.0136513    0.265795    0.324488   -0.148239     0.0179297   -0.368309     0.366031    -0.327771     -0.364581     0.302136     -0.140997   -0.222637   -0.0655543    0.21184    -0.00262148  -0.319009   -0.0213071   -0.105101   -0.312322    -0.0319824    0.23202      0.0700799  -0.72355      0.399174
 -0.0620409    0.241427      0.132371    -0.220774   -0.0823653   0.0309012   -0.0474575    0.0156641    0.00256198  -0.084934     -0.621385     0.312782     -0.0948352   0.410451    0.0466627    0.0574994   0.197848    -0.182966   -0.168804    -0.0911199  -0.41613     -0.144126     0.139405     0.0570693   0.144151     0.10113
 -0.102429    -0.269968     -0.353508    -0.572505   -0.427779    0.0289663   -0.480022     0.183041    -0.319655    -0.228464     -0.207036    -0.102178      0.387462   -0.127215    0.518464    -0.0104392  -0.341919    -0.475398   -0.0791974   -0.56375    -0.195202    -0.303098    -0.0458343   -0.0666074   0.19123      0.0999276
 -0.242047     0.21708      -0.744634     0.28396     0.805561   -0.0848943   -0.269466     0.276772    -0.223617     0.0282766    -0.283197     0.195112     -0.0530745  -0.408715    0.0724219    0.230113   -0.35458     -0.267262   -0.255736    -0.155554    0.15712     -0.586237    -0.190014     0.215036    0.285453    -0.0388716
  0.0482216    0.266524     -0.0942491   -0.196296   -0.0422183  -0.0392193    0.502494    -0.223347    -0.53891      0.0532611    -0.373942    -0.352986     -0.049148    0.133698    0.167708    -0.328422   -0.647867     0.202239    0.11684     -0.174484   -0.413991     0.254922    -0.451745    -0.313306    0.819518     0.1607
 -0.776581     0.195044     -0.359894    -0.127797   -0.190326   -0.140335     0.421524     0.563591     0.206833     0.24052      -0.162858    -0.467309      0.383144   -0.0136872   0.231194    -0.338062   -0.0678142   -0.300882   -0.213074    -0.126093    0.215488     0.730898    -0.301805     0.219293    0.667805     0.00461392
  0.0670958   -0.0436192    -0.37735     -0.106479    0.422136   -0.0664165    0.0925595    0.167075     0.199494    -0.0777766    -0.28203     -0.530491      0.0822208  -0.11277     0.341354    -0.10029    -0.0657931   -0.493508    0.083591    -0.356588    0.11294      0.0221869    0.0476706   -0.052677    0.398437    -0.155746
 -0.506951    -0.784131      0.399051     0.524469   -0.0578377  -0.548881     0.335617    -0.570647     0.326984    -0.131417     -0.00755198  -0.181414      0.180309    0.288865    0.800931    -0.417063    0.0457939    0.268405    0.475575    -0.548863    0.075057     0.458168     0.700197    -0.264674   -0.00948966   0.375786
 -0.242151    -0.369081      0.366439    -0.100663   -0.129265    0.429381    -0.0596236    0.454701    -0.464327     0.437548      0.534462    -0.662424      0.32732    -0.0447053   0.37834     -0.1797     -0.34478      0.728837    0.307783     0.258198    0.344264     0.252646    -0.56051     -0.50672     0.19913     -0.554247
 -0.030959     0.1512        0.228755     0.030417   -0.227409    0.114004    -0.161569    -0.154225    -0.0222774   -0.0399822     0.158104     0.626409     -0.121237    0.0184658  -0.377348     0.131252   -0.0552827    0.166961   -0.00755058   0.181695   -0.0721181   -0.107789     0.0303053    0.0176917  -0.442022     0.124436
 -0.0187534   -0.46045      -0.876261     0.224144    0.0608942  -0.0442378    0.176551     0.0442663   -0.11883      0.152177     -0.621648     0.206354      0.660415    0.275683   -0.00417898   0.508381    0.435786    -0.369391   -0.123813    -0.308072   -0.441553     0.209971    -1.23417     -0.632724   -0.0869243    0.392385
  0.0480501    0.285486     -0.42458     -0.103964    0.0414812  -0.172261    -0.377944    -0.471022     0.601786     0.130622     -0.211524     0.630969      0.396534    0.459131    0.042383     0.102733   -0.0463591   -0.382781    0.147359    -0.220916   -0.360394    -0.310465     0.850832     0.218112    0.236206     0.273024
 -0.00632693  -0.000492127   0.479064    -0.156427   -0.504245    0.0726406   -0.587649    -0.563608    -0.520378    -0.279147     -0.542693    -0.267633      0.36075     0.16747     0.122397     0.146612    0.233409     0.516185   -0.673856     0.386186   -0.536158    -0.421136    -0.181356     0.402283   -0.215275     0.573058
  0.241539    -0.046599      0.751553    -0.166875    0.472653    0.246557     0.791154     0.249613    -0.174937    -0.538678     -0.382676     0.116943     -0.100348   -0.277205    0.203331     0.536431    0.059196     0.467113   -0.0543879    0.214736   -0.108907    -0.304737    -0.571203    -0.174564    0.236832     0.00275826
  0.576881    -0.807582     -0.30805     -0.729699    0.570011   -0.212365    -0.0547011    0.0154802    0.484025     0.233671     -0.0198666   -0.0561371     0.51758    -0.377987   -0.0209258   -0.242006    0.310702    -0.209614    0.214698     0.0920211   0.502131     0.044937     0.240096    -0.32811     0.196968    -0.087233
  0.602305     0.399746     -0.18468     -0.536572    0.379787    0.275016    -0.45274      0.616806     0.331288     0.146262     -0.0947382    0.0571201     0.280708   -0.665224   -0.204193     0.397178    0.407887    -0.480874   -0.428274     0.508473    0.122773     0.0188275   -0.390596     0.156272   -0.330469    -0.334585
  0.0808602    0.172436     -0.11097      0.573063    0.789271   -0.146781     0.147381    -0.387896     0.499604     0.317904      0.257913     0.0302734     0.181742    0.11608    -0.37886     -0.0263092   0.347        0.402956    0.187598     0.351908    0.455117     0.0788543   -0.113155    -0.0998334  -0.0359592   -0.15391
 -0.318087    -0.520121      0.0634596    0.557885    0.176974    0.038007    -0.176374     0.39311      0.575181     0.000801056   0.455167     0.137273      0.138164   -0.196877    0.188912     0.328823    0.457165    -0.312431   -0.171187     0.410209    0.571836    -0.0210905    0.467536     0.0499912  -0.39195     -0.413073
  0.209349    -0.290694     -0.327753    -0.240625   -0.0211209   0.0986527   -0.186939    -0.148658    -0.124024    -0.000352446  -0.385031     0.362848     -0.514798    0.165701   -0.137025    -0.70422     0.425836    -0.884793   -1.06387     -0.102878    0.199229     0.273132     0.319807     0.114818   -0.33655      0.0715672
  0.272247    -0.311651     -0.0575801    0.0583826   0.0863346  -0.351542     0.207495     0.309222    -0.227446    -0.339275      0.134146    -0.912442     -1.10769     0.0707515   0.0162846   -0.138216    0.282662    -0.332554   -0.346129     0.570175   -0.27368      0.70912     -0.35211     -0.001829   -0.255511    -0.18163
  0.750633     0.90191       0.555601    -0.419242    0.277585    0.424657     0.00789715   0.00210792   0.53944     -0.350633     -0.277548    -0.017335     -0.469956   -0.7173     -0.290629    -0.784463   -0.0852949   -0.0496295  -0.0643566    0.168648    0.720317    -0.00851091   0.653346     0.463165    0.194261     0.249045
  0.820122     0.169301     -0.623399    -0.612314   -0.0902118   0.375164    -0.148687    -0.230451    -0.0601882   -0.0851927    -0.404509     0.432294     -0.119697    0.802446   -0.772336    -0.577789   -0.0757863   -0.0878058  -0.204201    -0.0410386  -0.00472517  -0.278649    -0.291579    -0.238517    0.609878    -0.0173864[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.409809
[ Info: iteration 2, average log likelihood -1.409798
[ Info: iteration 3, average log likelihood -1.409788
[ Info: iteration 4, average log likelihood -1.409778
[ Info: iteration 5, average log likelihood -1.409769
[ Info: iteration 6, average log likelihood -1.409760
[ Info: iteration 7, average log likelihood -1.409751
[ Info: iteration 8, average log likelihood -1.409743
[ Info: iteration 9, average log likelihood -1.409735
[ Info: iteration 10, average log likelihood -1.409727
┌ Info: EM with 100000 data points 10 iterations avll -1.409727
└ 59.0 data points per parameter
kind full, method kmeans
[ Info: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.958048e+05
      1       7.132372e+05      -1.825676e+05 |       32
      2       6.933569e+05      -1.988035e+04 |       32
      3       6.871018e+05      -6.255036e+03 |       32
      4       6.840708e+05      -3.031000e+03 |       32
      5       6.822596e+05      -1.811243e+03 |       32
      6       6.810059e+05      -1.253697e+03 |       32
      7       6.800288e+05      -9.771527e+02 |       32
      8       6.792647e+05      -7.640374e+02 |       32
      9       6.785958e+05      -6.689235e+02 |       32
     10       6.780312e+05      -5.646051e+02 |       32
     11       6.775663e+05      -4.648990e+02 |       32
     12       6.771903e+05      -3.760301e+02 |       32
     13       6.769192e+05      -2.710974e+02 |       32
     14       6.767008e+05      -2.183492e+02 |       32
     15       6.765127e+05      -1.880971e+02 |       32
     16       6.763457e+05      -1.670574e+02 |       32
     17       6.762031e+05      -1.425219e+02 |       32
     18       6.760734e+05      -1.297643e+02 |       32
     19       6.759512e+05      -1.221946e+02 |       32
     20       6.758221e+05      -1.291156e+02 |       32
     21       6.756881e+05      -1.339501e+02 |       32
     22       6.755439e+05      -1.442272e+02 |       32
     23       6.754133e+05      -1.305406e+02 |       32
     24       6.752966e+05      -1.167669e+02 |       32
     25       6.751791e+05      -1.174489e+02 |       32
     26       6.750481e+05      -1.310195e+02 |       32
     27       6.749282e+05      -1.198744e+02 |       32
     28       6.748234e+05      -1.048570e+02 |       32
     29       6.747214e+05      -1.019310e+02 |       32
     30       6.746279e+05      -9.357641e+01 |       32
     31       6.745406e+05      -8.730798e+01 |       32
     32       6.744569e+05      -8.366557e+01 |       32
     33       6.743737e+05      -8.319216e+01 |       32
     34       6.742969e+05      -7.681350e+01 |       32
     35       6.742269e+05      -6.996948e+01 |       32
     36       6.741533e+05      -7.361148e+01 |       32
     37       6.740872e+05      -6.613415e+01 |       32
     38       6.740203e+05      -6.682714e+01 |       32
     39       6.739562e+05      -6.413030e+01 |       32
     40       6.738949e+05      -6.133924e+01 |       32
     41       6.738420e+05      -5.282538e+01 |       32
     42       6.737881e+05      -5.395916e+01 |       32
     43       6.737402e+05      -4.793430e+01 |       32
     44       6.736992e+05      -4.090672e+01 |       32
     45       6.736566e+05      -4.265153e+01 |       32
     46       6.736149e+05      -4.166184e+01 |       32
     47       6.735771e+05      -3.779253e+01 |       32
     48       6.735408e+05      -3.636388e+01 |       32
     49       6.735097e+05      -3.108044e+01 |       32
     50       6.734781e+05      -3.158932e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 673478.1099975593)
┌ Info: K-means with 32000 data points using 50 iterations
└ 37.0 data points per parameter
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.421979
[ Info: iteration 2, average log likelihood -1.416874
[ Info: iteration 3, average log likelihood -1.415473
[ Info: iteration 4, average log likelihood -1.414425
[ Info: iteration 5, average log likelihood -1.413340
[ Info: iteration 6, average log likelihood -1.412363
[ Info: iteration 7, average log likelihood -1.411702
[ Info: iteration 8, average log likelihood -1.411338
[ Info: iteration 9, average log likelihood -1.411138
[ Info: iteration 10, average log likelihood -1.411010
[ Info: iteration 11, average log likelihood -1.410916
[ Info: iteration 12, average log likelihood -1.410838
[ Info: iteration 13, average log likelihood -1.410770
[ Info: iteration 14, average log likelihood -1.410709
[ Info: iteration 15, average log likelihood -1.410653
[ Info: iteration 16, average log likelihood -1.410601
[ Info: iteration 17, average log likelihood -1.410551
[ Info: iteration 18, average log likelihood -1.410504
[ Info: iteration 19, average log likelihood -1.410459
[ Info: iteration 20, average log likelihood -1.410417
[ Info: iteration 21, average log likelihood -1.410376
[ Info: iteration 22, average log likelihood -1.410337
[ Info: iteration 23, average log likelihood -1.410300
[ Info: iteration 24, average log likelihood -1.410264
[ Info: iteration 25, average log likelihood -1.410230
[ Info: iteration 26, average log likelihood -1.410198
[ Info: iteration 27, average log likelihood -1.410167
[ Info: iteration 28, average log likelihood -1.410137
[ Info: iteration 29, average log likelihood -1.410109
[ Info: iteration 30, average log likelihood -1.410083
[ Info: iteration 31, average log likelihood -1.410057
[ Info: iteration 32, average log likelihood -1.410033
[ Info: iteration 33, average log likelihood -1.410009
[ Info: iteration 34, average log likelihood -1.409987
[ Info: iteration 35, average log likelihood -1.409966
[ Info: iteration 36, average log likelihood -1.409945
[ Info: iteration 37, average log likelihood -1.409926
[ Info: iteration 38, average log likelihood -1.409907
[ Info: iteration 39, average log likelihood -1.409890
[ Info: iteration 40, average log likelihood -1.409872
[ Info: iteration 41, average log likelihood -1.409856
[ Info: iteration 42, average log likelihood -1.409841
[ Info: iteration 43, average log likelihood -1.409825
[ Info: iteration 44, average log likelihood -1.409811
[ Info: iteration 45, average log likelihood -1.409797
[ Info: iteration 46, average log likelihood -1.409784
[ Info: iteration 47, average log likelihood -1.409771
[ Info: iteration 48, average log likelihood -1.409758
[ Info: iteration 49, average log likelihood -1.409746
[ Info: iteration 50, average log likelihood -1.409735
┌ Info: EM with 100000 data points 50 iterations avll -1.409735
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
  0.728321     0.861067    0.430376   -0.358798    0.236161    0.452487    0.0340298    -0.0312692    0.525814   -0.174461   -0.238245     0.069649   -0.442965   -0.479547   -0.461828   -0.725153    -0.0677488   -0.00679633  -0.0246283   0.0900112    0.630462    0.0441539     0.638683     0.491288    0.151372    0.096148
 -0.218185    -0.277193    0.38682     0.257879   -0.0687626   0.213652    0.181103      0.228861    -0.331844    0.146339    0.566808    -0.58978     0.212699   -0.23809     0.397211    0.083329    -0.872126     0.657052     0.526318    0.00322632   0.0931208  -0.0413873    -0.59019     -0.831912    0.0720593  -0.188095
 -0.35763     -0.162864    0.0537512  -0.18355    -0.265617   -0.308083   -0.0879613     0.154341     0.355516    0.276344    0.264183    -0.0764266   0.472792   -0.22918     0.379119   -0.0617552    0.0853537    0.0640763    0.248152   -0.186591     0.100593    0.272354      0.104892     0.128247   -0.0826634   0.0254685
  0.00910992   0.0157953   0.073657    0.291888    0.135149    0.0173724   0.0647863    -0.120887    -0.0022397  -0.0270767   0.131609    -0.0724826  -0.0725293  -0.0224906   0.0703919   0.299811    -0.0487902    0.181701     0.112807    0.203337    -0.0183607  -0.108064     -0.0604289   -0.0808731  -0.170678   -0.00906378
 -0.205618     0.559746   -0.986156    0.24159     0.777919   -0.13394     0.277827      0.363556     0.0113339   0.195091    0.0328088    0.356101    0.059802   -0.391679   -0.494986    0.0233182   -0.11042     -0.373895    -0.10291    -0.113644     0.276444   -0.240029     -0.732306    -0.249229    0.155431    0.0577104
 -0.668766    -0.209851   -0.110772    0.68533    -0.579983   -0.337359    0.0634896     0.0933117   -0.171796    0.586469    0.403109     0.355621    0.0531603   0.618379    0.367929    0.886951    -0.274724     0.281552     0.290523   -0.0529903   -0.542385    0.000989659   0.00377724  -0.262247   -0.319387   -0.301119
  0.351685    -0.262732   -0.124712    0.084792    0.553311    0.0371767  -0.375941      0.27151      0.619025    0.175673    0.192498     0.262635    0.29748    -0.477421   -0.076734    0.388332     0.540845    -0.281133    -0.129022    0.479647     0.641559   -0.216948      0.241501     0.0297808  -0.359055   -0.272566
 -0.675791     0.193054   -0.334438    0.159076    0.26059    -0.270763    0.332655      0.124914     0.0746223   0.0266624  -0.193937     0.0969844  -0.09641     0.449471    0.221823   -0.695744    -0.233431    -0.65917     -0.520124   -0.0820859    0.827868    0.153597      0.554724    -0.16554     0.523678    0.0404059
 -0.338975     0.433287   -0.383014    0.792286   -0.184512    0.291336   -0.0529672    -0.00528749  -0.176935   -0.259147    0.622495    -0.055538   -0.506047   -0.223859    0.035511   -0.0782858   -0.00731022  -0.145038    -0.488337    0.177955     0.30357     0.697051     -0.0745802    0.183606   -0.554109   -0.0150707
  0.216898    -0.333918    0.590167   -0.35757     0.136648    0.0116419   0.418431      0.0516782    0.122668   -0.0593575  -0.56776      0.0137217   0.498705   -0.464753   -0.172357    0.602813     0.101379     0.787787     0.306756   -0.145971    -0.225787   -0.413558     -0.537861    -0.502577    0.107422    0.353231
  0.474947     0.072948   -0.426517   -0.543323   -0.083942    0.581608   -0.448525     -0.101459    -0.47406     0.227115   -0.516996     0.344054    0.202461    0.424296   -0.490049   -0.36811      0.0428836   -0.0459095   -0.581215    0.169498    -0.145384   -0.301926     -0.459477    -0.419282    0.105593    0.0128981
  0.221506     0.0895763   0.713276    0.227236   -0.291294    0.187085    0.269143     -0.142915    -0.0841948   0.279451    0.132342     0.341804   -0.381843    0.576939   -0.293158   -0.00792647   0.442884     0.360502     0.149222    0.549373    -0.187796    0.309109     -0.0186628   -0.218579   -0.43575    -0.214459
  0.313771    -0.549485    0.851595   -0.044708   -0.43212     0.188695   -0.683242     -0.482866    -0.208031   -0.29968    -0.287571    -0.716235    0.0593793   0.0922979   0.412011    0.362924     0.305386     0.488736     0.100381    0.0873569   -0.332755   -0.0758676     0.426536     0.316181   -0.354348    0.0506384
  0.392393    -0.263099   -0.487474   -0.124783    0.270832   -0.524327   -0.479475     -0.567822     0.79017     0.0841251   0.0937683    0.423979    0.577118    0.623824   -0.190554   -0.373718     0.13117      0.0124926    0.229762   -0.101731     0.0581073   0.0188873     0.366631    -0.121709    0.316136    0.307852
  0.14975      0.285448   -0.0101955   0.158557   -0.0482575   0.226953   -0.000312481  -0.371255     0.32284    -0.244893   -0.383249     0.458009    0.074908    0.132389    0.286831    0.425144    -0.2685      -0.524349    -0.0261911  -0.218456    -0.641203   -0.27219       0.577411     0.0830524  -0.218356    0.241387
  0.300902    -0.342603   -0.458253   -0.388059    0.336002    0.0966216   0.249038      0.336204     0.16949     0.523992   -0.207509    -0.558521    0.207165    0.277901   -0.0884945  -0.0449925    0.434607    -0.149849     0.445351    0.45104     -0.217114    0.752361     -0.193088    -0.188071    0.204073   -0.672655
  0.134261     0.188579    0.206013    0.277296    0.103061    0.329127    0.461793     -0.253463    -0.403238   -0.69521    -0.299584     0.110923   -0.957776    0.264088   -0.37094     0.469035    -0.397181    -0.0196648   -0.404229    0.352363    -0.0975363  -0.421979     -0.4793      -0.0750161   0.421213   -0.273585
  0.240334    -0.0825086  -0.105914   -0.334718    0.0721163   0.120915   -0.0852        0.0502909    0.118945   -0.104767   -0.226203     0.0789851   0.0736517  -0.0570568  -0.219738   -0.214031     0.15547     -0.26371     -0.145436   -0.0276701    0.212586   -0.0155814     0.0572417   -0.0283147   0.221947   -0.0855823
  0.128956    -0.521198    0.179862   -0.295797   -0.129064   -0.155603    0.0624651    -0.139199    -0.137887   -0.0995588   0.589495    -0.295212   -0.156169   -0.384537   -0.435932   -0.535947     0.175923     0.442613    -0.145034    0.311287     0.74202     0.244286     -0.345723    -0.0301972  -0.227403    0.0344
 -0.00643721  -0.0361365  -0.146411   -0.0168772  -0.0100967   0.0211448  -0.0129506     0.0227969    0.0467227   0.0576388  -0.121913     0.0624926   0.141071    0.107518    0.0873681   0.0357546    0.0918301   -0.180566    -0.0239643  -0.120443    -0.0983991   0.102082     -0.102226    -0.0859042  -0.0449556   0.0442743
 -0.524774    -0.335959    0.344576   -0.509498   -0.345591    0.0591087  -0.0255338     0.410808    -0.115644    0.233301    0.0696246   -0.666115    0.42842    -0.0647508   0.459018   -0.039663     0.141676     0.411388     0.278877    0.01813      0.417646    0.220661      0.00947425   0.189925    1.05143    -0.675686
 -0.190208     0.0616835  -0.619636    0.0764457   0.305102   -0.0165896  -0.691612      0.16695     -0.365052    0.0179379  -0.41253      0.015825    0.217291   -0.343844    0.540247    0.260887    -0.326619    -0.255641    -0.203125   -0.427402    -0.176558   -0.404295     -0.142441     0.256604    0.206704   -0.0499121
  0.368518    -0.524554   -0.261218   -0.453347    0.573146   -0.228457    0.151714      0.177307     0.25976    -0.422116   -0.432889    -0.616006    0.137906   -0.743808    0.35274    -0.360589    -0.0775665   -0.708435    -0.0292576  -0.480922     0.353985   -0.0951413    -0.0484185   -0.327522    0.400189    0.139955
 -0.431633     0.291104    0.243306    0.176588   -0.831965   -0.460766   -0.16386      -0.185713    -0.61872    -0.819606   -0.0463757   -0.117662    0.0262559  -0.0541572   0.522063   -0.371356    -0.59489      0.0354264   -0.380248   -0.338204    -0.428865   -0.160752     -0.325203     0.498397   -0.0915857   0.83583
 -0.16161      0.111111    0.030571   -0.221272   -0.368619    0.0793309   0.268544     -0.0663529   -0.445797   -0.0013111  -0.247016    -0.0188318  -0.0580296   0.380557    0.245285   -0.294803    -0.278961     0.0197149    0.149854   -0.398824    -0.38406     0.205579     -0.0890795   -0.210284    0.380228   -0.00650352
 -0.267916     0.0812037  -0.204657   -0.381282   -0.243326    0.147763   -0.681927     -0.092175     0.134699    0.622892    0.359662     0.016667    0.315775   -0.344225   -0.295878    0.648743    -0.616054     0.300604     0.606432   -0.0565994    0.0272511  -0.4129        0.24039      0.183125    0.030174   -0.126437
  0.206548    -0.608414   -0.205087   -0.241726   -0.089382    0.0879288  -0.0754604    -0.0553897   -0.315854   -0.123341   -0.265626    -0.0024305  -0.553111    0.107179    0.143543   -0.529394     0.454091    -0.880857    -0.971631    0.00750638  -0.265725    0.375818      0.0562367    0.196453   -0.443114   -0.0673292
  0.0391634    0.430088   -0.226542   -0.181191    0.0577856  -0.813769   -0.397464     -0.12659      0.323499   -0.209267   -0.94097      0.469096   -0.322039    0.27222    -0.570526    0.145916     0.559657    -0.210931    -0.359871   -0.182859    -0.328631   -0.163127      0.167412     0.280108   -0.355535    0.572224
 -0.232131     0.115547    0.489823   -0.0229981   0.159522   -0.143472   -0.118135     -0.283301    -0.171124   -0.246117    0.476242     0.133412   -0.341736    0.0260598   0.336123   -0.529888    -0.58526      0.313694     0.641016    0.0535881    0.612607   -0.296054      1.25195      0.234075    0.0913257  -0.21163
 -0.00912601   0.620793    0.761946   -0.537524    0.186636    0.291678    0.611632      0.536814     0.0524389  -0.316984   -0.00178345   0.14643    -0.0694814  -0.50992     0.555582    0.315118     0.337794    -0.109414    -0.13759     0.493875    -0.466458   -0.0460158    -0.350639    -0.021887   -0.775493   -0.455596
 -0.305963     0.278969   -0.17289     0.258683    0.508246    0.285938    0.309067      0.100201    -0.0303339  -0.0974173  -0.0474447    0.0271463   0.503743    0.151312    0.488282    0.809261    -0.128979     0.352705     0.164312    0.504105    -0.569753   -0.225989     -0.422956     0.585021    0.477196    0.12761
 -0.669084    -0.304926    0.535188    0.856255    0.233467   -0.352781    0.650883     -0.248705     0.770989   -0.16382     0.214214    -0.113929    0.0668394  -0.122667    0.47089     0.190559     0.309756     0.370015     0.513133   -0.519149     0.0425828   0.28073       0.252845    -0.0217289  -0.213937    0.0984735[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.409724
[ Info: iteration 2, average log likelihood -1.409713
[ Info: iteration 3, average log likelihood -1.409702
[ Info: iteration 4, average log likelihood -1.409692
[ Info: iteration 5, average log likelihood -1.409682
[ Info: iteration 6, average log likelihood -1.409672
[ Info: iteration 7, average log likelihood -1.409662
[ Info: iteration 8, average log likelihood -1.409653
[ Info: iteration 9, average log likelihood -1.409644
[ Info: iteration 10, average log likelihood -1.409636
┌ Info: EM with 100000 data points 10 iterations avll -1.409636
└ 59.0 data points per parameter
[ Info: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.678561e+05
      1       2.230230e+04      -1.455538e+05 |        2
      2       7.823675e+03      -1.447862e+04 |        0
      3       7.823675e+03       0.000000e+00 |        0
K-means converged with 3 iterations (objv = 7823.67549422947)
┌ Info: K-means with 900 data points using 3 iterations
└ 150.0 data points per parameter
[ Info: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
[ Info: iteration 1, average log likelihood -2.043155
[ Info: iteration 2, average log likelihood -2.043154
[ Info: iteration 3, average log likelihood -2.043154
[ Info: iteration 4, average log likelihood -2.043154
[ Info: iteration 5, average log likelihood -2.043154
[ Info: iteration 6, average log likelihood -2.043154
[ Info: iteration 7, average log likelihood -2.043154
[ Info: iteration 8, average log likelihood -2.043154
[ Info: iteration 9, average log likelihood -2.043154
[ Info: iteration 10, average log likelihood -2.043154
┌ Info: EM with 900 data points 10 iterations avll -2.043154
└ 81.8 data points per parameter
   Testing GaussianMixtures tests passed 
