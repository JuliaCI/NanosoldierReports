Julia Version 1.5.2-pre.0
Commit c4acbf93fb (2020-08-26 10:58 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-9.0.1 (ORCJIT, skylake-avx512)
Environment:
  JULIA_DEPOT_PATH = ::/usr/local/share/julia
  JULIA_NUM_THREADS = 2

  Resolving package versions...
  Installed Requires ───────────────────── v1.0.1
  Installed CodecZlib ──────────────────── v0.7.0
  Installed JLD2 ───────────────────────── v0.1.14
  Installed OrderedCollections ─────────── v1.3.0
  Installed MacroTools ─────────────────── v0.5.5
  Installed BinaryProvider ─────────────── v0.5.10
  Installed GPUArrays ──────────────────── v5.1.0
  Installed Knet ───────────────────────── v1.4.0
  Installed Zlib_jll ───────────────────── v1.2.11+15
  Installed CEnum ──────────────────────── v0.4.1
  Installed AbstractFFTs ───────────────── v0.5.0
  Installed Reexport ───────────────────── v0.2.0
  Installed OpenSpecFun_jll ────────────── v0.5.3+3
  Installed TranscodingStreams ─────────── v0.9.5
  Installed DataStructures ─────────────── v0.17.20
  Installed NNlib ──────────────────────── v0.7.4
  Installed LLVM ───────────────────────── v2.0.0
  Installed CUDA ───────────────────────── v1.3.3
  Installed FileIO ─────────────────────── v1.4.1
  Installed CompilerSupportLibraries_jll ─ v0.3.3+0
  Installed AutoGrad ───────────────────── v1.2.3
  Installed SpecialFunctions ───────────── v0.10.3
  Installed GPUCompiler ────────────────── v0.6.0
  Installed TimerOutputs ───────────────── v0.5.6
  Installed Adapt ──────────────────────── v2.0.2
  Installed ExprTools ──────────────────── v0.1.1
Updating `~/.julia/environments/v1.5/Project.toml`
  [1902f260] + Knet v1.4.0
Updating `~/.julia/environments/v1.5/Manifest.toml`
  [621f4979] + AbstractFFTs v0.5.0
  [79e6a3ab] + Adapt v2.0.2
  [6710c13c] + AutoGrad v1.2.3
  [b99e7846] + BinaryProvider v0.5.10
  [fa961155] + CEnum v0.4.1
  [052768ef] + CUDA v1.3.3
  [944b1d66] + CodecZlib v0.7.0
  [e66e0078] + CompilerSupportLibraries_jll v0.3.3+0
  [864edb3b] + DataStructures v0.17.20
  [e2ba6199] + ExprTools v0.1.1
  [5789e2e9] + FileIO v1.4.1
  [0c68f7d7] + GPUArrays v5.1.0
  [61eb1bfa] + GPUCompiler v0.6.0
  [033835bb] + JLD2 v0.1.14
  [1902f260] + Knet v1.4.0
  [929cbde3] + LLVM v2.0.0
  [1914dd2f] + MacroTools v0.5.5
  [872c559c] + NNlib v0.7.4
  [efe28fd5] + OpenSpecFun_jll v0.5.3+3
  [bac558e1] + OrderedCollections v1.3.0
  [189a3867] + Reexport v0.2.0
  [ae029012] + Requires v1.0.1
  [276daf66] + SpecialFunctions v0.10.3
  [a759f4b9] + TimerOutputs v0.5.6
  [3bb67fe8] + TranscodingStreams v0.9.5
  [83775a58] + Zlib_jll v1.2.11+15
  [2a0f44e3] + Base64
  [ade2ca70] + Dates
  [8ba89e20] + Distributed
  [b77e0a4c] + InteractiveUtils
  [76f85450] + LibGit2
  [8f399da3] + Libdl
  [37e2e46d] + LinearAlgebra
  [56ddb016] + Logging
  [d6f4376e] + Markdown
  [a63ad114] + Mmap
  [44cfe95a] + Pkg
  [de0858da] + Printf
  [3fa0cd96] + REPL
  [9a3f8284] + Random
  [ea8e919c] + SHA
  [9e88b42a] + Serialization
  [6462fe0b] + Sockets
  [2f01184e] + SparseArrays
  [10745b16] + Statistics
  [8dfed614] + Test
  [cf7118a7] + UUIDs
  [4ec0a83e] + Unicode
    Testing Knet
Status `/tmp/jl_UnJaTt/Project.toml`
  [6710c13c] AutoGrad v1.2.3
  [052768ef] CUDA v1.3.3
  [5789e2e9] FileIO v1.4.1
  [033835bb] JLD2 v0.1.14
  [1902f260] Knet v1.4.0
  [872c559c] NNlib v0.7.4
  [276daf66] SpecialFunctions v0.10.3
  [8f399da3] Libdl
  [37e2e46d] LinearAlgebra
  [44cfe95a] Pkg
  [de0858da] Printf
  [9a3f8284] Random
  [10745b16] Statistics
  [8dfed614] Test
Status `/tmp/jl_UnJaTt/Manifest.toml`
  [621f4979] AbstractFFTs v0.5.0
  [79e6a3ab] Adapt v2.0.2
  [6710c13c] AutoGrad v1.2.3
  [b99e7846] BinaryProvider v0.5.10
  [fa961155] CEnum v0.4.1
  [052768ef] CUDA v1.3.3
  [944b1d66] CodecZlib v0.7.0
  [e66e0078] CompilerSupportLibraries_jll v0.3.3+0
  [864edb3b] DataStructures v0.17.20
  [e2ba6199] ExprTools v0.1.1
  [5789e2e9] FileIO v1.4.1
  [0c68f7d7] GPUArrays v5.1.0
  [61eb1bfa] GPUCompiler v0.6.0
  [033835bb] JLD2 v0.1.14
  [1902f260] Knet v1.4.0
  [929cbde3] LLVM v2.0.0
  [1914dd2f] MacroTools v0.5.5
  [872c559c] NNlib v0.7.4
  [efe28fd5] OpenSpecFun_jll v0.5.3+3
  [bac558e1] OrderedCollections v1.3.0
  [189a3867] Reexport v0.2.0
  [ae029012] Requires v1.0.1
  [276daf66] SpecialFunctions v0.10.3
  [a759f4b9] TimerOutputs v0.5.6
  [3bb67fe8] TranscodingStreams v0.9.5
  [83775a58] Zlib_jll v1.2.11+15
  [2a0f44e3] Base64
  [ade2ca70] Dates
  [8ba89e20] Distributed
  [b77e0a4c] InteractiveUtils
  [76f85450] LibGit2
  [8f399da3] Libdl
  [37e2e46d] LinearAlgebra
  [56ddb016] Logging
  [d6f4376e] Markdown
  [a63ad114] Mmap
  [44cfe95a] Pkg
  [de0858da] Printf
  [3fa0cd96] REPL
  [9a3f8284] Random
  [ea8e919c] SHA
  [9e88b42a] Serialization
  [6462fe0b] Sockets
  [2f01184e] SparseArrays
  [10745b16] Statistics
  [8dfed614] Test
  [cf7118a7] UUIDs
  [4ec0a83e] Unicode
kptr.jl	306.792611 seconds (32.09 M allocations: 1.667 GiB, 0.78% gc time)
gpu.jl	Knet.LibKnet8.libknet8 = "/home/pkgeval/.julia/artifacts/5e1e317677e88277f0ee67ab9e17587a8edc4f7a/libknet8"
readdir(artifact"libknet8") = ["libknet8.so"]
CuDevice(0): Tesla T4
length(CUDA.devices()) = 1
CUDA.capability(CUDA.device()) = v"7.5.0"
CUDA.warpsize(CUDA.device()) = 32
CUDA.find_toolkit() = ["/usr/local/cuda-10.2/targets/x86_64-linux", "/usr/local/cuda-10.2"]
CUDA.version() = v"11.0.0"
Mem.info() = (15614083072, 15843721216)
CUDA.synchronize() = nothing
NVML.driver_version() = v"450.36.6"
NVML.version() = v"11.0.0+450.36.6"
NVML.cuda_driver_version() = v"11.0.0"
NVML.memory_info(nvmldev) = (total = 15843721216, free = 15614083072, used = 229638144)
CUBLAS.handle() = Ptr{Nothing} @0x0000000009fd0f80
CUBLAS.version() = v"10.2.2"
gpu: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/gpu.jl:3
  Got exception outside of a @test
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] top-level scope at show.jl:641
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/gpu.jl:39
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/gpu.jl:8
   [17] include(::String) at ./client.jl:457
   [18] macro expansion at ./timing.jl:174 [inlined]
   [19] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
   [20] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
   [22] include(::String) at ./client.jl:457
   [23] top-level scope at none:6
   [24] eval(::Module, ::Any) at ./boot.jl:331
   [25] exec_options(::Base.JLOptions) at ./client.jl:272
   [26] _start() at ./client.jl:506
  
 14.628406 seconds (12.13 M allocations: 602.236 MiB, 2.04% gc time)
distributions.jl	  3.227369 seconds (4.46 M allocations: 226.285 MiB, 4.72% gc time)
dropout.jl	 17.328159 seconds (5.68 M allocations: 299.880 MiB, 0.84% gc time)
gcnode.jl	gcnode: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/gcnode.jl:8
  Got exception outside of a @test
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] (::RNN)(::KnetArray{Float32,3}; batchSizes::Nothing) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:348
   [14] (::RNN)(::KnetArray{Float32,3}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:328
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/gcnode.jl:18
   [16] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/gcnode.jl:11
   [18] include(::String) at ./client.jl:457
   [19] macro expansion at ./timing.jl:174 [inlined]
   [20] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
   [21] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
   [23] include(::String) at ./client.jl:457
   [24] top-level scope at none:6
   [25] eval(::Module, ::Any) at ./boot.jl:331
   [26] exec_options(::Base.JLOptions) at ./client.jl:272
   [27] _start() at ./client.jl:506
  
  2.620951 seconds (2.40 M allocations: 127.443 MiB, 2.57% gc time)
jld.jl	 30.000413 seconds (37.96 M allocations: 1.832 GiB, 5.49% gc time)
statistics.jl	 27.339781 seconds (25.32 M allocations: 1.313 GiB, 3.43% gc time)
bmm.jl	 61.399322 seconds (52.52 M allocations: 2.656 GiB, 3.11% gc time)
serialize.jl	serialize: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/serialize.jl:10
  Got exception outside of a @test
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] (::RNN)(::KnetArray{Float32,3}; batchSizes::Nothing) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:348
   [14] RNN at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:328 [inlined]
   [15] (::var"#m1test#61")(::RNN, ::KnetArray{Float32,3}, ::Array{Float32,3}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/serialize.jl:40
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/serialize.jl:50
   [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/serialize.jl:11
   [19] include(::String) at ./client.jl:457
   [20] macro expansion at ./timing.jl:174 [inlined]
   [21] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
   [22] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
   [24] include(::String) at ./client.jl:457
   [25] top-level scope at none:6
   [26] eval(::Module, ::Any) at ./boot.jl:331
   [27] exec_options(::Base.JLOptions) at ./client.jl:272
   [28] _start() at ./client.jl:506
  
  9.256071 seconds (9.96 M allocations: 514.643 MiB, 3.83% gc time)
loss.jl	
Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:7 [inlined]
 [14] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:6 [inlined]
 [16] _cudnnSoftmaxForward(::KnetArray{Float64,4}; algo::CUDA.CUDNN.cudnnSoftmaxAlgorithm_t) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:47
 [17] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [18] _cudnnSoftmaxForward(::AutoGrad.Result{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at ./none:0
 [19] logsoftmax(::Param{KnetArray{Float64,2}}; dims::Function) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:13
 [20] logsoftmax(::Param{KnetArray{Float64,2}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:11
 [21] gcsum(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [22] gcsum(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [23] (::AutoGrad.var"#203#205"{Tuple{},typeof(logsoftmax),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [24] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [25] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [26] gradcheck(::typeof(logsoftmax), ::KnetArray{Float64,2}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [27] gradcheck(::Function, ::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
 [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:17
 [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
 [31] include(::String) at ./client.jl:457
 [32] macro expansion at ./timing.jl:174 [inlined]
 [33] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [35] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [36] include(::String) at ./client.jl:457
 [37] top-level scope at none:6
 [38] eval(::Module, ::Any) at ./boot.jl:331
 [39] exec_options(::Base.JLOptions) at ./client.jl:272
 [40] _start() at ./client.jl:506
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:17
  Test threw exception
  Expression: gradcheck(f, k)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(logsoftmax), ::KnetArray{Float64,2}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:17
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:7 [inlined]
   [14] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:6 [inlined]
   [16] _cudnnSoftmaxForward(::KnetArray{Float64,4}; algo::CUDA.CUDNN.cudnnSoftmaxAlgorithm_t) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:47
   [17] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [18] _cudnnSoftmaxForward(::AutoGrad.Result{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at ./none:0
   [19] logsoftmax(::Param{KnetArray{Float64,2}}; dims::Function) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:13
   [20] logsoftmax(::Param{KnetArray{Float64,2}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:11
   [21] gcsum(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [22] gcsum(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [23] (::AutoGrad.var"#203#205"{Tuple{},typeof(logsoftmax),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [24] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [25] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [26] gradcheck(::typeof(logsoftmax), ::KnetArray{Float64,2}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [27] gradcheck(::Function, ::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:17
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:7 [inlined]
 [14] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:6 [inlined]
 [16] _cudnnSoftmaxForward(::KnetArray{Float64,4}; algo::CUDA.CUDNN.cudnnSoftmaxAlgorithm_t) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:47
 [17] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [18] _cudnnSoftmaxForward(::AutoGrad.Result{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at ./none:0
 [19] logsoftmax(::Param{KnetArray{Float64,2}}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:17
 [20] gcsum(::Function, ::Param{KnetArray{Float64,2}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [21] (::AutoGrad.var"#203#205"{Tuple{Pair{Symbol,Int64}},typeof(logsoftmax),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [22] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [23] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [24] gradcheck(::typeof(logsoftmax), ::KnetArray{Float64,2}; kw::Tuple{Pair{Symbol,Int64}}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:18
 [26] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [27] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
 [28] include(::String) at ./client.jl:457
 [29] macro expansion at ./timing.jl:174 [inlined]
 [30] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [33] include(::String) at ./client.jl:457
 [34] top-level scope at none:6
 [35] eval(::Module, ::Any) at ./boot.jl:331
 [36] exec_options(::Base.JLOptions) at ./client.jl:272
 [37] _start() at ./client.jl:506
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:18
  Test threw exception
  Expression: gradcheck(f, k, kw = (:dims => 1,))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(logsoftmax), ::KnetArray{Float64,2}; kw::Tuple{Pair{Symbol,Int64}}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:18
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:7 [inlined]
   [14] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:6 [inlined]
   [16] _cudnnSoftmaxForward(::KnetArray{Float64,4}; algo::CUDA.CUDNN.cudnnSoftmaxAlgorithm_t) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:47
   [17] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [18] _cudnnSoftmaxForward(::AutoGrad.Result{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at ./none:0
   [19] logsoftmax(::Param{KnetArray{Float64,2}}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:17
   [20] gcsum(::Function, ::Param{KnetArray{Float64,2}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [21] (::AutoGrad.var"#203#205"{Tuple{Pair{Symbol,Int64}},typeof(logsoftmax),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [22] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [23] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [24] gradcheck(::typeof(logsoftmax), ::KnetArray{Float64,2}; kw::Tuple{Pair{Symbol,Int64}}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:18
   [26] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [27] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:7 [inlined]
 [14] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:6 [inlined]
 [16] _cudnnSoftmaxForward(::KnetArray{Float64,4}; algo::CUDA.CUDNN.cudnnSoftmaxAlgorithm_t) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:47
 [17] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [18] _cudnnSoftmaxForward(::AutoGrad.Result{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at ./none:0
 [19] logsoftmax(::AutoGrad.Result{KnetArray{Float64,2}}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:17
 [20] logsoftmax(::Param{KnetArray{Float64,2}}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:20
 [21] gcsum(::Function, ::Param{KnetArray{Float64,2}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [22] (::AutoGrad.var"#203#205"{Tuple{Pair{Symbol,Int64}},typeof(logsoftmax),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [23] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [24] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [25] gradcheck(::typeof(logsoftmax), ::KnetArray{Float64,2}; kw::Tuple{Pair{Symbol,Int64}}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [26] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:19
 [27] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
 [29] include(::String) at ./client.jl:457
 [30] macro expansion at ./timing.jl:174 [inlined]
 [31] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [33] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [34] include(::String) at ./client.jl:457
 [35] top-level scope at none:6
 [36] eval(::Module, ::Any) at ./boot.jl:331
 [37] exec_options(::Base.JLOptions) at ./client.jl:272
 [38] _start() at ./client.jl:506
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:19
  Test threw exception
  Expression: gradcheck(f, k, kw = (:dims => 2,))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(logsoftmax), ::KnetArray{Float64,2}; kw::Tuple{Pair{Symbol,Int64}}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:19
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:7 [inlined]
   [14] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:6 [inlined]
   [16] _cudnnSoftmaxForward(::KnetArray{Float64,4}; algo::CUDA.CUDNN.cudnnSoftmaxAlgorithm_t) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:47
   [17] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [18] _cudnnSoftmaxForward(::AutoGrad.Result{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at ./none:0
   [19] logsoftmax(::AutoGrad.Result{KnetArray{Float64,2}}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:17
   [20] logsoftmax(::Param{KnetArray{Float64,2}}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:20
   [21] gcsum(::Function, ::Param{KnetArray{Float64,2}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [22] (::AutoGrad.var"#203#205"{Tuple{Pair{Symbol,Int64}},typeof(logsoftmax),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [23] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [24] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [25] gradcheck(::typeof(logsoftmax), ::KnetArray{Float64,2}; kw::Tuple{Pair{Symbol,Int64}}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [26] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:19
   [27] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:20
  Test threw exception
  Expression: isapprox(f(a), f(k))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:7 [inlined]
   [14] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:6 [inlined]
   [16] _cudnnSoftmaxForward(::KnetArray{Float64,4}; algo::CUDA.CUDNN.cudnnSoftmaxAlgorithm_t) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:47
   [17] #logsoftmax#46 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:13 [inlined]
   [18] logsoftmax(::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:11
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:20
   [20] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:21
  Test threw exception
  Expression: isapprox(f(a, dims = 1), f(k, dims = 1))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:7 [inlined]
   [14] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:6 [inlined]
   [16] _cudnnSoftmaxForward(::KnetArray{Float64,4}; algo::CUDA.CUDNN.cudnnSoftmaxAlgorithm_t) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:47
   [17] logsoftmax(::KnetArray{Float64,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:17
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:21
   [19] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:22
  Test threw exception
  Expression: isapprox(f(a, dims = 2), f(k, dims = 2))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:7 [inlined]
   [14] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:6 [inlined]
   [16] _cudnnSoftmaxForward(::KnetArray{Float64,4}; algo::CUDA.CUDNN.cudnnSoftmaxAlgorithm_t) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:47
   [17] logsoftmax(::KnetArray{Float64,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:17
   [18] logsoftmax(::KnetArray{Float64,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:20
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:22
   [20] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:7 [inlined]
 [14] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:6 [inlined]
 [16] _cudnnSoftmaxForward(::KnetArray{Float64,4}; algo::CUDA.CUDNN.cudnnSoftmaxAlgorithm_t) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:47
 [17] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [18] _cudnnSoftmaxForward(::AutoGrad.Result{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at ./none:0
 [19] logsoftmax(::Param{KnetArray{Float64,3}}; dims::Function) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:13
 [20] logsoftmax(::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:11
 [21] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [22] gcsum(::Function, ::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [23] (::AutoGrad.var"#203#205"{Tuple{},typeof(logsoftmax),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [24] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [25] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [26] gradcheck(::typeof(logsoftmax), ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [27] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
 [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:36
 [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
 [31] include(::String) at ./client.jl:457
 [32] macro expansion at ./timing.jl:174 [inlined]
 [33] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [35] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [36] include(::String) at ./client.jl:457
 [37] top-level scope at none:6
 [38] eval(::Module, ::Any) at ./boot.jl:331
 [39] exec_options(::Base.JLOptions) at ./client.jl:272
 [40] _start() at ./client.jl:506
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:36
  Test threw exception
  Expression: gradcheck(f, k)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(logsoftmax), ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:36
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:7 [inlined]
   [14] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:6 [inlined]
   [16] _cudnnSoftmaxForward(::KnetArray{Float64,4}; algo::CUDA.CUDNN.cudnnSoftmaxAlgorithm_t) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:47
   [17] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [18] _cudnnSoftmaxForward(::AutoGrad.Result{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at ./none:0
   [19] logsoftmax(::Param{KnetArray{Float64,3}}; dims::Function) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:13
   [20] logsoftmax(::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:11
   [21] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [22] gcsum(::Function, ::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [23] (::AutoGrad.var"#203#205"{Tuple{},typeof(logsoftmax),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [24] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [25] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [26] gradcheck(::typeof(logsoftmax), ::KnetArray{Float64,3}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [27] gradcheck(::Function, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:36
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:37
  Test threw exception
  Expression: isapprox(f(a), f(k))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:7 [inlined]
   [14] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:6 [inlined]
   [16] _cudnnSoftmaxForward(::KnetArray{Float64,4}; algo::CUDA.CUDNN.cudnnSoftmaxAlgorithm_t) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:47
   [17] #logsoftmax#46 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:13 [inlined]
   [18] logsoftmax(::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:11
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:37
   [20] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:7 [inlined]
 [14] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:6 [inlined]
 [16] _cudnnSoftmaxForward(::KnetArray{Float64,4}; algo::CUDA.CUDNN.cudnnSoftmaxAlgorithm_t) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:47
 [17] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [18] _cudnnSoftmaxForward(::AutoGrad.Result{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at ./none:0
 [19] logsoftmax(::Param{KnetArray{Float64,3}}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:17
 [20] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [21] (::AutoGrad.var"#203#205"{Tuple{Pair{Symbol,Int64}},typeof(logsoftmax),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [22] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [23] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [24] gradcheck(::typeof(logsoftmax), ::KnetArray{Float64,3}; kw::Tuple{Pair{Symbol,Int64}}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:39
 [26] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [27] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
 [28] include(::String) at ./client.jl:457
 [29] macro expansion at ./timing.jl:174 [inlined]
 [30] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [31] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [33] include(::String) at ./client.jl:457
 [34] top-level scope at none:6
 [35] eval(::Module, ::Any) at ./boot.jl:331
 [36] exec_options(::Base.JLOptions) at ./client.jl:272
 [37] _start() at ./client.jl:506
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:39
  Test threw exception
  Expression: gradcheck(f, k, kw = (:dims => d,))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(logsoftmax), ::KnetArray{Float64,3}; kw::Tuple{Pair{Symbol,Int64}}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:39
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:7 [inlined]
   [14] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:6 [inlined]
   [16] _cudnnSoftmaxForward(::KnetArray{Float64,4}; algo::CUDA.CUDNN.cudnnSoftmaxAlgorithm_t) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:47
   [17] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [18] _cudnnSoftmaxForward(::AutoGrad.Result{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at ./none:0
   [19] logsoftmax(::Param{KnetArray{Float64,3}}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:17
   [20] gcsum(::Function, ::Param{KnetArray{Float64,3}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [21] (::AutoGrad.var"#203#205"{Tuple{Pair{Symbol,Int64}},typeof(logsoftmax),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [22] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [23] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [24] gradcheck(::typeof(logsoftmax), ::KnetArray{Float64,3}; kw::Tuple{Pair{Symbol,Int64}}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:39
   [26] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [27] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:40
  Test threw exception
  Expression: isapprox(f(a, dims = d), f(k, dims = d))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:7 [inlined]
   [14] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:6 [inlined]
   [16] _cudnnSoftmaxForward(::KnetArray{Float64,4}; algo::CUDA.CUDNN.cudnnSoftmaxAlgorithm_t) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:47
   [17] logsoftmax(::KnetArray{Float64,3}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:17
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:40
   [19] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:53
  Test threw exception
  Expression: softmax(k, dims = d) ≈ exp.(logsoftmax(k, dims = d))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:7 [inlined]
   [14] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:6 [inlined]
   [16] _cudnnSoftmaxForward(::KnetArray{Float64,4}; algo::CUDA.CUDNN.cudnnSoftmaxAlgorithm_t) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:47
   [17] softmax(::KnetArray{Float64,3}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:34
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:53
   [19] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:54
  Test threw exception
  Expression: all(Array(sum(softmax(k, dims = d), dims = d)) .≈ 1)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:7 [inlined]
   [14] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:6 [inlined]
   [16] _cudnnSoftmaxForward(::KnetArray{Float64,4}; algo::CUDA.CUDNN.cudnnSoftmaxAlgorithm_t) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:47
   [17] softmax(::KnetArray{Float64,3}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:34
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:54
   [19] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:53
  Test threw exception
  Expression: softmax(k, dims = d) ≈ exp.(logsoftmax(k, dims = d))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:7 [inlined]
   [14] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:6 [inlined]
   [16] _cudnnSoftmaxForward(::KnetArray{Float64,4}; algo::CUDA.CUDNN.cudnnSoftmaxAlgorithm_t) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:47
   [17] softmax(::KnetArray{Float64,3}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:34
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:53
   [19] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:54
  Test threw exception
  Expression: all(Array(sum(softmax(k, dims = d), dims = d)) .≈ 1)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:7 [inlined]
   [14] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:6 [inlined]
   [16] _cudnnSoftmaxForward(::KnetArray{Float64,4}; algo::CUDA.CUDNN.cudnnSoftmaxAlgorithm_t) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:47
   [17] softmax(::KnetArray{Float64,3}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:34
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:54
   [19] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:7 [inlined]
 [14] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:6 [inlined]
 [16] _cudnnSoftmaxForward(::KnetArray{Float64,4}; algo::CUDA.CUDNN.cudnnSoftmaxAlgorithm_t) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:47
 [17] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [18] _cudnnSoftmaxForward(::AutoGrad.Result{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at ./none:0
 [19] logsoftmax(::Param{KnetArray{Float64,2}}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:17
 [20] nll(::Param{KnetArray{Float64,2}}, ::Array{Int64,1}; dims::Int64, average::Bool) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/loss.jl:39
 [21] gcsum(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [22] (::AutoGrad.var"#203#205"{Tuple{Pair{Symbol,Int64}},typeof(nll),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [23] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [24] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [25] gradcheck(::typeof(nll), ::KnetArray{Float64,2}, ::Vararg{Any,N} where N; kw::Tuple{Pair{Symbol,Int64}}, args::Int64, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [26] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:69
 [27] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
 [29] include(::String) at ./client.jl:457
 [30] macro expansion at ./timing.jl:174 [inlined]
 [31] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [33] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [34] include(::String) at ./client.jl:457
 [35] top-level scope at none:6
 [36] eval(::Module, ::Any) at ./boot.jl:331
 [37] exec_options(::Base.JLOptions) at ./client.jl:272
 [38] _start() at ./client.jl:506
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:69
  Test threw exception
  Expression: gradcheck(nll, k, indices, kw = (:dims => 1,), args = 1)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(nll), ::KnetArray{Float64,2}, ::Vararg{Any,N} where N; kw::Tuple{Pair{Symbol,Int64}}, args::Int64, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:69
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:7 [inlined]
   [14] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:6 [inlined]
   [16] _cudnnSoftmaxForward(::KnetArray{Float64,4}; algo::CUDA.CUDNN.cudnnSoftmaxAlgorithm_t) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:47
   [17] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [18] _cudnnSoftmaxForward(::AutoGrad.Result{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at ./none:0
   [19] logsoftmax(::Param{KnetArray{Float64,2}}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:17
   [20] nll(::Param{KnetArray{Float64,2}}, ::Array{Int64,1}; dims::Int64, average::Bool) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/loss.jl:39
   [21] gcsum(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [22] (::AutoGrad.var"#203#205"{Tuple{Pair{Symbol,Int64}},typeof(nll),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [23] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [24] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [25] gradcheck(::typeof(nll), ::KnetArray{Float64,2}, ::Vararg{Any,N} where N; kw::Tuple{Pair{Symbol,Int64}}, args::Int64, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [26] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:69
   [27] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:7 [inlined]
 [14] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:6 [inlined]
 [16] _cudnnSoftmaxForward(::KnetArray{Float64,4}; algo::CUDA.CUDNN.cudnnSoftmaxAlgorithm_t) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:47
 [17] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [18] _cudnnSoftmaxForward(::AutoGrad.Result{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at ./none:0
 [19] logsoftmax(::AutoGrad.Result{KnetArray{Float64,2}}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:17
 [20] logsoftmax(::Param{KnetArray{Float64,2}}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:20
 [21] nll(::Param{KnetArray{Float64,2}}, ::Array{Int64,1}; dims::Int64, average::Bool) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/loss.jl:39
 [22] gcsum(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [23] (::AutoGrad.var"#203#205"{Tuple{Pair{Symbol,Int64}},typeof(nll),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [24] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [25] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [26] gradcheck(::typeof(nll), ::KnetArray{Float64,2}, ::Vararg{Any,N} where N; kw::Tuple{Pair{Symbol,Int64}}, args::Int64, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [27] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:70
 [28] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
 [30] include(::String) at ./client.jl:457
 [31] macro expansion at ./timing.jl:174 [inlined]
 [32] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [34] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [35] include(::String) at ./client.jl:457
 [36] top-level scope at none:6
 [37] eval(::Module, ::Any) at ./boot.jl:331
 [38] exec_options(::Base.JLOptions) at ./client.jl:272
 [39] _start() at ./client.jl:506
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:70
  Test threw exception
  Expression: gradcheck(nll, k, indices, kw = (:dims => 2,), args = 1)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(nll), ::KnetArray{Float64,2}, ::Vararg{Any,N} where N; kw::Tuple{Pair{Symbol,Int64}}, args::Int64, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:70
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:7 [inlined]
   [14] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:6 [inlined]
   [16] _cudnnSoftmaxForward(::KnetArray{Float64,4}; algo::CUDA.CUDNN.cudnnSoftmaxAlgorithm_t) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:47
   [17] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [18] _cudnnSoftmaxForward(::AutoGrad.Result{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at ./none:0
   [19] logsoftmax(::AutoGrad.Result{KnetArray{Float64,2}}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:17
   [20] logsoftmax(::Param{KnetArray{Float64,2}}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:20
   [21] nll(::Param{KnetArray{Float64,2}}, ::Array{Int64,1}; dims::Int64, average::Bool) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/loss.jl:39
   [22] gcsum(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:dims,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [23] (::AutoGrad.var"#203#205"{Tuple{Pair{Symbol,Int64}},typeof(nll),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [24] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [25] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [26] gradcheck(::typeof(nll), ::KnetArray{Float64,2}, ::Vararg{Any,N} where N; kw::Tuple{Pair{Symbol,Int64}}, args::Int64, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [27] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:70
   [28] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:73
  Test threw exception
  Expression: isapprox(nll(k, indices, dims = 1), nll(a, indices, dims = 1))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:7 [inlined]
   [14] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:6 [inlined]
   [16] _cudnnSoftmaxForward(::KnetArray{Float64,4}; algo::CUDA.CUDNN.cudnnSoftmaxAlgorithm_t) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:47
   [17] logsoftmax(::KnetArray{Float64,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:17
   [18] nll(::KnetArray{Float64,2}, ::Array{Int64,1}; dims::Int64, average::Bool) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/loss.jl:39
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:73
   [20] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:74
  Test threw exception
  Expression: isapprox(nll(k, indices, dims = 2), nll(a, indices, dims = 2))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:7 [inlined]
   [14] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:6 [inlined]
   [16] _cudnnSoftmaxForward(::KnetArray{Float64,4}; algo::CUDA.CUDNN.cudnnSoftmaxAlgorithm_t) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:47
   [17] logsoftmax(::KnetArray{Float64,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:17
   [18] logsoftmax(::KnetArray{Float64,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:20
   [19] nll(::KnetArray{Float64,2}, ::Array{Int64,1}; dims::Int64, average::Bool) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/loss.jl:39
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:74
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:87
  Test threw exception
  Expression: isapprox(softmax(x, dims = 1), _cudnnSoftmaxForward(x, algo = CUDNN_SOFTMAX_FAST))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:7 [inlined]
   [14] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:6 [inlined]
   [16] _cudnnSoftmaxForward(::KnetArray{Float64,4}; algo::CUDA.CUDNN.cudnnSoftmaxAlgorithm_t) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:47
   [17] softmax(::KnetArray{Float64,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:34
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:87
   [19] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:88
  Test threw exception
  Expression: isapprox(softmax(x, dims = 1), _cudnnSoftmaxForward(x, algo = CUDNN_SOFTMAX_ACCURATE))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:7 [inlined]
   [14] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:6 [inlined]
   [16] _cudnnSoftmaxForward(::KnetArray{Float64,4}; algo::CUDA.CUDNN.cudnnSoftmaxAlgorithm_t) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:47
   [17] softmax(::KnetArray{Float64,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:34
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:88
   [19] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:89
  Test threw exception
  Expression: isapprox(logsoftmax(x, dims = 1), _cudnnSoftmaxForward(x, algo = CUDNN_SOFTMAX_LOG))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:7 [inlined]
   [14] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:6 [inlined]
   [16] _cudnnSoftmaxForward(::KnetArray{Float64,4}; algo::CUDA.CUDNN.cudnnSoftmaxAlgorithm_t) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:47
   [17] logsoftmax(::KnetArray{Float64,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:17
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:89
   [19] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:90
  Test threw exception
  Expression: isapprox(∇softmax(x, y1, dy, dims = 1), _cudnnSoftmaxBackward(y1, dy, algo = CUDNN_SOFTMAX_FAST))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:7 [inlined]
   [14] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:6 [inlined]
   [16] _cudnnSoftmaxBackward(::KnetArray{Float64,2}, ::KnetArray{Float64,2}; algo::CUDA.CUDNN.cudnnSoftmaxAlgorithm_t) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:54
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:90
   [18] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:91
  Test threw exception
  Expression: isapprox(∇softmax(x, y1, dy, dims = 1), _cudnnSoftmaxBackward(y1, dy, algo = CUDNN_SOFTMAX_ACCURATE))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:7 [inlined]
   [14] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:6 [inlined]
   [16] _cudnnSoftmaxBackward(::KnetArray{Float64,2}, ::KnetArray{Float64,2}; algo::CUDA.CUDNN.cudnnSoftmaxAlgorithm_t) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:54
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:91
   [18] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:92
  Test threw exception
  Expression: isapprox(∇logsoftmax(x, y2, dy, dims = 1), _cudnnSoftmaxBackward(y2, dy, algo = CUDNN_SOFTMAX_LOG))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:7 [inlined]
   [14] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:6 [inlined]
   [16] _cudnnSoftmaxBackward(::KnetArray{Float64,2}, ::KnetArray{Float64,2}; algo::CUDA.CUDNN.cudnnSoftmaxAlgorithm_t) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:54
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:92
   [18] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:7 [inlined]
 [14] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:6 [inlined]
 [16] _cudnnSoftmaxForward(::KnetArray{Float64,2}; algo::CUDA.CUDNN.cudnnSoftmaxAlgorithm_t) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:47
 [17] forw(::Function, ::Param{KnetArray{Float64,2}}; kwargs::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [18] _cudnnSoftmaxForward(::Param{KnetArray{Float64,2}}; o::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at ./none:0
 [19] (::var"#70#80")() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
 [20] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [21] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
 [22] (::AutoGrad.var"#220#225"{Tuple{},var"#70#80",Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [23] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [24] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [25] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
 [26] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
 [27] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:93
 [28] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
 [30] include(::String) at ./client.jl:457
 [31] macro expansion at ./timing.jl:174 [inlined]
 [32] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [34] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [35] include(::String) at ./client.jl:457
 [36] top-level scope at none:6
 [37] eval(::Module, ::Any) at ./boot.jl:331
 [38] exec_options(::Base.JLOptions) at ./client.jl:272
 [39] _start() at ./client.jl:506
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:93
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:93 =# @gcheck _cudnnSoftmaxForward(Param(x), algo = CUDNN_SOFTMAX_FAST)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:93
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:7 [inlined]
   [14] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:6 [inlined]
   [16] _cudnnSoftmaxForward(::KnetArray{Float64,2}; algo::CUDA.CUDNN.cudnnSoftmaxAlgorithm_t) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:47
   [17] forw(::Function, ::Param{KnetArray{Float64,2}}; kwargs::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [18] _cudnnSoftmaxForward(::Param{KnetArray{Float64,2}}; o::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at ./none:0
   [19] (::var"#70#80")() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
   [20] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [21] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
   [22] (::AutoGrad.var"#220#225"{Tuple{},var"#70#80",Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [23] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [24] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [25] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [26] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [27] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:93
   [28] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:7 [inlined]
 [14] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:6 [inlined]
 [16] _cudnnSoftmaxForward(::KnetArray{Float64,2}; algo::CUDA.CUDNN.cudnnSoftmaxAlgorithm_t) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:47
 [17] forw(::Function, ::Param{KnetArray{Float64,2}}; kwargs::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [18] _cudnnSoftmaxForward(::Param{KnetArray{Float64,2}}; o::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at ./none:0
 [19] (::var"#71#81")() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
 [20] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [21] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
 [22] (::AutoGrad.var"#220#225"{Tuple{},var"#71#81",Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [23] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [24] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [25] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
 [26] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
 [27] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:94
 [28] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
 [30] include(::String) at ./client.jl:457
 [31] macro expansion at ./timing.jl:174 [inlined]
 [32] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [34] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [35] include(::String) at ./client.jl:457
 [36] top-level scope at none:6
 [37] eval(::Module, ::Any) at ./boot.jl:331
 [38] exec_options(::Base.JLOptions) at ./client.jl:272
 [39] _start() at ./client.jl:506
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:94
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:94 =# @gcheck _cudnnSoftmaxForward(Param(x), algo = CUDNN_SOFTMAX_ACCURATE)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:94
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:7 [inlined]
   [14] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:6 [inlined]
   [16] _cudnnSoftmaxForward(::KnetArray{Float64,2}; algo::CUDA.CUDNN.cudnnSoftmaxAlgorithm_t) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:47
   [17] forw(::Function, ::Param{KnetArray{Float64,2}}; kwargs::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [18] _cudnnSoftmaxForward(::Param{KnetArray{Float64,2}}; o::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at ./none:0
   [19] (::var"#71#81")() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
   [20] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [21] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
   [22] (::AutoGrad.var"#220#225"{Tuple{},var"#71#81",Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [23] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [24] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [25] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [26] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [27] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:94
   [28] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:7 [inlined]
 [14] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:6 [inlined]
 [16] _cudnnSoftmaxForward(::KnetArray{Float64,2}; algo::CUDA.CUDNN.cudnnSoftmaxAlgorithm_t) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:47
 [17] forw(::Function, ::Param{KnetArray{Float64,2}}; kwargs::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [18] _cudnnSoftmaxForward(::Param{KnetArray{Float64,2}}; o::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at ./none:0
 [19] (::var"#72#82")() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
 [20] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [21] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
 [22] (::AutoGrad.var"#220#225"{Tuple{},var"#72#82",Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [23] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [24] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [25] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
 [26] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
 [27] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:95
 [28] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
 [30] include(::String) at ./client.jl:457
 [31] macro expansion at ./timing.jl:174 [inlined]
 [32] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [34] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [35] include(::String) at ./client.jl:457
 [36] top-level scope at none:6
 [37] eval(::Module, ::Any) at ./boot.jl:331
 [38] exec_options(::Base.JLOptions) at ./client.jl:272
 [39] _start() at ./client.jl:506
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:95
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:95 =# @gcheck _cudnnSoftmaxForward(Param(x), algo = CUDNN_SOFTMAX_LOG)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:95
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:7 [inlined]
   [14] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:6 [inlined]
   [16] _cudnnSoftmaxForward(::KnetArray{Float64,2}; algo::CUDA.CUDNN.cudnnSoftmaxAlgorithm_t) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:47
   [17] forw(::Function, ::Param{KnetArray{Float64,2}}; kwargs::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [18] _cudnnSoftmaxForward(::Param{KnetArray{Float64,2}}; o::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at ./none:0
   [19] (::var"#72#82")() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
   [20] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [21] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
   [22] (::AutoGrad.var"#220#225"{Tuple{},var"#72#82",Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [23] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [24] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [25] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [26] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [27] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:95
   [28] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:7 [inlined]
 [14] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:6 [inlined]
 [16] _cudnnSoftmaxBackward(::KnetArray{Float64,2}, ::KnetArray{Float64,2}; algo::CUDA.CUDNN.cudnnSoftmaxAlgorithm_t) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:54
 [17] forw(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Param{KnetArray{Float64,2}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [18] _cudnnSoftmaxBackward(::Param{KnetArray{Float64,2}}, ::Param{KnetArray{Float64,2}}; o::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at ./none:0
 [19] (::var"#73#83")() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
 [20] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [21] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
 [22] (::AutoGrad.var"#220#225"{Tuple{},var"#73#83",Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [23] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [24] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [25] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
 [26] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
 [27] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:96
 [28] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
 [30] include(::String) at ./client.jl:457
 [31] macro expansion at ./timing.jl:174 [inlined]
 [32] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [34] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [35] include(::String) at ./client.jl:457
 [36] top-level scope at none:6
 [37] eval(::Module, ::Any) at ./boot.jl:331
 [38] exec_options(::Base.JLOptions) at ./client.jl:272
 [39] _start() at ./client.jl:506
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:96
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:96 =# @gcheck _cudnnSoftmaxBackward(Param(y1), Param(dy), algo = CUDNN_SOFTMAX_FAST)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:96
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:7 [inlined]
   [14] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:6 [inlined]
   [16] _cudnnSoftmaxBackward(::KnetArray{Float64,2}, ::KnetArray{Float64,2}; algo::CUDA.CUDNN.cudnnSoftmaxAlgorithm_t) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:54
   [17] forw(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Param{KnetArray{Float64,2}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [18] _cudnnSoftmaxBackward(::Param{KnetArray{Float64,2}}, ::Param{KnetArray{Float64,2}}; o::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at ./none:0
   [19] (::var"#73#83")() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
   [20] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [21] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
   [22] (::AutoGrad.var"#220#225"{Tuple{},var"#73#83",Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [23] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [24] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [25] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [26] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [27] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:96
   [28] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:7 [inlined]
 [14] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:6 [inlined]
 [16] _cudnnSoftmaxBackward(::KnetArray{Float64,2}, ::KnetArray{Float64,2}; algo::CUDA.CUDNN.cudnnSoftmaxAlgorithm_t) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:54
 [17] forw(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Param{KnetArray{Float64,2}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [18] _cudnnSoftmaxBackward(::Param{KnetArray{Float64,2}}, ::Param{KnetArray{Float64,2}}; o::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at ./none:0
 [19] (::var"#74#84")() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
 [20] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [21] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
 [22] (::AutoGrad.var"#220#225"{Tuple{},var"#74#84",Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [23] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [24] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [25] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
 [26] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
 [27] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:97
 [28] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
 [30] include(::String) at ./client.jl:457
 [31] macro expansion at ./timing.jl:174 [inlined]
 [32] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [34] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [35] include(::String) at ./client.jl:457
 [36] top-level scope at none:6
 [37] eval(::Module, ::Any) at ./boot.jl:331
 [38] exec_options(::Base.JLOptions) at ./client.jl:272
 [39] _start() at ./client.jl:506
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:97
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:97 =# @gcheck _cudnnSoftmaxBackward(Param(y1), Param(dy), algo = CUDNN_SOFTMAX_ACCURATE)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:97
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:7 [inlined]
   [14] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:6 [inlined]
   [16] _cudnnSoftmaxBackward(::KnetArray{Float64,2}, ::KnetArray{Float64,2}; algo::CUDA.CUDNN.cudnnSoftmaxAlgorithm_t) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:54
   [17] forw(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Param{KnetArray{Float64,2}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [18] _cudnnSoftmaxBackward(::Param{KnetArray{Float64,2}}, ::Param{KnetArray{Float64,2}}; o::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at ./none:0
   [19] (::var"#74#84")() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
   [20] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [21] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
   [22] (::AutoGrad.var"#220#225"{Tuple{},var"#74#84",Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [23] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [24] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [25] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [26] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [27] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:97
   [28] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:7 [inlined]
 [14] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:6 [inlined]
 [16] _cudnnSoftmaxBackward(::KnetArray{Float64,2}, ::KnetArray{Float64,2}; algo::CUDA.CUDNN.cudnnSoftmaxAlgorithm_t) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:54
 [17] forw(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Param{KnetArray{Float64,2}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [18] _cudnnSoftmaxBackward(::Param{KnetArray{Float64,2}}, ::Param{KnetArray{Float64,2}}; o::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at ./none:0
 [19] (::var"#75#85")() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
 [20] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [21] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
 [22] (::AutoGrad.var"#220#225"{Tuple{},var"#75#85",Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [23] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [24] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [25] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
 [26] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
 [27] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:98
 [28] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
 [30] include(::String) at ./client.jl:457
 [31] macro expansion at ./timing.jl:174 [inlined]
 [32] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [34] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [35] include(::String) at ./client.jl:457
 [36] top-level scope at none:6
 [37] eval(::Module, ::Any) at ./boot.jl:331
 [38] exec_options(::Base.JLOptions) at ./client.jl:272
 [39] _start() at ./client.jl:506
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:98
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:98 =# @gcheck _cudnnSoftmaxBackward(Param(y2), Param(dy), algo = CUDNN_SOFTMAX_LOG)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [4] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:98
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:7 [inlined]
   [14] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:6 [inlined]
   [16] _cudnnSoftmaxBackward(::KnetArray{Float64,2}, ::KnetArray{Float64,2}; algo::CUDA.CUDNN.cudnnSoftmaxAlgorithm_t) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:54
   [17] forw(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Param{KnetArray{Float64,2}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [18] _cudnnSoftmaxBackward(::Param{KnetArray{Float64,2}}, ::Param{KnetArray{Float64,2}}; o::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at ./none:0
   [19] (::var"#75#85")() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
   [20] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [21] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
   [22] (::AutoGrad.var"#220#225"{Tuple{},var"#75#85",Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [23] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [24] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [25] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [26] gcheck(::Function) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [27] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:98
   [28] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:107
  Test threw exception
  Expression: isapprox(f(a, b, c), f(A, B, C))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:7 [inlined]
   [14] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:6 [inlined]
   [16] _cudnnSoftmaxForward(::KnetArray{Float64,4}; algo::CUDA.CUDNN.cudnnSoftmaxAlgorithm_t) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:47
   [17] logsoftmax(::KnetArray{Float64,2}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:17
   [18] nll(::KnetArray{Float64,2}, ::Array{Int64,1}; dims::Int64, average::Bool) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/loss.jl:39
   [19] nll at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/loss.jl:38 [inlined]
   [20] (::var"#f#86")(::KnetArray{Float64,2}, ::KnetArray{Float64,2}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:101
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:107
   [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:7 [inlined]
 [14] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:6 [inlined]
 [16] _cudnnSoftmaxForward(::KnetArray{Float64,4}; algo::CUDA.CUDNN.cudnnSoftmaxAlgorithm_t) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:47
 [17] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [18] _cudnnSoftmaxForward(::AutoGrad.Result{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at ./none:0
 [19] logsoftmax(::AutoGrad.Result{KnetArray{Float64,2}}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:17
 [20] nll(::AutoGrad.Result{KnetArray{Float64,2}}, ::Array{Int64,1}; dims::Int64, average::Bool) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/loss.jl:39
 [21] nll(::AutoGrad.Result{KnetArray{Float64,2}}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/loss.jl:38
 [22] (::var"#f#86")(::Param{KnetArray{Float64,2}}, ::KnetArray{Float64,2}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:101
 [23] differentiate(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [24] differentiate(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135
 [25] (::AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#f#86",Int64,Bool}})(::KnetArray{Float64,2}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:225
 [26] (::AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#f#86",Int64,Bool}})(::KnetArray{Float64,2}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:221
 [27] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:108
 [28] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
 [30] include(::String) at ./client.jl:457
 [31] macro expansion at ./timing.jl:174 [inlined]
 [32] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [34] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [35] include(::String) at ./client.jl:457
 [36] top-level scope at none:6
 [37] eval(::Module, ::Any) at ./boot.jl:331
 [38] exec_options(::Base.JLOptions) at ./client.jl:272
 [39] _start() at ./client.jl:506
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:108
  Test threw exception
  Expression: isapprox(∇f(a, b, c), ∇f(A, B, C))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135
   [3] (::AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#f#86",Int64,Bool}})(::KnetArray{Float64,2}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:225
   [4] (::AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#f#86",Int64,Bool}})(::KnetArray{Float64,2}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:221
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:108
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:7 [inlined]
   [14] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:6 [inlined]
   [16] _cudnnSoftmaxForward(::KnetArray{Float64,4}; algo::CUDA.CUDNN.cudnnSoftmaxAlgorithm_t) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:47
   [17] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [18] _cudnnSoftmaxForward(::AutoGrad.Result{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at ./none:0
   [19] logsoftmax(::AutoGrad.Result{KnetArray{Float64,2}}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:17
   [20] nll(::AutoGrad.Result{KnetArray{Float64,2}}, ::Array{Int64,1}; dims::Int64, average::Bool) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/loss.jl:39
   [21] nll(::AutoGrad.Result{KnetArray{Float64,2}}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/loss.jl:38
   [22] (::var"#f#86")(::Param{KnetArray{Float64,2}}, ::KnetArray{Float64,2}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:101
   [23] differentiate(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [24] differentiate(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135
   [25] (::AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#f#86",Int64,Bool}})(::KnetArray{Float64,2}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:225
   [26] (::AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#f#86",Int64,Bool}})(::KnetArray{Float64,2}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:221
   [27] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:108
   [28] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:7 [inlined]
 [14] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:6 [inlined]
 [16] _cudnnSoftmaxForward(::KnetArray{Float64,4}; algo::CUDA.CUDNN.cudnnSoftmaxAlgorithm_t) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:47
 [17] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [18] _cudnnSoftmaxForward(::AutoGrad.Result{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at ./none:0
 [19] logsoftmax(::AutoGrad.Result{KnetArray{Float64,2}}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:17
 [20] nll(::AutoGrad.Result{KnetArray{Float64,2}}, ::Array{Int64,1}; dims::Int64, average::Bool) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/loss.jl:39
 [21] nll(::AutoGrad.Result{KnetArray{Float64,2}}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/loss.jl:38
 [22] (::var"#f#86")(::AutoGrad.Result{KnetArray{Float64,2}}, ::KnetArray{Float64,2}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:101
 [23] differentiate(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [24] differentiate(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135
 [25] (::AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#f#86",Int64,Bool}})(::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:225
 [26] gradfun at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:221 [inlined]
 [27] (::var"#∇fj#87"{AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#f#86",Int64,Bool}}})(::Param{KnetArray{Float64,2}}, ::KnetArray{Float64,2}, ::Array{Int64,1}, ::Int64) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:103
 [28] differentiate(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [29] differentiate(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135
 [30] (::AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#∇fj#87"{AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#f#86",Int64,Bool}}},Int64,Bool}})(::KnetArray{Float64,2}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:225
 [31] (::AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#∇fj#87"{AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#f#86",Int64,Bool}}},Int64,Bool}})(::KnetArray{Float64,2}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:221
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:110
 [33] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [34] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
 [35] include(::String) at ./client.jl:457
 [36] macro expansion at ./timing.jl:174 [inlined]
 [37] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [38] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [39] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [40] include(::String) at ./client.jl:457
 [41] top-level scope at none:6
 [42] eval(::Module, ::Any) at ./boot.jl:331
 [43] exec_options(::Base.JLOptions) at ./client.jl:272
 [44] _start() at ./client.jl:506

Stacktrace:
 [1] differentiate(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
 [2] differentiate(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135
 [3] (::AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#f#86",Int64,Bool}})(::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:225
 [4] gradfun at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:221 [inlined]
 [5] (::var"#∇fj#87"{AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#f#86",Int64,Bool}}})(::Param{KnetArray{Float64,2}}, ::KnetArray{Float64,2}, ::Array{Int64,1}, ::Int64) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:103
 [6] differentiate(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [7] differentiate(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135
 [8] (::AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#∇fj#87"{AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#f#86",Int64,Bool}}},Int64,Bool}})(::KnetArray{Float64,2}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:225
 [9] (::AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#∇fj#87"{AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#f#86",Int64,Bool}}},Int64,Bool}})(::KnetArray{Float64,2}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:221
 [10] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:110
 [11] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [12] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
 [13] include(::String) at ./client.jl:457
 [14] macro expansion at ./timing.jl:174 [inlined]
 [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [16] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [17] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [18] include(::String) at ./client.jl:457
 [19] top-level scope at none:6
 [20] eval(::Module, ::Any) at ./boot.jl:331
 [21] exec_options(::Base.JLOptions) at ./client.jl:272
 [22] _start() at ./client.jl:506
loss: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:110
  Test threw exception
  Expression: isapprox(∇∇fj(a, b, c, i), ∇∇fj(A, B, C, i))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135
   [3] (::AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#∇fj#87"{AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#f#86",Int64,Bool}}},Int64,Bool}})(::KnetArray{Float64,2}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:225
   [4] (::AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#∇fj#87"{AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#f#86",Int64,Bool}}},Int64,Bool}})(::KnetArray{Float64,2}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:221
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:110
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  caused by [exception 2]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135
   [3] (::AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#f#86",Int64,Bool}})(::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:225
   [4] gradfun at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:221 [inlined]
   [5] (::var"#∇fj#87"{AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#f#86",Int64,Bool}}})(::Param{KnetArray{Float64,2}}, ::KnetArray{Float64,2}, ::Array{Int64,1}, ::Int64) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:103
   [6] differentiate(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [7] differentiate(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135
   [8] (::AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#∇fj#87"{AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#f#86",Int64,Bool}}},Int64,Bool}})(::KnetArray{Float64,2}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:225
   [9] (::AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#∇fj#87"{AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#f#86",Int64,Bool}}},Int64,Bool}})(::KnetArray{Float64,2}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:221
   [10] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:110
   [11] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [12] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:7 [inlined]
   [14] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [15] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/cudnn_retry.jl:6 [inlined]
   [16] _cudnnSoftmaxForward(::KnetArray{Float64,4}; algo::CUDA.CUDNN.cudnnSoftmaxAlgorithm_t) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:47
   [17] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [18] _cudnnSoftmaxForward(::AutoGrad.Result{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,CUDA.CUDNN.cudnnSoftmaxAlgorithm_t,Tuple{Symbol},NamedTuple{(:algo,),Tuple{CUDA.CUDNN.cudnnSoftmaxAlgorithm_t}}}) at ./none:0
   [19] logsoftmax(::AutoGrad.Result{KnetArray{Float64,2}}; dims::Int64) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/softmax.jl:17
   [20] nll(::AutoGrad.Result{KnetArray{Float64,2}}, ::Array{Int64,1}; dims::Int64, average::Bool) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/loss.jl:39
   [21] nll(::AutoGrad.Result{KnetArray{Float64,2}}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/loss.jl:38
   [22] (::var"#f#86")(::AutoGrad.Result{KnetArray{Float64,2}}, ::KnetArray{Float64,2}, ::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:101
   [23] differentiate(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [24] differentiate(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135
   [25] (::AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#f#86",Int64,Bool}})(::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:225
   [26] gradfun at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:221 [inlined]
   [27] (::var"#∇fj#87"{AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#f#86",Int64,Bool}}})(::Param{KnetArray{Float64,2}}, ::KnetArray{Float64,2}, ::Array{Int64,1}, ::Int64) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:103
   [28] differentiate(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [29] differentiate(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135
   [30] (::AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#∇fj#87"{AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#f#86",Int64,Bool}}},Int64,Bool}})(::KnetArray{Float64,2}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:225
   [31] (::AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#∇fj#87"{AutoGrad.var"#gradfun#7"{AutoGrad.var"#gradfun#6#8"{var"#f#86",Int64,Bool}}},Int64,Bool}})(::KnetArray{Float64,2}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:221
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:110
   [33] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [34] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/loss.jl:10
  
 65.170294 seconds (49.87 M allocations: 2.488 GiB, 3.22% gc time)
cuarray.jl	 33.972283 seconds (26.69 M allocations: 1.378 GiB, 2.74% gc time)
update.jl	┌ Warning: optimizers is deprecated, use sgd, adam etc. instead.
└ @ Knet.Train20 ~/.julia/packages/Knet/Mfd6L/src/train20/update.jl:598
 78.460767 seconds (63.39 M allocations: 2.864 GiB, 4.20% gc time)
linalg.jl	 48.972937 seconds (42.75 M allocations: 2.156 GiB, 2.80% gc time)
batchnorm.jl	gpu-stats: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:58
  Test threw exception
  Expression: abs(mean(batchnorm(kax; training = true))) < 0.001
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] #batchnorm4#1 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:11 [inlined]
   [14] batchnorm4(::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:77
   [15] batchnorm2(::Nothing, ::Nothing, ::KnetArray{Float64,2}; moments::Nothing, training::Bool, o::Base.Iterators.Pairs{Symbol,Knet.Ops20.BNCache,Tuple{Symbol},NamedTuple{(:cache,),Tuple{Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:236
   [16] #batchnorm2#185 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:247 [inlined]
   [17] #batchnorm#176 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:65 [inlined]
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:58
   [19] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:58
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:44
   [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:12
  
gpu-stats: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:59
  Test threw exception
  Expression: isapprox(std2(batchnorm(kax; training = true)), 1.0; rtol = 0.01)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] #batchnorm4#1 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:11 [inlined]
   [14] batchnorm4(::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:77
   [15] batchnorm2(::Nothing, ::Nothing, ::KnetArray{Float64,2}; moments::Nothing, training::Bool, o::Base.Iterators.Pairs{Symbol,Knet.Ops20.BNCache,Tuple{Symbol},NamedTuple{(:cache,),Tuple{Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:236
   [16] #batchnorm2#185 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:247 [inlined]
   [17] #batchnorm#176 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:65 [inlined]
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:59
   [19] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:58
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:44
   [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] #batchnorm4#1 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:11 [inlined]
 [14] batchnorm4(::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:77
 [15] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [16] #batchnorm4#193 at ./none:0 [inlined]
 [17] batchnorm2(::Nothing, ::Nothing, ::Param{KnetArray{Float64,2}}; moments::Nothing, training::Bool, o::Base.Iterators.Pairs{Symbol,Knet.Ops20.BNCache,Tuple{Symbol},NamedTuple{(:cache,),Tuple{Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:236
 [18] #batchnorm2#185 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:247 [inlined]
 [19] #batchnorm#176 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:65 [inlined]
 [20] (::var"#bn1#147")(::Param{KnetArray{Float64,2}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:18
 [21] gcsum(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [22] gcsum(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [23] (::AutoGrad.var"#203#205"{Tuple{},var"#bn1#147",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [24] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [25] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [26] gradcheck(::var"#bn1#147", ::KnetArray{Float64,2}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [27] gradcheck(::Function, ::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
 [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:63
 [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:63
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:44
 [33] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [34] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:12
 [35] include(::String) at ./client.jl:457
 [36] macro expansion at ./timing.jl:174 [inlined]
 [37] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [38] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [39] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [40] include(::String) at ./client.jl:457
 [41] top-level scope at none:6
 [42] eval(::Module, ::Any) at ./boot.jl:331
 [43] exec_options(::Base.JLOptions) at ./client.jl:272
 [44] _start() at ./client.jl:506
gpu-grads: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:63
  Test threw exception
  Expression: gradcheck(bn1, kax)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#bn1#147", ::KnetArray{Float64,2}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:63
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:63
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:44
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [11] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] #batchnorm4#1 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:11 [inlined]
   [14] batchnorm4(::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:77
   [15] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [16] #batchnorm4#193 at ./none:0 [inlined]
   [17] batchnorm2(::Nothing, ::Nothing, ::Param{KnetArray{Float64,2}}; moments::Nothing, training::Bool, o::Base.Iterators.Pairs{Symbol,Knet.Ops20.BNCache,Tuple{Symbol},NamedTuple{(:cache,),Tuple{Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:236
   [18] #batchnorm2#185 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:247 [inlined]
   [19] #batchnorm#176 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:65 [inlined]
   [20] (::var"#bn1#147")(::Param{KnetArray{Float64,2}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:18
   [21] gcsum(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [22] gcsum(::Function, ::Param{KnetArray{Float64,2}}, ::Vararg{Any,N} where N) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [23] (::AutoGrad.var"#203#205"{Tuple{},var"#bn1#147",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [24] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [25] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [26] gradcheck(::var"#bn1#147", ::KnetArray{Float64,2}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [27] gradcheck(::Function, ::KnetArray{Float64,2}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:63
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:63
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:44
   [33] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [34] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] #batchnorm4#1 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:11 [inlined]
 [14] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [15] #batchnorm4#195 at ./none:0 [inlined]
 [16] batchnorm2(::AutoGrad.Result{KnetArray{Float64,2}}, ::AutoGrad.Result{KnetArray{Float64,2}}, ::AutoGrad.Result{KnetArray{Float64,2}}; moments::Nothing, training::Bool, o::Base.Iterators.Pairs{Symbol,Knet.Ops20.BNCache,Tuple{Symbol},NamedTuple{(:cache,),Tuple{Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:236
 [17] batchnorm(::AutoGrad.Result{KnetArray{Float64,2}}, ::Nothing, ::AutoGrad.Result{KnetArray{Float64,1}}; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:65
 [18] (::var"#bn3#146")(::Param{Tuple{KnetArray{Float64,2},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:17
 [19] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,2},KnetArray{Float64,1}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [20] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,2},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [21] (::AutoGrad.var"#203#205"{Tuple{},var"#bn3#146",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [22] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [23] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [24] gradcheck(::var"#bn3#146", ::Tuple{KnetArray{Float64,2},KnetArray{Float64,1}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [25] gradcheck(::Function, ::Tuple{KnetArray{Float64,2},KnetArray{Float64,1}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
 [26] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:64
 [27] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:63
 [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:44
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:12
 [33] include(::String) at ./client.jl:457
 [34] macro expansion at ./timing.jl:174 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [38] include(::String) at ./client.jl:457
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:272
 [42] _start() at ./client.jl:506
gpu-grads: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:64
  Test threw exception
  Expression: gradcheck(bn3, (kax, kaw))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#bn3#146", ::Tuple{KnetArray{Float64,2},KnetArray{Float64,1}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::Tuple{KnetArray{Float64,2},KnetArray{Float64,1}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:64
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:63
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:44
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [11] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] #batchnorm4#1 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:11 [inlined]
   [14] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [15] #batchnorm4#195 at ./none:0 [inlined]
   [16] batchnorm2(::AutoGrad.Result{KnetArray{Float64,2}}, ::AutoGrad.Result{KnetArray{Float64,2}}, ::AutoGrad.Result{KnetArray{Float64,2}}; moments::Nothing, training::Bool, o::Base.Iterators.Pairs{Symbol,Knet.Ops20.BNCache,Tuple{Symbol},NamedTuple{(:cache,),Tuple{Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:236
   [17] batchnorm(::AutoGrad.Result{KnetArray{Float64,2}}, ::Nothing, ::AutoGrad.Result{KnetArray{Float64,1}}; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:65
   [18] (::var"#bn3#146")(::Param{Tuple{KnetArray{Float64,2},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:17
   [19] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,2},KnetArray{Float64,1}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [20] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,2},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [21] (::AutoGrad.var"#203#205"{Tuple{},var"#bn3#146",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [22] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [23] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [24] gradcheck(::var"#bn3#146", ::Tuple{KnetArray{Float64,2},KnetArray{Float64,1}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [25] gradcheck(::Function, ::Tuple{KnetArray{Float64,2},KnetArray{Float64,1}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [26] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:64
   [27] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:63
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:44
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:12
  
dev-consistency: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:67
  Got exception outside of a @test
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] #batchnorm4#1 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:11 [inlined]
   [14] batchnorm4(::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.Ops20.BNMoments,Bool,Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:77
   [15] batchnorm2(::Nothing, ::Nothing, ::KnetArray{Float64,2}; moments::Knet.Ops20.BNMoments, training::Bool, o::Base.Iterators.Pairs{Symbol,Knet.Ops20.BNCache,Tuple{Symbol},NamedTuple{(:cache,),Tuple{Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:236
   [16] #batchnorm2#185 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:247 [inlined]
   [17] #batchnorm#176 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:65 [inlined]
   [18] batchnorm at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:57 [inlined] (repeats 2 times)
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:71
   [20] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:68
   [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:44
   [24] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:12
   [26] include(::String) at ./client.jl:457
   [27] macro expansion at ./timing.jl:174 [inlined]
   [28] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
   [31] include(::String) at ./client.jl:457
   [32] top-level scope at none:6
   [33] eval(::Module, ::Any) at ./boot.jl:331
   [34] exec_options(::Base.JLOptions) at ./client.jl:272
   [35] _start() at ./client.jl:506
  
test-moments: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:79
  Got exception outside of a @test
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] #batchnorm4#1 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:11 [inlined]
   [14] batchnorm4(::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.Ops20.BNMoments,Bool,Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:77
   [15] batchnorm2(::Nothing, ::Nothing, ::KnetArray{Float64,2}; moments::Knet.Ops20.BNMoments, training::Bool, o::Base.Iterators.Pairs{Symbol,Knet.Ops20.BNCache,Tuple{Symbol},NamedTuple{(:cache,),Tuple{Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:236
   [16] #batchnorm2#185 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:247 [inlined]
   [17] #batchnorm#176 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:65 [inlined]
   [18] batchnorm at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:57 [inlined] (repeats 2 times)
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:88
   [20] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:80
   [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:44
   [24] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:12
   [26] include(::String) at ./client.jl:457
   [27] macro expansion at ./timing.jl:174 [inlined]
   [28] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
   [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
   [31] include(::String) at ./client.jl:457
   [32] top-level scope at none:6
   [33] eval(::Module, ::Any) at ./boot.jl:331
   [34] exec_options(::Base.JLOptions) at ./client.jl:272
   [35] _start() at ./client.jl:506
  
training-moments: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:96
  Got exception outside of a @test
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] #batchnorm4#1 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:11 [inlined]
   [14] batchnorm4(::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.Ops20.BNMoments,Bool,Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:77
   [15] batchnorm2(::Nothing, ::Nothing, ::KnetArray{Float64,2}; moments::Knet.Ops20.BNMoments, training::Bool, o::Base.Iterators.Pairs{Symbol,Knet.Ops20.BNCache,Tuple{Symbol},NamedTuple{(:cache,),Tuple{Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:236
   [16] #batchnorm2#185 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:247 [inlined]
   [17] #batchnorm#176 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:65 [inlined]
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:105
   [19] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:97
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:44
   [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:12
   [25] include(::String) at ./client.jl:457
   [26] macro expansion at ./timing.jl:174 [inlined]
   [27] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
   [28] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
   [30] include(::String) at ./client.jl:457
   [31] top-level scope at none:6
   [32] eval(::Module, ::Any) at ./boot.jl:331
   [33] exec_options(::Base.JLOptions) at ./client.jl:272
   [34] _start() at ./client.jl:506
  
gpu-stats: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:58
  Test threw exception
  Expression: abs(mean(batchnorm(kax; training = true))) < 0.001
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] #batchnorm4#1 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:11 [inlined]
   [14] batchnorm4(::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:77
   [15] batchnorm(::KnetArray{Float64,4}, ::Nothing, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:70
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:58
   [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:58
   [19] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:44
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:12
  
gpu-stats: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:59
  Test threw exception
  Expression: isapprox(std2(batchnorm(kax; training = true)), 1.0; rtol = 0.01)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] #batchnorm4#1 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:11 [inlined]
   [14] batchnorm4(::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:77
   [15] batchnorm(::KnetArray{Float64,4}, ::Nothing, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:70
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:59
   [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:58
   [19] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:44
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] #batchnorm4#1 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:11 [inlined]
 [14] batchnorm4(::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:77
 [15] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [16] #batchnorm4#193 at ./none:0 [inlined]
 [17] batchnorm(::Param{KnetArray{Float64,4}}, ::Nothing, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:70
 [18] (::var"#bn1#147")(::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:18
 [19] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [20] gcsum(::Function, ::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [21] (::AutoGrad.var"#203#205"{Tuple{},var"#bn1#147",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [22] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [23] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [24] gradcheck(::var"#bn1#147", ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [25] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
 [26] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:63
 [27] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:63
 [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:44
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:12
 [33] include(::String) at ./client.jl:457
 [34] macro expansion at ./timing.jl:174 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [38] include(::String) at ./client.jl:457
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:272
 [42] _start() at ./client.jl:506
gpu-grads: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:63
  Test threw exception
  Expression: gradcheck(bn1, kax)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#bn1#147", ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:63
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:63
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:44
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [11] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] #batchnorm4#1 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:11 [inlined]
   [14] batchnorm4(::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:77
   [15] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [16] #batchnorm4#193 at ./none:0 [inlined]
   [17] batchnorm(::Param{KnetArray{Float64,4}}, ::Nothing, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:70
   [18] (::var"#bn1#147")(::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:18
   [19] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [20] gcsum(::Function, ::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [21] (::AutoGrad.var"#203#205"{Tuple{},var"#bn1#147",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [22] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [23] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [24] gradcheck(::var"#bn1#147", ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [25] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [26] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:63
   [27] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:63
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:44
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] #batchnorm4#1 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:11 [inlined]
 [14] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [15] #batchnorm4#195 at ./none:0 [inlined]
 [16] batchnorm(::AutoGrad.Result{KnetArray{Float64,4}}, ::Nothing, ::AutoGrad.Result{KnetArray{Float64,1}}; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:70
 [17] (::var"#bn3#146")(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:17
 [18] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [19] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [20] (::AutoGrad.var"#203#205"{Tuple{},var"#bn3#146",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [21] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [22] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [23] gradcheck(::var"#bn3#146", ::Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [24] gradcheck(::Function, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:64
 [26] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [27] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:63
 [28] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:44
 [30] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [31] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:12
 [32] include(::String) at ./client.jl:457
 [33] macro expansion at ./timing.jl:174 [inlined]
 [34] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [35] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [36] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [37] include(::String) at ./client.jl:457
 [38] top-level scope at none:6
 [39] eval(::Module, ::Any) at ./boot.jl:331
 [40] exec_options(::Base.JLOptions) at ./client.jl:272
 [41] _start() at ./client.jl:506
gpu-grads: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:64
  Test threw exception
  Expression: gradcheck(bn3, (kax, kaw))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#bn3#146", ::Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:64
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:63
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:44
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [11] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] #batchnorm4#1 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:11 [inlined]
   [14] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [15] #batchnorm4#195 at ./none:0 [inlined]
   [16] batchnorm(::AutoGrad.Result{KnetArray{Float64,4}}, ::Nothing, ::AutoGrad.Result{KnetArray{Float64,1}}; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:70
   [17] (::var"#bn3#146")(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:17
   [18] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [19] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [20] (::AutoGrad.var"#203#205"{Tuple{},var"#bn3#146",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [21] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [22] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [23] gradcheck(::var"#bn3#146", ::Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [24] gradcheck(::Function, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:64
   [26] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [27] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:63
   [28] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:44
   [30] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [31] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:12
  
dev-consistency: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:67
  Got exception outside of a @test
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] #batchnorm4#1 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:11 [inlined]
   [14] batchnorm4(::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.Ops20.BNMoments,Bool,Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:77
   [15] batchnorm(::KnetArray{Float64,4}, ::Knet.Ops20.BNMoments, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:70
   [16] batchnorm at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:57 [inlined] (repeats 2 times)
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:71
   [18] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:68
   [20] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:44
   [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:12
   [24] include(::String) at ./client.jl:457
   [25] macro expansion at ./timing.jl:174 [inlined]
   [26] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
   [27] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
   [29] include(::String) at ./client.jl:457
   [30] top-level scope at none:6
   [31] eval(::Module, ::Any) at ./boot.jl:331
   [32] exec_options(::Base.JLOptions) at ./client.jl:272
   [33] _start() at ./client.jl:506
  
test-moments: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:79
  Got exception outside of a @test
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] #batchnorm4#1 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:11 [inlined]
   [14] batchnorm4(::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.Ops20.BNMoments,Bool,Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:77
   [15] batchnorm(::KnetArray{Float64,4}, ::Knet.Ops20.BNMoments, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:70
   [16] batchnorm at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:57 [inlined] (repeats 2 times)
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:88
   [18] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:80
   [20] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:44
   [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:12
   [24] include(::String) at ./client.jl:457
   [25] macro expansion at ./timing.jl:174 [inlined]
   [26] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
   [27] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
   [29] include(::String) at ./client.jl:457
   [30] top-level scope at none:6
   [31] eval(::Module, ::Any) at ./boot.jl:331
   [32] exec_options(::Base.JLOptions) at ./client.jl:272
   [33] _start() at ./client.jl:506
  
training-moments: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:96
  Got exception outside of a @test
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] #batchnorm4#1 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:11 [inlined]
   [14] batchnorm4(::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.Ops20.BNMoments,Bool,Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:77
   [15] batchnorm(::KnetArray{Float64,4}, ::Knet.Ops20.BNMoments, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:70
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:105
   [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:97
   [19] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:44
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:12
   [23] include(::String) at ./client.jl:457
   [24] macro expansion at ./timing.jl:174 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
   [28] include(::String) at ./client.jl:457
   [29] top-level scope at none:6
   [30] eval(::Module, ::Any) at ./boot.jl:331
   [31] exec_options(::Base.JLOptions) at ./client.jl:272
   [32] _start() at ./client.jl:506
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] #batchnorm4#1 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:11 [inlined]
 [14] batchnorm4(::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.Ops20.BNMoments,Bool,Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:77
 [15] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.Ops20.BNMoments,Bool,Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [16] #batchnorm4#193 at ./none:0 [inlined]
 [17] batchnorm(::Param{KnetArray{Float64,4}}, ::Knet.Ops20.BNMoments, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:70
 [18] (::var"#bn1ts#149")(::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:20
 [19] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [20] gcsum(::Function, ::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [21] (::AutoGrad.var"#203#205"{Tuple{},var"#bn1ts#149",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [22] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [23] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [24] gradcheck(::var"#bn1ts#149", ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [25] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
 [26] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:122
 [27] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:122
 [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:44
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:12
 [33] include(::String) at ./client.jl:457
 [34] macro expansion at ./timing.jl:174 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [38] include(::String) at ./client.jl:457
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:272
 [42] _start() at ./client.jl:506
gpu-grads-testing: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:122
  Test threw exception
  Expression: gradcheck(bn1ts, kax)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#bn1ts#149", ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:122
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:122
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:44
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [11] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] #batchnorm4#1 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:11 [inlined]
   [14] batchnorm4(::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.Ops20.BNMoments,Bool,Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:77
   [15] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.Ops20.BNMoments,Bool,Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [16] #batchnorm4#193 at ./none:0 [inlined]
   [17] batchnorm(::Param{KnetArray{Float64,4}}, ::Knet.Ops20.BNMoments, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:70
   [18] (::var"#bn1ts#149")(::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:20
   [19] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [20] gcsum(::Function, ::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [21] (::AutoGrad.var"#203#205"{Tuple{},var"#bn1ts#149",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [22] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [23] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [24] gradcheck(::var"#bn1ts#149", ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [25] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [26] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:122
   [27] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:122
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:44
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] #batchnorm4#1 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:11 [inlined]
 [14] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.Ops20.BNMoments,Bool,Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [15] #batchnorm4#195 at ./none:0 [inlined]
 [16] batchnorm(::AutoGrad.Result{KnetArray{Float64,4}}, ::Knet.Ops20.BNMoments, ::AutoGrad.Result{KnetArray{Float64,1}}; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:70
 [17] (::var"#bn3ts#148")(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:19
 [18] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [19] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [20] (::AutoGrad.var"#203#205"{Tuple{},var"#bn3ts#148",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [21] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [22] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [23] gradcheck(::var"#bn3ts#148", ::Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [24] gradcheck(::Function, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:123
 [26] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [27] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:122
 [28] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:44
 [30] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [31] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:12
 [32] include(::String) at ./client.jl:457
 [33] macro expansion at ./timing.jl:174 [inlined]
 [34] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [35] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [36] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [37] include(::String) at ./client.jl:457
 [38] top-level scope at none:6
 [39] eval(::Module, ::Any) at ./boot.jl:331
 [40] exec_options(::Base.JLOptions) at ./client.jl:272
 [41] _start() at ./client.jl:506
gpu-grads-testing: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:123
  Test threw exception
  Expression: gradcheck(bn3ts, (kax, kaw))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#bn3ts#148", ::Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:123
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:122
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:44
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [11] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] #batchnorm4#1 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:11 [inlined]
   [14] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.Ops20.BNMoments,Bool,Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [15] #batchnorm4#195 at ./none:0 [inlined]
   [16] batchnorm(::AutoGrad.Result{KnetArray{Float64,4}}, ::Knet.Ops20.BNMoments, ::AutoGrad.Result{KnetArray{Float64,1}}; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:70
   [17] (::var"#bn3ts#148")(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:19
   [18] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [19] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [20] (::AutoGrad.var"#203#205"{Tuple{},var"#bn3ts#148",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [21] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [22] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [23] gradcheck(::var"#bn3ts#148", ::Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [24] gradcheck(::Function, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,1}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:123
   [26] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [27] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:122
   [28] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:44
   [30] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [31] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:12
  
gpu-stats: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:58
  Test threw exception
  Expression: abs(mean(batchnorm(kax; training = true))) < 0.001
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] #batchnorm4#1 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:11 [inlined]
   [14] batchnorm4(::KnetArray{Float64,5}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:77
   [15] batchnorm(::KnetArray{Float64,5}, ::Nothing, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:70
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:58
   [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:58
   [19] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:44
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:12
  
gpu-stats: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:59
  Test threw exception
  Expression: isapprox(std2(batchnorm(kax; training = true)), 1.0; rtol = 0.01)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] #batchnorm4#1 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:11 [inlined]
   [14] batchnorm4(::KnetArray{Float64,5}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:77
   [15] batchnorm(::KnetArray{Float64,5}, ::Nothing, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:70
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:59
   [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:58
   [19] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:44
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] #batchnorm4#1 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:11 [inlined]
 [14] batchnorm4(::KnetArray{Float64,5}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:77
 [15] forw(::Function, ::Param{KnetArray{Float64,5}}; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [16] #batchnorm4#193 at ./none:0 [inlined]
 [17] batchnorm(::Param{KnetArray{Float64,5}}, ::Nothing, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:70
 [18] (::var"#bn1#147")(::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:18
 [19] gcsum(::Function, ::Param{KnetArray{Float64,5}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [20] gcsum(::Function, ::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [21] (::AutoGrad.var"#203#205"{Tuple{},var"#bn1#147",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [22] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [23] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [24] gradcheck(::var"#bn1#147", ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [25] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
 [26] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:63
 [27] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:63
 [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:44
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:12
 [33] include(::String) at ./client.jl:457
 [34] macro expansion at ./timing.jl:174 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [38] include(::String) at ./client.jl:457
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:272
 [42] _start() at ./client.jl:506
gpu-grads: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:63
  Test threw exception
  Expression: gradcheck(bn1, kax)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#bn1#147", ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:63
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:63
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:44
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [11] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] #batchnorm4#1 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:11 [inlined]
   [14] batchnorm4(::KnetArray{Float64,5}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:77
   [15] forw(::Function, ::Param{KnetArray{Float64,5}}; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [16] #batchnorm4#193 at ./none:0 [inlined]
   [17] batchnorm(::Param{KnetArray{Float64,5}}, ::Nothing, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:70
   [18] (::var"#bn1#147")(::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:18
   [19] gcsum(::Function, ::Param{KnetArray{Float64,5}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [20] gcsum(::Function, ::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [21] (::AutoGrad.var"#203#205"{Tuple{},var"#bn1#147",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [22] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [23] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [24] gradcheck(::var"#bn1#147", ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [25] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [26] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:63
   [27] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:63
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:44
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] #batchnorm4#1 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:11 [inlined]
 [14] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,5}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,5}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [15] #batchnorm4#195 at ./none:0 [inlined]
 [16] batchnorm(::AutoGrad.Result{KnetArray{Float64,5}}, ::Nothing, ::AutoGrad.Result{KnetArray{Float64,1}}; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:70
 [17] (::var"#bn3#146")(::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:17
 [18] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [19] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [20] (::AutoGrad.var"#203#205"{Tuple{},var"#bn3#146",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [21] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [22] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [23] gradcheck(::var"#bn3#146", ::Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [24] gradcheck(::Function, ::Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:64
 [26] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [27] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:63
 [28] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:44
 [30] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [31] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:12
 [32] include(::String) at ./client.jl:457
 [33] macro expansion at ./timing.jl:174 [inlined]
 [34] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [35] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [36] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [37] include(::String) at ./client.jl:457
 [38] top-level scope at none:6
 [39] eval(::Module, ::Any) at ./boot.jl:331
 [40] exec_options(::Base.JLOptions) at ./client.jl:272
 [41] _start() at ./client.jl:506
gpu-grads: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:64
  Test threw exception
  Expression: gradcheck(bn3, (kax, kaw))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#bn3#146", ::Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:64
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:63
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:44
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [11] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] #batchnorm4#1 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:11 [inlined]
   [14] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,5}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,5}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Nothing,Bool,Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [15] #batchnorm4#195 at ./none:0 [inlined]
   [16] batchnorm(::AutoGrad.Result{KnetArray{Float64,5}}, ::Nothing, ::AutoGrad.Result{KnetArray{Float64,1}}; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:70
   [17] (::var"#bn3#146")(::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:17
   [18] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [19] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [20] (::AutoGrad.var"#203#205"{Tuple{},var"#bn3#146",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [21] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [22] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [23] gradcheck(::var"#bn3#146", ::Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [24] gradcheck(::Function, ::Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:64
   [26] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [27] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:63
   [28] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:44
   [30] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [31] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:12
  
dev-consistency: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:67
  Got exception outside of a @test
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] #batchnorm4#1 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:11 [inlined]
   [14] batchnorm4(::KnetArray{Float64,5}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.Ops20.BNMoments,Bool,Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:77
   [15] batchnorm(::KnetArray{Float64,5}, ::Knet.Ops20.BNMoments, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:70
   [16] batchnorm at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:57 [inlined] (repeats 2 times)
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:71
   [18] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:68
   [20] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:44
   [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:12
   [24] include(::String) at ./client.jl:457
   [25] macro expansion at ./timing.jl:174 [inlined]
   [26] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
   [27] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
   [29] include(::String) at ./client.jl:457
   [30] top-level scope at none:6
   [31] eval(::Module, ::Any) at ./boot.jl:331
   [32] exec_options(::Base.JLOptions) at ./client.jl:272
   [33] _start() at ./client.jl:506
  
test-moments: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:79
  Got exception outside of a @test
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] #batchnorm4#1 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:11 [inlined]
   [14] batchnorm4(::KnetArray{Float64,5}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.Ops20.BNMoments,Bool,Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:77
   [15] batchnorm(::KnetArray{Float64,5}, ::Knet.Ops20.BNMoments, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:70
   [16] batchnorm at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:57 [inlined] (repeats 2 times)
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:88
   [18] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:80
   [20] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:44
   [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:12
   [24] include(::String) at ./client.jl:457
   [25] macro expansion at ./timing.jl:174 [inlined]
   [26] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
   [27] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
   [29] include(::String) at ./client.jl:457
   [30] top-level scope at none:6
   [31] eval(::Module, ::Any) at ./boot.jl:331
   [32] exec_options(::Base.JLOptions) at ./client.jl:272
   [33] _start() at ./client.jl:506
  
training-moments: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:96
  Got exception outside of a @test
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] #batchnorm4#1 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:11 [inlined]
   [14] batchnorm4(::KnetArray{Float64,5}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.Ops20.BNMoments,Bool,Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:77
   [15] batchnorm(::KnetArray{Float64,5}, ::Knet.Ops20.BNMoments, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:70
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:105
   [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:97
   [19] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:44
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:12
   [23] include(::String) at ./client.jl:457
   [24] macro expansion at ./timing.jl:174 [inlined]
   [25] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
   [26] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [27] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
   [28] include(::String) at ./client.jl:457
   [29] top-level scope at none:6
   [30] eval(::Module, ::Any) at ./boot.jl:331
   [31] exec_options(::Base.JLOptions) at ./client.jl:272
   [32] _start() at ./client.jl:506
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] #batchnorm4#1 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:11 [inlined]
 [14] batchnorm4(::KnetArray{Float64,5}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.Ops20.BNMoments,Bool,Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:77
 [15] forw(::Function, ::Param{KnetArray{Float64,5}}; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.Ops20.BNMoments,Bool,Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [16] #batchnorm4#193 at ./none:0 [inlined]
 [17] batchnorm(::Param{KnetArray{Float64,5}}, ::Knet.Ops20.BNMoments, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:70
 [18] (::var"#bn1ts#149")(::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:20
 [19] gcsum(::Function, ::Param{KnetArray{Float64,5}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [20] gcsum(::Function, ::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [21] (::AutoGrad.var"#203#205"{Tuple{},var"#bn1ts#149",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [22] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [23] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [24] gradcheck(::var"#bn1ts#149", ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [25] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
 [26] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:122
 [27] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:122
 [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:44
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:12
 [33] include(::String) at ./client.jl:457
 [34] macro expansion at ./timing.jl:174 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [38] include(::String) at ./client.jl:457
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:272
 [42] _start() at ./client.jl:506
gpu-grads-testing: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:122
  Test threw exception
  Expression: gradcheck(bn1ts, kax)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#bn1ts#149", ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:122
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:122
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:44
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [11] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] #batchnorm4#1 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:11 [inlined]
   [14] batchnorm4(::KnetArray{Float64,5}; o::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.Ops20.BNMoments,Bool,Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:77
   [15] forw(::Function, ::Param{KnetArray{Float64,5}}; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.Ops20.BNMoments,Bool,Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [16] #batchnorm4#193 at ./none:0 [inlined]
   [17] batchnorm(::Param{KnetArray{Float64,5}}, ::Knet.Ops20.BNMoments, ::Nothing; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:70
   [18] (::var"#bn1ts#149")(::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:20
   [19] gcsum(::Function, ::Param{KnetArray{Float64,5}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [20] gcsum(::Function, ::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [21] (::AutoGrad.var"#203#205"{Tuple{},var"#bn1ts#149",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [22] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [23] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [24] gradcheck(::var"#bn1ts#149", ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [25] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [26] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:122
   [27] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:122
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:44
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] #batchnorm4#1 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:11 [inlined]
 [14] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,5}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,5}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.Ops20.BNMoments,Bool,Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [15] #batchnorm4#195 at ./none:0 [inlined]
 [16] batchnorm(::AutoGrad.Result{KnetArray{Float64,5}}, ::Knet.Ops20.BNMoments, ::AutoGrad.Result{KnetArray{Float64,1}}; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:70
 [17] (::var"#bn3ts#148")(::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:19
 [18] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [19] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [20] (::AutoGrad.var"#203#205"{Tuple{},var"#bn3ts#148",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [21] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [22] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [23] gradcheck(::var"#bn3ts#148", ::Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [24] gradcheck(::Function, ::Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:123
 [26] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [27] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:122
 [28] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:44
 [30] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [31] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:12
 [32] include(::String) at ./client.jl:457
 [33] macro expansion at ./timing.jl:174 [inlined]
 [34] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [35] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [36] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [37] include(::String) at ./client.jl:457
 [38] top-level scope at none:6
 [39] eval(::Module, ::Any) at ./boot.jl:331
 [40] exec_options(::Base.JLOptions) at ./client.jl:272
 [41] _start() at ./client.jl:506
gpu-grads-testing: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:123
  Test threw exception
  Expression: gradcheck(bn3ts, (kax, kaw))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#bn3ts#148", ::Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:123
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:122
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:44
   [10] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [11] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] #batchnorm4#1 at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/batchnorm.jl:11 [inlined]
   [14] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,5}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,5}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:moments, :training, :cache),Tuple{Knet.Ops20.BNMoments,Bool,Knet.Ops20.BNCache}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [15] #batchnorm4#195 at ./none:0 [inlined]
   [16] batchnorm(::AutoGrad.Result{KnetArray{Float64,5}}, ::Knet.Ops20.BNMoments, ::AutoGrad.Result{KnetArray{Float64,1}}; training::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/batchnorm.jl:70
   [17] (::var"#bn3ts#148")(::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:19
   [18] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [19] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [20] (::AutoGrad.var"#203#205"{Tuple{},var"#bn3ts#148",Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [21] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [22] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [23] gradcheck(::var"#bn3ts#148", ::Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [24] gradcheck(::Function, ::Tuple{KnetArray{Float64,5},KnetArray{Float64,1}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:123
   [26] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [27] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:122
   [28] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:44
   [30] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [31] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/batchnorm.jl:12
  
 44.812403 seconds (38.98 M allocations: 1.894 GiB, 2.54% gc time)
ops20.jl	(pa, xi, f0, nd, ad) = ("3×3×2×4 Param{Array{Float64,4}}", -1.078481861382521, -164.91303757256028, -503685.5721281784, 0)
conv: Test Failed at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/ops20.jl:44
  Expression: convtest(w, x; padding = 0, stride = 1, dilation = 1, mode = 0, alpha = 1, group = 1)
Stacktrace:
 [1] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/ops20.jl:44
 [2] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [3] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/ops20.jl:43
 [4] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [5] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/ops20.jl:14
(pa, xi, f0, nd, ad) = ("3×3×2×4 Param{Array{Float64,4}}", 0.19804624652667283, -123.07865929651024, 284206.68977276015, 0)
conv: Test Failed at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/ops20.jl:45
  Expression: convtest(w, x; padding = 1, stride = 1, dilation = 1, mode = 0, alpha = 1, group = 1)
Stacktrace:
 [1] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/ops20.jl:45
 [2] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [3] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/ops20.jl:43
 [4] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [5] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/ops20.jl:14
(pa, xi, f0, nd, ad) = ("3×3×2×4 Param{Array{Float64,4}}", -1.4967674347335624, -51.596731482360894, -158516.93632922703, 0)
conv: Test Failed at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/ops20.jl:46
  Expression: convtest(w, x; padding = 0, stride = 2, dilation = 1, mode = 0, alpha = 1, group = 1)
Stacktrace:
 [1] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/ops20.jl:46
 [2] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [3] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/ops20.jl:43
 [4] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [5] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/ops20.jl:14
(pa, xi, f0, nd, ad) = ("3×3×2×4 Param{Array{Float64,4}}", 0.6436675332634261, -17.499935659235113, -696346.1516585269, 0)
conv: Test Failed at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/ops20.jl:47
  Expression: convtest(w, x; padding = 0, stride = 1, dilation = 2, mode = 0, alpha = 1, group = 1)
Stacktrace:
 [1] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/ops20.jl:47
 [2] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [3] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/ops20.jl:43
 [4] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [5] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/ops20.jl:14
(pa, xi, f0, nd, ad) = ("3×3×2×4 Param{Array{Float64,4}}", -1.078481861382521, -33.895071636510465, 6.268746281251205, 4.773137232629573)
conv: Test Failed at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/ops20.jl:48
  Expression: convtest(w, x; padding = 0, stride = 1, dilation = 1, mode = 1, alpha = 1, group = 1)
Stacktrace:
 [1] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/ops20.jl:48
 [2] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [3] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/ops20.jl:43
 [4] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [5] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/ops20.jl:14
(pa, xi, f0, nd, ad) = ("3×3×2×4 Param{Array{Float64,4}}", -0.3638436486795584, -25.893743490575503, -550724.622532623, 0)
conv: Test Failed at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/ops20.jl:49
  Expression: convtest(w, x; padding = 0, stride = 1, dilation = 1, mode = 0, alpha = 2, group = 1)
Stacktrace:
 [1] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/ops20.jl:49
 [2] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [3] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/ops20.jl:43
 [4] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [5] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/ops20.jl:14
┌ Warning: Pool mode=2 not yet implemented in NNlib, using 1 instead. See https://github.com/FluxML/NNlib.jl/issues/218
└ @ Knet.Ops20 ~/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:174
┌ Warning: Pool maxpoolingNanOpt=0 not yet implemented in NNlib, using 1 instead. See https://github.com/FluxML/NNlib.jl/issues/218
└ @ Knet.Ops20 ~/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:179
 69.239182 seconds (58.42 M allocations: 2.910 GiB, 4.16% gc time)
karray.jl	 78.530135 seconds (61.66 M allocations: 3.066 GiB, 3.12% gc time)
conv.jl	(pa, xi, f0, nd, ad) = ("3×3×4×3 Array{Float32,4}", 0.45f0, 1476.2517f0, 23.196423f0, 18.1f0)
cpuconv: Test Failed at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:51
  Expression: gradcheck(deconv41, (ad32, ax32); rtol = TOL)
Stacktrace:
 [1] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:51
 [2] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [3] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:40
 [4] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [5] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
┌ Warning: Pool padding is buggy in NNlib, use with caution. See https://github.com/FluxML/NNlib.jl/issues/229
└ @ Knet.Ops20 ~/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:183
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
  Test threw exception
  Expression: isapprox(pool(kx), pool(ax))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] pool(::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/conv.jl:34
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] pool(::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/conv.jl:34
 [14] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [15] forw at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:65 [inlined]
 [16] #pool#99 at ./none:0 [inlined]
 [17] pool(::Param{KnetArray{Float64,4}}) at ./none:0
 [18] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [19] gcsum(::Function, ::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [20] (::AutoGrad.var"#203#205"{Tuple{},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [21] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [22] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [23] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [24] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:121
 [26] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [27] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
 [28] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [30] include(::String) at ./client.jl:457
 [31] macro expansion at ./timing.jl:174 [inlined]
 [32] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [34] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [35] include(::String) at ./client.jl:457
 [36] top-level scope at none:6
 [37] eval(::Module, ::Any) at ./boot.jl:331
 [38] exec_options(::Base.JLOptions) at ./client.jl:272
 [39] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:121
  Test threw exception
  Expression: gradcheck(pool, kx)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:121
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] pool(::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/conv.jl:34
   [14] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [15] forw at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:65 [inlined]
   [16] #pool#99 at ./none:0 [inlined]
   [17] pool(::Param{KnetArray{Float64,4}}) at ./none:0
   [18] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [19] gcsum(::Function, ::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [20] (::AutoGrad.var"#203#205"{Tuple{},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [21] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [22] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [23] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [24] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:121
   [26] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [27] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [28] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:122
  Test threw exception
  Expression: isapprox(unpool(kx), unpool(ax))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] unpool(::KnetArray{Float64,4}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:198
   [14] unpool(::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:195
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:122
   [16] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [18] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::KnetArray{Float64,4}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Symbol,Real,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:window, :mode, :alpha),Tuple{Int64,Int64,Float64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] #poolx#105 at ./none:0 [inlined]
 [15] unpool(::Param{KnetArray{Float64,4}}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:198
 [16] unpool(::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:195
 [17] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [18] gcsum(::Function, ::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [19] (::AutoGrad.var"#203#205"{Tuple{},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [20] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [21] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [22] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [23] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:123
 [25] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [26] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
 [27] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [29] include(::String) at ./client.jl:457
 [30] macro expansion at ./timing.jl:174 [inlined]
 [31] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [33] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [34] include(::String) at ./client.jl:457
 [35] top-level scope at none:6
 [36] eval(::Module, ::Any) at ./boot.jl:331
 [37] exec_options(::Base.JLOptions) at ./client.jl:272
 [38] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:123
  Test threw exception
  Expression: gradcheck(unpool, kx)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:123
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::KnetArray{Float64,4}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Symbol,Real,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:window, :mode, :alpha),Tuple{Int64,Int64,Float64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] #poolx#105 at ./none:0 [inlined]
   [15] unpool(::Param{KnetArray{Float64,4}}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:198
   [16] unpool(::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:195
   [17] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [18] gcsum(::Function, ::Param{KnetArray{Float64,4}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [19] (::AutoGrad.var"#203#205"{Tuple{},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [20] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [21] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [22] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [23] gradcheck(::Function, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:123
   [25] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [26] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [27] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:124
  Test threw exception
  Expression: isapprox(conv4(kw, kx), conv4(aw, ax))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] conv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/conv.jl:7
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:124
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] conv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/conv.jl:7
 [14] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [15] forw at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:65 [inlined]
 [16] #conv4#22 at ./none:0 [inlined]
 [17] conv4(::AutoGrad.Result{KnetArray{Float64,4}}, ::AutoGrad.Result{KnetArray{Float64,4}}) at ./none:0
 [18] (::var"#conv41#251"{var"#conv41#249#252"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [19] (::var"#conv41#251"{var"#conv41#249#252"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [20] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [21] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [22] (::AutoGrad.var"#203#205"{Tuple{},var"#conv41#251"{var"#conv41#249#252"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [23] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [24] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [25] gradcheck(::var"#conv41#251"{var"#conv41#249#252"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [26] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:125
 [27] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
 [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [31] include(::String) at ./client.jl:457
 [32] macro expansion at ./timing.jl:174 [inlined]
 [33] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [35] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [36] include(::String) at ./client.jl:457
 [37] top-level scope at none:6
 [38] eval(::Module, ::Any) at ./boot.jl:331
 [39] exec_options(::Base.JLOptions) at ./client.jl:272
 [40] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:125
  Test threw exception
  Expression: gradcheck(conv41, (kw, kx); rtol = TOL)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#conv41#251"{var"#conv41#249#252"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:125
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [7] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] conv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/conv.jl:7
   [14] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [15] forw at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:65 [inlined]
   [16] #conv4#22 at ./none:0 [inlined]
   [17] conv4(::AutoGrad.Result{KnetArray{Float64,4}}, ::AutoGrad.Result{KnetArray{Float64,4}}) at ./none:0
   [18] (::var"#conv41#251"{var"#conv41#249#252"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
   [19] (::var"#conv41#251"{var"#conv41#249#252"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
   [20] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [21] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [22] (::AutoGrad.var"#203#205"{Tuple{},var"#conv41#251"{var"#conv41#249#252"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [23] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [24] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [25] gradcheck(::var"#conv41#251"{var"#conv41#249#252"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [26] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:125
   [27] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:126
  Test threw exception
  Expression: isapprox(deconv4(kd, kx), deconv4(ad, ax))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] conv4x at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/conv.jl:16 [inlined]
   [14] deconv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:103
   [15] deconv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:102
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:126
   [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [19] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] conv4x at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/conv.jl:16 [inlined]
 [14] deconv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:103
 [15] deconv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:102
 [16] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [17] forw at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:65 [inlined]
 [18] #deconv4#88 at ./none:0 [inlined]
 [19] deconv4(::AutoGrad.Result{KnetArray{Float64,4}}, ::AutoGrad.Result{KnetArray{Float64,4}}) at ./none:0
 [20] (::var"#deconv41#253"{var"#deconv41#250#254"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:13
 [21] (::var"#deconv41#253"{var"#deconv41#250#254"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:13
 [22] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [23] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [24] (::AutoGrad.var"#203#205"{Tuple{},var"#deconv41#253"{var"#deconv41#250#254"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [25] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [26] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [27] gradcheck(::var"#deconv41#253"{var"#deconv41#250#254"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:127
 [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [33] include(::String) at ./client.jl:457
 [34] macro expansion at ./timing.jl:174 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [38] include(::String) at ./client.jl:457
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:272
 [42] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:127
  Test threw exception
  Expression: gradcheck(deconv41, (kd, kx); rtol = TOL)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#deconv41#253"{var"#deconv41#250#254"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:127
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [7] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] conv4x at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/conv.jl:16 [inlined]
   [14] deconv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:103
   [15] deconv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:102
   [16] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [17] forw at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:65 [inlined]
   [18] #deconv4#88 at ./none:0 [inlined]
   [19] deconv4(::AutoGrad.Result{KnetArray{Float64,4}}, ::AutoGrad.Result{KnetArray{Float64,4}}) at ./none:0
   [20] (::var"#deconv41#253"{var"#deconv41#250#254"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:13
   [21] (::var"#deconv41#253"{var"#deconv41#250#254"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:13
   [22] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [23] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [24] (::AutoGrad.var"#203#205"{Tuple{},var"#deconv41#253"{var"#deconv41#250#254"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [25] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [26] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [27] gradcheck(::var"#deconv41#253"{var"#deconv41#250#254"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:127
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:130
  Test threw exception
  Expression: isapprox(pool(kx32), pool(ax32))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] pool(::KnetArray{Float32,4}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/conv.jl:34
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:130
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] pool(::KnetArray{Float32,4}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/conv.jl:34
 [14] forw(::Function, ::Param{KnetArray{Float32,4}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [15] forw at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:65 [inlined]
 [16] #pool#99 at ./none:0 [inlined]
 [17] pool(::Param{KnetArray{Float32,4}}) at ./none:0
 [18] gcsum(::Function, ::Param{KnetArray{Float32,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [19] gcsum(::Function, ::Param{KnetArray{Float32,4}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [20] (::AutoGrad.var"#203#205"{Tuple{},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [21] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [22] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [23] gradcheck(::typeof(pool), ::KnetArray{Float32,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [24] gradcheck(::Function, ::KnetArray{Float32,4}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:131
 [26] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [27] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
 [28] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [30] include(::String) at ./client.jl:457
 [31] macro expansion at ./timing.jl:174 [inlined]
 [32] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [34] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [35] include(::String) at ./client.jl:457
 [36] top-level scope at none:6
 [37] eval(::Module, ::Any) at ./boot.jl:331
 [38] exec_options(::Base.JLOptions) at ./client.jl:272
 [39] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:131
  Test threw exception
  Expression: gradcheck(pool, kx32)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(pool), ::KnetArray{Float32,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float32,4}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:131
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] pool(::KnetArray{Float32,4}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/conv.jl:34
   [14] forw(::Function, ::Param{KnetArray{Float32,4}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [15] forw at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:65 [inlined]
   [16] #pool#99 at ./none:0 [inlined]
   [17] pool(::Param{KnetArray{Float32,4}}) at ./none:0
   [18] gcsum(::Function, ::Param{KnetArray{Float32,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [19] gcsum(::Function, ::Param{KnetArray{Float32,4}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [20] (::AutoGrad.var"#203#205"{Tuple{},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [21] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [22] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [23] gradcheck(::typeof(pool), ::KnetArray{Float32,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [24] gradcheck(::Function, ::KnetArray{Float32,4}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:131
   [26] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [27] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [28] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:132
  Test threw exception
  Expression: isapprox(unpool(kx32), unpool(ax32))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] unpool(::KnetArray{Float32,4}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:198
   [14] unpool(::KnetArray{Float32,4}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:195
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:132
   [16] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [18] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::KnetArray{Float32,4}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Symbol,Real,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:window, :mode, :alpha),Tuple{Int64,Int64,Float64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] #poolx#105 at ./none:0 [inlined]
 [15] unpool(::Param{KnetArray{Float32,4}}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:198
 [16] unpool(::Param{KnetArray{Float32,4}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:195
 [17] gcsum(::Function, ::Param{KnetArray{Float32,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [18] gcsum(::Function, ::Param{KnetArray{Float32,4}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [19] (::AutoGrad.var"#203#205"{Tuple{},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [20] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [21] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [22] gradcheck(::typeof(unpool), ::KnetArray{Float32,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [23] gradcheck(::Function, ::KnetArray{Float32,4}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:133
 [25] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [26] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
 [27] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [29] include(::String) at ./client.jl:457
 [30] macro expansion at ./timing.jl:174 [inlined]
 [31] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [33] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [34] include(::String) at ./client.jl:457
 [35] top-level scope at none:6
 [36] eval(::Module, ::Any) at ./boot.jl:331
 [37] exec_options(::Base.JLOptions) at ./client.jl:272
 [38] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:133
  Test threw exception
  Expression: gradcheck(unpool, kx32)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(unpool), ::KnetArray{Float32,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float32,4}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:133
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::KnetArray{Float32,4}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Symbol,Real,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:window, :mode, :alpha),Tuple{Int64,Int64,Float64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] #poolx#105 at ./none:0 [inlined]
   [15] unpool(::Param{KnetArray{Float32,4}}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:198
   [16] unpool(::Param{KnetArray{Float32,4}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:195
   [17] gcsum(::Function, ::Param{KnetArray{Float32,4}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [18] gcsum(::Function, ::Param{KnetArray{Float32,4}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [19] (::AutoGrad.var"#203#205"{Tuple{},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [20] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [21] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [22] gradcheck(::typeof(unpool), ::KnetArray{Float32,4}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [23] gradcheck(::Function, ::KnetArray{Float32,4}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:133
   [25] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [26] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [27] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:134
  Test threw exception
  Expression: isapprox(conv4(kw32, kx32), conv4(aw32, ax32))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] conv4(::KnetArray{Float32,4}, ::KnetArray{Float32,4}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/conv.jl:7
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:134
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] conv4(::KnetArray{Float32,4}, ::KnetArray{Float32,4}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/conv.jl:7
 [14] forw(::Function, ::AutoGrad.Result{KnetArray{Float32,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float32,4}},N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [15] forw at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:65 [inlined]
 [16] #conv4#22 at ./none:0 [inlined]
 [17] conv4(::AutoGrad.Result{KnetArray{Float32,4}}, ::AutoGrad.Result{KnetArray{Float32,4}}) at ./none:0
 [18] (::var"#conv41#251"{var"#conv41#249#252"})(::Param{Tuple{KnetArray{Float32,4},KnetArray{Float32,4}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [19] (::var"#conv41#251"{var"#conv41#249#252"})(::Param{Tuple{KnetArray{Float32,4},KnetArray{Float32,4}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [20] gcsum(::Function, ::Param{Tuple{KnetArray{Float32,4},KnetArray{Float32,4}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [21] gcsum(::Function, ::Param{Tuple{KnetArray{Float32,4},KnetArray{Float32,4}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [22] (::AutoGrad.var"#203#205"{Tuple{},var"#conv41#251"{var"#conv41#249#252"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [23] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [24] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [25] gradcheck(::var"#conv41#251"{var"#conv41#249#252"}, ::Tuple{KnetArray{Float32,4},KnetArray{Float32,4}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [26] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:135
 [27] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
 [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [31] include(::String) at ./client.jl:457
 [32] macro expansion at ./timing.jl:174 [inlined]
 [33] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [35] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [36] include(::String) at ./client.jl:457
 [37] top-level scope at none:6
 [38] eval(::Module, ::Any) at ./boot.jl:331
 [39] exec_options(::Base.JLOptions) at ./client.jl:272
 [40] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:135
  Test threw exception
  Expression: gradcheck(conv41, (kw32, kx32); rtol = TOL)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#conv41#251"{var"#conv41#249#252"}, ::Tuple{KnetArray{Float32,4},KnetArray{Float32,4}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:135
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [7] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] conv4(::KnetArray{Float32,4}, ::KnetArray{Float32,4}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/conv.jl:7
   [14] forw(::Function, ::AutoGrad.Result{KnetArray{Float32,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float32,4}},N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [15] forw at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:65 [inlined]
   [16] #conv4#22 at ./none:0 [inlined]
   [17] conv4(::AutoGrad.Result{KnetArray{Float32,4}}, ::AutoGrad.Result{KnetArray{Float32,4}}) at ./none:0
   [18] (::var"#conv41#251"{var"#conv41#249#252"})(::Param{Tuple{KnetArray{Float32,4},KnetArray{Float32,4}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
   [19] (::var"#conv41#251"{var"#conv41#249#252"})(::Param{Tuple{KnetArray{Float32,4},KnetArray{Float32,4}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
   [20] gcsum(::Function, ::Param{Tuple{KnetArray{Float32,4},KnetArray{Float32,4}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [21] gcsum(::Function, ::Param{Tuple{KnetArray{Float32,4},KnetArray{Float32,4}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [22] (::AutoGrad.var"#203#205"{Tuple{},var"#conv41#251"{var"#conv41#249#252"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [23] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [24] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [25] gradcheck(::var"#conv41#251"{var"#conv41#249#252"}, ::Tuple{KnetArray{Float32,4},KnetArray{Float32,4}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [26] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:135
   [27] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:136
  Test threw exception
  Expression: isapprox(deconv4(kd32, kx32), deconv4(ad32, ax32))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] conv4x at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/conv.jl:16 [inlined]
   [14] deconv4(::KnetArray{Float32,4}, ::KnetArray{Float32,4}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:103
   [15] deconv4(::KnetArray{Float32,4}, ::KnetArray{Float32,4}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:102
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:136
   [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [19] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] conv4x at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/conv.jl:16 [inlined]
 [14] deconv4(::KnetArray{Float32,4}, ::KnetArray{Float32,4}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:103
 [15] deconv4(::KnetArray{Float32,4}, ::KnetArray{Float32,4}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:102
 [16] forw(::Function, ::AutoGrad.Result{KnetArray{Float32,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float32,4}},N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [17] forw at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:65 [inlined]
 [18] #deconv4#88 at ./none:0 [inlined]
 [19] deconv4(::AutoGrad.Result{KnetArray{Float32,4}}, ::AutoGrad.Result{KnetArray{Float32,4}}) at ./none:0
 [20] (::var"#deconv41#253"{var"#deconv41#250#254"})(::Param{Tuple{KnetArray{Float32,4},KnetArray{Float32,4}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:13
 [21] (::var"#deconv41#253"{var"#deconv41#250#254"})(::Param{Tuple{KnetArray{Float32,4},KnetArray{Float32,4}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:13
 [22] gcsum(::Function, ::Param{Tuple{KnetArray{Float32,4},KnetArray{Float32,4}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [23] gcsum(::Function, ::Param{Tuple{KnetArray{Float32,4},KnetArray{Float32,4}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [24] (::AutoGrad.var"#203#205"{Tuple{},var"#deconv41#253"{var"#deconv41#250#254"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [25] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [26] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [27] gradcheck(::var"#deconv41#253"{var"#deconv41#250#254"}, ::Tuple{KnetArray{Float32,4},KnetArray{Float32,4}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:137
 [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [33] include(::String) at ./client.jl:457
 [34] macro expansion at ./timing.jl:174 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [38] include(::String) at ./client.jl:457
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:272
 [42] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:137
  Test threw exception
  Expression: gradcheck(deconv41, (kd32, kx32); rtol = TOL)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#deconv41#253"{var"#deconv41#250#254"}, ::Tuple{KnetArray{Float32,4},KnetArray{Float32,4}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:137
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [7] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] conv4x at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/conv.jl:16 [inlined]
   [14] deconv4(::KnetArray{Float32,4}, ::KnetArray{Float32,4}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:103
   [15] deconv4(::KnetArray{Float32,4}, ::KnetArray{Float32,4}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:102
   [16] forw(::Function, ::AutoGrad.Result{KnetArray{Float32,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float32,4}},N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [17] forw at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:65 [inlined]
   [18] #deconv4#88 at ./none:0 [inlined]
   [19] deconv4(::AutoGrad.Result{KnetArray{Float32,4}}, ::AutoGrad.Result{KnetArray{Float32,4}}) at ./none:0
   [20] (::var"#deconv41#253"{var"#deconv41#250#254"})(::Param{Tuple{KnetArray{Float32,4},KnetArray{Float32,4}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:13
   [21] (::var"#deconv41#253"{var"#deconv41#250#254"})(::Param{Tuple{KnetArray{Float32,4},KnetArray{Float32,4}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:13
   [22] gcsum(::Function, ::Param{Tuple{KnetArray{Float32,4},KnetArray{Float32,4}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [23] gcsum(::Function, ::Param{Tuple{KnetArray{Float32,4},KnetArray{Float32,4}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [24] (::AutoGrad.var"#203#205"{Tuple{},var"#deconv41#253"{var"#deconv41#250#254"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [25] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [26] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [27] gradcheck(::var"#deconv41#253"{var"#deconv41#250#254"}, ::Tuple{KnetArray{Float32,4},KnetArray{Float32,4}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:137
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:140
  Test threw exception
  Expression: isapprox(pool(kx5), pool(ax5))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] pool(::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/conv.jl:34
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:140
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] pool(::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/conv.jl:34
 [14] forw(::Function, ::Param{KnetArray{Float64,5}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [15] forw at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:65 [inlined]
 [16] #pool#99 at ./none:0 [inlined]
 [17] pool(::Param{KnetArray{Float64,5}}) at ./none:0
 [18] gcsum(::Function, ::Param{KnetArray{Float64,5}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [19] gcsum(::Function, ::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [20] (::AutoGrad.var"#203#205"{Tuple{},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [21] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [22] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [23] gradcheck(::typeof(pool), ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [24] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:141
 [26] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [27] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
 [28] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [30] include(::String) at ./client.jl:457
 [31] macro expansion at ./timing.jl:174 [inlined]
 [32] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [33] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [34] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [35] include(::String) at ./client.jl:457
 [36] top-level scope at none:6
 [37] eval(::Module, ::Any) at ./boot.jl:331
 [38] exec_options(::Base.JLOptions) at ./client.jl:272
 [39] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:141
  Test threw exception
  Expression: gradcheck(pool, kx5)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(pool), ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:141
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] pool(::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/conv.jl:34
   [14] forw(::Function, ::Param{KnetArray{Float64,5}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [15] forw at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:65 [inlined]
   [16] #pool#99 at ./none:0 [inlined]
   [17] pool(::Param{KnetArray{Float64,5}}) at ./none:0
   [18] gcsum(::Function, ::Param{KnetArray{Float64,5}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [19] gcsum(::Function, ::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [20] (::AutoGrad.var"#203#205"{Tuple{},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [21] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [22] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [23] gradcheck(::typeof(pool), ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [24] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:141
   [26] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [27] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [28] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:142
  Test threw exception
  Expression: isapprox(unpool(kx5), unpool(ax5))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] unpool(::KnetArray{Float64,5}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:198
   [14] unpool(::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:195
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:142
   [16] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [18] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [19] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::KnetArray{Float64,5}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Symbol,Real,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:window, :mode, :alpha),Tuple{Int64,Int64,Float64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] #poolx#105 at ./none:0 [inlined]
 [15] unpool(::Param{KnetArray{Float64,5}}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:198
 [16] unpool(::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:195
 [17] gcsum(::Function, ::Param{KnetArray{Float64,5}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [18] gcsum(::Function, ::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [19] (::AutoGrad.var"#203#205"{Tuple{},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [20] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [21] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [22] gradcheck(::typeof(unpool), ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [23] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:143
 [25] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [26] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
 [27] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [29] include(::String) at ./client.jl:457
 [30] macro expansion at ./timing.jl:174 [inlined]
 [31] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [32] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [33] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [34] include(::String) at ./client.jl:457
 [35] top-level scope at none:6
 [36] eval(::Module, ::Any) at ./boot.jl:331
 [37] exec_options(::Base.JLOptions) at ./client.jl:272
 [38] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:143
  Test threw exception
  Expression: gradcheck(unpool, kx5)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(unpool), ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [5] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:143
   [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [7] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [8] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [9] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::KnetArray{Float64,5}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Symbol,Real,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:window, :mode, :alpha),Tuple{Int64,Int64,Float64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] #poolx#105 at ./none:0 [inlined]
   [15] unpool(::Param{KnetArray{Float64,5}}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:198
   [16] unpool(::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:195
   [17] gcsum(::Function, ::Param{KnetArray{Float64,5}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [18] gcsum(::Function, ::Param{KnetArray{Float64,5}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [19] (::AutoGrad.var"#203#205"{Tuple{},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [20] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [21] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [22] gradcheck(::typeof(unpool), ::KnetArray{Float64,5}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [23] gradcheck(::Function, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:36
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:143
   [25] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [26] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [27] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:144
  Test threw exception
  Expression: isapprox(conv4(kw5, kx5), conv4(aw5, ax5))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] conv4(::KnetArray{Float64,5}, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/conv.jl:7
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:144
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] conv4(::KnetArray{Float64,5}, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/conv.jl:7
 [14] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,5}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,5}},N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [15] forw at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:65 [inlined]
 [16] #conv4#22 at ./none:0 [inlined]
 [17] conv4(::AutoGrad.Result{KnetArray{Float64,5}}, ::AutoGrad.Result{KnetArray{Float64,5}}) at ./none:0
 [18] (::var"#conv41#251"{var"#conv41#249#252"})(::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,5}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [19] (::var"#conv41#251"{var"#conv41#249#252"})(::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,5}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [20] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,5}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [21] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,5}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [22] (::AutoGrad.var"#203#205"{Tuple{},var"#conv41#251"{var"#conv41#249#252"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [23] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [24] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [25] gradcheck(::var"#conv41#251"{var"#conv41#249#252"}, ::Tuple{KnetArray{Float64,5},KnetArray{Float64,5}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [26] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:145
 [27] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
 [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [31] include(::String) at ./client.jl:457
 [32] macro expansion at ./timing.jl:174 [inlined]
 [33] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [34] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [35] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [36] include(::String) at ./client.jl:457
 [37] top-level scope at none:6
 [38] eval(::Module, ::Any) at ./boot.jl:331
 [39] exec_options(::Base.JLOptions) at ./client.jl:272
 [40] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:145
  Test threw exception
  Expression: gradcheck(conv41, (kw5, kx5); rtol = TOL)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#conv41#251"{var"#conv41#249#252"}, ::Tuple{KnetArray{Float64,5},KnetArray{Float64,5}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:145
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [7] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] conv4(::KnetArray{Float64,5}, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/conv.jl:7
   [14] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,5}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,5}},N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [15] forw at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:65 [inlined]
   [16] #conv4#22 at ./none:0 [inlined]
   [17] conv4(::AutoGrad.Result{KnetArray{Float64,5}}, ::AutoGrad.Result{KnetArray{Float64,5}}) at ./none:0
   [18] (::var"#conv41#251"{var"#conv41#249#252"})(::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,5}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
   [19] (::var"#conv41#251"{var"#conv41#249#252"})(::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,5}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
   [20] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,5}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [21] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,5}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [22] (::AutoGrad.var"#203#205"{Tuple{},var"#conv41#251"{var"#conv41#249#252"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [23] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [24] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [25] gradcheck(::var"#conv41#251"{var"#conv41#249#252"}, ::Tuple{KnetArray{Float64,5},KnetArray{Float64,5}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [26] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:145
   [27] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:146
  Test threw exception
  Expression: isapprox(deconv4(kw5, kx5), deconv4(aw5, ax5))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] conv4x at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/conv.jl:16 [inlined]
   [14] deconv4(::KnetArray{Float64,5}, ::KnetArray{Float64,5}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:103
   [15] deconv4(::KnetArray{Float64,5}, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:102
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:146
   [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [19] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] conv4x at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/conv.jl:16 [inlined]
 [14] deconv4(::KnetArray{Float64,5}, ::KnetArray{Float64,5}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:103
 [15] deconv4(::KnetArray{Float64,5}, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:102
 [16] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,5}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,5}},N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [17] forw at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:65 [inlined]
 [18] #deconv4#88 at ./none:0 [inlined]
 [19] deconv4(::AutoGrad.Result{KnetArray{Float64,5}}, ::AutoGrad.Result{KnetArray{Float64,5}}) at ./none:0
 [20] (::var"#deconv41#253"{var"#deconv41#250#254"})(::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,5}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:13
 [21] (::var"#deconv41#253"{var"#deconv41#250#254"})(::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,5}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:13
 [22] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,5}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [23] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,5}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [24] (::AutoGrad.var"#203#205"{Tuple{},var"#deconv41#253"{var"#deconv41#250#254"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [25] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [26] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [27] gradcheck(::var"#deconv41#253"{var"#deconv41#250#254"}, ::Tuple{KnetArray{Float64,5},KnetArray{Float64,5}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:147
 [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
 [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [32] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [33] include(::String) at ./client.jl:457
 [34] macro expansion at ./timing.jl:174 [inlined]
 [35] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [36] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [37] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [38] include(::String) at ./client.jl:457
 [39] top-level scope at none:6
 [40] eval(::Module, ::Any) at ./boot.jl:331
 [41] exec_options(::Base.JLOptions) at ./client.jl:272
 [42] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:147
  Test threw exception
  Expression: gradcheck(deconv41, (kw5, kx5); rtol = TOL)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#deconv41#253"{var"#deconv41#250#254"}, ::Tuple{KnetArray{Float64,5},KnetArray{Float64,5}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:147
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [7] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] conv4x at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/conv.jl:16 [inlined]
   [14] deconv4(::KnetArray{Float64,5}, ::KnetArray{Float64,5}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:103
   [15] deconv4(::KnetArray{Float64,5}, ::KnetArray{Float64,5}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:102
   [16] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,5}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,5}},N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [17] forw at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:65 [inlined]
   [18] #deconv4#88 at ./none:0 [inlined]
   [19] deconv4(::AutoGrad.Result{KnetArray{Float64,5}}, ::AutoGrad.Result{KnetArray{Float64,5}}) at ./none:0
   [20] (::var"#deconv41#253"{var"#deconv41#250#254"})(::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,5}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:13
   [21] (::var"#deconv41#253"{var"#deconv41#250#254"})(::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,5}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:13
   [22] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,5}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [23] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,5},KnetArray{Float64,5}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [24] (::AutoGrad.var"#203#205"{Tuple{},var"#deconv41#253"{var"#deconv41#250#254"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [25] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [26] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [27] gradcheck(::var"#deconv41#253"{var"#deconv41#250#254"}, ::Tuple{KnetArray{Float64,5},KnetArray{Float64,5}}; kw::Tuple{}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:147
   [29] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [30] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [31] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [32] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:150
  Test threw exception
  Expression: isapprox(pool(kx; window = 3), pool(ax; window = 3))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:150
   [14] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [16] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:window,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] pool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:window,),Tuple{Int64}}}) at ./none:0
 [15] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:window,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [16] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [19] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [20] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:151
 [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
 [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [25] include(::String) at ./client.jl:457
 [26] macro expansion at ./timing.jl:174 [inlined]
 [27] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [28] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [30] include(::String) at ./client.jl:457
 [31] top-level scope at none:6
 [32] eval(::Module, ::Any) at ./boot.jl:331
 [33] exec_options(::Base.JLOptions) at ./client.jl:272
 [34] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:151
  Test threw exception
  Expression: gradcheck(pool, kx; kw = [(:window, 3)])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:151
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [7] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:window,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] pool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:window,),Tuple{Int64}}}) at ./none:0
   [15] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:window,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [16] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [19] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:151
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:152
  Test threw exception
  Expression: isapprox(unpool(kx; window = 3), unpool(ax; window = 3))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] unpool(::KnetArray{Float64,4}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:198
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:152
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::KnetArray{Float64,4}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Symbol,Real,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:window, :mode, :alpha),Tuple{Int64,Int64,Float64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] #poolx#105 at ./none:0 [inlined]
 [15] unpool(::Param{KnetArray{Float64,4}}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:198
 [16] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:window,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [20] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:153
 [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
 [24] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [26] include(::String) at ./client.jl:457
 [27] macro expansion at ./timing.jl:174 [inlined]
 [28] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [31] include(::String) at ./client.jl:457
 [32] top-level scope at none:6
 [33] eval(::Module, ::Any) at ./boot.jl:331
 [34] exec_options(::Base.JLOptions) at ./client.jl:272
 [35] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:153
  Test threw exception
  Expression: gradcheck(unpool, kx; kw = [(:window, 3)])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:153
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [7] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::KnetArray{Float64,4}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Symbol,Real,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:window, :mode, :alpha),Tuple{Int64,Int64,Float64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] #poolx#105 at ./none:0 [inlined]
   [15] unpool(::Param{KnetArray{Float64,4}}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:198
   [16] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:window,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [20] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:153
   [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [24] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:154
  Test threw exception
  Expression: isapprox(pool(kx; window = (3, 3)), pool(ax; window = (3, 3)))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:154
   [14] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [16] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:window,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] pool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:window,),Tuple{Tuple{Int64,Int64}}}}) at ./none:0
 [15] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:window,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [16] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Tuple{Int64,Int64}},1},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [19] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [20] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:155
 [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
 [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [25] include(::String) at ./client.jl:457
 [26] macro expansion at ./timing.jl:174 [inlined]
 [27] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [28] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [30] include(::String) at ./client.jl:457
 [31] top-level scope at none:6
 [32] eval(::Module, ::Any) at ./boot.jl:331
 [33] exec_options(::Base.JLOptions) at ./client.jl:272
 [34] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:155
  Test threw exception
  Expression: gradcheck(pool, kx; kw = [(:window, (3, 3))])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:155
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [7] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:window,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] pool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:window,),Tuple{Tuple{Int64,Int64}}}}) at ./none:0
   [15] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:window,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [16] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Tuple{Int64,Int64}},1},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [19] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:155
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:156
  Test threw exception
  Expression: isapprox(unpool(kx; window = (3, 3)), unpool(ax; window = (3, 3)))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] unpool(::KnetArray{Float64,4}; window::Tuple{Int64,Int64}, alpha::Int64, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:198
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:156
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::KnetArray{Float64,4}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:window, :mode, :alpha),Tuple{Tuple{Int64,Int64},Int64,Float64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] #poolx#105 at ./none:0 [inlined]
 [15] unpool(::Param{KnetArray{Float64,4}}; window::Tuple{Int64,Int64}, alpha::Int64, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:198
 [16] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:window,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Tuple{Int64,Int64}},1},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [20] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:157
 [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
 [24] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [26] include(::String) at ./client.jl:457
 [27] macro expansion at ./timing.jl:174 [inlined]
 [28] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [31] include(::String) at ./client.jl:457
 [32] top-level scope at none:6
 [33] eval(::Module, ::Any) at ./boot.jl:331
 [34] exec_options(::Base.JLOptions) at ./client.jl:272
 [35] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:157
  Test threw exception
  Expression: gradcheck(unpool, kx; kw = [(:window, (3, 3))])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:157
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [7] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::KnetArray{Float64,4}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:window, :mode, :alpha),Tuple{Tuple{Int64,Int64},Int64,Float64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] #poolx#105 at ./none:0 [inlined]
   [15] unpool(::Param{KnetArray{Float64,4}}; window::Tuple{Int64,Int64}, alpha::Int64, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:198
   [16] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:window,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Tuple{Int64,Int64}},1},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [20] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:157
   [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [24] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:160
  Test threw exception
  Expression: isapprox(pool(kx; padding = 1), pool(ax; padding = 1))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:160
   [14] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [16] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] pool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at ./none:0
 [15] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [16] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [19] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [20] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:161
 [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
 [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [25] include(::String) at ./client.jl:457
 [26] macro expansion at ./timing.jl:174 [inlined]
 [27] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [28] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [30] include(::String) at ./client.jl:457
 [31] top-level scope at none:6
 [32] eval(::Module, ::Any) at ./boot.jl:331
 [33] exec_options(::Base.JLOptions) at ./client.jl:272
 [34] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:161
  Test threw exception
  Expression: gradcheck(pool, kx; kw = [(:padding, 1)])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:161
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [7] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] pool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at ./none:0
   [15] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [16] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [19] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:161
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:162
  Test threw exception
  Expression: isapprox(unpool(kx; padding = 1), unpool(ax; padding = 1))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] unpool(::KnetArray{Float64,4}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:198
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:162
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::KnetArray{Float64,4}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Symbol,Real,NTuple{4,Symbol},NamedTuple{(:padding, :window, :mode, :alpha),Tuple{Int64,Int64,Int64,Float64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] #poolx#105 at ./none:0 [inlined]
 [15] unpool(::Param{KnetArray{Float64,4}}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:198
 [16] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [20] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:163
 [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
 [24] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [26] include(::String) at ./client.jl:457
 [27] macro expansion at ./timing.jl:174 [inlined]
 [28] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [31] include(::String) at ./client.jl:457
 [32] top-level scope at none:6
 [33] eval(::Module, ::Any) at ./boot.jl:331
 [34] exec_options(::Base.JLOptions) at ./client.jl:272
 [35] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:163
  Test threw exception
  Expression: gradcheck(unpool, kx; kw = [(:padding, 1)])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:163
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [7] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::KnetArray{Float64,4}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Symbol,Real,NTuple{4,Symbol},NamedTuple{(:padding, :window, :mode, :alpha),Tuple{Int64,Int64,Int64,Float64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] #poolx#105 at ./none:0 [inlined]
   [15] unpool(::Param{KnetArray{Float64,4}}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:198
   [16] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [20] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:163
   [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [24] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:164
  Test threw exception
  Expression: isapprox(conv4(kw, kx; padding = 1), conv4(aw, ax; padding = 1))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:164
   [14] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [16] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] #conv4#22 at ./none:0 [inlined]
 [15] (::var"#conv41#251"{var"#conv41#249#252"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [16] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},var"#conv41#251"{var"#conv41#249#252"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [20] gradcheck(::var"#conv41#251"{var"#conv41#249#252"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:165
 [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
 [24] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [26] include(::String) at ./client.jl:457
 [27] macro expansion at ./timing.jl:174 [inlined]
 [28] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [31] include(::String) at ./client.jl:457
 [32] top-level scope at none:6
 [33] eval(::Module, ::Any) at ./boot.jl:331
 [34] exec_options(::Base.JLOptions) at ./client.jl:272
 [35] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:165
  Test threw exception
  Expression: gradcheck(conv41, (kw, kx); rtol = TOL, kw = [(:padding, 1)])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#conv41#251"{var"#conv41#249#252"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:165
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [7] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] #conv4#22 at ./none:0 [inlined]
   [15] (::var"#conv41#251"{var"#conv41#249#252"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
   [16] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},var"#conv41#251"{var"#conv41#249#252"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [20] gradcheck(::var"#conv41#251"{var"#conv41#249#252"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:165
   [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [24] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:166
  Test threw exception
  Expression: isapprox(deconv4(kd, kx; padding = 1), deconv4(ad, ax; padding = 1))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] deconv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:103
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:166
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] deconv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:103
 [14] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [15] deconv4(::AutoGrad.Result{KnetArray{Float64,4}}, ::AutoGrad.Result{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at ./none:0
 [16] (::var"#deconv41#253"{var"#deconv41#250#254"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:13
 [17] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [18] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},var"#deconv41#253"{var"#deconv41#250#254"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [21] gradcheck(::var"#deconv41#253"{var"#deconv41#250#254"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:167
 [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
 [25] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [26] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [27] include(::String) at ./client.jl:457
 [28] macro expansion at ./timing.jl:174 [inlined]
 [29] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [31] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [32] include(::String) at ./client.jl:457
 [33] top-level scope at none:6
 [34] eval(::Module, ::Any) at ./boot.jl:331
 [35] exec_options(::Base.JLOptions) at ./client.jl:272
 [36] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:167
  Test threw exception
  Expression: gradcheck(deconv41, (kd, kx); rtol = TOL, kw = [(:padding, 1)])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#deconv41#253"{var"#deconv41#250#254"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:167
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [7] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] deconv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:103
   [14] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [15] deconv4(::AutoGrad.Result{KnetArray{Float64,4}}, ::AutoGrad.Result{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at ./none:0
   [16] (::var"#deconv41#253"{var"#deconv41#250#254"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:13
   [17] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:padding,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [18] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},var"#deconv41#253"{var"#deconv41#250#254"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [21] gradcheck(::var"#deconv41#253"{var"#deconv41#250#254"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:167
   [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [25] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [26] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:169
  Test threw exception
  Expression: isapprox(pool(kx; padding = (1, 1)), pool(ax; padding = (1, 1)))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:169
   [14] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [16] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] pool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at ./none:0
 [15] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [16] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Tuple{Int64,Int64}},1},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [19] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [20] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:170
 [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
 [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [25] include(::String) at ./client.jl:457
 [26] macro expansion at ./timing.jl:174 [inlined]
 [27] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [28] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [30] include(::String) at ./client.jl:457
 [31] top-level scope at none:6
 [32] eval(::Module, ::Any) at ./boot.jl:331
 [33] exec_options(::Base.JLOptions) at ./client.jl:272
 [34] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:170
  Test threw exception
  Expression: gradcheck(pool, kx; kw = [(:padding, (1, 1))])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:170
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [7] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] pool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at ./none:0
   [15] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [16] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Tuple{Int64,Int64}},1},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [19] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:170
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:171
  Test threw exception
  Expression: isapprox(unpool(kx; padding = (1, 1)), unpool(ax; padding = (1, 1)))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] unpool(::KnetArray{Float64,4}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:198
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:171
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::KnetArray{Float64,4}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Symbol,Any,NTuple{4,Symbol},NamedTuple{(:padding, :window, :mode, :alpha),Tuple{Tuple{Int64,Int64},Int64,Int64,Float64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] #poolx#105 at ./none:0 [inlined]
 [15] unpool(::Param{KnetArray{Float64,4}}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:198
 [16] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Tuple{Int64,Int64}},1},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [20] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:172
 [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
 [24] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [26] include(::String) at ./client.jl:457
 [27] macro expansion at ./timing.jl:174 [inlined]
 [28] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [31] include(::String) at ./client.jl:457
 [32] top-level scope at none:6
 [33] eval(::Module, ::Any) at ./boot.jl:331
 [34] exec_options(::Base.JLOptions) at ./client.jl:272
 [35] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:172
  Test threw exception
  Expression: gradcheck(unpool, kx; kw = [(:padding, (1, 1))])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:172
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [7] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::KnetArray{Float64,4}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Symbol,Any,NTuple{4,Symbol},NamedTuple{(:padding, :window, :mode, :alpha),Tuple{Tuple{Int64,Int64},Int64,Int64,Float64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] #poolx#105 at ./none:0 [inlined]
   [15] unpool(::Param{KnetArray{Float64,4}}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:198
   [16] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Tuple{Int64,Int64}},1},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [20] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:172
   [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [24] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:173
  Test threw exception
  Expression: isapprox(conv4(kw, kx; padding = (1, 1)), conv4(aw, ax; padding = (1, 1)))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:173
   [14] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [16] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] #conv4#22 at ./none:0 [inlined]
 [15] (::var"#conv41#251"{var"#conv41#249#252"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [16] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Tuple{Int64,Int64}},1},var"#conv41#251"{var"#conv41#249#252"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [20] gradcheck(::var"#conv41#251"{var"#conv41#249#252"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:174
 [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
 [24] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [26] include(::String) at ./client.jl:457
 [27] macro expansion at ./timing.jl:174 [inlined]
 [28] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [31] include(::String) at ./client.jl:457
 [32] top-level scope at none:6
 [33] eval(::Module, ::Any) at ./boot.jl:331
 [34] exec_options(::Base.JLOptions) at ./client.jl:272
 [35] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:174
  Test threw exception
  Expression: gradcheck(conv41, (kw, kx); rtol = TOL, kw = [(:padding, (1, 1))])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#conv41#251"{var"#conv41#249#252"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:174
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [7] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] #conv4#22 at ./none:0 [inlined]
   [15] (::var"#conv41#251"{var"#conv41#249#252"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
   [16] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Tuple{Int64,Int64}},1},var"#conv41#251"{var"#conv41#249#252"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [20] gradcheck(::var"#conv41#251"{var"#conv41#249#252"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:174
   [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [24] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:175
  Test threw exception
  Expression: isapprox(deconv4(kd, kx; padding = (1, 1)), deconv4(ad, ax; padding = (1, 1)))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] deconv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:103
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:175
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] deconv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:103
 [14] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [15] deconv4(::AutoGrad.Result{KnetArray{Float64,4}}, ::AutoGrad.Result{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at ./none:0
 [16] (::var"#deconv41#253"{var"#deconv41#250#254"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:13
 [17] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [18] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Tuple{Int64,Int64}},1},var"#deconv41#253"{var"#deconv41#250#254"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [21] gradcheck(::var"#deconv41#253"{var"#deconv41#250#254"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:176
 [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
 [25] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [26] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [27] include(::String) at ./client.jl:457
 [28] macro expansion at ./timing.jl:174 [inlined]
 [29] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [31] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [32] include(::String) at ./client.jl:457
 [33] top-level scope at none:6
 [34] eval(::Module, ::Any) at ./boot.jl:331
 [35] exec_options(::Base.JLOptions) at ./client.jl:272
 [36] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:176
  Test threw exception
  Expression: gradcheck(deconv41, (kd, kx); rtol = TOL, kw = [(:padding, (1, 1))])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#deconv41#253"{var"#deconv41#250#254"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:176
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [7] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] deconv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:103
   [14] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [15] deconv4(::AutoGrad.Result{KnetArray{Float64,4}}, ::AutoGrad.Result{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at ./none:0
   [16] (::var"#deconv41#253"{var"#deconv41#250#254"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:13
   [17] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:padding,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [18] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Tuple{Int64,Int64}},1},var"#deconv41#253"{var"#deconv41#250#254"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [21] gradcheck(::var"#deconv41#253"{var"#deconv41#250#254"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:176
   [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [25] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [26] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:179
  Test threw exception
  Expression: isapprox(pool(kx; stride = 3), pool(ax; stride = 3))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:179
   [14] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [16] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] pool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at ./none:0
 [15] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [16] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [19] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [20] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:180
 [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
 [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [25] include(::String) at ./client.jl:457
 [26] macro expansion at ./timing.jl:174 [inlined]
 [27] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [28] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [30] include(::String) at ./client.jl:457
 [31] top-level scope at none:6
 [32] eval(::Module, ::Any) at ./boot.jl:331
 [33] exec_options(::Base.JLOptions) at ./client.jl:272
 [34] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:180
  Test threw exception
  Expression: gradcheck(pool, kx; kw = [(:stride, 3)])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:180
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [7] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] pool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at ./none:0
   [15] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [16] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [19] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:180
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:181
  Test threw exception
  Expression: isapprox(unpool(kx; stride = 3), unpool(ax; stride = 3))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] unpool(::KnetArray{Float64,4}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:198
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:181
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::KnetArray{Float64,4}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Symbol,Real,NTuple{4,Symbol},NamedTuple{(:stride, :window, :mode, :alpha),Tuple{Int64,Int64,Int64,Float64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] #poolx#105 at ./none:0 [inlined]
 [15] unpool(::Param{KnetArray{Float64,4}}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:198
 [16] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [20] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:182
 [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
 [24] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [26] include(::String) at ./client.jl:457
 [27] macro expansion at ./timing.jl:174 [inlined]
 [28] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [31] include(::String) at ./client.jl:457
 [32] top-level scope at none:6
 [33] eval(::Module, ::Any) at ./boot.jl:331
 [34] exec_options(::Base.JLOptions) at ./client.jl:272
 [35] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:182
  Test threw exception
  Expression: gradcheck(unpool, kx; kw = [(:stride, 3)])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:182
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [7] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::KnetArray{Float64,4}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Symbol,Real,NTuple{4,Symbol},NamedTuple{(:stride, :window, :mode, :alpha),Tuple{Int64,Int64,Int64,Float64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] #poolx#105 at ./none:0 [inlined]
   [15] unpool(::Param{KnetArray{Float64,4}}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:198
   [16] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [20] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:182
   [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [24] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:183
  Test threw exception
  Expression: isapprox(conv4(kw, kx; stride = 3), conv4(aw, ax; stride = 3))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:183
   [14] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [16] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] #conv4#22 at ./none:0 [inlined]
 [15] (::var"#conv41#251"{var"#conv41#249#252"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [16] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},var"#conv41#251"{var"#conv41#249#252"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [20] gradcheck(::var"#conv41#251"{var"#conv41#249#252"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:184
 [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
 [24] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [26] include(::String) at ./client.jl:457
 [27] macro expansion at ./timing.jl:174 [inlined]
 [28] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [31] include(::String) at ./client.jl:457
 [32] top-level scope at none:6
 [33] eval(::Module, ::Any) at ./boot.jl:331
 [34] exec_options(::Base.JLOptions) at ./client.jl:272
 [35] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:184
  Test threw exception
  Expression: gradcheck(conv41, (kw, kx); rtol = TOL, kw = [(:stride, 3)])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#conv41#251"{var"#conv41#249#252"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:184
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [7] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] #conv4#22 at ./none:0 [inlined]
   [15] (::var"#conv41#251"{var"#conv41#249#252"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
   [16] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},var"#conv41#251"{var"#conv41#249#252"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [20] gradcheck(::var"#conv41#251"{var"#conv41#249#252"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:184
   [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [24] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:185
  Test threw exception
  Expression: isapprox(deconv4(kd, kx; stride = 3), deconv4(ad, ax; stride = 3); rtol = 1.0e-6)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] deconv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:103
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:185
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] deconv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:103
 [14] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [15] deconv4(::AutoGrad.Result{KnetArray{Float64,4}}, ::AutoGrad.Result{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at ./none:0
 [16] (::var"#deconv41#253"{var"#deconv41#250#254"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:13
 [17] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [18] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},var"#deconv41#253"{var"#deconv41#250#254"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [21] gradcheck(::var"#deconv41#253"{var"#deconv41#250#254"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:186
 [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
 [25] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [26] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [27] include(::String) at ./client.jl:457
 [28] macro expansion at ./timing.jl:174 [inlined]
 [29] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [31] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [32] include(::String) at ./client.jl:457
 [33] top-level scope at none:6
 [34] eval(::Module, ::Any) at ./boot.jl:331
 [35] exec_options(::Base.JLOptions) at ./client.jl:272
 [36] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:186
  Test threw exception
  Expression: gradcheck(deconv41, (kd, kx); rtol = TOL, kw = [(:stride, 3)])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#deconv41#253"{var"#deconv41#250#254"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:186
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [7] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] deconv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:103
   [14] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [15] deconv4(::AutoGrad.Result{KnetArray{Float64,4}}, ::AutoGrad.Result{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at ./none:0
   [16] (::var"#deconv41#253"{var"#deconv41#250#254"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:13
   [17] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:stride,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [18] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},var"#deconv41#253"{var"#deconv41#250#254"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [21] gradcheck(::var"#deconv41#253"{var"#deconv41#250#254"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:186
   [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [25] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [26] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:188
  Test threw exception
  Expression: isapprox(pool(kx; stride = (3, 3)), pool(ax; stride = (3, 3)))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:188
   [14] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [16] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] pool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at ./none:0
 [15] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [16] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Tuple{Int64,Int64}},1},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [19] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [20] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:189
 [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
 [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [25] include(::String) at ./client.jl:457
 [26] macro expansion at ./timing.jl:174 [inlined]
 [27] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [28] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [30] include(::String) at ./client.jl:457
 [31] top-level scope at none:6
 [32] eval(::Module, ::Any) at ./boot.jl:331
 [33] exec_options(::Base.JLOptions) at ./client.jl:272
 [34] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:189
  Test threw exception
  Expression: gradcheck(pool, kx; kw = [(:stride, (3, 3))])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:189
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [7] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] pool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at ./none:0
   [15] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [16] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Tuple{Int64,Int64}},1},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [19] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:189
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:190
  Test threw exception
  Expression: isapprox(unpool(kx; stride = (3, 3)), unpool(ax; stride = (3, 3)))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] unpool(::KnetArray{Float64,4}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:198
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:190
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::KnetArray{Float64,4}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Symbol,Any,NTuple{4,Symbol},NamedTuple{(:stride, :window, :mode, :alpha),Tuple{Tuple{Int64,Int64},Int64,Int64,Float64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] #poolx#105 at ./none:0 [inlined]
 [15] unpool(::Param{KnetArray{Float64,4}}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:198
 [16] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Tuple{Int64,Int64}},1},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [20] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:191
 [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
 [24] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [26] include(::String) at ./client.jl:457
 [27] macro expansion at ./timing.jl:174 [inlined]
 [28] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [31] include(::String) at ./client.jl:457
 [32] top-level scope at none:6
 [33] eval(::Module, ::Any) at ./boot.jl:331
 [34] exec_options(::Base.JLOptions) at ./client.jl:272
 [35] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:191
  Test threw exception
  Expression: gradcheck(unpool, kx; kw = [(:stride, (3, 3))])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:191
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [7] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::KnetArray{Float64,4}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Symbol,Any,NTuple{4,Symbol},NamedTuple{(:stride, :window, :mode, :alpha),Tuple{Tuple{Int64,Int64},Int64,Int64,Float64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] #poolx#105 at ./none:0 [inlined]
   [15] unpool(::Param{KnetArray{Float64,4}}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:198
   [16] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Tuple{Int64,Int64}},1},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [20] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:191
   [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [24] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:192
  Test threw exception
  Expression: isapprox(conv4(kw, kx; stride = (3, 3)), conv4(aw, ax; stride = (3, 3)))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:192
   [14] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [16] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] #conv4#22 at ./none:0 [inlined]
 [15] (::var"#conv41#251"{var"#conv41#249#252"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [16] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Tuple{Int64,Int64}},1},var"#conv41#251"{var"#conv41#249#252"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [20] gradcheck(::var"#conv41#251"{var"#conv41#249#252"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:193
 [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
 [24] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [26] include(::String) at ./client.jl:457
 [27] macro expansion at ./timing.jl:174 [inlined]
 [28] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [31] include(::String) at ./client.jl:457
 [32] top-level scope at none:6
 [33] eval(::Module, ::Any) at ./boot.jl:331
 [34] exec_options(::Base.JLOptions) at ./client.jl:272
 [35] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:193
  Test threw exception
  Expression: gradcheck(conv41, (kw, kx); rtol = TOL, kw = [(:stride, (3, 3))])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#conv41#251"{var"#conv41#249#252"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:193
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [7] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] #conv4#22 at ./none:0 [inlined]
   [15] (::var"#conv41#251"{var"#conv41#249#252"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
   [16] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Tuple{Int64,Int64}},1},var"#conv41#251"{var"#conv41#249#252"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [20] gradcheck(::var"#conv41#251"{var"#conv41#249#252"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:193
   [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [24] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:194
  Test threw exception
  Expression: isapprox(deconv4(kd, kx; stride = (3, 3)), deconv4(ad, ax; stride = (3, 3)); rtol = 1.0e-6)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] deconv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:103
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:194
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] deconv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:103
 [14] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [15] deconv4(::AutoGrad.Result{KnetArray{Float64,4}}, ::AutoGrad.Result{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at ./none:0
 [16] (::var"#deconv41#253"{var"#deconv41#250#254"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:13
 [17] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [18] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Tuple{Int64,Int64}},1},var"#deconv41#253"{var"#deconv41#250#254"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [21] gradcheck(::var"#deconv41#253"{var"#deconv41#250#254"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:195
 [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
 [25] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [26] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [27] include(::String) at ./client.jl:457
 [28] macro expansion at ./timing.jl:174 [inlined]
 [29] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [31] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [32] include(::String) at ./client.jl:457
 [33] top-level scope at none:6
 [34] eval(::Module, ::Any) at ./boot.jl:331
 [35] exec_options(::Base.JLOptions) at ./client.jl:272
 [36] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:195
  Test threw exception
  Expression: gradcheck(deconv41, (kd, kx); rtol = TOL, kw = [(:stride, (3, 3))])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#deconv41#253"{var"#deconv41#250#254"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:195
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [7] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] deconv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:103
   [14] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [15] deconv4(::AutoGrad.Result{KnetArray{Float64,4}}, ::AutoGrad.Result{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at ./none:0
   [16] (::var"#deconv41#253"{var"#deconv41#250#254"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:13
   [17] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64},Tuple{Symbol},NamedTuple{(:stride,),Tuple{Tuple{Int64,Int64}}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [18] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Tuple{Int64,Int64}},1},var"#deconv41#253"{var"#deconv41#250#254"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [21] gradcheck(::var"#deconv41#253"{var"#deconv41#250#254"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Tuple{Int64,Int64}},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:195
   [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [25] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [26] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:198
  Test threw exception
  Expression: isapprox(pool(kx; mode = 1, padding = 1), pool(ax; mode = 1, padding = 1))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:198
   [14] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [16] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] pool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at ./none:0
 [15] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [16] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [19] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [20] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:199
 [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
 [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [25] include(::String) at ./client.jl:457
 [26] macro expansion at ./timing.jl:174 [inlined]
 [27] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [28] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [30] include(::String) at ./client.jl:457
 [31] top-level scope at none:6
 [32] eval(::Module, ::Any) at ./boot.jl:331
 [33] exec_options(::Base.JLOptions) at ./client.jl:272
 [34] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:199
  Test threw exception
  Expression: gradcheck(pool, kx; kw = [(:mode, 1), (:padding, 1)])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:199
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [7] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] pool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at ./none:0
   [15] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [16] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [19] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:199
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:200
  Test threw exception
  Expression: isapprox(unpool(kx; mode = 1, padding = 1), unpool(ax; mode = 1, padding = 1))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] unpool(::KnetArray{Float64,4}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:198
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:200
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::KnetArray{Float64,4}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Symbol,Real,NTuple{4,Symbol},NamedTuple{(:mode, :padding, :window, :alpha),Tuple{Int64,Int64,Int64,Float64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] #poolx#105 at ./none:0 [inlined]
 [15] unpool(::Param{KnetArray{Float64,4}}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:198
 [16] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [20] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:201
 [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
 [24] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [26] include(::String) at ./client.jl:457
 [27] macro expansion at ./timing.jl:174 [inlined]
 [28] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [31] include(::String) at ./client.jl:457
 [32] top-level scope at none:6
 [33] eval(::Module, ::Any) at ./boot.jl:331
 [34] exec_options(::Base.JLOptions) at ./client.jl:272
 [35] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:201
  Test threw exception
  Expression: gradcheck(unpool, kx; kw = [(:mode, 1), (:padding, 1)])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:201
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [7] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::KnetArray{Float64,4}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Symbol,Real,NTuple{4,Symbol},NamedTuple{(:mode, :padding, :window, :alpha),Tuple{Int64,Int64,Int64,Float64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] #poolx#105 at ./none:0 [inlined]
   [15] unpool(::Param{KnetArray{Float64,4}}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:198
   [16] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [20] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:201
   [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [24] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:202
  Test threw exception
  Expression: isapprox(conv4(kw, kx; mode = 1, padding = 1), conv4(aw, ax; mode = 1, padding = 1))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:202
   [14] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [16] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] #conv4#22 at ./none:0 [inlined]
 [15] (::var"#conv41#251"{var"#conv41#249#252"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [16] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},var"#conv41#251"{var"#conv41#249#252"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [20] gradcheck(::var"#conv41#251"{var"#conv41#249#252"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:203
 [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
 [24] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [26] include(::String) at ./client.jl:457
 [27] macro expansion at ./timing.jl:174 [inlined]
 [28] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [31] include(::String) at ./client.jl:457
 [32] top-level scope at none:6
 [33] eval(::Module, ::Any) at ./boot.jl:331
 [34] exec_options(::Base.JLOptions) at ./client.jl:272
 [35] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:203
  Test threw exception
  Expression: gradcheck(conv41, (kw, kx); rtol = TOL, kw = [(:mode, 1), (:padding, 1)])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#conv41#251"{var"#conv41#249#252"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:203
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [7] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] #conv4#22 at ./none:0 [inlined]
   [15] (::var"#conv41#251"{var"#conv41#249#252"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
   [16] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},var"#conv41#251"{var"#conv41#249#252"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [20] gradcheck(::var"#conv41#251"{var"#conv41#249#252"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:203
   [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [24] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:204
  Test threw exception
  Expression: isapprox(deconv4(kd, kx; mode = 1, padding = 1), deconv4(ad, ax; mode = 1, padding = 1))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] deconv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:103
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:204
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] deconv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:103
 [14] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [15] deconv4(::AutoGrad.Result{KnetArray{Float64,4}}, ::AutoGrad.Result{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at ./none:0
 [16] (::var"#deconv41#253"{var"#deconv41#250#254"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:13
 [17] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [18] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},var"#deconv41#253"{var"#deconv41#250#254"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [21] gradcheck(::var"#deconv41#253"{var"#deconv41#250#254"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:205
 [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
 [25] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [26] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [27] include(::String) at ./client.jl:457
 [28] macro expansion at ./timing.jl:174 [inlined]
 [29] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [31] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [32] include(::String) at ./client.jl:457
 [33] top-level scope at none:6
 [34] eval(::Module, ::Any) at ./boot.jl:331
 [35] exec_options(::Base.JLOptions) at ./client.jl:272
 [36] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:205
  Test threw exception
  Expression: gradcheck(deconv41, (kd, kx); rtol = TOL, kw = [(:mode, 1), (:padding, 1)])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#deconv41#253"{var"#deconv41#250#254"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:205
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [7] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] deconv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:103
   [14] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [15] deconv4(::AutoGrad.Result{KnetArray{Float64,4}}, ::AutoGrad.Result{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at ./none:0
   [16] (::var"#deconv41#253"{var"#deconv41#250#254"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:13
   [17] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [18] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},var"#deconv41#253"{var"#deconv41#250#254"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [21] gradcheck(::var"#deconv41#253"{var"#deconv41#250#254"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:205
   [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [25] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [26] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] pool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at ./none:0
 [15] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [16] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [19] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [20] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:209
 [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
 [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [25] include(::String) at ./client.jl:457
 [26] macro expansion at ./timing.jl:174 [inlined]
 [27] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [28] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [30] include(::String) at ./client.jl:457
 [31] top-level scope at none:6
 [32] eval(::Module, ::Any) at ./boot.jl:331
 [33] exec_options(::Base.JLOptions) at ./client.jl:272
 [34] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:209
  Test threw exception
  Expression: gradcheck(pool, kx; kw = [(:mode, 2), (:padding, 1)])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:209
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [7] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] pool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at ./none:0
   [15] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [16] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [19] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:209
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::KnetArray{Float64,4}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Symbol,Real,NTuple{4,Symbol},NamedTuple{(:mode, :padding, :window, :alpha),Tuple{Int64,Int64,Int64,Float64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] #poolx#105 at ./none:0 [inlined]
 [15] unpool(::Param{KnetArray{Float64,4}}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:198
 [16] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [20] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:211
 [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
 [24] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [26] include(::String) at ./client.jl:457
 [27] macro expansion at ./timing.jl:174 [inlined]
 [28] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [31] include(::String) at ./client.jl:457
 [32] top-level scope at none:6
 [33] eval(::Module, ::Any) at ./boot.jl:331
 [34] exec_options(::Base.JLOptions) at ./client.jl:272
 [35] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:211
  Test threw exception
  Expression: gradcheck(unpool, kx; kw = [(:mode, 2), (:padding, 1)])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:211
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [7] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::KnetArray{Float64,4}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Symbol,Real,NTuple{4,Symbol},NamedTuple{(:mode, :padding, :window, :alpha),Tuple{Int64,Int64,Int64,Float64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] #poolx#105 at ./none:0 [inlined]
   [15] unpool(::Param{KnetArray{Float64,4}}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:198
   [16] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [20] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:211
   [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [24] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:214
  Test threw exception
  Expression: isapprox(pool(kx; alpha = 2), pool(ax; alpha = 2))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:214
   [14] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [16] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] pool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at ./none:0
 [15] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [16] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [19] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [20] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:215
 [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
 [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [25] include(::String) at ./client.jl:457
 [26] macro expansion at ./timing.jl:174 [inlined]
 [27] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [28] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [30] include(::String) at ./client.jl:457
 [31] top-level scope at none:6
 [32] eval(::Module, ::Any) at ./boot.jl:331
 [33] exec_options(::Base.JLOptions) at ./client.jl:272
 [34] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:215
  Test threw exception
  Expression: gradcheck(pool, kx; kw = [(:alpha, 2)])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:215
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [7] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] pool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at ./none:0
   [15] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [16] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [19] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:215
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:216
  Test threw exception
  Expression: isapprox(unpool(kx; alpha = 2), unpool(ax; alpha = 2))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] unpool(::KnetArray{Float64,4}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:198
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:216
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::KnetArray{Float64,4}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Symbol,Real,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:window, :mode, :alpha),Tuple{Int64,Int64,Float64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] #poolx#105 at ./none:0 [inlined]
 [15] unpool(::Param{KnetArray{Float64,4}}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:198
 [16] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [20] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:217
 [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
 [24] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [26] include(::String) at ./client.jl:457
 [27] macro expansion at ./timing.jl:174 [inlined]
 [28] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [31] include(::String) at ./client.jl:457
 [32] top-level scope at none:6
 [33] eval(::Module, ::Any) at ./boot.jl:331
 [34] exec_options(::Base.JLOptions) at ./client.jl:272
 [35] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:217
  Test threw exception
  Expression: gradcheck(unpool, kx; kw = [(:alpha, 2)])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:217
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [7] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::KnetArray{Float64,4}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Symbol,Real,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:window, :mode, :alpha),Tuple{Int64,Int64,Float64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] #poolx#105 at ./none:0 [inlined]
   [15] unpool(::Param{KnetArray{Float64,4}}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:198
   [16] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [20] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:217
   [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [24] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:218
  Test threw exception
  Expression: isapprox(pool(kx; alpha = 2, mode = 1, padding = 1), pool(ax; alpha = 2, mode = 1, padding = 1))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:218
   [14] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [16] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:alpha, :mode, :padding),Tuple{Int64,Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] pool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:alpha, :mode, :padding),Tuple{Int64,Int64,Int64}}}) at ./none:0
 [15] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:alpha, :mode, :padding),Tuple{Int64,Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [16] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [19] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [20] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:219
 [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
 [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [25] include(::String) at ./client.jl:457
 [26] macro expansion at ./timing.jl:174 [inlined]
 [27] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [28] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [30] include(::String) at ./client.jl:457
 [31] top-level scope at none:6
 [32] eval(::Module, ::Any) at ./boot.jl:331
 [33] exec_options(::Base.JLOptions) at ./client.jl:272
 [34] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:219
  Test threw exception
  Expression: gradcheck(pool, kx; kw = [(:alpha, 2), (:mode, 1), (:padding, 1)])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:219
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [7] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:alpha, :mode, :padding),Tuple{Int64,Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] pool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:alpha, :mode, :padding),Tuple{Int64,Int64,Int64}}}) at ./none:0
   [15] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:alpha, :mode, :padding),Tuple{Int64,Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [16] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [19] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:219
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:220
  Test threw exception
  Expression: isapprox(unpool(kx; alpha = 2, mode = 1, padding = 1), unpool(ax; alpha = 2, mode = 1, padding = 1))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] unpool(::KnetArray{Float64,4}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:198
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:220
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::KnetArray{Float64,4}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Symbol,Real,NTuple{4,Symbol},NamedTuple{(:mode, :padding, :window, :alpha),Tuple{Int64,Int64,Int64,Float64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] #poolx#105 at ./none:0 [inlined]
 [15] unpool(::Param{KnetArray{Float64,4}}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:198
 [16] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:alpha, :mode, :padding),Tuple{Int64,Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [20] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:221
 [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
 [24] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [26] include(::String) at ./client.jl:457
 [27] macro expansion at ./timing.jl:174 [inlined]
 [28] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [31] include(::String) at ./client.jl:457
 [32] top-level scope at none:6
 [33] eval(::Module, ::Any) at ./boot.jl:331
 [34] exec_options(::Base.JLOptions) at ./client.jl:272
 [35] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:221
  Test threw exception
  Expression: gradcheck(unpool, kx; kw = [(:alpha, 2), (:mode, 1), (:padding, 1)])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:221
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [7] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::KnetArray{Float64,4}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Symbol,Real,NTuple{4,Symbol},NamedTuple{(:mode, :padding, :window, :alpha),Tuple{Int64,Int64,Int64,Float64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] #poolx#105 at ./none:0 [inlined]
   [15] unpool(::Param{KnetArray{Float64,4}}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:198
   [16] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:alpha, :mode, :padding),Tuple{Int64,Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [20] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:221
   [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [24] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:alpha, :mode, :padding),Tuple{Int64,Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] pool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:alpha, :mode, :padding),Tuple{Int64,Int64,Int64}}}) at ./none:0
 [15] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:alpha, :mode, :padding),Tuple{Int64,Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [16] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [19] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [20] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:224
 [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
 [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [25] include(::String) at ./client.jl:457
 [26] macro expansion at ./timing.jl:174 [inlined]
 [27] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [28] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [30] include(::String) at ./client.jl:457
 [31] top-level scope at none:6
 [32] eval(::Module, ::Any) at ./boot.jl:331
 [33] exec_options(::Base.JLOptions) at ./client.jl:272
 [34] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:224
  Test threw exception
  Expression: gradcheck(pool, kx; kw = [(:alpha, 2), (:mode, 2), (:padding, 1)])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:224
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [7] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::Param{KnetArray{Float64,4}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:alpha, :mode, :padding),Tuple{Int64,Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] pool(::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:alpha, :mode, :padding),Tuple{Int64,Int64,Int64}}}) at ./none:0
   [15] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:alpha, :mode, :padding),Tuple{Int64,Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [16] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(pool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [17] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [18] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [19] gradcheck(::typeof(pool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [20] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:224
   [21] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::KnetArray{Float64,4}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Symbol,Real,NTuple{4,Symbol},NamedTuple{(:mode, :padding, :window, :alpha),Tuple{Int64,Int64,Int64,Float64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] #poolx#105 at ./none:0 [inlined]
 [15] unpool(::Param{KnetArray{Float64,4}}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:198
 [16] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:alpha, :mode, :padding),Tuple{Int64,Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [20] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:226
 [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
 [24] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [26] include(::String) at ./client.jl:457
 [27] macro expansion at ./timing.jl:174 [inlined]
 [28] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [31] include(::String) at ./client.jl:457
 [32] top-level scope at none:6
 [33] eval(::Module, ::Any) at ./boot.jl:331
 [34] exec_options(::Base.JLOptions) at ./client.jl:272
 [35] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:226
  Test threw exception
  Expression: gradcheck(unpool, kx; kw = [(:alpha, 2), (:mode, 2), (:padding, 1)])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:226
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [7] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::KnetArray{Float64,4}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Symbol,Real,NTuple{4,Symbol},NamedTuple{(:mode, :padding, :window, :alpha),Tuple{Int64,Int64,Int64,Float64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] #poolx#105 at ./none:0 [inlined]
   [15] unpool(::Param{KnetArray{Float64,4}}; window::Int64, alpha::Int64, o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:mode, :padding),Tuple{Int64,Int64}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:198
   [16] gcsum(::Function, ::Param{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:alpha, :mode, :padding),Tuple{Int64,Int64,Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},typeof(unpool),Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [20] gradcheck(::typeof(unpool), ::KnetArray{Float64,4}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:226
   [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [24] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:228
  Test threw exception
  Expression: isapprox(conv4(kw, kx; alpha = 2), conv4(aw, ax; alpha = 2))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:228
   [14] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [16] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [17] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [14] #conv4#22 at ./none:0 [inlined]
 [15] (::var"#conv41#251"{var"#conv41#249#252"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [16] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},var"#conv41#251"{var"#conv41#249#252"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [20] gradcheck(::var"#conv41#251"{var"#conv41#249#252"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:229
 [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
 [24] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [26] include(::String) at ./client.jl:457
 [27] macro expansion at ./timing.jl:174 [inlined]
 [28] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [29] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [30] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [31] include(::String) at ./client.jl:457
 [32] top-level scope at none:6
 [33] eval(::Module, ::Any) at ./boot.jl:331
 [34] exec_options(::Base.JLOptions) at ./client.jl:272
 [35] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:229
  Test threw exception
  Expression: gradcheck(conv41, (kw, kx); rtol = TOL, kw = [(:alpha, 2)])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#conv41#251"{var"#conv41#249#252"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:229
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [7] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [14] #conv4#22 at ./none:0 [inlined]
   [15] (::var"#conv41#251"{var"#conv41#249#252"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
   [16] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},var"#conv41#251"{var"#conv41#249#252"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [20] gradcheck(::var"#conv41#251"{var"#conv41#249#252"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:229
   [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [24] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [25] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:230
  Test threw exception
  Expression: isapprox(deconv4(kd, kx; alpha = 2), deconv4(ad, ax; alpha = 2))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] deconv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:103
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:230
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [17] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [18] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] deconv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:103
 [14] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
 [15] deconv4(::AutoGrad.Result{KnetArray{Float64,4}}, ::AutoGrad.Result{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at ./none:0
 [16] (::var"#deconv41#253"{var"#deconv41#250#254"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:13
 [17] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [18] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},var"#deconv41#253"{var"#deconv41#250#254"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [21] gradcheck(::var"#deconv41#253"{var"#deconv41#250#254"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:231
 [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
 [25] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [26] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
 [27] include(::String) at ./client.jl:457
 [28] macro expansion at ./timing.jl:174 [inlined]
 [29] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [31] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [32] include(::String) at ./client.jl:457
 [33] top-level scope at none:6
 [34] eval(::Module, ::Any) at ./boot.jl:331
 [35] exec_options(::Base.JLOptions) at ./client.jl:272
 [36] _start() at ./client.jl:506
gpuconv: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:231
  Test threw exception
  Expression: gradcheck(deconv41, (kd, kx); rtol = TOL, kw = [(:alpha, 2)])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gradcheck(::var"#deconv41#253"{var"#deconv41#250#254"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:231
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [7] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [8] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] deconv4(::KnetArray{Float64,4}, ::KnetArray{Float64,4}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/conv.jl:103
   [14] forw(::Function, ::AutoGrad.Result{KnetArray{Float64,4}}, ::Vararg{AutoGrad.Result{KnetArray{Float64,4}},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:66
   [15] deconv4(::AutoGrad.Result{KnetArray{Float64,4}}, ::AutoGrad.Result{KnetArray{Float64,4}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at ./none:0
   [16] (::var"#deconv41#253"{var"#deconv41#250#254"})(::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:13
   [17] gcsum(::Function, ::Param{Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}}; o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:alpha,),Tuple{Int64}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [18] (::AutoGrad.var"#203#205"{Array{Tuple{Symbol,Int64},1},var"#deconv41#253"{var"#deconv41#250#254"},Array{Any,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [21] gradcheck(::var"#deconv41#253"{var"#deconv41#250#254"}, ::Tuple{KnetArray{Float64,4},KnetArray{Float64,4}}; kw::Array{Tuple{Symbol,Int64},1}, args::Function, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:39
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:231
   [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:120
   [25] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [26] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/conv.jl:12
  
122.538481 seconds (73.75 M allocations: 3.710 GiB, 2.30% gc time)
rnn.jl	rnn: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:48
  Test threw exception
  Expression: eq(rnnforw(r, w, x1), rnntest(r, w, x1))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] rnnforw at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/rnn.jl:93 [inlined] (repeats 2 times)
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:48
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  
rnn: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:49
  Test threw exception
  Expression: eq(rnnforw(r, w, x1), rnnforw(rcpu, wcpu, x1cpu))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] rnnforw at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/rnn.jl:93 [inlined] (repeats 2 times)
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:49
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] (::RNN)(::Param{KnetArray{Float64,1}}; batchSizes::Nothing) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:348
 [14] (::RNN)(::Param{KnetArray{Float64,1}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:328
 [15] (::var"#258#289"{UnionAll,KnetArray{Float64,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
 [16] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
 [18] (::AutoGrad.var"#220#225"{Tuple{},var"#258#289"{UnionAll,KnetArray{Float64,1}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [21] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:50
 [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
 [25] include(::String) at ./client.jl:457
 [26] macro expansion at ./timing.jl:174 [inlined]
 [27] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [28] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [30] include(::String) at ./client.jl:457
 [31] top-level scope at none:6
 [32] eval(::Module, ::Any) at ./boot.jl:331
 [33] exec_options(::Base.JLOptions) at ./client.jl:272
 [34] _start() at ./client.jl:506
rnn: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:50
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:50 =# @gcheck1 r(P(x1))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:50
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] (::RNN)(::Param{KnetArray{Float64,1}}; batchSizes::Nothing) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:348
   [14] (::RNN)(::Param{KnetArray{Float64,1}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:328
   [15] (::var"#258#289"{UnionAll,KnetArray{Float64,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
   [16] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
   [18] (::AutoGrad.var"#220#225"{Tuple{},var"#258#289"{UnionAll,KnetArray{Float64,1}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [21] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:50
   [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  
rnn: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:53
  Test threw exception
  Expression: eq(rnnforw(r, w, x1; batchSizes = [1]), rnntest(r, w, x1))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:53
   [14] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] (::RNN)(::Param{KnetArray{Float64,1}}; batchSizes::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:348
 [14] (::var"#260#291"{UnionAll,KnetArray{Float64,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
 [15] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [16] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
 [17] (::AutoGrad.var"#220#225"{Tuple{},var"#260#291"{UnionAll,KnetArray{Float64,1}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [20] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:55
 [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
 [24] include(::String) at ./client.jl:457
 [25] macro expansion at ./timing.jl:174 [inlined]
 [26] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [27] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [29] include(::String) at ./client.jl:457
 [30] top-level scope at none:6
 [31] eval(::Module, ::Any) at ./boot.jl:331
 [32] exec_options(::Base.JLOptions) at ./client.jl:272
 [33] _start() at ./client.jl:506
rnn: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:55
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:55 =# @gcheck1 r(P(x1), batchSizes = [1])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:55
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] (::RNN)(::Param{KnetArray{Float64,1}}; batchSizes::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:348
   [14] (::var"#260#291"{UnionAll,KnetArray{Float64,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
   [15] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [16] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
   [17] (::AutoGrad.var"#220#225"{Tuple{},var"#260#291"{UnionAll,KnetArray{Float64,1}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [20] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:55
   [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  
rnn: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:58
  Test threw exception
  Expression: eq(rnnforw(r, w, x1, hx1, cx1), rnntest(r, w, x1, hx1, cx1))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] rnnforw(::RNN, ::Param{KnetArray{Float64,3}}, ::KnetArray{Float64,1}, ::KnetArray{Float64,3}, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/rnn.jl:93
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:58
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  
rnn: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:59
  Test threw exception
  Expression: eq(rnnforw(r, w, x1, hx1, cx1), rnnforw(rcpu, wcpu, x1cpu, hx1cpu, cx1cpu))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] rnnforw(::RNN, ::Param{KnetArray{Float64,3}}, ::KnetArray{Float64,1}, ::KnetArray{Float64,3}, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/rnn.jl:93
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:59
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] (::RNN)(::Param{KnetArray{Float64,1}}; batchSizes::Nothing) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:348
 [14] RNN at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:328 [inlined]
 [15] rxhc at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:15 [inlined]
 [16] (::var"#rxhc#286"{var"#rxhc#257#287"})(::RNN, ::Param{KnetArray{Float64,1}}, ::Param{KnetArray{Float64,3}}, ::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:15
 [17] (::var"#261#292"{UnionAll,KnetArray{Float64,1},KnetArray{Float64,3},KnetArray{Float64,3}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
 [18] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [19] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
 [20] (::AutoGrad.var"#220#225"{Tuple{},var"#261#292"{UnionAll,KnetArray{Float64,1},KnetArray{Float64,3},KnetArray{Float64,3}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [21] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [22] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [23] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:60
 [25] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [26] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
 [27] include(::String) at ./client.jl:457
 [28] macro expansion at ./timing.jl:174 [inlined]
 [29] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [31] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [32] include(::String) at ./client.jl:457
 [33] top-level scope at none:6
 [34] eval(::Module, ::Any) at ./boot.jl:331
 [35] exec_options(::Base.JLOptions) at ./client.jl:272
 [36] _start() at ./client.jl:506
rnn: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:60
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:60 =# @gcheck1 rxhc(r, P(x1), P(hx1), P(cx1))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:60
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] (::RNN)(::Param{KnetArray{Float64,1}}; batchSizes::Nothing) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:348
   [14] RNN at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:328 [inlined]
   [15] rxhc at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:15 [inlined]
   [16] (::var"#rxhc#286"{var"#rxhc#257#287"})(::RNN, ::Param{KnetArray{Float64,1}}, ::Param{KnetArray{Float64,3}}, ::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:15
   [17] (::var"#261#292"{UnionAll,KnetArray{Float64,1},KnetArray{Float64,3},KnetArray{Float64,3}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
   [18] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [19] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
   [20] (::AutoGrad.var"#220#225"{Tuple{},var"#261#292"{UnionAll,KnetArray{Float64,1},KnetArray{Float64,3},KnetArray{Float64,3}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [21] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [22] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [23] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:60
   [25] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [26] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  
rnn: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:63
  Test threw exception
  Expression: eq(rnnforw(r, w, x1, hx1, cx1; batchSizes = [1]), rnntest(r, w, x1, hx1, cx1))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:63
   [14] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] (::RNN)(::Param{KnetArray{Float64,1}}; batchSizes::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:348
 [14] rxhc at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:15 [inlined]
 [15] (::var"#263#294"{UnionAll,KnetArray{Float64,1},KnetArray{Float64,3},KnetArray{Float64,3}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
 [16] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
 [18] (::AutoGrad.var"#220#225"{Tuple{},var"#263#294"{UnionAll,KnetArray{Float64,1},KnetArray{Float64,3},KnetArray{Float64,3}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [21] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:65
 [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
 [25] include(::String) at ./client.jl:457
 [26] macro expansion at ./timing.jl:174 [inlined]
 [27] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [28] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [30] include(::String) at ./client.jl:457
 [31] top-level scope at none:6
 [32] eval(::Module, ::Any) at ./boot.jl:331
 [33] exec_options(::Base.JLOptions) at ./client.jl:272
 [34] _start() at ./client.jl:506
rnn: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:65
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:65 =# @gcheck1 rxhc(r, P(x1), P(hx1), P(cx1), batchSizes = [1])
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:65
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] (::RNN)(::Param{KnetArray{Float64,1}}; batchSizes::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:348
   [14] rxhc at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:15 [inlined]
   [15] (::var"#263#294"{UnionAll,KnetArray{Float64,1},KnetArray{Float64,3},KnetArray{Float64,3}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
   [16] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
   [18] (::AutoGrad.var"#220#225"{Tuple{},var"#263#294"{UnionAll,KnetArray{Float64,1},KnetArray{Float64,3},KnetArray{Float64,3}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [21] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:65
   [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  
rnn: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:74
  Test threw exception
  Expression: eq(rnnforw(r, w, x2), rnntest(r, w, x2))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] rnnforw at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/rnn.jl:93 [inlined] (repeats 2 times)
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:74
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  
rnn: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:75
  Test threw exception
  Expression: eq(rnnforw(r, w, x2), rnnforw(rcpu, wcpu, x2cpu))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] rnnforw at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/rnn.jl:93 [inlined] (repeats 2 times)
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:75
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] (::RNN)(::Param{KnetArray{Float64,2}}; batchSizes::Nothing) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:348
 [14] (::RNN)(::Param{KnetArray{Float64,2}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:328
 [15] (::var"#264#295"{UnionAll,KnetArray{Float64,2}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
 [16] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
 [18] (::AutoGrad.var"#220#225"{Tuple{},var"#264#295"{UnionAll,KnetArray{Float64,2}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [21] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:76
 [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
 [25] include(::String) at ./client.jl:457
 [26] macro expansion at ./timing.jl:174 [inlined]
 [27] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [28] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [30] include(::String) at ./client.jl:457
 [31] top-level scope at none:6
 [32] eval(::Module, ::Any) at ./boot.jl:331
 [33] exec_options(::Base.JLOptions) at ./client.jl:272
 [34] _start() at ./client.jl:506
rnn: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:76
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:76 =# @gcheck1 r(P(x2))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:76
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] (::RNN)(::Param{KnetArray{Float64,2}}; batchSizes::Nothing) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:348
   [14] (::RNN)(::Param{KnetArray{Float64,2}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:328
   [15] (::var"#264#295"{UnionAll,KnetArray{Float64,2}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
   [16] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
   [18] (::AutoGrad.var"#220#225"{Tuple{},var"#264#295"{UnionAll,KnetArray{Float64,2}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [21] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:76
   [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  
rnn: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:79
  Test threw exception
  Expression: eq(rnnforw(r, w, x2, hx2, cx2), rnntest(r, w, x2, hx2, cx2))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] rnnforw(::RNN, ::Param{KnetArray{Float64,3}}, ::KnetArray{Float64,2}, ::KnetArray{Float64,3}, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/rnn.jl:93
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:79
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  
rnn: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:80
  Test threw exception
  Expression: eq(rnnforw(r, w, x2, hx2, cx2), rnnforw(rcpu, wcpu, x2cpu, hx2cpu, cx2cpu))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] rnnforw(::RNN, ::Param{KnetArray{Float64,3}}, ::KnetArray{Float64,2}, ::KnetArray{Float64,3}, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/rnn.jl:93
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:80
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] (::RNN)(::Param{KnetArray{Float64,2}}; batchSizes::Nothing) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:348
 [14] RNN at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:328 [inlined]
 [15] rxhc at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:15 [inlined]
 [16] (::var"#rxhc#286"{var"#rxhc#257#287"})(::RNN, ::Param{KnetArray{Float64,2}}, ::Param{KnetArray{Float64,3}}, ::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:15
 [17] (::var"#266#297"{UnionAll,KnetArray{Float64,2},KnetArray{Float64,3},KnetArray{Float64,3}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
 [18] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [19] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
 [20] (::AutoGrad.var"#220#225"{Tuple{},var"#266#297"{UnionAll,KnetArray{Float64,2},KnetArray{Float64,3},KnetArray{Float64,3}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [21] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [22] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [23] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:81
 [25] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [26] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
 [27] include(::String) at ./client.jl:457
 [28] macro expansion at ./timing.jl:174 [inlined]
 [29] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [31] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [32] include(::String) at ./client.jl:457
 [33] top-level scope at none:6
 [34] eval(::Module, ::Any) at ./boot.jl:331
 [35] exec_options(::Base.JLOptions) at ./client.jl:272
 [36] _start() at ./client.jl:506
rnn: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:81
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:81 =# @gcheck1 rxhc(r, P(x2), P(hx2), P(cx2))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:81
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] (::RNN)(::Param{KnetArray{Float64,2}}; batchSizes::Nothing) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:348
   [14] RNN at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:328 [inlined]
   [15] rxhc at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:15 [inlined]
   [16] (::var"#rxhc#286"{var"#rxhc#257#287"})(::RNN, ::Param{KnetArray{Float64,2}}, ::Param{KnetArray{Float64,3}}, ::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:15
   [17] (::var"#266#297"{UnionAll,KnetArray{Float64,2},KnetArray{Float64,3},KnetArray{Float64,3}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
   [18] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [19] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
   [20] (::AutoGrad.var"#220#225"{Tuple{},var"#266#297"{UnionAll,KnetArray{Float64,2},KnetArray{Float64,3},KnetArray{Float64,3}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [21] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [22] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [23] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:81
   [25] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [26] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  
rnn: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:84
  Test threw exception
  Expression: eq(rnnforw(r, w, x2, hx2, cx2; batchSizes = [B]), rnntest(r, w, x2, hx2, cx2))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:84
   [14] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] (::RNN)(::Param{KnetArray{Float64,2}}; batchSizes::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:348
 [14] rxhc at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:15 [inlined]
 [15] (::var"#268#299"{UnionAll,KnetArray{Float64,2},Array{Int64,1},KnetArray{Float64,3},KnetArray{Float64,3}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
 [16] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
 [18] (::AutoGrad.var"#220#225"{Tuple{},var"#268#299"{UnionAll,KnetArray{Float64,2},Array{Int64,1},KnetArray{Float64,3},KnetArray{Float64,3}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [21] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:89
 [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
 [25] include(::String) at ./client.jl:457
 [26] macro expansion at ./timing.jl:174 [inlined]
 [27] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [28] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [30] include(::String) at ./client.jl:457
 [31] top-level scope at none:6
 [32] eval(::Module, ::Any) at ./boot.jl:331
 [33] exec_options(::Base.JLOptions) at ./client.jl:272
 [34] _start() at ./client.jl:506
rnn: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:89
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:89 =# @gcheck1 rxhc(r, P(x2), P(hx2a), P(cx2a), batchSizes = b)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:89
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] (::RNN)(::Param{KnetArray{Float64,2}}; batchSizes::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:348
   [14] rxhc at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:15 [inlined]
   [15] (::var"#268#299"{UnionAll,KnetArray{Float64,2},Array{Int64,1},KnetArray{Float64,3},KnetArray{Float64,3}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
   [16] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
   [18] (::AutoGrad.var"#220#225"{Tuple{},var"#268#299"{UnionAll,KnetArray{Float64,2},Array{Int64,1},KnetArray{Float64,3},KnetArray{Float64,3}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [21] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:89
   [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] (::RNN)(::Param{KnetArray{Float64,2}}; batchSizes::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:348
 [14] (::var"#269#300"{UnionAll,KnetArray{Float64,2},Array{Int64,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
 [15] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [16] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
 [17] (::AutoGrad.var"#220#225"{Tuple{},var"#269#300"{UnionAll,KnetArray{Float64,2},Array{Int64,1}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [20] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:92
 [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
 [24] include(::String) at ./client.jl:457
 [25] macro expansion at ./timing.jl:174 [inlined]
 [26] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [27] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [29] include(::String) at ./client.jl:457
 [30] top-level scope at none:6
 [31] eval(::Module, ::Any) at ./boot.jl:331
 [32] exec_options(::Base.JLOptions) at ./client.jl:272
 [33] _start() at ./client.jl:506
rnn: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:92
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:92 =# @gcheck1 r(P(x2), batchSizes = b)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:92
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] (::RNN)(::Param{KnetArray{Float64,2}}; batchSizes::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:348
   [14] (::var"#269#300"{UnionAll,KnetArray{Float64,2},Array{Int64,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
   [15] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [16] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
   [17] (::AutoGrad.var"#220#225"{Tuple{},var"#269#300"{UnionAll,KnetArray{Float64,2},Array{Int64,1}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [20] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:92
   [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] (::RNN)(::Param{KnetArray{Float64,2}}; batchSizes::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:348
 [14] rxhc at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:15 [inlined]
 [15] (::var"#268#299"{UnionAll,KnetArray{Float64,2},Array{Int64,1},KnetArray{Float64,3},KnetArray{Float64,3}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
 [16] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
 [18] (::AutoGrad.var"#220#225"{Tuple{},var"#268#299"{UnionAll,KnetArray{Float64,2},Array{Int64,1},KnetArray{Float64,3},KnetArray{Float64,3}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [21] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:89
 [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
 [25] include(::String) at ./client.jl:457
 [26] macro expansion at ./timing.jl:174 [inlined]
 [27] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [28] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [30] include(::String) at ./client.jl:457
 [31] top-level scope at none:6
 [32] eval(::Module, ::Any) at ./boot.jl:331
 [33] exec_options(::Base.JLOptions) at ./client.jl:272
 [34] _start() at ./client.jl:506
rnn: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:89
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:89 =# @gcheck1 rxhc(r, P(x2), P(hx2a), P(cx2a), batchSizes = b)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:89
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] (::RNN)(::Param{KnetArray{Float64,2}}; batchSizes::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:348
   [14] rxhc at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:15 [inlined]
   [15] (::var"#268#299"{UnionAll,KnetArray{Float64,2},Array{Int64,1},KnetArray{Float64,3},KnetArray{Float64,3}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
   [16] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
   [18] (::AutoGrad.var"#220#225"{Tuple{},var"#268#299"{UnionAll,KnetArray{Float64,2},Array{Int64,1},KnetArray{Float64,3},KnetArray{Float64,3}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [21] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:89
   [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] (::RNN)(::Param{KnetArray{Float64,2}}; batchSizes::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:348
 [14] (::var"#269#300"{UnionAll,KnetArray{Float64,2},Array{Int64,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
 [15] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [16] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
 [17] (::AutoGrad.var"#220#225"{Tuple{},var"#269#300"{UnionAll,KnetArray{Float64,2},Array{Int64,1}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [20] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:92
 [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
 [24] include(::String) at ./client.jl:457
 [25] macro expansion at ./timing.jl:174 [inlined]
 [26] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [27] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [29] include(::String) at ./client.jl:457
 [30] top-level scope at none:6
 [31] eval(::Module, ::Any) at ./boot.jl:331
 [32] exec_options(::Base.JLOptions) at ./client.jl:272
 [33] _start() at ./client.jl:506
rnn: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:92
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:92 =# @gcheck1 r(P(x2), batchSizes = b)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:92
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] (::RNN)(::Param{KnetArray{Float64,2}}; batchSizes::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:348
   [14] (::var"#269#300"{UnionAll,KnetArray{Float64,2},Array{Int64,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
   [15] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [16] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
   [17] (::AutoGrad.var"#220#225"{Tuple{},var"#269#300"{UnionAll,KnetArray{Float64,2},Array{Int64,1}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [20] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:92
   [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] (::RNN)(::Param{KnetArray{Float64,2}}; batchSizes::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:348
 [14] rxhc at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:15 [inlined]
 [15] (::var"#268#299"{UnionAll,KnetArray{Float64,2},Array{Int64,1},KnetArray{Float64,3},KnetArray{Float64,3}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
 [16] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
 [18] (::AutoGrad.var"#220#225"{Tuple{},var"#268#299"{UnionAll,KnetArray{Float64,2},Array{Int64,1},KnetArray{Float64,3},KnetArray{Float64,3}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [21] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:89
 [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
 [25] include(::String) at ./client.jl:457
 [26] macro expansion at ./timing.jl:174 [inlined]
 [27] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [28] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [30] include(::String) at ./client.jl:457
 [31] top-level scope at none:6
 [32] eval(::Module, ::Any) at ./boot.jl:331
 [33] exec_options(::Base.JLOptions) at ./client.jl:272
 [34] _start() at ./client.jl:506
rnn: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:89
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:89 =# @gcheck1 rxhc(r, P(x2), P(hx2a), P(cx2a), batchSizes = b)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:89
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] (::RNN)(::Param{KnetArray{Float64,2}}; batchSizes::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:348
   [14] rxhc at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:15 [inlined]
   [15] (::var"#268#299"{UnionAll,KnetArray{Float64,2},Array{Int64,1},KnetArray{Float64,3},KnetArray{Float64,3}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
   [16] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
   [18] (::AutoGrad.var"#220#225"{Tuple{},var"#268#299"{UnionAll,KnetArray{Float64,2},Array{Int64,1},KnetArray{Float64,3},KnetArray{Float64,3}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [21] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:89
   [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] (::RNN)(::Param{KnetArray{Float64,2}}; batchSizes::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:348
 [14] (::var"#269#300"{UnionAll,KnetArray{Float64,2},Array{Int64,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
 [15] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [16] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
 [17] (::AutoGrad.var"#220#225"{Tuple{},var"#269#300"{UnionAll,KnetArray{Float64,2},Array{Int64,1}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [20] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:92
 [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
 [24] include(::String) at ./client.jl:457
 [25] macro expansion at ./timing.jl:174 [inlined]
 [26] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [27] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [29] include(::String) at ./client.jl:457
 [30] top-level scope at none:6
 [31] eval(::Module, ::Any) at ./boot.jl:331
 [32] exec_options(::Base.JLOptions) at ./client.jl:272
 [33] _start() at ./client.jl:506
rnn: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:92
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:92 =# @gcheck1 r(P(x2), batchSizes = b)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:92
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] (::RNN)(::Param{KnetArray{Float64,2}}; batchSizes::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:348
   [14] (::var"#269#300"{UnionAll,KnetArray{Float64,2},Array{Int64,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
   [15] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [16] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
   [17] (::AutoGrad.var"#220#225"{Tuple{},var"#269#300"{UnionAll,KnetArray{Float64,2},Array{Int64,1}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [20] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:92
   [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  
rnn: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:102
  Test threw exception
  Expression: eq(rnnforw(r, w, x3), rnntest(r, w, x3))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] rnnforw at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/rnn.jl:93 [inlined] (repeats 2 times)
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:102
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  
rnn: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:103
  Test threw exception
  Expression: eq(rnnforw(r, w, x3), rnnforw(rcpu, wcpu, x3cpu))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] rnnforw at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/rnn.jl:93 [inlined] (repeats 2 times)
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:103
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] (::RNN)(::Param{KnetArray{Float64,3}}; batchSizes::Nothing) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:348
 [14] (::RNN)(::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:328
 [15] (::var"#270#301"{UnionAll,KnetArray{Float64,3}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
 [16] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
 [18] (::AutoGrad.var"#220#225"{Tuple{},var"#270#301"{UnionAll,KnetArray{Float64,3}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [21] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:104
 [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
 [25] include(::String) at ./client.jl:457
 [26] macro expansion at ./timing.jl:174 [inlined]
 [27] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [28] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [30] include(::String) at ./client.jl:457
 [31] top-level scope at none:6
 [32] eval(::Module, ::Any) at ./boot.jl:331
 [33] exec_options(::Base.JLOptions) at ./client.jl:272
 [34] _start() at ./client.jl:506
rnn: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:104
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:104 =# @gcheck1 r(P(x3))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:104
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] (::RNN)(::Param{KnetArray{Float64,3}}; batchSizes::Nothing) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:348
   [14] (::RNN)(::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:328
   [15] (::var"#270#301"{UnionAll,KnetArray{Float64,3}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
   [16] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
   [18] (::AutoGrad.var"#220#225"{Tuple{},var"#270#301"{UnionAll,KnetArray{Float64,3}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [21] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:104
   [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  
rnn: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:107
  Test threw exception
  Expression: eq(rnnforw(r, w, x3, hx3, cx3), rnntest(r, w, x3, hx3, cx3))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] rnnforw(::RNN, ::Param{KnetArray{Float64,3}}, ::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/rnn.jl:93
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:107
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  
rnn: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:108
  Test threw exception
  Expression: eq(rnnforw(r, w, x3, hx3, cx3), rnnforw(rcpu, wcpu, x3cpu, hx3cpu, cx3cpu))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] rnnforw(::RNN, ::Param{KnetArray{Float64,3}}, ::KnetArray{Float64,3}, ::KnetArray{Float64,3}, ::KnetArray{Float64,3}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/rnn.jl:93
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:108
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] (::RNN)(::Param{KnetArray{Float64,3}}; batchSizes::Nothing) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:348
 [14] RNN at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:328 [inlined]
 [15] rxhc at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:15 [inlined]
 [16] (::var"#rxhc#286"{var"#rxhc#257#287"})(::RNN, ::Param{KnetArray{Float64,3}}, ::Param{KnetArray{Float64,3}}, ::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:15
 [17] (::var"#272#303"{UnionAll,KnetArray{Float64,3},KnetArray{Float64,3},KnetArray{Float64,3}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
 [18] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [19] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
 [20] (::AutoGrad.var"#220#225"{Tuple{},var"#272#303"{UnionAll,KnetArray{Float64,3},KnetArray{Float64,3},KnetArray{Float64,3}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [21] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [22] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [23] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:109
 [25] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [26] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
 [27] include(::String) at ./client.jl:457
 [28] macro expansion at ./timing.jl:174 [inlined]
 [29] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [30] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [31] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [32] include(::String) at ./client.jl:457
 [33] top-level scope at none:6
 [34] eval(::Module, ::Any) at ./boot.jl:331
 [35] exec_options(::Base.JLOptions) at ./client.jl:272
 [36] _start() at ./client.jl:506
rnn: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:109
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:109 =# @gcheck1 rxhc(r, P(x3), P(hx3), P(cx3))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:109
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] (::RNN)(::Param{KnetArray{Float64,3}}; batchSizes::Nothing) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:348
   [14] RNN at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:328 [inlined]
   [15] rxhc at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:15 [inlined]
   [16] (::var"#rxhc#286"{var"#rxhc#257#287"})(::RNN, ::Param{KnetArray{Float64,3}}, ::Param{KnetArray{Float64,3}}, ::Param{KnetArray{Float64,3}}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:15
   [17] (::var"#272#303"{UnionAll,KnetArray{Float64,3},KnetArray{Float64,3},KnetArray{Float64,3}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
   [18] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [19] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
   [20] (::AutoGrad.var"#220#225"{Tuple{},var"#272#303"{UnionAll,KnetArray{Float64,3},KnetArray{Float64,3},KnetArray{Float64,3}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [21] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [22] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [23] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:109
   [25] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [26] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  
rnn: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:112
  Test threw exception
  Expression: eq(rnnforw(r, w, x3, hx3, cx3; batchSizes = [B for t = 1:T]), rnntest(r, w, x3, hx3, cx3))
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:112
   [14] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [15] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] (::RNN)(::Param{KnetArray{Float64,3}}; batchSizes::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:348
 [14] rxhc at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:15 [inlined]
 [15] (::var"#275#306"{UnionAll,KnetArray{Float64,3},KnetArray{Float64,3},KnetArray{Float64,3},Array{Int64,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
 [16] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
 [18] (::AutoGrad.var"#220#225"{Tuple{},var"#275#306"{UnionAll,KnetArray{Float64,3},KnetArray{Float64,3},KnetArray{Float64,3},Array{Int64,1}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [21] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:117
 [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
 [25] include(::String) at ./client.jl:457
 [26] macro expansion at ./timing.jl:174 [inlined]
 [27] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [28] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [30] include(::String) at ./client.jl:457
 [31] top-level scope at none:6
 [32] eval(::Module, ::Any) at ./boot.jl:331
 [33] exec_options(::Base.JLOptions) at ./client.jl:272
 [34] _start() at ./client.jl:506
rnn: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:117
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:117 =# @gcheck1 rxhc(r, P(x3), P(hx3a), P(cx3a), batchSizes = b)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:117
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] (::RNN)(::Param{KnetArray{Float64,3}}; batchSizes::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:348
   [14] rxhc at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:15 [inlined]
   [15] (::var"#275#306"{UnionAll,KnetArray{Float64,3},KnetArray{Float64,3},KnetArray{Float64,3},Array{Int64,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
   [16] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
   [18] (::AutoGrad.var"#220#225"{Tuple{},var"#275#306"{UnionAll,KnetArray{Float64,3},KnetArray{Float64,3},KnetArray{Float64,3},Array{Int64,1}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [21] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:117
   [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] (::RNN)(::Param{KnetArray{Float64,3}}; batchSizes::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:348
 [14] (::var"#276#307"{UnionAll,KnetArray{Float64,3},Array{Int64,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
 [15] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [16] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
 [17] (::AutoGrad.var"#220#225"{Tuple{},var"#276#307"{UnionAll,KnetArray{Float64,3},Array{Int64,1}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [20] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:120
 [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
 [24] include(::String) at ./client.jl:457
 [25] macro expansion at ./timing.jl:174 [inlined]
 [26] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [27] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [29] include(::String) at ./client.jl:457
 [30] top-level scope at none:6
 [31] eval(::Module, ::Any) at ./boot.jl:331
 [32] exec_options(::Base.JLOptions) at ./client.jl:272
 [33] _start() at ./client.jl:506
rnn: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:120
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:120 =# @gcheck1 r(P(x3), batchSizes = b)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:120
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] (::RNN)(::Param{KnetArray{Float64,3}}; batchSizes::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:348
   [14] (::var"#276#307"{UnionAll,KnetArray{Float64,3},Array{Int64,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
   [15] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [16] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
   [17] (::AutoGrad.var"#220#225"{Tuple{},var"#276#307"{UnionAll,KnetArray{Float64,3},Array{Int64,1}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [20] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:120
   [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] (::RNN)(::Param{KnetArray{Float64,3}}; batchSizes::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:348
 [14] rxhc at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:15 [inlined]
 [15] (::var"#275#306"{UnionAll,KnetArray{Float64,3},KnetArray{Float64,3},KnetArray{Float64,3},Array{Int64,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
 [16] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
 [18] (::AutoGrad.var"#220#225"{Tuple{},var"#275#306"{UnionAll,KnetArray{Float64,3},KnetArray{Float64,3},KnetArray{Float64,3},Array{Int64,1}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [21] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:117
 [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
 [25] include(::String) at ./client.jl:457
 [26] macro expansion at ./timing.jl:174 [inlined]
 [27] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [28] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [30] include(::String) at ./client.jl:457
 [31] top-level scope at none:6
 [32] eval(::Module, ::Any) at ./boot.jl:331
 [33] exec_options(::Base.JLOptions) at ./client.jl:272
 [34] _start() at ./client.jl:506
rnn: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:117
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:117 =# @gcheck1 rxhc(r, P(x3), P(hx3a), P(cx3a), batchSizes = b)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:117
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] (::RNN)(::Param{KnetArray{Float64,3}}; batchSizes::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:348
   [14] rxhc at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:15 [inlined]
   [15] (::var"#275#306"{UnionAll,KnetArray{Float64,3},KnetArray{Float64,3},KnetArray{Float64,3},Array{Int64,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
   [16] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
   [18] (::AutoGrad.var"#220#225"{Tuple{},var"#275#306"{UnionAll,KnetArray{Float64,3},KnetArray{Float64,3},KnetArray{Float64,3},Array{Int64,1}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [21] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:117
   [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] (::RNN)(::Param{KnetArray{Float64,3}}; batchSizes::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:348
 [14] (::var"#276#307"{UnionAll,KnetArray{Float64,3},Array{Int64,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
 [15] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [16] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
 [17] (::AutoGrad.var"#220#225"{Tuple{},var"#276#307"{UnionAll,KnetArray{Float64,3},Array{Int64,1}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [20] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:120
 [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
 [24] include(::String) at ./client.jl:457
 [25] macro expansion at ./timing.jl:174 [inlined]
 [26] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [27] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [29] include(::String) at ./client.jl:457
 [30] top-level scope at none:6
 [31] eval(::Module, ::Any) at ./boot.jl:331
 [32] exec_options(::Base.JLOptions) at ./client.jl:272
 [33] _start() at ./client.jl:506
rnn: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:120
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:120 =# @gcheck1 r(P(x3), batchSizes = b)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:120
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] (::RNN)(::Param{KnetArray{Float64,3}}; batchSizes::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:348
   [14] (::var"#276#307"{UnionAll,KnetArray{Float64,3},Array{Int64,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
   [15] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [16] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
   [17] (::AutoGrad.var"#220#225"{Tuple{},var"#276#307"{UnionAll,KnetArray{Float64,3},Array{Int64,1}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [20] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:120
   [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] (::RNN)(::Param{KnetArray{Float64,3}}; batchSizes::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:348
 [14] rxhc at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:15 [inlined]
 [15] (::var"#275#306"{UnionAll,KnetArray{Float64,3},KnetArray{Float64,3},KnetArray{Float64,3},Array{Int64,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
 [16] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [17] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
 [18] (::AutoGrad.var"#220#225"{Tuple{},var"#275#306"{UnionAll,KnetArray{Float64,3},KnetArray{Float64,3},KnetArray{Float64,3},Array{Int64,1}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [21] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
 [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:117
 [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
 [25] include(::String) at ./client.jl:457
 [26] macro expansion at ./timing.jl:174 [inlined]
 [27] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [28] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [29] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [30] include(::String) at ./client.jl:457
 [31] top-level scope at none:6
 [32] eval(::Module, ::Any) at ./boot.jl:331
 [33] exec_options(::Base.JLOptions) at ./client.jl:272
 [34] _start() at ./client.jl:506
rnn: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:117
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:117 =# @gcheck1 rxhc(r, P(x3), P(hx3a), P(cx3a), batchSizes = b)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:117
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] (::RNN)(::Param{KnetArray{Float64,3}}; batchSizes::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:348
   [14] rxhc at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:15 [inlined]
   [15] (::var"#275#306"{UnionAll,KnetArray{Float64,3},KnetArray{Float64,3},KnetArray{Float64,3},Array{Int64,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
   [16] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [17] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
   [18] (::AutoGrad.var"#220#225"{Tuple{},var"#275#306"{UnionAll,KnetArray{Float64,3},KnetArray{Float64,3},KnetArray{Float64,3},Array{Int64,1}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [19] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [20] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [21] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [22] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:117
   [23] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [24] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  

Stacktrace:
 [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
 [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
 [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
 [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
 [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
 [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
 [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
 [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
 [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
 [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
 [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
 [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
 [13] (::RNN)(::Param{KnetArray{Float64,3}}; batchSizes::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:348
 [14] (::var"#276#307"{UnionAll,KnetArray{Float64,3},Array{Int64,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
 [15] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
 [16] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
 [17] (::AutoGrad.var"#220#225"{Tuple{},var"#276#307"{UnionAll,KnetArray{Float64,3},Array{Int64,1}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
 [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
 [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
 [20] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
 [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:120
 [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
 [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
 [24] include(::String) at ./client.jl:457
 [25] macro expansion at ./timing.jl:174 [inlined]
 [26] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
 [27] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
 [28] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
 [29] include(::String) at ./client.jl:457
 [30] top-level scope at none:6
 [31] eval(::Module, ::Any) at ./boot.jl:331
 [32] exec_options(::Base.JLOptions) at ./client.jl:272
 [33] _start() at ./client.jl:506
rnn: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:120
  Test threw exception
  Expression: #= /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:120 =# @gcheck1 r(P(x3), batchSizes = b)
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:148
   [2] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [3] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [4] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:120
   [5] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [6] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  caused by [exception 1]
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] (::RNN)(::Param{KnetArray{Float64,3}}; batchSizes::Array{Int64,1}) at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20/rnn.jl:348
   [14] (::var"#276#307"{UnionAll,KnetArray{Float64,3},Array{Int64,1}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:172
   [15] gcsum(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50
   [16] gcsum at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:50 [inlined]
   [17] (::AutoGrad.var"#220#225"{Tuple{},var"#276#307"{UnionAll,KnetArray{Float64,3},Array{Int64,1}},Tuple{}})() at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:205
   [18] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:144
   [19] differentiate at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]
   [20] gcheck(::Function; kw::Tuple{}, nsample::Int64, verbose::Int64, rtol::Float64, atol::Float64, delta::Float64) at /home/pkgeval/.julia/packages/AutoGrad/VFrAv/test/gradcheck.jl:158
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:120
   [22] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [23] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
  
rnn: Error During Test at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:12
  Got exception outside of a @test
  AssertionError: This functionality is unavailabe as CUDNN is missing.
  Stacktrace:
   [1] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:74 [inlined]
   [2] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/initialization.jl:51 [inlined]
   [3] libcudnn at /home/pkgeval/.julia/packages/CUDA/dZvbp/deps/bindeps.jl:73 [inlined]
   [4] (::CUDA.CUDNN.var"#19170#cache_fptr!#9")() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:31
   [5] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/utils/call.jl:39 [inlined]
   [6] unsafe_cudnnCreate(::Base.RefValue{Ptr{Nothing}}) at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/libcudnn.jl:39
   [7] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:6 [inlined]
   [8] macro expansion at /home/pkgeval/.julia/packages/CUDA/dZvbp/src/pool.jl:403 [inlined]
   [9] cudnnCreate() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/base.jl:3
   [10] #516 at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:44 [inlined]
   [11] get!(::CUDA.CUDNN.var"#516#519"{CuContext}, ::IdDict{Any,Any}, ::Any) at ./iddict.jl:152
   [12] handle() at /home/pkgeval/.julia/packages/CUDA/dZvbp/lib/cudnn/CUDNN.jl:43
   [13] rnnforw at /home/pkgeval/.julia/packages/Knet/Mfd6L/src/ops20_gpu/rnn.jl:93 [inlined] (repeats 2 times)
   [14] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:129
   [15] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115
   [16] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/rnn.jl:14
   [17] include(::String) at ./client.jl:457
   [18] macro expansion at ./timing.jl:174 [inlined]
   [19] macro expansion at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:4 [inlined]
   [20] macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1115 [inlined]
   [21] top-level scope at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:11
   [22] include(::String) at ./client.jl:457
   [23] top-level scope at none:6
   [24] eval(::Module, ::Any) at ./boot.jl:331
   [25] exec_options(::Base.JLOptions) at ./client.jl:272
   [26] _start() at ./client.jl:506
  
 36.745507 seconds (16.20 M allocations: 821.587 MiB, 3.28% gc time)
reduction.jl	126.992958 seconds (111.68 M allocations: 5.341 GiB, 2.94% gc time)
unary.jl	223.250636 seconds (156.49 M allocations: 7.965 GiB, 2.53% gc time)
binary.jl	292.414695 seconds (196.51 M allocations: 9.625 GiB, 2.95% gc time)
Test Summary:           | Pass  Fail  Error  Broken  Total
Knet                    | 7047     7    186       5   7245
  kptr:alloc            |    8                           8
  kptr:gc               |    7                           7
  kptr:realloc          |    8                           8
  kptr:cuda             |    1                           1
  kptr:cudagc           |    1                           1
  gpu                   |                 1              1
  distributions         |    4                           4
  dropout               |    3                           3
  gcnode                |                 1              1
  JLD                   |    4                           4
  statistics            |   68                          68
  bmm                   |   39                          39
  serialize             |    8            1              9
  loss                  |  110           33            143
  cuarray               |   99                          99
  update!               |   20                          20
  optimizers            |    2                           2
  linalg                |  200                         200
  batchnorm             |   28           25             53
    {Float64, 2}        |    8            7             15
      cpu-stat          |    2                           2
      cpu-grads         |    2                           2
      gpu-stats         |                 2              2
      gpu-grads         |                 2              2
      dev-consistency   |                 1              1
      test-moments      |    2            1              3
      training-moments  |    2            1              3
    {Float64, 4}        |   10            9             19
      cpu-stat          |    2                           2
      cpu-grads         |    2                           2
      gpu-stats         |                 2              2
      gpu-grads         |                 2              2
      dev-consistency   |                 1              1
      test-moments      |    2            1              3
      training-moments  |    2            1              3
      cpu-grads-testing |    2                           2
      gpu-grads-testing |                 2              2
    {Float64, 5}        |   10            9             19
      cpu-stat          |    2                           2
      cpu-grads         |    2                           2
      gpu-stats         |                 2              2
      gpu-grads         |                 2              2
      dev-consistency   |                 1              1
      test-moments      |    2            1              3
      training-moments  |    2            1              3
      cpu-grads-testing |    2                           2
      gpu-grads-testing |                 2              2
  ops20                 |   46     6              3     55
    activation          |    5                           5
    bmm                 |    5                           5
    conv                |          6              1      7
    pool                |    8                    1      9
    mat                 |    7                           7
    batchnorm           |    2                           2
    softmax/loss        |   10                          10
    rnn                 |    9                    1     10
  karray                |  328                         328
  conv                  |   53     1     88       2    144
    cpuconv             |   53     1              2     56
    gpuconv             |                88             88
  rnn                   |   10           37             47
  reduction             | 1620                        1620
  unary                 |  598                         598
  binary                | 3782                        3782
ERROR: LoadError: Some tests did not pass: 7047 passed, 7 failed, 186 errored, 5 broken.
in expression starting at /home/pkgeval/.julia/packages/Knet/Mfd6L/test/runtests.jl:7
ERROR: Package Knet errored during testing
Stacktrace:
 [1] pkgerror(::String) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Pkg/src/Types.jl:52
 [2] test(::Pkg.Types.Context, ::Array{Pkg.Types.PackageSpec,1}; coverage::Bool, julia_args::Cmd, test_args::Cmd, test_fn::Nothing) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Pkg/src/Operations.jl:1578
 [3] test(::Pkg.Types.Context, ::Array{Pkg.Types.PackageSpec,1}; coverage::Bool, test_fn::Nothing, julia_args::Cmd, test_args::Cmd, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:328
 [4] test(::Pkg.Types.Context, ::Array{Pkg.Types.PackageSpec,1}) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:315
 [5] #test#61 at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:67 [inlined]
 [6] test at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:67 [inlined]
 [7] #test#60 at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:66 [inlined]
 [8] test at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:66 [inlined]
 [9] test(::String; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:65
 [10] test(::String) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:65
 [11] top-level scope at none:16
