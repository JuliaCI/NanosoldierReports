Julia Version 1.4.2-pre.0
Commit ef4fe83698 (2020-04-15 16:24 UTC)
Platform Info:
  OS: Linux (x86_64-linux-gnu)
  CPU: Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-8.0.1 (ORCJIT, skylake)
Environment:
  JULIA_DEPOT_PATH = ::/usr/local/share/julia
  JULIA_NUM_THREADS = 2

  Resolving package versions...
  Installed Requires ─────────── v1.0.1
  Installed Reexport ─────────── v0.2.0
  Installed ExprTools ────────── v0.1.1
  Installed MacroTools ───────── v0.5.5
  Installed TensorOperations ─── v2.2.0
  Installed NNlib ────────────── v0.6.6
  Installed LLVM ─────────────── v1.4.1
  Installed CUDAnative ───────── v3.1.0
  Installed CodeTracking ─────── v0.5.11
  Installed AbstractFFTs ─────── v0.5.0
  Installed GPUCompiler ──────── v0.2.0
  Installed Strided ──────────── v0.3.5
  Installed DataStructures ───── v0.17.15
  Installed GPUArrays ────────── v3.3.0
  Installed LRUCache ─────────── v1.1.0
  Installed Adapt ────────────── v1.0.1
  Installed CUDAdrv ──────────── v6.3.0
  Installed TimerOutputs ─────── v0.5.5
  Installed OrderedCollections ─ v1.2.0
  Installed CUDAapi ──────────── v4.0.0
  Installed BinaryProvider ───── v0.5.9
  Installed TupleTools ───────── v1.2.0
  Installed CEnum ────────────── v0.3.0
  Installed CuArrays ─────────── v2.2.0
  Installed Cthulhu ──────────── v1.0.2
   Updating `~/.julia/environments/v1.4/Project.toml`
  [6aa20fa7] + TensorOperations v2.2.0
   Updating `~/.julia/environments/v1.4/Manifest.toml`
  [621f4979] + AbstractFFTs v0.5.0
  [79e6a3ab] + Adapt v1.0.1
  [b99e7846] + BinaryProvider v0.5.9
  [fa961155] + CEnum v0.3.0
  [3895d2a7] + CUDAapi v4.0.0
  [c5f51814] + CUDAdrv v6.3.0
  [be33ccc6] + CUDAnative v3.1.0
  [da1fd8a2] + CodeTracking v0.5.11
  [f68482b8] + Cthulhu v1.0.2
  [3a865a2d] + CuArrays v2.2.0
  [864edb3b] + DataStructures v0.17.15
  [e2ba6199] + ExprTools v0.1.1
  [0c68f7d7] + GPUArrays v3.3.0
  [61eb1bfa] + GPUCompiler v0.2.0
  [929cbde3] + LLVM v1.4.1
  [8ac3fa9e] + LRUCache v1.1.0
  [1914dd2f] + MacroTools v0.5.5
  [872c559c] + NNlib v0.6.6
  [bac558e1] + OrderedCollections v1.2.0
  [189a3867] + Reexport v0.2.0
  [ae029012] + Requires v1.0.1
  [5e0ebb24] + Strided v0.3.5
  [6aa20fa7] + TensorOperations v2.2.0
  [a759f4b9] + TimerOutputs v0.5.5
  [9d95972d] + TupleTools v1.2.0
  [2a0f44e3] + Base64 
  [ade2ca70] + Dates 
  [b77e0a4c] + InteractiveUtils 
  [76f85450] + LibGit2 
  [8f399da3] + Libdl 
  [37e2e46d] + LinearAlgebra 
  [56ddb016] + Logging 
  [d6f4376e] + Markdown 
  [44cfe95a] + Pkg 
  [de0858da] + Printf 
  [3fa0cd96] + REPL 
  [9a3f8284] + Random 
  [ea8e919c] + SHA 
  [9e88b42a] + Serialization 
  [6462fe0b] + Sockets 
  [2f01184e] + SparseArrays 
  [10745b16] + Statistics 
  [cf7118a7] + UUIDs 
  [4ec0a83e] + Unicode 
   Building NNlib → `~/.julia/packages/NNlib/FAI3o/deps/build.log`
    Testing TensorOperations
Status `/tmp/jl_ho2ywz/Manifest.toml`
  [621f4979] AbstractFFTs v0.5.0
  [79e6a3ab] Adapt v1.0.1
  [b99e7846] BinaryProvider v0.5.9
  [fa961155] CEnum v0.3.0
  [3895d2a7] CUDAapi v4.0.0
  [c5f51814] CUDAdrv v6.3.0
  [be33ccc6] CUDAnative v3.1.0
  [da1fd8a2] CodeTracking v0.5.11
  [f68482b8] Cthulhu v1.0.2
  [3a865a2d] CuArrays v2.2.0
  [864edb3b] DataStructures v0.17.15
  [e2ba6199] ExprTools v0.1.1
  [0c68f7d7] GPUArrays v3.3.0
  [61eb1bfa] GPUCompiler v0.2.0
  [929cbde3] LLVM v1.4.1
  [8ac3fa9e] LRUCache v1.1.0
  [1914dd2f] MacroTools v0.5.5
  [872c559c] NNlib v0.6.6
  [bac558e1] OrderedCollections v1.2.0
  [189a3867] Reexport v0.2.0
  [ae029012] Requires v1.0.1
  [5e0ebb24] Strided v0.3.5
  [6aa20fa7] TensorOperations v2.2.0
  [a759f4b9] TimerOutputs v0.5.5
  [9d95972d] TupleTools v1.2.0
  [2a0f44e3] Base64 
  [ade2ca70] Dates 
  [8ba89e20] Distributed 
  [b77e0a4c] InteractiveUtils 
  [76f85450] LibGit2 
  [8f399da3] Libdl 
  [37e2e46d] LinearAlgebra 
  [56ddb016] Logging 
  [d6f4376e] Markdown 
  [44cfe95a] Pkg 
  [de0858da] Printf 
  [3fa0cd96] REPL 
  [9a3f8284] Random 
  [ea8e919c] SHA 
  [9e88b42a] Serialization 
  [6462fe0b] Sockets 
  [2f01184e] SparseArrays 
  [10745b16] Statistics 
  [8dfed614] Test 
  [cf7118a7] UUIDs 
  [4ec0a83e] Unicode 
Test Summary:                          | Pass  Total
Method syntax with BLAS and with cache |   32     32
tensorcopy: 1.1622579097747803 seconds
tensoradd: 1.1140170097351074 seconds
tensortrace: 2.5592751502990723 seconds
tensorcontract 1: 8.089006900787354 seconds
tensorcontract 2: 13.631011009216309 seconds
tensorcontract 3: 41.39183306694031 seconds
views: 1.6596448421478271 seconds
more views: 2.672722816467285 seconds
even more views: 0.9817850589752197 seconds
and some more views: 10.478375911712646 seconds
Float32 views: 0.5626308917999268 seconds
readme example: 45.11541295051575 seconds
tensor network examples: 162.42224287986755 seconds
diagonal examples: 115.08203315734863 seconds
Test Summary:                           | Pass  Total
Index Notation with BLAS and with cache |  286    286
Test Summary:                             | Pass  Total
Method syntax with BLAS and without cache |   32     32
tensorcopy: 0.0014379024505615234 seconds
tensoradd: 0.0009431838989257812 seconds
tensortrace: 0.08774518966674805 seconds
tensorcontract 1: 0.9916088581085205 seconds
tensorcontract 2: 0.2575979232788086 seconds
tensorcontract 3: 15.619629144668579 seconds
views: 0.15958309173583984 seconds
more views: 0.16553688049316406 seconds
even more views: 0.010298013687133789 seconds
and some more views: 0.5409421920776367 seconds
Float32 views: 0.5359790325164795 seconds
readme example: 0.06857895851135254 seconds
tensor network examples: 1.1101348400115967 seconds
diagonal examples: 1.9401240348815918 seconds
Test Summary:                              | Pass  Total
Index Notation with BLAS and without cache |  286    286
Test Summary:                                | Pass  Total
Method syntax without BLAS and without cache |   32     32
tensorcopy: 0.001191854476928711 seconds
tensoradd: 0.0006430149078369141 seconds
tensortrace: 0.11172199249267578 seconds
tensorcontract 1: 0.8115718364715576 seconds
tensorcontract 2: 0.04259204864501953 seconds
tensorcontract 3: 17.819478034973145 seconds
views: 0.1504039764404297 seconds
more views: 0.3337578773498535 seconds
even more views: 0.008554935455322266 seconds
and some more views: 0.9776709079742432 seconds
Float32 views: 0.5608820915222168 seconds
readme example: 4.077246904373169 seconds
tensor network examples: 16.97420883178711 seconds
diagonal examples: 3.9463589191436768 seconds
Test Summary:                                 | Pass  Total
Index Notation without BLAS and without cache |  286    286
cutensor macro for elementary operations: Error During Test at /home/pkgeval/.julia/packages/TensorOperations/9ft0x/test/cutensor.jl:7
  Test threw exception
  Expression: C1 ≈ C2
  GPU compilation of kernel broadcast(CuArrays.CuKernelContext, CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global}, Base.Broadcast.Broadcasted{Nothing,NTuple{4,Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{Array{Float64,4},NTuple{4,Bool},NTuple{4,Int64}},Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Bool},NTuple{4,Int64}}}}) failed
  KernelError: passing and using non-bitstype argument
  
  Argument 4 to your kernel function is of type Base.Broadcast.Broadcasted{Nothing,NTuple{4,Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{Array{Float64,4},NTuple{4,Bool},NTuple{4,Int64}},Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Bool},NTuple{4,Int64}}}}, which is not isbits:
    .args is of type Tuple{Base.Broadcast.Extruded{Array{Float64,4},NTuple{4,Bool},NTuple{4,Int64}},Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Bool},NTuple{4,Int64}}} which is not isbits.
      .1 is of type Base.Broadcast.Extruded{Array{Float64,4},NTuple{4,Bool},NTuple{4,Int64}} which is not isbits.
        .x is of type Array{Float64,4} which is not isbits.
  
  Passing non-isbits types is only allowed if they they are unused by the kernel.
  
  Stacktrace:
   [1] check_invocation(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::LLVM.Function) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/validation.jl:75
   [2] macro expansion at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:184 [inlined]
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [4] codegen(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:183
   [5] compile(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:36
   [6] _cufunction(::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,NTuple{4,Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{Array{Float64,4},NTuple{4,Bool},NTuple{4,Int64}},Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Bool},NTuple{4,Int64}}}}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:308
   [7] _cufunction at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:302 [inlined]
   [8] #77 at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:21 [inlined]
   [9] get!(::GPUCompiler.var"#77#78"{Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CUDAnative._cufunction),GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,NTuple{4,Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{Array{Float64,4},NTuple{4,Bool},NTuple{4,Int64}},Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Bool},NTuple{4,Int64}}}}}}}, ::Dict{UInt64,Any}, ::UInt64) at ./dict.jl:452
   [10] macro expansion at ./lock.jl:183 [inlined]
   [11] check_cache(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,NTuple{4,Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{Array{Float64,4},NTuple{4,Bool},NTuple{4,Int64}},Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Bool},NTuple{4,Int64}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:19
   [12] + at ./int.jl:53 [inlined]
   [13] hash_64_64 at ./hashing.jl:35 [inlined]
   [14] hash_uint64 at ./hashing.jl:62 [inlined]
   [15] hx at ./float.jl:568 [inlined]
   [16] hash at ./float.jl:571 [inlined]
   [17] cached_compilation(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,NTuple{4,Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{Array{Float64,4},NTuple{4,Bool},NTuple{4,Int64}},Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Bool},NTuple{4,Int64}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:0
   [18] cached_compilation(::Function, ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,NTuple{4,Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{Array{Float64,4},NTuple{4,Bool},NTuple{4,Int64}},Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Bool},NTuple{4,Int64}}}}}}, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:37
   [19] cufunction(::Function, ::Type; name::String, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:296
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:108 [inlined]
   [21] gpu_call(::CuArrays.CuArrayBackend, ::Function, ::Tuple{CuArray{Float64,4,Nothing},Base.Broadcast.Broadcasted{Nothing,NTuple{4,Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{Array{Float64,4},NTuple{4,Bool},NTuple{4,Int64}},Base.Broadcast.Extruded{CuArray{Float64,4,Nothing},NTuple{4,Bool},NTuple{4,Int64}}}}}, ::Int64; name::String) at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/gpuarrays.jl:32
   [22] #gpu_call#1 at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/device/execution.jl:61 [inlined]
   [23] copyto! at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/broadcast.jl:63 [inlined]
   [24] copyto! at ./broadcast.jl:864 [inlined]
   [25] copy(::Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{4},NTuple{4,Base.OneTo{Int64}},typeof(-),Tuple{Array{Float64,4},CuArray{Float64,4,Nothing}}}) at ./broadcast.jl:840
   [26] materialize at ./broadcast.jl:820 [inlined]
   [27] broadcast_preserving_zero_d at ./broadcast.jl:809 [inlined]
   [28] -(::Array{Float64,4}, ::CuArray{Float64,4,Nothing}) at ./arraymath.jl:39
   [29] isapprox(::Array{Float64,4}, ::CuArray{Float64,4,Nothing}; atol::Int64, rtol::Float64, nans::Bool, norm::typeof(norm)) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/generic.jl:1588
   [30] isapprox(::Array{Float64,4}, ::CuArray{Float64,4,Nothing}) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/generic.jl:1588
   [31] eval_test(::Expr, ::Expr, ::LineNumberNode, ::Bool) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:246
   [32] macro expansion at /home/pkgeval/.julia/packages/TensorOperations/9ft0x/test/cutensor.jl:7 [inlined]
   [33] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/TensorOperations/9ft0x/test/cutensor.jl:2
  
tensorcopy: 30.492693185806274 seconds
cutensor macro for elementary operations: Error During Test at /home/pkgeval/.julia/packages/TensorOperations/9ft0x/test/cutensor.jl:21
  Test threw exception
  Expression: C1 ≈ C2
  GPU compilation of kernel broadcast(CuArrays.CuKernelContext, CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global}, Base.Broadcast.Broadcasted{Nothing,NTuple{4,Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Bool},NTuple{4,Int64}},Base.Broadcast.Extruded{Array{Float64,4},NTuple{4,Bool},NTuple{4,Int64}}}}) failed
  KernelError: passing and using non-bitstype argument
  
  Argument 4 to your kernel function is of type Base.Broadcast.Broadcasted{Nothing,NTuple{4,Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Bool},NTuple{4,Int64}},Base.Broadcast.Extruded{Array{Float64,4},NTuple{4,Bool},NTuple{4,Int64}}}}, which is not isbits:
    .args is of type Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Bool},NTuple{4,Int64}},Base.Broadcast.Extruded{Array{Float64,4},NTuple{4,Bool},NTuple{4,Int64}}} which is not isbits.
      .2 is of type Base.Broadcast.Extruded{Array{Float64,4},NTuple{4,Bool},NTuple{4,Int64}} which is not isbits.
        .x is of type Array{Float64,4} which is not isbits.
  
  Passing non-isbits types is only allowed if they they are unused by the kernel.
  
  Stacktrace:
   [1] check_invocation(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::LLVM.Function) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/validation.jl:75
   [2] macro expansion at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:184 [inlined]
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [4] codegen(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:183
   [5] compile(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:36
   [6] _cufunction(::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,NTuple{4,Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Bool},NTuple{4,Int64}},Base.Broadcast.Extruded{Array{Float64,4},NTuple{4,Bool},NTuple{4,Int64}}}}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:308
   [7] _cufunction at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:302 [inlined]
   [8] #77 at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:21 [inlined]
   [9] get!(::GPUCompiler.var"#77#78"{Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CUDAnative._cufunction),GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,NTuple{4,Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Bool},NTuple{4,Int64}},Base.Broadcast.Extruded{Array{Float64,4},NTuple{4,Bool},NTuple{4,Int64}}}}}}}, ::Dict{UInt64,Any}, ::UInt64) at ./dict.jl:452
   [10] macro expansion at ./lock.jl:183 [inlined]
   [11] check_cache(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,NTuple{4,Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Bool},NTuple{4,Int64}},Base.Broadcast.Extruded{Array{Float64,4},NTuple{4,Bool},NTuple{4,Int64}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:19
   [12] + at ./int.jl:53 [inlined]
   [13] hash_64_64 at ./hashing.jl:35 [inlined]
   [14] hash_uint64 at ./hashing.jl:62 [inlined]
   [15] hx at ./float.jl:568 [inlined]
   [16] hash at ./float.jl:571 [inlined]
   [17] cached_compilation(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,NTuple{4,Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Bool},NTuple{4,Int64}},Base.Broadcast.Extruded{Array{Float64,4},NTuple{4,Bool},NTuple{4,Int64}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:0
   [18] cached_compilation(::Function, ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,NTuple{4,Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,4,CUDAnative.AS.Global},NTuple{4,Bool},NTuple{4,Int64}},Base.Broadcast.Extruded{Array{Float64,4},NTuple{4,Bool},NTuple{4,Int64}}}}}}, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:37
   [19] cufunction(::Function, ::Type; name::String, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:296
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:108 [inlined]
   [21] gpu_call(::CuArrays.CuArrayBackend, ::Function, ::Tuple{CuArray{Float64,4,Nothing},Base.Broadcast.Broadcasted{Nothing,NTuple{4,Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CuArray{Float64,4,Nothing},NTuple{4,Bool},NTuple{4,Int64}},Base.Broadcast.Extruded{Array{Float64,4},NTuple{4,Bool},NTuple{4,Int64}}}}}, ::Int64; name::String) at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/gpuarrays.jl:32
   [22] #gpu_call#1 at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/device/execution.jl:61 [inlined]
   [23] copyto! at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/broadcast.jl:63 [inlined]
   [24] copyto! at ./broadcast.jl:864 [inlined]
   [25] copy at ./broadcast.jl:840 [inlined]
   [26] materialize at ./broadcast.jl:820 [inlined]
   [27] broadcast_preserving_zero_d at ./broadcast.jl:809 [inlined]
   [28] -(::CuArray{Float64,4,Nothing}, ::Array{Float64,4}) at ./arraymath.jl:39
   [29] isapprox(::CuArray{Float64,4,Nothing}, ::Array{Float64,4}; atol::Int64, rtol::Float64, nans::Bool, norm::typeof(norm)) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/generic.jl:1588
   [30] isapprox(::CuArray{Float64,4,Nothing}, ::Array{Float64,4}) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/generic.jl:1588
   [31] eval_test(::Expr, ::Expr, ::LineNumberNode, ::Bool) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:246
   [32] macro expansion at /home/pkgeval/.julia/packages/TensorOperations/9ft0x/test/cutensor.jl:21 [inlined]
   [33] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/TensorOperations/9ft0x/test/cutensor.jl:2
  
CUTENSOR ERROR: extent of mode 1 does not match.
tensoradd: 0.9316039085388184 seconds
cutensor macro for elementary operations: Error During Test at /home/pkgeval/.julia/packages/TensorOperations/9ft0x/test/cutensor.jl:36
  Test threw exception
  Expression: C1 ≈ C2
  GPU compilation of kernel broadcast(CuArrays.CuKernelContext, CUDAnative.CuDeviceArray{Float64,1,CUDAnative.AS.Global}, Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,1,CUDAnative.AS.Global},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Extruded{Array{Float64,1},Tuple{Bool},Tuple{Int64}}}}) failed
  KernelError: passing and using non-bitstype argument
  
  Argument 4 to your kernel function is of type Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,1,CUDAnative.AS.Global},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Extruded{Array{Float64,1},Tuple{Bool},Tuple{Int64}}}}, which is not isbits:
    .args is of type Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,1,CUDAnative.AS.Global},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Extruded{Array{Float64,1},Tuple{Bool},Tuple{Int64}}} which is not isbits.
      .2 is of type Base.Broadcast.Extruded{Array{Float64,1},Tuple{Bool},Tuple{Int64}} which is not isbits.
        .x is of type Array{Float64,1} which is not isbits.
  
  Passing non-isbits types is only allowed if they they are unused by the kernel.
  
  Stacktrace:
   [1] check_invocation(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::LLVM.Function) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/validation.jl:75
   [2] macro expansion at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:184 [inlined]
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [4] codegen(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:183
   [5] compile(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:36
   [6] _cufunction(::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,1,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,1,CUDAnative.AS.Global},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Extruded{Array{Float64,1},Tuple{Bool},Tuple{Int64}}}}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:308
   [7] _cufunction at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:302 [inlined]
   [8] #77 at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:21 [inlined]
   [9] get!(::GPUCompiler.var"#77#78"{Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CUDAnative._cufunction),GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,1,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,1,CUDAnative.AS.Global},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Extruded{Array{Float64,1},Tuple{Bool},Tuple{Int64}}}}}}}, ::Dict{UInt64,Any}, ::UInt64) at ./dict.jl:452
   [10] macro expansion at ./lock.jl:183 [inlined]
   [11] check_cache(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,1,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,1,CUDAnative.AS.Global},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Extruded{Array{Float64,1},Tuple{Bool},Tuple{Int64}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:19
   [12] + at ./int.jl:53 [inlined]
   [13] hash_64_64 at ./hashing.jl:35 [inlined]
   [14] hash_uint64 at ./hashing.jl:62 [inlined]
   [15] hx at ./float.jl:568 [inlined]
   [16] hash at ./float.jl:571 [inlined]
   [17] cached_compilation(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,1,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,1,CUDAnative.AS.Global},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Extruded{Array{Float64,1},Tuple{Bool},Tuple{Int64}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:0
   [18] cached_compilation(::Function, ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,1,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,1,CUDAnative.AS.Global},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Extruded{Array{Float64,1},Tuple{Bool},Tuple{Int64}}}}}}, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:37
   [19] cufunction(::Function, ::Type; name::String, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:296
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:108 [inlined]
   [21] gpu_call(::CuArrays.CuArrayBackend, ::Function, ::Tuple{CuArray{Float64,1,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CuArray{Float64,1,Nothing},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Extruded{Array{Float64,1},Tuple{Bool},Tuple{Int64}}}}}, ::Int64; name::String) at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/gpuarrays.jl:32
   [22] #gpu_call#1 at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/device/execution.jl:61 [inlined]
   [23] copyto! at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/broadcast.jl:63 [inlined]
   [24] copyto! at ./broadcast.jl:864 [inlined]
   [25] copy(::Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},typeof(-),Tuple{CuArray{Float64,1,Nothing},Array{Float64,1}}}) at ./broadcast.jl:840
   [26] materialize at ./broadcast.jl:820 [inlined]
   [27] broadcast_preserving_zero_d at ./broadcast.jl:809 [inlined]
   [28] - at ./arraymath.jl:39 [inlined]
   [29] isapprox(::CuArray{Float64,1,Nothing}, ::Array{Float64,1}; atol::Int64, rtol::Float64, nans::Bool, norm::typeof(norm)) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/generic.jl:1588
   [30] isapprox(::CuArray{Float64,1,Nothing}, ::Array{Float64,1}) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/generic.jl:1588
   [31] eval_test(::Expr, ::Expr, ::LineNumberNode, ::Bool) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:246
   [32] macro expansion at /home/pkgeval/.julia/packages/TensorOperations/9ft0x/test/cutensor.jl:36 [inlined]
   [33] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/TensorOperations/9ft0x/test/cutensor.jl:2
  
cutensor macro for elementary operations: Error During Test at /home/pkgeval/.julia/packages/TensorOperations/9ft0x/test/cutensor.jl:45
  Test threw exception
  Expression: C1 ≈ C2
  GPU compilation of kernel broadcast(CuArrays.CuKernelContext, CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global}, Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,3},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}) failed
  KernelError: passing and using non-bitstype argument
  
  Argument 4 to your kernel function is of type Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,3},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}, which is not isbits:
    .args is of type Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,3},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}} which is not isbits.
      .2 is of type Base.Broadcast.Extruded{Array{Float64,3},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}} which is not isbits.
        .x is of type Array{Float64,3} which is not isbits.
  
  Passing non-isbits types is only allowed if they they are unused by the kernel.
  
  Stacktrace:
   [1] check_invocation(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::LLVM.Function) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/validation.jl:75
   [2] macro expansion at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:184 [inlined]
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [4] codegen(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:183
   [5] compile(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:36
   [6] _cufunction(::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,3},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:308
   [7] _cufunction at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:302 [inlined]
   [8] #77 at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:21 [inlined]
   [9] get!(::GPUCompiler.var"#77#78"{Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CUDAnative._cufunction),GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,3},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}}, ::Dict{UInt64,Any}, ::UInt64) at ./dict.jl:452
   [10] macro expansion at ./lock.jl:183 [inlined]
   [11] check_cache(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,3},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:19
   [12] + at ./int.jl:53 [inlined]
   [13] hash_64_64 at ./hashing.jl:35 [inlined]
   [14] hash_uint64 at ./hashing.jl:62 [inlined]
   [15] hx at ./float.jl:568 [inlined]
   [16] hash at ./float.jl:571 [inlined]
   [17] cached_compilation(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,3},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:0
   [18] cached_compilation(::Function, ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,3},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}}, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:37
   [19] cufunction(::Function, ::Type; name::String, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:296
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:108 [inlined]
   [21] gpu_call(::CuArrays.CuArrayBackend, ::Function, ::Tuple{CuArray{Float64,3,Nothing},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CuArray{Float64,3,Nothing},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}},Base.Broadcast.Extruded{Array{Float64,3},Tuple{Bool,Bool,Bool},Tuple{Int64,Int64,Int64}}}}}, ::Int64; name::String) at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/gpuarrays.jl:32
   [22] #gpu_call#1 at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/device/execution.jl:61 [inlined]
   [23] copyto! at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/broadcast.jl:63 [inlined]
   [24] copyto! at ./broadcast.jl:864 [inlined]
   [25] copy(::Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{3},Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},typeof(-),Tuple{CuArray{Float64,3,Nothing},Array{Float64,3}}}) at ./broadcast.jl:840
   [26] materialize at ./broadcast.jl:820 [inlined]
   [27] broadcast_preserving_zero_d at ./broadcast.jl:809 [inlined]
   [28] -(::CuArray{Float64,3,Nothing}, ::Array{Float64,3}) at ./arraymath.jl:39
   [29] isapprox(::CuArray{Float64,3,Nothing}, ::Array{Float64,3}; atol::Int64, rtol::Float64, nans::Bool, norm::typeof(norm)) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/generic.jl:1588
   [30] isapprox(::CuArray{Float64,3,Nothing}, ::Array{Float64,3}) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/generic.jl:1588
   [31] eval_test(::Expr, ::Expr, ::LineNumberNode, ::Bool) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:246
   [32] macro expansion at /home/pkgeval/.julia/packages/TensorOperations/9ft0x/test/cutensor.jl:45 [inlined]
   [33] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/TensorOperations/9ft0x/test/cutensor.jl:2
  
tensortrace: 4.726691961288452 seconds
cutensor macro for elementary operations: Error During Test at /home/pkgeval/.julia/packages/TensorOperations/9ft0x/test/cutensor.jl:56
  Test threw exception
  Expression: C1 ≈ C2
  GPU compilation of kernel broadcast(CuArrays.CuKernelContext, CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global}, Base.Broadcast.Broadcasted{Nothing,NTuple{5,Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},NTuple{5,Bool},NTuple{5,Int64}},Base.Broadcast.Extruded{Array{Float64,5},NTuple{5,Bool},NTuple{5,Int64}}}}) failed
  KernelError: passing and using non-bitstype argument
  
  Argument 4 to your kernel function is of type Base.Broadcast.Broadcasted{Nothing,NTuple{5,Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},NTuple{5,Bool},NTuple{5,Int64}},Base.Broadcast.Extruded{Array{Float64,5},NTuple{5,Bool},NTuple{5,Int64}}}}, which is not isbits:
    .args is of type Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},NTuple{5,Bool},NTuple{5,Int64}},Base.Broadcast.Extruded{Array{Float64,5},NTuple{5,Bool},NTuple{5,Int64}}} which is not isbits.
      .2 is of type Base.Broadcast.Extruded{Array{Float64,5},NTuple{5,Bool},NTuple{5,Int64}} which is not isbits.
        .x is of type Array{Float64,5} which is not isbits.
  
  Passing non-isbits types is only allowed if they they are unused by the kernel.
  
  Stacktrace:
   [1] check_invocation(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}, ::LLVM.Function) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/validation.jl:75
   [2] macro expansion at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:184 [inlined]
   [3] macro expansion at /home/pkgeval/.julia/packages/TimerOutputs/NvIUx/src/TimerOutput.jl:245 [inlined]
   [4] codegen(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:183
   [5] compile(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDAnative.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, strict::Bool) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/driver.jl:36
   [6] _cufunction(::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,NTuple{5,Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},NTuple{5,Bool},NTuple{5,Int64}},Base.Broadcast.Extruded{Array{Float64,5},NTuple{5,Bool},NTuple{5,Int64}}}}}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:308
   [7] _cufunction at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:302 [inlined]
   [8] #77 at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:21 [inlined]
   [9] get!(::GPUCompiler.var"#77#78"{Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(CUDAnative._cufunction),GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,NTuple{5,Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},NTuple{5,Bool},NTuple{5,Int64}},Base.Broadcast.Extruded{Array{Float64,5},NTuple{5,Bool},NTuple{5,Int64}}}}}}}, ::Dict{UInt64,Any}, ::UInt64) at ./dict.jl:452
   [10] macro expansion at ./lock.jl:183 [inlined]
   [11] check_cache(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,NTuple{5,Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},NTuple{5,Bool},NTuple{5,Int64}},Base.Broadcast.Extruded{Array{Float64,5},NTuple{5,Bool},NTuple{5,Int64}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:19
   [12] + at ./int.jl:53 [inlined]
   [13] hash_64_64 at ./hashing.jl:35 [inlined]
   [14] hash_uint64 at ./hashing.jl:62 [inlined]
   [15] hx at ./float.jl:568 [inlined]
   [16] hash at ./float.jl:571 [inlined]
   [17] cached_compilation(::typeof(CUDAnative._cufunction), ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,NTuple{5,Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},NTuple{5,Bool},NTuple{5,Int64}},Base.Broadcast.Extruded{Array{Float64,5},NTuple{5,Bool},NTuple{5,Int64}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:0
   [18] cached_compilation(::Function, ::GPUCompiler.FunctionSpec{GPUArrays.var"#26#27",Tuple{CuArrays.CuKernelContext,CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,NTuple{5,Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CUDAnative.CuDeviceArray{Float64,5,CUDAnative.AS.Global},NTuple{5,Bool},NTuple{5,Int64}},Base.Broadcast.Extruded{Array{Float64,5},NTuple{5,Bool},NTuple{5,Int64}}}}}}, ::UInt64) at /home/pkgeval/.julia/packages/GPUCompiler/bwcs0/src/cache.jl:37
   [19] cufunction(::Function, ::Type; name::String, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:296
   [20] macro expansion at /home/pkgeval/.julia/packages/CUDAnative/e0IdN/src/execution.jl:108 [inlined]
   [21] gpu_call(::CuArrays.CuArrayBackend, ::Function, ::Tuple{CuArray{Float64,5,Nothing},Base.Broadcast.Broadcasted{Nothing,NTuple{5,Base.OneTo{Int64}},typeof(-),Tuple{Base.Broadcast.Extruded{CuArray{Float64,5,Nothing},NTuple{5,Bool},NTuple{5,Int64}},Base.Broadcast.Extruded{Array{Float64,5},NTuple{5,Bool},NTuple{5,Int64}}}}}, ::Int64; name::String) at /home/pkgeval/.julia/packages/CuArrays/l0gXB/src/gpuarrays.jl:32
   [22] #gpu_call#1 at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/device/execution.jl:61 [inlined]
   [23] copyto! at /home/pkgeval/.julia/packages/GPUArrays/OXvxB/src/host/broadcast.jl:63 [inlined]
   [24] copyto! at ./broadcast.jl:864 [inlined]
   [25] copy(::Base.Broadcast.Broadcasted{CuArrays.CuArrayStyle{5},NTuple{5,Base.OneTo{Int64}},typeof(-),Tuple{CuArray{Float64,5,Nothing},Array{Float64,5}}}) at ./broadcast.jl:840
   [26] materialize at ./broadcast.jl:820 [inlined]
   [27] broadcast_preserving_zero_d at ./broadcast.jl:809 [inlined]
   [28] -(::CuArray{Float64,5,Nothing}, ::Array{Float64,5}) at ./arraymath.jl:39
   [29] isapprox(::CuArray{Float64,5,Nothing}, ::Array{Float64,5}; atol::Int64, rtol::Float64, nans::Bool, norm::typeof(norm)) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/generic.jl:1588
   [30] isapprox(::CuArray{Float64,5,Nothing}, ::Array{Float64,5}) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/generic.jl:1588
   [31] eval_test(::Expr, ::Expr, ::LineNumberNode, ::Bool) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:246
   [32] macro expansion at /home/pkgeval/.julia/packages/TensorOperations/9ft0x/test/cutensor.jl:56 [inlined]
   [33] macro expansion at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113 [inlined]
   [34] top-level scope at /home/pkgeval/.julia/packages/TensorOperations/9ft0x/test/cutensor.jl:2
  
tensorcontract 1: 2.7634201049804688 seconds
tensorcontract 2: 0.0004589557647705078 seconds
Test Summary:                            | Pass  Error  Total
cutensor macro for elementary operations |    5      5     10
ERROR: LoadError: LoadError: Some tests did not pass: 5 passed, 0 failed, 5 errored, 0 broken.
in expression starting at /home/pkgeval/.julia/packages/TensorOperations/9ft0x/test/cutensor.jl:1
in expression starting at /home/pkgeval/.julia/packages/TensorOperations/9ft0x/test/runtests.jl:22
ERROR: Package TensorOperations errored during testing
Stacktrace:
 [1] pkgerror(::String, ::Vararg{String,N} where N) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Pkg/src/Types.jl:53
 [2] test(::Pkg.Types.Context, ::Array{Pkg.Types.PackageSpec,1}; coverage::Bool, julia_args::Cmd, test_args::Cmd, test_fn::Nothing) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Pkg/src/Operations.jl:1503
 [3] test(::Pkg.Types.Context, ::Array{Pkg.Types.PackageSpec,1}; coverage::Bool, test_fn::Nothing, julia_args::Cmd, test_args::Cmd, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Pkg/src/API.jl:316
 [4] test(::Pkg.Types.Context, ::Array{Pkg.Types.PackageSpec,1}) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Pkg/src/API.jl:303
 [5] #test#68 at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Pkg/src/API.jl:297 [inlined]
 [6] test at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Pkg/src/API.jl:297 [inlined]
 [7] #test#67 at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Pkg/src/API.jl:296 [inlined]
 [8] test at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Pkg/src/API.jl:296 [inlined]
 [9] test(::String; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Pkg/src/API.jl:295
 [10] test(::String) at /workspace/srcdir/usr/share/julia/stdlib/v1.4/Pkg/src/API.jl:295
 [11] top-level scope at none:16
